<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Memory Management - My Notes</title>


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon-de23e50b.svg">
        <link rel="shortcut icon" href="../favicon-8114d1fc.png">
        <link rel="stylesheet" href="../css/variables-8adf115d.css">
        <link rel="stylesheet" href="../css/general-2459343d.css">
        <link rel="stylesheet" href="../css/chrome-ae938929.css">
        <link rel="stylesheet" href="../css/print-9e4910d8.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../fonts/fonts-9644e21d.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="mdbook-highlight-css" href="../highlight-493f70e1.css">
        <link rel="stylesheet" id="mdbook-tomorrow-night-css" href="../tomorrow-night-4c0ae647.css">
        <link rel="stylesheet" id="mdbook-ayu-highlight-css" href="../ayu-highlight-3fdfc3ac.css">

        <!-- Custom theme stylesheets -->

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "../";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "../searchindex-ef1cd730.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../toc-b765e602.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="mdbook-body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="mdbook-sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("mdbook-sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="mdbook-sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="../toc.html"></iframe>
            </noscript>
            <div id="mdbook-sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="mdbook-page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="mdbook-menu-bar-hover-placeholder"></div>
                <div id="mdbook-menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="mdbook-sidebar-toggle" class="icon-button" for="mdbook-sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="mdbook-sidebar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"/></svg></span>
                        </label>
                        <button id="mdbook-theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="mdbook-theme-list">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M371.3 367.1c27.3-3.9 51.9-19.4 67.2-42.9L600.2 74.1c12.6-19.5 9.4-45.3-7.6-61.2S549.7-4.4 531.1 9.6L294.4 187.2c-24 18-38.2 46.1-38.4 76.1L371.3 367.1zm-19.6 25.4l-116-104.4C175.9 290.3 128 339.6 128 400c0 3.9 .2 7.8 .6 11.6c1.8 17.5-10.2 36.4-27.8 36.4H96c-17.7 0-32 14.3-32 32s14.3 32 32 32H240c61.9 0 112-50.1 112-112c0-2.5-.1-5-.2-7.5z"/></svg></span>
                        </button>
                        <ul id="mdbook-theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-ayu">Ayu</button></li>
                        </ul>
                        <button id="mdbook-search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="mdbook-searchbar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M416 208c0 45.9-14.9 88.3-40 122.7L502.6 457.4c12.5 12.5 12.5 32.8 0 45.3s-32.8 12.5-45.3 0L330.7 376c-34.4 25.2-76.8 40-122.7 40C93.1 416 0 322.9 0 208S93.1 0 208 0S416 93.1 416 208zM208 352c79.5 0 144-64.5 144-144s-64.5-144-144-144S64 128.5 64 208s64.5 144 144 144z"/></svg></span>
                        </button>
                    </div>

                    <h1 class="menu-title">My Notes</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <span class=fa-svg id="print-button"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M128 0C92.7 0 64 28.7 64 64v96h64V64H354.7L384 93.3V160h64V93.3c0-17-6.7-33.3-18.7-45.3L400 18.7C388 6.7 371.7 0 354.7 0H128zM384 352v32 64H128V384 368 352H384zm64 32h32c17.7 0 32-14.3 32-32V256c0-35.3-28.7-64-64-64H64c-35.3 0-64 28.7-64 64v96c0 17.7 14.3 32 32 32H64v64c0 35.3 28.7 64 64 64H384c35.3 0 64-28.7 64-64V384zm-16-88c-13.3 0-24-10.7-24-24s10.7-24 24-24s24 10.7 24 24s-10.7 24-24 24z"/></svg></span>
                        </a>

                    </div>
                </div>

                <div id="mdbook-search-wrapper" class="hidden">
                    <form id="mdbook-searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="mdbook-searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="mdbook-searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <span class=fa-svg id="fa-spin"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M304 48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zm0 416c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM48 304c26.5 0 48-21.5 48-48s-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48zm464-48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM142.9 437c18.7-18.7 18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zm0-294.2c18.7-18.7 18.7-49.1 0-67.9S93.7 56.2 75 75s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zM369.1 437c18.7 18.7 49.1 18.7 67.9 0s18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9z"/></svg></span>
                            </div>
                        </div>
                    </form>
                    <div id="mdbook-searchresults-outer" class="searchresults-outer hidden">
                        <div id="mdbook-searchresults-header" class="searchresults-header"></div>
                        <ul id="mdbook-searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('mdbook-sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('mdbook-sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#mdbook-sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="mdbook-content" class="content">
                    <main>
                        <h1 id="memory-management"><a class="header" href="#memory-management">Memory Management</a></h1>
<h2 id="table-of-contents"><a class="header" href="#table-of-contents">Table of Contents</a></h2>
<ul>
<li><a href="#memory-fundamentals">Memory Fundamentals</a>
<ul>
<li><a href="#stack-vs-heap-allocation">Stack vs Heap Allocation</a></li>
<li><a href="#memory-layout">Memory Layout</a></li>
<li><a href="#virtual-memory">Virtual Memory</a></li>
<li><a href="#memory-alignment">Memory Alignment</a></li>
<li><a href="#fragmentation">Fragmentation</a></li>
</ul>
</li>
<li><a href="#allocation-strategies">Allocation Strategies</a>
<ul>
<li><a href="#static-allocation">Static Allocation</a></li>
<li><a href="#stack-allocation">Stack Allocation</a></li>
<li><a href="#heap-allocation">Heap Allocation</a></li>
<li><a href="#memory-pools">Memory Pools</a></li>
<li><a href="#arena-allocators">Arena Allocators</a></li>
</ul>
</li>
<li><a href="#garbage-collection">Garbage Collection</a>
<ul>
<li><a href="#reference-counting">Reference Counting</a></li>
<li><a href="#mark-and-sweep">Mark and Sweep</a></li>
<li><a href="#generational-gc">Generational GC</a></li>
<li><a href="#tri-color-marking">Tri-Color Marking</a></li>
<li><a href="#gc-tuning">GC Tuning</a></li>
<li><a href="#gc-pauses">GC Pauses</a></li>
</ul>
</li>
<li><a href="#manual-memory-management">Manual Memory Management</a>
<ul>
<li><a href="#mallocfree-in-c">malloc/free in C</a></li>
<li><a href="#newdelete-in-c">new/delete in C++</a></li>
<li><a href="#memory-leak-detection">Memory Leak Detection</a></li>
<li><a href="#use-after-free-bugs">Use-After-Free Bugs</a></li>
<li><a href="#double-free-errors">Double-Free Errors</a></li>
</ul>
</li>
<li><a href="#smart-pointers-c">Smart Pointers (C++)</a>
<ul>
<li><a href="#unique_ptr">unique_ptr</a></li>
<li><a href="#shared_ptr">shared_ptr</a></li>
<li><a href="#weak_ptr">weak_ptr</a></li>
<li><a href="#raii-pattern">RAII Pattern</a></li>
</ul>
</li>
<li><a href="#language-specific-memory-management">Language-Specific Memory Management</a>
<ul>
<li><a href="#python">Python</a></li>
<li><a href="#javascript">JavaScript</a></li>
<li><a href="#go">Go</a></li>
<li><a href="#rust">Rust</a></li>
<li><a href="#java">Java</a></li>
</ul>
</li>
<li><a href="#memory-profiling">Memory Profiling</a>
<ul>
<li><a href="#profiling-tools">Profiling Tools</a></li>
<li><a href="#memory-leak-detection-tools">Memory Leak Detection Tools</a></li>
<li><a href="#heap-profiling">Heap Profiling</a></li>
</ul>
</li>
<li><a href="#performance-optimization">Performance Optimization</a>
<ul>
<li><a href="#cache-friendly-data-structures">Cache-Friendly Data Structures</a></li>
<li><a href="#memory-access-patterns">Memory Access Patterns</a></li>
<li><a href="#copy-on-write">Copy-on-Write</a></li>
<li><a href="#memory-mapped-files">Memory-Mapped Files</a></li>
</ul>
</li>
<li><a href="#common-pitfalls-and-best-practices">Common Pitfalls and Best Practices</a></li>
</ul>
<hr>
<h2 id="memory-fundamentals"><a class="header" href="#memory-fundamentals">Memory Fundamentals</a></h2>
<h3 id="stack-vs-heap-allocation"><a class="header" href="#stack-vs-heap-allocation">Stack vs Heap Allocation</a></h3>
<p>Memory in programs is primarily divided into two main areas: the stack and the heap. Understanding the differences is crucial for writing efficient and correct code.</p>
<h4 id="the-stack"><a class="header" href="#the-stack">The Stack</a></h4>
<p><strong>Characteristics:</strong></p>
<ul>
<li><strong>Fast allocation/deallocation</strong>: Push/pop operations (O(1))</li>
<li><strong>Automatic management</strong>: Variables automatically cleaned up when out of scope</li>
<li><strong>Limited size</strong>: Typically 1-8 MB (platform-dependent)</li>
<li><strong>LIFO structure</strong>: Last In, First Out</li>
<li><strong>Thread-local</strong>: Each thread has its own stack</li>
<li><strong>Contiguous memory</strong>: Sequential allocation</li>
</ul>
<p><strong>What goes on the stack:</strong></p>
<ul>
<li>Local variables</li>
<li>Function parameters</li>
<li>Return addresses</li>
<li>Function call frames</li>
</ul>
<p><strong>Example in C:</strong></p>
<pre><code class="language-c">void function() {
    int x = 10;           // Allocated on stack
    char buffer[100];     // Allocated on stack
    double y = 3.14;      // Allocated on stack
}  // All variables automatically destroyed here
</code></pre>
<p><strong>Stack Frame Structure:</strong></p>
<pre><code>High Address
+------------------+
| Previous Frame   |
+------------------+
| Return Address   |
+------------------+
| Saved Registers  |
+------------------+
| Local Variables  |
+------------------+
| Arguments        |
+------------------+ &lt;- Stack Pointer (SP)
Low Address
</code></pre>
<p><strong>Advantages:</strong></p>
<ul>
<li>Very fast allocation (just move stack pointer)</li>
<li>No fragmentation</li>
<li>Automatic cleanup</li>
<li>Cache-friendly (locality of reference)</li>
</ul>
<p><strong>Disadvantages:</strong></p>
<ul>
<li>Limited size (stack overflow risk)</li>
<li>Variables destroyed when function returns</li>
<li>Size must be known at compile time</li>
</ul>
<h4 id="the-heap"><a class="header" href="#the-heap">The Heap</a></h4>
<p><strong>Characteristics:</strong></p>
<ul>
<li><strong>Slower allocation/deallocation</strong>: Requires bookkeeping</li>
<li><strong>Manual or GC management</strong>: Must explicitly free or use garbage collection</li>
<li><strong>Large size</strong>: Limited by available system memory</li>
<li><strong>Flexible structure</strong>: Can allocate any size at runtime</li>
<li><strong>Shared</strong>: Accessible by all threads (requires synchronization)</li>
<li><strong>Fragmented</strong>: Non-contiguous allocations</li>
</ul>
<p><strong>What goes on the heap:</strong></p>
<ul>
<li>Dynamically allocated objects</li>
<li>Large data structures</li>
<li>Objects with unknown size at compile time</li>
<li>Objects that need to outlive their scope</li>
</ul>
<p><strong>Example in C:</strong></p>
<pre><code class="language-c">void function() {
    int* ptr = malloc(sizeof(int) * 100);  // Allocated on heap
    // Use ptr...
    free(ptr);  // Must manually free
}
</code></pre>
<p><strong>Heap Structure:</strong></p>
<pre><code>+------------------+
| Free Block       |
+------------------+
| Allocated Block  |
+------------------+
| Free Block       |
+------------------+
| Allocated Block  |
+------------------+
</code></pre>
<p><strong>Advantages:</strong></p>
<ul>
<li>Large size available</li>
<li>Variables persist beyond function scope</li>
<li>Runtime-sized allocations</li>
<li>Flexible lifetime control</li>
</ul>
<p><strong>Disadvantages:</strong></p>
<ul>
<li>Slower allocation</li>
<li>Manual management (C/C++) or GC overhead</li>
<li>Fragmentation issues</li>
<li>Potential for memory leaks</li>
</ul>
<h4 id="comparison-table"><a class="header" href="#comparison-table">Comparison Table</a></h4>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Feature</th><th>Stack</th><th>Heap</th></tr>
</thead>
<tbody>
<tr><td>Speed</td><td>Very fast (nanoseconds)</td><td>Slower (microseconds)</td></tr>
<tr><td>Size</td><td>Limited (1-8 MB)</td><td>Large (GB+)</td></tr>
<tr><td>Management</td><td>Automatic</td><td>Manual/GC</td></tr>
<tr><td>Lifetime</td><td>Function scope</td><td>Explicit control</td></tr>
<tr><td>Fragmentation</td><td>None</td><td>Possible</td></tr>
<tr><td>Thread-safety</td><td>Thread-local</td><td>Requires sync</td></tr>
<tr><td>Access pattern</td><td>Sequential</td><td>Random</td></tr>
</tbody>
</table>
</div>
<h4 id="when-to-use-each"><a class="header" href="#when-to-use-each">When to Use Each</a></h4>
<p><strong>Use Stack:</strong></p>
<ul>
<li>Small, fixed-size data</li>
<li>Short-lived variables</li>
<li>When you need maximum speed</li>
<li>When automatic cleanup is desired</li>
</ul>
<p><strong>Use Heap:</strong></p>
<ul>
<li>Large data structures</li>
<li>Data that outlives function scope</li>
<li>Runtime-sized allocations</li>
<li>Shared data between threads</li>
</ul>
<h3 id="memory-layout"><a class="header" href="#memory-layout">Memory Layout</a></h3>
<p>Understanding how a program’s memory is organized is essential for debugging and optimization.</p>
<h4 id="typical-memory-layout-32-bit64-bit-systems"><a class="header" href="#typical-memory-layout-32-bit64-bit-systems">Typical Memory Layout (32-bit/64-bit systems)</a></h4>
<pre><code>High Address (0xFFFFFFFF / 0xFFFFFFFFFFFFFFFF)
+------------------------+
|    Kernel Space        |
|  (OS, system calls)    |
+------------------------+ &lt;- 0xC0000000 (varies)
|        Stack           |
|    (grows down)        |
|          ↓             |
+------------------------+
|         ...            |
|     (unmapped)         |
|         ...            |
+------------------------+
|          ↑             |
|    (grows up)          |
|         Heap           |
+------------------------+
|    BSS Segment         |
|  (uninitialized data)  |
+------------------------+
|    Data Segment        |
|  (initialized data)    |
+------------------------+
|    Text Segment        |
|   (code/instructions)  |
+------------------------+ &lt;- 0x08048000 (typical)
|       Reserved         |
+------------------------+
Low Address (0x00000000)
</code></pre>
<h4 id="segment-details"><a class="header" href="#segment-details">Segment Details</a></h4>
<p><strong>1. Text Segment (Code Segment)</strong></p>
<ul>
<li>Contains executable instructions</li>
<li>Read-only and shareable</li>
<li>Fixed size determined at compile time</li>
<li>Contains program code and constants</li>
</ul>
<pre><code class="language-c">// This function's machine code goes in text segment
int add(int a, int b) {
    return a + b;
}

// String literal in text segment (read-only)
const char* msg = "Hello, World!";
</code></pre>
<p><strong>2. Data Segment (Initialized Data)</strong></p>
<ul>
<li>Contains global and static variables with initial values</li>
<li>Read-write</li>
<li>Fixed size</li>
</ul>
<pre><code class="language-c">// Goes in data segment
int global_initialized = 42;
static int static_initialized = 100;

void function() {
    static int func_static = 5;  // Also in data segment
}
</code></pre>
<p><strong>3. BSS Segment (Block Started by Symbol)</strong></p>
<ul>
<li>Contains uninitialized global and static variables</li>
<li>Automatically zeroed by OS</li>
<li>Doesn’t take space in executable file (just a marker)</li>
</ul>
<pre><code class="language-c">// Goes in BSS segment
int global_uninitialized;
static int static_uninitialized;

void function() {
    static int func_static;  // Also in BSS
}
</code></pre>
<p><strong>Why separate BSS from Data?</strong></p>
<ul>
<li>Reduces executable file size</li>
<li>No need to store zeros in the binary</li>
<li>OS zeros memory pages when loading</li>
</ul>
<p><strong>4. Heap</strong></p>
<ul>
<li>Dynamic memory allocation</li>
<li>Grows upward (toward higher addresses)</li>
<li>Managed by allocators (malloc/new)</li>
<li>Shared by all threads</li>
</ul>
<p><strong>5. Stack</strong></p>
<ul>
<li>Local variables and function calls</li>
<li>Grows downward (toward lower addresses)</li>
<li>Each thread has its own stack</li>
<li>Limited size (configurable)</li>
</ul>
<p><strong>6. Memory-Mapped Region</strong></p>
<ul>
<li>Shared libraries</li>
<li>Memory-mapped files</li>
<li>Between heap and stack</li>
</ul>
<h4 id="example-program-memory"><a class="header" href="#example-program-memory">Example Program Memory</a></h4>
<pre><code class="language-c">#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;

// BSS segment
int global_uninit;

// Data segment
int global_init = 42;

// Text segment
int add(int a, int b) {
    return a + b;
}

int main() {
    // Stack
    int stack_var = 10;

    // Heap
    int* heap_var = malloc(sizeof(int));
    *heap_var = 20;

    // Text segment (string literal)
    char* str = "Hello";

    printf("Stack var address: %p\n", (void*)&amp;stack_var);
    printf("Heap var address: %p\n", (void*)heap_var);
    printf("Global init address: %p\n", (void*)&amp;global_init);
    printf("Global uninit address: %p\n", (void*)&amp;global_uninit);
    printf("Function address: %p\n", (void*)add);
    printf("String literal address: %p\n", (void*)str);

    free(heap_var);
    return 0;
}
</code></pre>
<p><strong>Output (example on Linux x86-64):</strong></p>
<pre><code>Stack var address: 0x7ffd1234abcd
Heap var address: 0x55e4d789ef00
Global init address: 0x55e4d6789010
Global uninit address: 0x55e4d6789020
Function address: 0x55e4d6789140
String literal address: 0x55e4d6789200
</code></pre>
<p>Notice the pattern:</p>
<ul>
<li>Stack: High address</li>
<li>Heap: Medium address</li>
<li>Global data: Lower address</li>
<li>Code/strings: Lowest address</li>
</ul>
<h4 id="inspecting-memory-layout"><a class="header" href="#inspecting-memory-layout">Inspecting Memory Layout</a></h4>
<p><strong>Linux:</strong></p>
<pre><code class="language-bash"># View process memory map
cat /proc/&lt;pid&gt;/maps

# Example output:
# 00400000-00401000 r-xp   text segment
# 00601000-00602000 rw-p   data segment
# 00602000-00623000 rw-p   heap
# 7fff12340000-7fff12361000 rw-p   stack
</code></pre>
<p><strong>Using size command:</strong></p>
<pre><code class="language-bash">$ size a.out
   text    data     bss     dec     hex filename
   1234     456     100    1790     6fe a.out
</code></pre>
<h3 id="virtual-memory"><a class="header" href="#virtual-memory">Virtual Memory</a></h3>
<p>Virtual memory is an abstraction that provides each process with the illusion of having its own private memory space.</p>
<h4 id="key-concepts"><a class="header" href="#key-concepts">Key Concepts</a></h4>
<p><strong>1. Virtual Address Space</strong></p>
<ul>
<li>Each process has its own virtual address space</li>
<li>Typically 2^32 bytes (4 GB) on 32-bit systems</li>
<li>Typically 2^48 bytes (256 TB) on 64-bit systems</li>
<li>Isolated from other processes</li>
</ul>
<p><strong>2. Physical Memory</strong></p>
<ul>
<li>Actual RAM installed in the system</li>
<li>Shared among all processes</li>
<li>Much smaller than total virtual memory</li>
</ul>
<p><strong>3. Address Translation</strong></p>
<pre><code>Virtual Address → MMU → Physical Address
</code></pre>
<p><strong>Components:</strong></p>
<ul>
<li><strong>MMU (Memory Management Unit)</strong>: Hardware that translates virtual to physical addresses</li>
<li><strong>Page Table</strong>: Maps virtual pages to physical frames</li>
<li><strong>TLB (Translation Lookaside Buffer)</strong>: Cache for page table entries</li>
</ul>
<h4 id="paging"><a class="header" href="#paging">Paging</a></h4>
<p>Memory is divided into fixed-size blocks:</p>
<ul>
<li><strong>Pages</strong>: Fixed-size blocks in virtual memory (typically 4 KB)</li>
<li><strong>Frames</strong>: Fixed-size blocks in physical memory (same size as pages)</li>
</ul>
<p><strong>Page Table Structure:</strong></p>
<pre><code>Virtual Page Number (VPN) → Page Table → Physical Frame Number (PFN)
</code></pre>
<p><strong>Example:</strong></p>
<pre><code>Virtual Address: 0x00403004
Page Size: 4096 bytes (4 KB)

VPN = 0x00403004 / 4096 = 0x403
Offset = 0x00403004 % 4096 = 0x004

Page Table Lookup: VPN 0x403 → PFN 0x1234

Physical Address: (0x1234 * 4096) + 0x004 = 0x01234004
</code></pre>
<h4 id="multi-level-page-tables"><a class="header" href="#multi-level-page-tables">Multi-Level Page Tables</a></h4>
<p>To save space, modern systems use hierarchical page tables:</p>
<pre><code>64-bit Virtual Address (x86-64):
+-------+-------+-------+-------+--------+
| PML4  |  PDP  |  PD   |  PT   | Offset |
+-------+-------+-------+-------+--------+
  9 bits  9 bits  9 bits  9 bits  12 bits

Process:
1. Use PML4 index to find PDP table
2. Use PDP index to find PD table
3. Use PD index to find PT table
4. Use PT index to find physical frame
5. Add offset to get physical address
</code></pre>
<p><strong>Advantages:</strong></p>
<ul>
<li>Only allocate page tables for used memory</li>
<li>Saves significant space compared to flat page table</li>
</ul>
<h4 id="page-faults"><a class="header" href="#page-faults">Page Faults</a></h4>
<p>A page fault occurs when accessing a virtual page not in physical memory.</p>
<p><strong>Types:</strong></p>
<p><strong>1. Minor (Soft) Page Fault</strong></p>
<ul>
<li>Page is in memory but not mapped in page table</li>
<li>Fast to handle</li>
<li>Example: First access to a newly allocated page</li>
</ul>
<p><strong>2. Major (Hard) Page Fault</strong></p>
<ul>
<li>Page must be loaded from disk (swap)</li>
<li>Very slow (milliseconds)</li>
<li>Example: Accessing swapped-out memory</li>
</ul>
<p><strong>3. Invalid Page Fault</strong></p>
<ul>
<li>Access to unmapped/protected memory</li>
<li>Results in segmentation fault</li>
</ul>
<p><strong>Page Fault Handling:</strong></p>
<pre><code>1. CPU generates page fault exception
2. OS page fault handler runs
3. Check if address is valid
4. If valid:
   a. Find free physical frame
   b. Load page from disk (if needed)
   c. Update page table
   d. Restart instruction
5. If invalid:
   a. Terminate process (SIGSEGV)
</code></pre>
<p><strong>Example - Monitoring Page Faults (Linux):</strong></p>
<pre><code class="language-bash"># Run command and show page fault statistics
/usr/bin/time -v ./myprogram

# Output includes:
# Major (requiring I/O) page faults: 123
# Minor (reclaiming a frame) page faults: 4567
</code></pre>
<h4 id="demand-paging"><a class="header" href="#demand-paging">Demand Paging</a></h4>
<p>Pages are loaded into memory only when accessed (lazy loading).</p>
<p><strong>Benefits:</strong></p>
<ul>
<li>Programs can be larger than physical RAM</li>
<li>Faster program startup (don’t load everything)</li>
<li>Better memory utilization</li>
</ul>
<p><strong>Process:</strong></p>
<pre><code class="language-c">int* big_array = malloc(1000000 * sizeof(int));
// Page tables created, but physical memory not allocated yet

big_array[0] = 42;  // Page fault! Allocate physical page
big_array[1000] = 100;  // Page fault! Allocate another page
</code></pre>
<h4 id="copy-on-write-cow"><a class="header" href="#copy-on-write-cow">Copy-on-Write (COW)</a></h4>
<p>Optimization technique where multiple processes share the same physical pages until one writes to them.</p>
<p><strong>fork() Example:</strong></p>
<pre><code class="language-c">int x = 42;  // Page containing x is marked COW

pid_t pid = fork();
// Child process shares parent's pages (read-only)

if (pid == 0) {
    // Child process
    x = 100;  // Write triggers COW:
              // 1. Page fault
              // 2. Copy page
              // 3. Update child's page table
              // 4. Mark both copies writable
}
</code></pre>
<p><strong>Benefits:</strong></p>
<ul>
<li>Fast fork() - no immediate copying</li>
<li>Saves memory if pages not modified</li>
<li>Common in modern Unix systems</li>
</ul>
<h4 id="swap-space"><a class="header" href="#swap-space">Swap Space</a></h4>
<p>When physical memory is full, OS can move pages to disk.</p>
<p><strong>Swapping Process:</strong></p>
<pre><code>1. Select victim page (LRU, etc.)
2. Write page to swap space if dirty
3. Mark page table entry as swapped
4. Free physical frame
5. On access:
   a. Page fault
   b. Read from swap
   c. Allocate frame
   d. Update page table
</code></pre>
<p><strong>Performance Impact:</strong></p>
<pre><code>Memory access: ~100 nanoseconds
Disk access: ~10 milliseconds
Ratio: 100,000x slower!
</code></pre>
<p><strong>Monitoring Swap (Linux):</strong></p>
<pre><code class="language-bash"># Check swap usage
free -h

# Monitor swap activity
vmstat 1
</code></pre>
<h4 id="memory-protection"><a class="header" href="#memory-protection">Memory Protection</a></h4>
<p>Virtual memory enables isolation and protection:</p>
<p><strong>Permission Bits:</strong></p>
<ul>
<li><strong>Read</strong>: Can read from page</li>
<li><strong>Write</strong>: Can write to page</li>
<li><strong>Execute</strong>: Can execute code from page</li>
</ul>
<p><strong>Example:</strong></p>
<pre><code>Text segment: Read + Execute (no Write)
Data segment: Read + Write (no Execute)
Stack: Read + Write (no Execute on modern systems - NX bit)
</code></pre>
<p><strong>Protection Violation:</strong></p>
<pre><code class="language-c">const char* str = "Hello";  // In read-only memory
str[0] = 'h';  // Segmentation fault! Write to read-only memory
</code></pre>
<h4 id="translation-lookaside-buffer-tlb"><a class="header" href="#translation-lookaside-buffer-tlb">Translation Lookaside Buffer (TLB)</a></h4>
<p>Hardware cache for page table entries.</p>
<p><strong>Why Needed:</strong></p>
<ul>
<li>Page table lookups are expensive (multiple memory accesses)</li>
<li>Most programs have high locality</li>
<li>Cache recent translations</li>
</ul>
<p><strong>Structure:</strong></p>
<pre><code>Virtual Page Number → TLB Lookup
  ↓ Hit                ↓ Miss
Physical Frame    Page Table Walk
</code></pre>
<p><strong>TLB Miss Handling:</strong></p>
<ul>
<li>Hardware-managed (x86): CPU walks page table</li>
<li>Software-managed (MIPS): OS exception handler</li>
</ul>
<p><strong>Performance Impact:</strong></p>
<pre><code class="language-c">// TLB-friendly: Sequential access
for (int i = 0; i &lt; N; i++) {
    array[i] = i;  // High TLB hit rate
}

// TLB-unfriendly: Random access across many pages
for (int i = 0; i &lt; N; i++) {
    int index = random() % N;
    array[index] = i;  // Many TLB misses
}
</code></pre>
<p><strong>Checking TLB Misses (Linux):</strong></p>
<pre><code class="language-bash">perf stat -e dTLB-loads,dTLB-load-misses ./myprogram
</code></pre>
<h3 id="memory-alignment"><a class="header" href="#memory-alignment">Memory Alignment</a></h3>
<p>Memory alignment refers to arranging data in memory at addresses that are multiples of certain boundaries.</p>
<h4 id="why-alignment-matters"><a class="header" href="#why-alignment-matters">Why Alignment Matters</a></h4>
<p><strong>1. Performance</strong></p>
<ul>
<li>Aligned accesses are faster on most architectures</li>
<li>Unaligned accesses may require multiple memory operations</li>
<li>Some CPUs (ARM) can crash on unaligned access</li>
</ul>
<p><strong>2. Atomic Operations</strong></p>
<ul>
<li>Atomic operations often require aligned addresses</li>
<li>Prevents word tearing</li>
</ul>
<p><strong>3. Hardware Requirements</strong></p>
<ul>
<li>Some SIMD instructions require 16-byte or 32-byte alignment</li>
<li>DMA operations may require specific alignment</li>
</ul>
<h4 id="alignment-requirements-by-type"><a class="header" href="#alignment-requirements-by-type">Alignment Requirements by Type</a></h4>
<pre><code class="language-c">// Typical alignment requirements (x86-64)
char:    1-byte alignment  (address % 1 == 0)
short:   2-byte alignment  (address % 2 == 0)
int:     4-byte alignment  (address % 4 == 0)
long:    8-byte alignment  (address % 8 == 0)
float:   4-byte alignment  (address % 4 == 0)
double:  8-byte alignment  (address % 8 == 0)
pointer: 8-byte alignment  (address % 8 == 0) on 64-bit
</code></pre>
<h4 id="structure-padding"><a class="header" href="#structure-padding">Structure Padding</a></h4>
<p>Compilers insert padding to maintain alignment:</p>
<p><strong>Example 1: Padding Between Fields</strong></p>
<pre><code class="language-c">struct Example1 {
    char a;    // 1 byte
    // 3 bytes padding
    int b;     // 4 bytes (needs 4-byte alignment)
    char c;    // 1 byte
    // 3 bytes padding (for next struct in array)
};
// Total: 12 bytes (not 6!)

printf("Size: %zu\n", sizeof(struct Example1));  // 12
</code></pre>
<p><strong>Memory Layout:</strong></p>
<pre><code>Offset: 0  1  2  3  4  5  6  7  8  9  10 11
       [a][  padding  ][   b      ][c][padding]
</code></pre>
<p><strong>Example 2: Reordering for Efficiency</strong></p>
<pre><code class="language-c">// Inefficient layout
struct Bad {
    char a;    // 1 byte
    double b;  // 8 bytes (needs 8-byte alignment)
    char c;    // 1 byte
};
// Size: 24 bytes

// Efficient layout
struct Good {
    double b;  // 8 bytes
    char a;    // 1 byte
    char c;    // 1 byte
    // 6 bytes padding
};
// Size: 16 bytes (33% smaller!)
</code></pre>
<p><strong>Memory Layouts:</strong></p>
<pre><code>Bad:
[a][      padding      ][        b        ][c][     padding     ]
1  +          7         +         8         + 1 +        7        = 24

Good:
[        b        ][a][c][    padding    ]
        8          + 1 + 1 +      6        = 16
</code></pre>
<h4 id="checking-and-controlling-alignment"><a class="header" href="#checking-and-controlling-alignment">Checking and Controlling Alignment</a></h4>
<p><strong>Check Field Offsets:</strong></p>
<pre><code class="language-c">#include &lt;stddef.h&gt;

struct Example {
    char a;
    int b;
    char c;
};

printf("Offset of a: %zu\n", offsetof(struct Example, a));  // 0
printf("Offset of b: %zu\n", offsetof(struct Example, b));  // 4
printf("Offset of c: %zu\n", offsetof(struct Example, c));  // 8
printf("Total size: %zu\n", sizeof(struct Example));        // 12
</code></pre>
<p><strong>Pack Structures (Remove Padding):</strong></p>
<pre><code class="language-c">// GCC/Clang
struct __attribute__((packed)) Packed {
    char a;
    int b;
    char c;
};
// Size: 6 bytes (no padding)

// MSVC
#pragma pack(push, 1)
struct Packed {
    char a;
    int b;
    char c;
};
#pragma pack(pop)
</code></pre>
<p><strong>Warning:</strong> Packed structures can cause:</p>
<ul>
<li>Slower access (unaligned reads)</li>
<li>Crashes on some architectures (ARM)</li>
<li>Inability to take aligned pointers</li>
</ul>
<p><strong>Specify Alignment:</strong></p>
<pre><code class="language-c">// Align to 16-byte boundary
struct alignas(16) Aligned {
    int x;
    int y;
};

// Or with GCC/Clang
struct __attribute__((aligned(16))) Aligned {
    int x;
    int y;
};
</code></pre>
<p><strong>C11 aligned_alloc:</strong></p>
<pre><code class="language-c">// Allocate 64 bytes aligned to 32-byte boundary
void* ptr = aligned_alloc(32, 64);
if (ptr) {
    // Use ptr
    free(ptr);
}
</code></pre>
<p><strong>C++ alignas:</strong></p>
<pre><code class="language-cpp">// Align variable to cache line (64 bytes)
alignas(64) int cache_aligned_var;

// Align structure
struct alignas(32) SimdData {
    float data[8];
};
</code></pre>
<h4 id="performance-impact"><a class="header" href="#performance-impact">Performance Impact</a></h4>
<p><strong>Benchmark: Aligned vs Unaligned Access</strong></p>
<pre><code class="language-c">#include &lt;time.h&gt;

// Aligned access
struct Aligned {
    int a;
    int b;
} __attribute__((aligned(8)));

// Unaligned access
struct __attribute__((packed)) Unaligned {
    char padding;
    int a;
    int b;
};

void benchmark_aligned() {
    struct Aligned data[1000000];
    clock_t start = clock();
    for (int i = 0; i &lt; 1000000; i++) {
        data[i].a = i;
        data[i].b = i * 2;
    }
    clock_t end = clock();
    printf("Aligned: %f seconds\n", (double)(end - start) / CLOCKS_PER_SEC);
}

void benchmark_unaligned() {
    struct Unaligned data[1000000];
    clock_t start = clock();
    for (int i = 0; i &lt; 1000000; i++) {
        data[i].a = i;
        data[i].b = i * 2;
    }
    clock_t end = clock();
    printf("Unaligned: %f seconds\n", (double)(end - start) / CLOCKS_PER_SEC);
}
</code></pre>
<p><strong>Typical Results:</strong></p>
<ul>
<li>x86-64: 10-50% slower for unaligned</li>
<li>ARM: May crash or be 2-3x slower</li>
</ul>
<h4 id="simd-alignment"><a class="header" href="#simd-alignment">SIMD Alignment</a></h4>
<p>SIMD (Single Instruction Multiple Data) operations often require strict alignment:</p>
<pre><code class="language-c">#include &lt;immintrin.h&gt;

// Must be 32-byte aligned for AVX
__attribute__((aligned(32))) float data[8];

// Load with alignment requirement
__m256 vec = _mm256_load_ps(data);  // Requires 32-byte alignment

// Load without alignment requirement (slower)
__m256 vec = _mm256_loadu_ps(data);  // Works with any alignment
</code></pre>
<h3 id="fragmentation"><a class="header" href="#fragmentation">Fragmentation</a></h3>
<p>Fragmentation occurs when memory is allocated and freed in a way that leaves unusable gaps.</p>
<h4 id="internal-fragmentation"><a class="header" href="#internal-fragmentation">Internal Fragmentation</a></h4>
<p>Memory wasted within allocated blocks.</p>
<p><strong>Causes:</strong></p>
<ul>
<li>Alignment requirements</li>
<li>Fixed-size allocation classes</li>
<li>Rounding up allocations</li>
</ul>
<p><strong>Example 1: Alignment</strong></p>
<pre><code class="language-c">// Request 9 bytes, but allocator rounds to 16 for alignment
char* ptr = malloc(9);
// Actual allocation: 16 bytes
// Internal fragmentation: 7 bytes (43%!)
</code></pre>
<p><strong>Example 2: Size Classes</strong></p>
<pre><code class="language-c">// Allocator has size classes: 8, 16, 32, 64, 128, 256...
char* small = malloc(17);
// Allocated from 32-byte class
// Internal fragmentation: 15 bytes
</code></pre>
<p><strong>Visualization:</strong></p>
<pre><code>Requested: 9 bytes
Allocated: 16 bytes

[xxxxxxxxx-------]
 used     wasted (internal fragmentation)
</code></pre>
<p><strong>Measuring Internal Fragmentation:</strong></p>
<pre><code>Internal Fragmentation = (Allocated - Requested) / Allocated

Example: (16 - 9) / 16 = 43.75%
</code></pre>
<h4 id="external-fragmentation"><a class="header" href="#external-fragmentation">External Fragmentation</a></h4>
<p>Free memory exists but is scattered in small, non-contiguous blocks.</p>
<p><strong>Example Scenario:</strong></p>
<pre><code class="language-c">// Initial state: 1000 bytes free
// [                1000 bytes free                    ]

char* a = malloc(100);
// [A:100][              900 bytes free               ]

char* b = malloc(100);
// [A:100][B:100][       800 bytes free              ]

char* c = malloc(100);
// [A:100][B:100][C:100][  700 bytes free           ]

free(b);
// [A:100][100 free][C:100][  700 bytes free        ]

// Now we have 800 bytes free total (100 + 700)
// But cannot allocate a 200-byte block!
char* d = malloc(200);  // Might fail or require compaction
</code></pre>
<p><strong>Visualization:</strong></p>
<pre><code>Memory State:
[Allocated][Free:100][Allocated][Free:700]
            ↑                     ↑
            Small hole           Larger hole

Cannot satisfy 200-byte request despite having 800 bytes free!
</code></pre>
<p><strong>Measuring External Fragmentation:</strong></p>
<pre><code>External Fragmentation = 1 - (Largest Free Block / Total Free Memory)

Example: 1 - (700 / 800) = 12.5%
</code></pre>
<h4 id="fragmentation-comparison"><a class="header" href="#fragmentation-comparison">Fragmentation Comparison</a></h4>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Type</th><th>Where</th><th>Cause</th><th>Solution</th></tr>
</thead>
<tbody>
<tr><td>Internal</td><td>Within blocks</td><td>Alignment, size classes</td><td>Better size classes, packing</td></tr>
<tr><td>External</td><td>Between blocks</td><td>Allocation patterns</td><td>Compaction, better algorithms</td></tr>
</tbody>
</table>
</div>
<h4 id="reducing-internal-fragmentation"><a class="header" href="#reducing-internal-fragmentation">Reducing Internal Fragmentation</a></h4>
<p><strong>1. Better Size Classes</strong></p>
<pre><code class="language-c">// Poor size classes (power of 2)
// 8, 16, 32, 64, 128, 256...
// Requesting 65 bytes wastes 63 bytes (49%)

// Better size classes (more granular)
// 8, 12, 16, 24, 32, 48, 64, 96, 128...
// Requesting 65 bytes wastes 31 bytes (32%)
</code></pre>
<p><strong>2. Exact-Fit Allocations</strong></p>
<pre><code class="language-c">// For known sizes, avoid overhead
struct Object {
    // Design to fit size class
    int data[6];  // 24 bytes - fits 32-byte class well
};
</code></pre>
<p><strong>3. Custom Allocators</strong></p>
<pre><code class="language-c">// Pool allocator for fixed-size objects
// Zero internal fragmentation
struct Pool {
    void* free_list;
    size_t object_size;
};
</code></pre>
<h4 id="reducing-external-fragmentation"><a class="header" href="#reducing-external-fragmentation">Reducing External Fragmentation</a></h4>
<p><strong>1. Buddy Allocation</strong></p>
<p>Splits memory into power-of-2 blocks that can be merged.</p>
<pre><code>Initial: [              128 bytes              ]

Request 16 bytes:
Split:   [      64      ][      64      ]
Split:   [  32  ][  32  ][      64      ]
Split:   [16][16][  32  ][      64      ]
Allocate:[A ][ F][  F   ][      F       ]

Request 32 bytes:
Allocate:[A ][ F][  B   ][      F       ]

Free A:
State:   [ F][ F][  B   ][      F       ]
Merge:   [  F   ][  B   ][      F       ]

Free B:
State:   [  F   ][  F   ][      F       ]
Merge:   [              F              ]
</code></pre>
<p><strong>2. Best-Fit Allocation</strong></p>
<pre><code class="language-c">// Find smallest block that fits request
// Minimizes wasted space
void* best_fit(size_t size, struct FreeList* list) {
    struct Block* best = NULL;
    size_t best_size = SIZE_MAX;

    for (struct Block* b = list-&gt;head; b; b = b-&gt;next) {
        if (b-&gt;size &gt;= size &amp;&amp; b-&gt;size &lt; best_size) {
            best = b;
            best_size = b-&gt;size;
        }
    }
    return best;
}
</code></pre>
<p><strong>3. First-Fit Allocation</strong></p>
<pre><code class="language-c">// Use first block that fits
// Faster than best-fit
void* first_fit(size_t size, struct FreeList* list) {
    for (struct Block* b = list-&gt;head; b; b = b-&gt;next) {
        if (b-&gt;size &gt;= size) {
            return b;
        }
    }
    return NULL;
}
</code></pre>
<p><strong>4. Memory Compaction</strong></p>
<p>Move allocated blocks together to consolidate free space.</p>
<pre><code>Before:
[A][Free][B][Free][C][Free]

After compaction:
[A][B][C][        Free       ]
</code></pre>
<p><strong>Challenge:</strong> Must update all pointers to moved objects!</p>
<p><strong>Solutions:</strong></p>
<ul>
<li>Handles/indirect pointers (Java, Go)</li>
<li>Moving GC with pointer tracking</li>
<li>Generally not possible in C/C++</li>
</ul>
<p><strong>5. Segregated Free Lists</strong></p>
<p>Maintain separate free lists for different size classes.</p>
<pre><code class="language-c">struct Allocator {
    struct FreeList* lists[NUM_SIZE_CLASSES];
};

// Size classes: 16, 32, 64, 128, 256, 512, 1024, 2048...
void* allocate(struct Allocator* alloc, size_t size) {
    int class = size_class(size);
    if (alloc-&gt;lists[class]-&gt;head) {
        return allocate_from_list(alloc-&gt;lists[class]);
    }
    // Fall back to larger class or request from OS
}
</code></pre>
<p><strong>Advantages:</strong></p>
<ul>
<li>Fast allocation (no search)</li>
<li>Reduced fragmentation within classes</li>
<li>Better cache locality</li>
</ul>
<h4 id="real-world-example-jemalloc"><a class="header" href="#real-world-example-jemalloc">Real-World Example: jemalloc</a></h4>
<p>jemalloc uses multiple techniques:</p>
<pre><code>Size Class Ranges:
- Small: 8, 16, 32, 48, 64, 80, 96, 112, 128... (up to 14 KB)
  → Segregated free lists, thread-local caching

- Large: 16 KB, 32 KB, 48 KB... (up to 4 MB)
  → Best-fit allocation

- Huge: &gt; 4 MB
  → Direct mmap() calls

Arenas:
- Multiple per thread to reduce contention
- Each arena has own metadata

Result:
- Low fragmentation (typically &lt; 10%)
- Good performance
</code></pre>
<h4 id="monitoring-fragmentation"><a class="header" href="#monitoring-fragmentation">Monitoring Fragmentation</a></h4>
<p><strong>Linux - /proc/meminfo:</strong></p>
<pre><code class="language-bash">$ cat /proc/meminfo | grep Frag
# Shows fragmentation index (0 = no fragmentation, 1 = max)
</code></pre>
<p><strong>Malloc Statistics (GNU libc):</strong></p>
<pre><code class="language-c">#include &lt;malloc.h&gt;

struct mallinfo info = mallinfo();
printf("Total allocated: %d\n", info.uordblks);
printf("Total free: %d\n", info.fordblks);
printf("Fragmentation: %.2f%%\n",
       100.0 * info.fordblks / (info.uordblks + info.fordblks));
</code></pre>
<p><strong>Custom Tracking:</strong></p>
<pre><code class="language-c">size_t total_requested = 0;
size_t total_allocated = 0;

void* my_malloc(size_t size) {
    size_t actual = round_up(size);
    total_requested += size;
    total_allocated += actual;

    double internal_frag = 100.0 * (total_allocated - total_requested)
                                 / total_allocated;
    printf("Internal fragmentation: %.2f%%\n", internal_frag);

    return malloc(actual);
}
</code></pre>
<hr>
<h2 id="allocation-strategies"><a class="header" href="#allocation-strategies">Allocation Strategies</a></h2>
<h3 id="static-allocation"><a class="header" href="#static-allocation">Static Allocation</a></h3>
<p>Memory allocated at compile time and exists for the program’s entire lifetime.</p>
<h4 id="characteristics"><a class="header" href="#characteristics">Characteristics</a></h4>
<ul>
<li><strong>Lifetime</strong>: Program start to program end</li>
<li><strong>Location</strong>: Data or BSS segment</li>
<li><strong>Size</strong>: Fixed at compile time</li>
<li><strong>Speed</strong>: No runtime overhead</li>
<li><strong>Thread-safety</strong>: Potential issues with shared mutable state</li>
</ul>
<h4 id="types-of-static-allocation"><a class="header" href="#types-of-static-allocation">Types of Static Allocation</a></h4>
<p><strong>1. Global Variables</strong></p>
<pre><code class="language-c">// Initialized global (data segment)
int global_counter = 0;

// Uninitialized global (BSS segment)
int global_array[1000];

void function() {
    global_counter++;  // Direct access, very fast
}
</code></pre>
<p><strong>2. Static Local Variables</strong></p>
<pre><code class="language-c">void function() {
    // Initialized once, persists across calls
    static int call_count = 0;
    call_count++;

    printf("Called %d times\n", call_count);
}

// First call: "Called 1 times"
// Second call: "Called 2 times"
</code></pre>
<p><strong>3. String Literals</strong></p>
<pre><code class="language-c">// String literal in read-only data segment
const char* message = "Hello, World!";

// Array initialized with string literal
char buffer[] = "Hello";  // Mutable copy on stack
</code></pre>
<p><strong>4. Static Arrays</strong></p>
<pre><code class="language-c">// Large lookup table
static const int fibonacci[20] = {
    0, 1, 1, 2, 3, 5, 8, 13, 21, 34,
    55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181
};

int get_fibonacci(int n) {
    return fibonacci[n];  // O(1) lookup
}
</code></pre>
<h4 id="advantages"><a class="header" href="#advantages">Advantages</a></h4>
<p><strong>1. Performance</strong></p>
<pre><code class="language-c">// Static: no allocation overhead
static int cache[1000];

// vs Dynamic: allocation overhead every time
int* cache = malloc(1000 * sizeof(int));
</code></pre>
<p><strong>2. Simplicity</strong></p>
<pre><code class="language-c">// No need to manage lifetime
static const char* error_messages[] = {
    "Success",
    "File not found",
    "Permission denied",
    "Out of memory"
};
</code></pre>
<p><strong>3. Guaranteed Initialization</strong></p>
<pre><code class="language-c">// BSS guarantees zero-initialization
static int counters[100];  // All zeros
static char buffer[1024];  // All zeros
</code></pre>
<h4 id="disadvantages"><a class="header" href="#disadvantages">Disadvantages</a></h4>
<p><strong>1. Memory Usage</strong></p>
<pre><code class="language-c">// Always allocated, even if never used
static char huge_buffer[1000000];  // 1 MB always consumed

void rarely_called_function() {
    // This buffer exists even if function never called
}
</code></pre>
<p><strong>2. No Dynamic Sizing</strong></p>
<pre><code class="language-c">// Must define maximum size at compile time
#define MAX_USERS 1000
static struct User users[MAX_USERS];

// Cannot grow beyond MAX_USERS
</code></pre>
<p><strong>3. Thread-Safety Issues</strong></p>
<pre><code class="language-c">// Global state is shared across threads
static int counter = 0;

void increment() {
    counter++;  // Race condition!
}

// Solution: use thread-local storage
_Thread_local int counter = 0;  // C11
// or
thread_local int counter = 0;    // C++11
</code></pre>
<h4 id="use-cases"><a class="header" href="#use-cases">Use Cases</a></h4>
<p><strong>1. Lookup Tables</strong></p>
<pre><code class="language-c">static const unsigned char reverse_bits[256] = {
    0x00, 0x80, 0x40, 0xC0, 0x20, 0xA0, 0x60, 0xE0,
    // ... precomputed values
};

unsigned char reverse(unsigned char b) {
    return reverse_bits[b];
}
</code></pre>
<p><strong>2. Singleton Pattern</strong></p>
<pre><code class="language-c">struct Logger* get_logger() {
    static struct Logger instance = {0};
    static int initialized = 0;

    if (!initialized) {
        logger_init(&amp;instance);
        initialized = 1;
    }

    return &amp;instance;
}
</code></pre>
<p><strong>3. String Constants</strong></p>
<pre><code class="language-c">const char* get_version() {
    return "1.0.0";  // String literal (static)
}
</code></pre>
<p><strong>4. State Machines</strong></p>
<pre><code class="language-c">enum State { START, RUNNING, STOPPED };

void state_machine() {
    static enum State current = START;

    switch (current) {
        case START:
            // ...
            current = RUNNING;
            break;
        case RUNNING:
            // ...
            break;
        case STOPPED:
            // ...
            break;
    }
}
</code></pre>
<h3 id="stack-allocation"><a class="header" href="#stack-allocation">Stack Allocation</a></h3>
<p>Memory automatically allocated on the call stack when entering a function and freed when exiting.</p>
<h4 id="characteristics-1"><a class="header" href="#characteristics-1">Characteristics</a></h4>
<ul>
<li><strong>Lifetime</strong>: Function scope</li>
<li><strong>Location</strong>: Stack segment</li>
<li><strong>Size</strong>: Must be known at compile time (typically)</li>
<li><strong>Speed</strong>: Extremely fast (just move stack pointer)</li>
<li><strong>Cleanup</strong>: Automatic</li>
</ul>
<h4 id="basic-stack-allocation"><a class="header" href="#basic-stack-allocation">Basic Stack Allocation</a></h4>
<pre><code class="language-c">void function() {
    int x = 42;              // 4 bytes on stack
    char buffer[100];        // 100 bytes on stack
    double values[10];       // 80 bytes on stack

    struct Point {
        int x, y;
    } p = {1, 2};           // 8 bytes on stack

}  // All automatically freed here
</code></pre>
<h4 id="variable-length-arrays-vla---c99"><a class="header" href="#variable-length-arrays-vla---c99">Variable Length Arrays (VLA) - C99</a></h4>
<pre><code class="language-c">void process(int n) {
    // Stack-allocated array with runtime size
    int array[n];  // VLA

    for (int i = 0; i &lt; n; i++) {
        array[i] = i * i;
    }

}  // array automatically freed

// Warning: Dangerous for large n (stack overflow)
process(1000000);  // May crash!
</code></pre>
<p><strong>VLA Limitations:</strong></p>
<ul>
<li>Not supported in C++ (except as compiler extension)</li>
<li>Dangerous for large sizes</li>
<li>Size must fit in stack (typically 1-8 MB)</li>
<li>No way to check if allocation succeeded</li>
</ul>
<h4 id="alloca---dynamic-stack-allocation"><a class="header" href="#alloca---dynamic-stack-allocation">alloca() - Dynamic Stack Allocation</a></h4>
<pre><code class="language-c">#include &lt;alloca.h&gt;

void function(size_t n) {
    // Allocate n bytes on stack
    char* buffer = alloca(n);

    // Use buffer...
    memset(buffer, 0, n);

}  // buffer automatically freed

// Warning: Same dangers as VLA
</code></pre>
<p><strong>Why alloca() is dangerous:</strong></p>
<ul>
<li>No error checking (can’t detect failure)</li>
<li>Stack overflow crashes program</li>
<li>Not portable (POSIX, not C standard)</li>
<li>Can’t be used in loops safely</li>
</ul>
<pre><code class="language-c">// DANGEROUS: unbounded stack growth
for (int i = 0; i &lt; n; i++) {
    char* buf = alloca(1000);  // Stack grows each iteration!
    // Memory not freed until function returns!
}
</code></pre>
<h4 id="advantages-of-stack-allocation"><a class="header" href="#advantages-of-stack-allocation">Advantages of Stack Allocation</a></h4>
<p><strong>1. Speed</strong></p>
<pre><code>Stack allocation: ~1 nanosecond
Heap allocation: ~100 nanoseconds
Ratio: 100x faster!
</code></pre>
<p><strong>2. Automatic Cleanup</strong></p>
<pre><code class="language-c">void function() {
    char buffer[1024];

    if (error_condition) {
        return;  // buffer automatically cleaned up
    }

    // Use buffer...

}  // buffer automatically cleaned up
</code></pre>
<p><strong>3. Cache-Friendly</strong></p>
<pre><code class="language-c">// Stack-allocated data has good locality
void process() {
    int a = 1;
    int b = 2;
    int c = 3;
    // a, b, c likely in same cache line
}
</code></pre>
<p><strong>4. No Fragmentation</strong></p>
<pre><code class="language-c">// Stack pointer just moves up/down
// No fragmentation issues
</code></pre>
<h4 id="disadvantages-of-stack-allocation"><a class="header" href="#disadvantages-of-stack-allocation">Disadvantages of Stack Allocation</a></h4>
<p><strong>1. Limited Size</strong></p>
<pre><code class="language-bash"># Check stack size limit (Linux)
$ ulimit -s
8192  # 8 MB default

# Set larger stack size
$ ulimit -s 16384  # 16 MB
</code></pre>
<pre><code class="language-c">// Stack overflow example
void recursive(int n) {
    char buffer[1024];  // 1 KB per call
    recursive(n + 1);   // Eventually crashes
}
</code></pre>
<p><strong>2. Lifetime Limitations</strong></p>
<pre><code class="language-c">char* create_string() {
    char buffer[100] = "Hello";
    return buffer;  // BUG! Returning pointer to stack memory
}

// Usage:
char* str = create_string();
printf("%s\n", str);  // Undefined behavior!
</code></pre>
<p><strong>3. Size Must Be Known</strong></p>
<pre><code class="language-c">void function(int n) {
    // Can't do this (without VLA):
    // int array[n];  // Not allowed in C++

    // Must use heap:
    int* array = new int[n];
    // ...
    delete[] array;
}
</code></pre>
<h4 id="best-practices"><a class="header" href="#best-practices">Best Practices</a></h4>
<p><strong>1. Prefer Stack for Small, Short-Lived Data</strong></p>
<pre><code class="language-c">// Good: small buffer, short lifetime
void process_line(const char* line) {
    char buffer[256];
    strncpy(buffer, line, 255);
    buffer[255] = '\0';
    // Process buffer...
}
</code></pre>
<p><strong>2. Use Heap for Large Data</strong></p>
<pre><code class="language-c">// Bad: large stack allocation
void bad() {
    char buffer[1000000];  // 1 MB - risky!
}

// Good: use heap
void good() {
    char* buffer = malloc(1000000);
    if (!buffer) {
        // Handle error
        return;
    }
    // Use buffer...
    free(buffer);
}
</code></pre>
<p><strong>3. Avoid Returning Stack Addresses</strong></p>
<pre><code class="language-c">// Bad
char* get_message() {
    char buffer[100] = "Hello";
    return buffer;  // Dangling pointer!
}

// Good: return string literal (static)
const char* get_message() {
    return "Hello";
}

// Good: use heap
char* get_message() {
    char* buffer = malloc(100);
    strcpy(buffer, "Hello");
    return buffer;  // Caller must free()
}

// Good: use output parameter
void get_message(char* buffer, size_t size) {
    strncpy(buffer, "Hello", size - 1);
    buffer[size - 1] = '\0';
}
</code></pre>
<p><strong>4. Check Stack Usage</strong></p>
<pre><code class="language-c">#include &lt;sys/resource.h&gt;

void check_stack() {
    struct rusage usage;
    getrusage(RUSAGE_SELF, &amp;usage);
    printf("Max stack size: %ld KB\n", usage.ru_maxrss);
}
</code></pre>
<h3 id="heap-allocation"><a class="header" href="#heap-allocation">Heap Allocation</a></h3>
<p>Dynamic memory allocation from the heap at runtime.</p>
<h4 id="basic-heap-allocation"><a class="header" href="#basic-heap-allocation">Basic Heap Allocation</a></h4>
<p><strong>C:</strong></p>
<pre><code class="language-c">#include &lt;stdlib.h&gt;

// Allocate
int* ptr = malloc(sizeof(int) * 10);
if (!ptr) {
    // Handle allocation failure
    return;
}

// Use
ptr[0] = 42;

// Free
free(ptr);
ptr = NULL;  // Avoid dangling pointer
</code></pre>
<p><strong>C++:</strong></p>
<pre><code class="language-cpp">// Allocate single object
int* ptr = new int(42);
delete ptr;

// Allocate array
int* arr = new int[10];
delete[] arr;

// Modern C++: use smart pointers instead
std::unique_ptr&lt;int&gt; ptr = std::make_unique&lt;int&gt;(42);
std::unique_ptr&lt;int[]&gt; arr = std::make_unique&lt;int[]&gt;(10);
// Automatic cleanup
</code></pre>
<h4 id="memory-allocation-functions-c"><a class="header" href="#memory-allocation-functions-c">Memory Allocation Functions (C)</a></h4>
<p><strong>malloc():</strong></p>
<pre><code class="language-c">void* malloc(size_t size);

// Allocates uninitialized memory
int* ptr = malloc(sizeof(int) * 100);
// Memory contains garbage values
</code></pre>
<p><strong>calloc():</strong></p>
<pre><code class="language-c">void* calloc(size_t count, size_t size);

// Allocates zero-initialized memory
int* ptr = calloc(100, sizeof(int));
// All elements are 0
</code></pre>
<p><strong>Performance:</strong></p>
<pre><code class="language-c">// malloc: faster (no initialization)
int* a = malloc(1000000 * sizeof(int));

// calloc: slower (zeros memory)
int* b = calloc(1000000, sizeof(int));
</code></pre>
<p><strong>realloc():</strong></p>
<pre><code class="language-c">void* realloc(void* ptr, size_t new_size);

// Resize allocation
int* ptr = malloc(sizeof(int) * 10);
// ...
ptr = realloc(ptr, sizeof(int) * 20);  // Grow to 20 elements

if (!ptr) {
    // realloc failed, original pointer still valid
    // (unless ptr was NULL)
}
</code></pre>
<p><strong>realloc() Behavior:</strong></p>
<pre><code class="language-c">// 1. new_size &gt; old_size: may move and copy data
// 2. new_size &lt; old_size: may shrink in place
// 3. new_size == 0: equivalent to free()
// 4. ptr == NULL: equivalent to malloc()

// Example: growing array
int* arr = NULL;
size_t capacity = 0;

for (int i = 0; i &lt; 100; i++) {
    if (i &gt;= capacity) {
        capacity = capacity ? capacity * 2 : 1;
        int* new_arr = realloc(arr, capacity * sizeof(int));
        if (!new_arr) {
            free(arr);
            return;  // Handle error
        }
        arr = new_arr;
    }
    arr[i] = i;
}
</code></pre>
<p><strong>free():</strong></p>
<pre><code class="language-c">void free(void* ptr);

// Free allocated memory
int* ptr = malloc(sizeof(int));
free(ptr);

// Safe to free NULL
free(NULL);  // No-op

// Double-free is undefined behavior
free(ptr);
free(ptr);  // BUG!

// Best practice: NULL after free
free(ptr);
ptr = NULL;
</code></pre>
<h4 id="alignment-and-allocation"><a class="header" href="#alignment-and-allocation">Alignment and Allocation</a></h4>
<p><strong>aligned_alloc() - C11:</strong></p>
<pre><code class="language-c">void* aligned_alloc(size_t alignment, size_t size);

// Allocate with specific alignment
// size must be multiple of alignment
void* ptr = aligned_alloc(64, 128);  // 64-byte aligned, 128 bytes
free(ptr);
</code></pre>
<p><strong>posix_memalign():</strong></p>
<pre><code class="language-c">int posix_memalign(void** ptr, size_t alignment, size_t size);

void* ptr;
if (posix_memalign(&amp;ptr, 64, 128) != 0) {
    // Handle error
}
free(ptr);
</code></pre>
<h4 id="allocation-patterns"><a class="header" href="#allocation-patterns">Allocation Patterns</a></h4>
<p><strong>Pattern 1: Fixed-Size Allocations</strong></p>
<pre><code class="language-c">struct Node {
    int data;
    struct Node* next;
};

struct Node* create_node(int data) {
    struct Node* node = malloc(sizeof(struct Node));
    if (node) {
        node-&gt;data = data;
        node-&gt;next = NULL;
    }
    return node;
}
</code></pre>
<p><strong>Pattern 2: Variable-Size Allocations</strong></p>
<pre><code class="language-c">struct String {
    size_t length;
    char* data;
};

struct String* create_string(const char* str) {
    struct String* s = malloc(sizeof(struct String));
    if (!s) return NULL;

    s-&gt;length = strlen(str);
    s-&gt;data = malloc(s-&gt;length + 1);
    if (!s-&gt;data) {
        free(s);
        return NULL;
    }

    strcpy(s-&gt;data, str);
    return s;
}

void free_string(struct String* s) {
    if (s) {
        free(s-&gt;data);
        free(s);
    }
}
</code></pre>
<p><strong>Pattern 3: Flexible Array Members (C99)</strong></p>
<pre><code class="language-c">struct Buffer {
    size_t size;
    char data[];  // Flexible array member
};

struct Buffer* create_buffer(size_t size) {
    // Allocate structure + array in one block
    struct Buffer* buf = malloc(sizeof(struct Buffer) + size);
    if (buf) {
        buf-&gt;size = size;
    }
    return buf;
}

void free_buffer(struct Buffer* buf) {
    free(buf);  // Single free for both struct and array
}
</code></pre>
<p><strong>Pattern 4: Growing Arrays</strong></p>
<pre><code class="language-c">struct DynamicArray {
    int* data;
    size_t size;
    size_t capacity;
};

void push_back(struct DynamicArray* arr, int value) {
    if (arr-&gt;size &gt;= arr-&gt;capacity) {
        size_t new_capacity = arr-&gt;capacity ? arr-&gt;capacity * 2 : 1;
        int* new_data = realloc(arr-&gt;data, new_capacity * sizeof(int));
        if (!new_data) {
            // Handle error
            return;
        }
        arr-&gt;data = new_data;
        arr-&gt;capacity = new_capacity;
    }
    arr-&gt;data[arr-&gt;size++] = value;
}
</code></pre>
<h4 id="allocation-performance"><a class="header" href="#allocation-performance">Allocation Performance</a></h4>
<p><strong>Size Class Optimization:</strong></p>
<p>Modern allocators use size classes to reduce overhead:</p>
<pre><code>jemalloc size classes (examples):
Small: 8, 16, 24, 32, 48, 64, 80, 96, 112, 128...
Large: 4K, 8K, 12K, 16K, 20K, 24K...
Huge: &gt; 4MB (direct mmap)
</code></pre>
<p><strong>Implications:</strong></p>
<pre><code class="language-c">// Request 17 bytes → get 24 bytes (from 24-byte class)
char* small = malloc(17);  // 7 bytes wasted

// Request 4097 bytes → get 8K (from 8K class)
char* medium = malloc(4097);  // ~4K wasted!

// Request 10 MB → direct mmap, exact size
char* large = malloc(10 * 1024 * 1024);
</code></pre>
<p><strong>Allocation Overhead:</strong></p>
<pre><code class="language-c">// Each allocation has metadata overhead
struct BlockHeader {
    size_t size;
    int flags;
    // Maybe more fields
};  // Typically 8-16 bytes

// So allocating 1 byte actually uses ~16 bytes!
char* tiny = malloc(1);  // 1 byte + 16 byte overhead = 17 bytes
</code></pre>
<p><strong>Reducing Overhead:</strong></p>
<pre><code class="language-c">// Bad: many small allocations
for (int i = 0; i &lt; 1000; i++) {
    int* p = malloc(sizeof(int));  // 1000 allocations
}

// Good: single large allocation
int* array = malloc(sizeof(int) * 1000);  // 1 allocation
</code></pre>
<h3 id="memory-pools"><a class="header" href="#memory-pools">Memory Pools</a></h3>
<p>Pre-allocated memory blocks for fixed-size objects, providing fast, predictable allocation.</p>
<h4 id="basic-concept"><a class="header" href="#basic-concept">Basic Concept</a></h4>
<pre><code>Memory Pool:
[Free][Free][Free][Used][Free][Used][Used][Free]
  ↓
Free List: → [0] → [1] → [2] → [4] → [7] → NULL
</code></pre>
<h4 id="simple-pool-implementation"><a class="header" href="#simple-pool-implementation">Simple Pool Implementation</a></h4>
<pre><code class="language-c">#define POOL_SIZE 1000
#define OBJECT_SIZE sizeof(struct Object)

struct Pool {
    void* memory;
    void* free_list;
    size_t object_size;
    size_t capacity;
};

struct FreeNode {
    struct FreeNode* next;
};

// Initialize pool
struct Pool* pool_create(size_t object_size, size_t capacity) {
    struct Pool* pool = malloc(sizeof(struct Pool));
    if (!pool) return NULL;

    pool-&gt;memory = malloc(object_size * capacity);
    if (!pool-&gt;memory) {
        free(pool);
        return NULL;
    }

    pool-&gt;object_size = object_size;
    pool-&gt;capacity = capacity;

    // Build free list
    pool-&gt;free_list = pool-&gt;memory;
    char* ptr = pool-&gt;memory;
    for (size_t i = 0; i &lt; capacity - 1; i++) {
        struct FreeNode* node = (struct FreeNode*)ptr;
        node-&gt;next = (struct FreeNode*)(ptr + object_size);
        ptr += object_size;
    }
    ((struct FreeNode*)ptr)-&gt;next = NULL;

    return pool;
}

// Allocate from pool
void* pool_alloc(struct Pool* pool) {
    if (!pool-&gt;free_list) {
        return NULL;  // Pool exhausted
    }

    void* ptr = pool-&gt;free_list;
    pool-&gt;free_list = ((struct FreeNode*)ptr)-&gt;next;
    return ptr;
}

// Free back to pool
void pool_free(struct Pool* pool, void* ptr) {
    struct FreeNode* node = (struct FreeNode*)ptr;
    node-&gt;next = pool-&gt;free_list;
    pool-&gt;free_list = node;
}

// Destroy pool
void pool_destroy(struct Pool* pool) {
    free(pool-&gt;memory);
    free(pool);
}
</code></pre>
<h4 id="usage-example"><a class="header" href="#usage-example">Usage Example</a></h4>
<pre><code class="language-c">struct Node {
    int data;
    struct Node* left;
    struct Node* right;
};

int main() {
    // Create pool for 1000 nodes
    struct Pool* node_pool = pool_create(sizeof(struct Node), 1000);

    // Allocate nodes from pool (very fast!)
    struct Node* n1 = pool_alloc(node_pool);
    struct Node* n2 = pool_alloc(node_pool);
    struct Node* n3 = pool_alloc(node_pool);

    n1-&gt;data = 1;
    n1-&gt;left = n2;
    n1-&gt;right = n3;

    // Free nodes back to pool
    pool_free(node_pool, n1);
    pool_free(node_pool, n2);
    pool_free(node_pool, n3);

    // Destroy pool
    pool_destroy(node_pool);

    return 0;
}
</code></pre>
<h4 id="performance-benefits"><a class="header" href="#performance-benefits">Performance Benefits</a></h4>
<pre><code class="language-c">// Benchmark: malloc vs pool allocation

// Using malloc
clock_t start = clock();
for (int i = 0; i &lt; 1000000; i++) {
    void* p = malloc(32);
    free(p);
}
clock_t malloc_time = clock() - start;

// Using pool
struct Pool* pool = pool_create(32, 1000000);
start = clock();
void* pointers[1000000];
for (int i = 0; i &lt; 1000000; i++) {
    pointers[i] = pool_alloc(pool);
}
for (int i = 0; i &lt; 1000000; i++) {
    pool_free(pool, pointers[i]);
}
clock_t pool_time = clock() - start;

printf("malloc: %f seconds\n", (double)malloc_time / CLOCKS_PER_SEC);
printf("pool: %f seconds\n", (double)pool_time / CLOCKS_PER_SEC);
printf("Speedup: %.2fx\n", (double)malloc_time / pool_time);

// Typical result: 10-50x faster!
</code></pre>
<h4 id="advantages-1"><a class="header" href="#advantages-1">Advantages</a></h4>
<ol>
<li><strong>Speed</strong>: O(1) allocation/deallocation</li>
<li><strong>No fragmentation</strong>: All objects same size</li>
<li><strong>Predictable performance</strong>: No syscalls</li>
<li><strong>Cache-friendly</strong>: Objects allocated together</li>
<li><strong>No individual overhead</strong>: Metadata only for pool, not each object</li>
</ol>
<h4 id="disadvantages-1"><a class="header" href="#disadvantages-1">Disadvantages</a></h4>
<ol>
<li><strong>Fixed object size</strong>: Can’t allocate different sizes</li>
<li><strong>Wasted memory</strong>: Unused pool capacity</li>
<li><strong>Manual management</strong>: Must return objects to pool</li>
<li><strong>Pool exhaustion</strong>: Can run out of objects</li>
</ol>
<h4 id="use-cases-1"><a class="header" href="#use-cases-1">Use Cases</a></h4>
<ul>
<li>Game engines (entities, particles)</li>
<li>Network servers (connection objects)</li>
<li>Database systems (query nodes)</li>
<li>Any system with many fixed-size allocations</li>
</ul>
<h3 id="arena-allocators"><a class="header" href="#arena-allocators">Arena Allocators</a></h3>
<p>Region-based memory management where allocations are freed all at once.</p>
<h4 id="basic-concept-1"><a class="header" href="#basic-concept-1">Basic Concept</a></h4>
<pre><code>Arena:
[Allocation 1][Allocation 2][Allocation 3][  Free Space  ]
                                           ↑
                                         Current position

Free all at once:
[                    All Free                            ]
</code></pre>
<h4 id="simple-arena-implementation"><a class="header" href="#simple-arena-implementation">Simple Arena Implementation</a></h4>
<pre><code class="language-c">struct Arena {
    char* buffer;
    size_t size;
    size_t used;
};

// Create arena
struct Arena* arena_create(size_t size) {
    struct Arena* arena = malloc(sizeof(struct Arena));
    if (!arena) return NULL;

    arena-&gt;buffer = malloc(size);
    if (!arena-&gt;buffer) {
        free(arena);
        return NULL;
    }

    arena-&gt;size = size;
    arena-&gt;used = 0;

    return arena;
}

// Allocate from arena
void* arena_alloc(struct Arena* arena, size_t size) {
    // Align to 8-byte boundary
    size_t aligned_size = (size + 7) &amp; ~7;

    if (arena-&gt;used + aligned_size &gt; arena-&gt;size) {
        return NULL;  // Arena full
    }

    void* ptr = arena-&gt;buffer + arena-&gt;used;
    arena-&gt;used += aligned_size;

    return ptr;
}

// Reset arena (free all allocations)
void arena_reset(struct Arena* arena) {
    arena-&gt;used = 0;
}

// Destroy arena
void arena_destroy(struct Arena* arena) {
    free(arena-&gt;buffer);
    free(arena);
}
</code></pre>
<h4 id="usage-example-1"><a class="header" href="#usage-example-1">Usage Example</a></h4>
<pre><code class="language-c">void process_request(struct Request* request) {
    // Create arena for this request
    struct Arena* arena = arena_create(1024 * 1024);  // 1 MB

    // Allocate temporary data from arena
    char* buffer = arena_alloc(arena, 4096);
    struct Parser* parser = arena_alloc(arena, sizeof(struct Parser));
    struct AST* ast = arena_alloc(arena, sizeof(struct AST));

    // Process request using allocated data
    parse_request(parser, request, buffer);
    build_ast(ast, parser);
    execute_ast(ast);

    // Free everything at once!
    arena_destroy(arena);
    // No need to free buffer, parser, ast individually
}
</code></pre>
<h4 id="advanced-arena-with-growing"><a class="header" href="#advanced-arena-with-growing">Advanced Arena with Growing</a></h4>
<pre><code class="language-c">struct ArenaBlock {
    char* buffer;
    size_t size;
    size_t used;
    struct ArenaBlock* next;
};

struct GrowingArena {
    struct ArenaBlock* current;
    size_t default_block_size;
};

struct GrowingArena* growing_arena_create(size_t default_size) {
    struct GrowingArena* arena = malloc(sizeof(struct GrowingArena));
    if (!arena) return NULL;

    arena-&gt;default_block_size = default_size;
    arena-&gt;current = calloc(1, sizeof(struct ArenaBlock));
    if (!arena-&gt;current) {
        free(arena);
        return NULL;
    }

    arena-&gt;current-&gt;buffer = malloc(default_size);
    if (!arena-&gt;current-&gt;buffer) {
        free(arena-&gt;current);
        free(arena);
        return NULL;
    }

    arena-&gt;current-&gt;size = default_size;
    arena-&gt;current-&gt;used = 0;
    arena-&gt;current-&gt;next = NULL;

    return arena;
}

void* growing_arena_alloc(struct GrowingArena* arena, size_t size) {
    size_t aligned_size = (size + 7) &amp; ~7;

    // Try current block
    if (arena-&gt;current-&gt;used + aligned_size &lt;= arena-&gt;current-&gt;size) {
        void* ptr = arena-&gt;current-&gt;buffer + arena-&gt;current-&gt;used;
        arena-&gt;current-&gt;used += aligned_size;
        return ptr;
    }

    // Need new block
    size_t block_size = arena-&gt;default_block_size;
    if (aligned_size &gt; block_size) {
        block_size = aligned_size;
    }

    struct ArenaBlock* new_block = calloc(1, sizeof(struct ArenaBlock));
    if (!new_block) return NULL;

    new_block-&gt;buffer = malloc(block_size);
    if (!new_block-&gt;buffer) {
        free(new_block);
        return NULL;
    }

    new_block-&gt;size = block_size;
    new_block-&gt;used = aligned_size;
    new_block-&gt;next = arena-&gt;current;
    arena-&gt;current = new_block;

    return new_block-&gt;buffer;
}

void growing_arena_destroy(struct GrowingArena* arena) {
    struct ArenaBlock* block = arena-&gt;current;
    while (block) {
        struct ArenaBlock* next = block-&gt;next;
        free(block-&gt;buffer);
        free(block);
        block = next;
    }
    free(arena);
}
</code></pre>
<h4 id="performance-characteristics"><a class="header" href="#performance-characteristics">Performance Characteristics</a></h4>
<pre><code class="language-c">// Benchmark: malloc vs arena

// Using malloc (must track and free each allocation)
clock_t start = clock();
char* pointers[10000];
for (int i = 0; i &lt; 10000; i++) {
    pointers[i] = malloc(100);
}
for (int i = 0; i &lt; 10000; i++) {
    free(pointers[i]);
}
clock_t malloc_time = clock() - start;

// Using arena
start = clock();
struct Arena* arena = arena_create(10000 * 100);
for (int i = 0; i &lt; 10000; i++) {
    arena_alloc(arena, 100);
}
arena_destroy(arena);  // Free all at once!
clock_t arena_time = clock() - start;

printf("malloc: %f seconds\n", (double)malloc_time / CLOCKS_PER_SEC);
printf("arena: %f seconds\n", (double)arena_time / CLOCKS_PER_SEC);
printf("Speedup: %.2fx\n", (double)malloc_time / arena_time);

// Typical result: 5-20x faster!
</code></pre>
<h4 id="advantages-2"><a class="header" href="#advantages-2">Advantages</a></h4>
<ol>
<li><strong>Very fast allocation</strong>: Just bump pointer</li>
<li><strong>Very fast deallocation</strong>: Free all at once</li>
<li><strong>No fragmentation</strong>: Linear allocation</li>
<li><strong>Simple implementation</strong>: Minimal code</li>
<li><strong>Cache-friendly</strong>: Sequential allocations</li>
<li><strong>No individual overhead</strong>: No per-allocation metadata</li>
</ol>
<h4 id="disadvantages-2"><a class="header" href="#disadvantages-2">Disadvantages</a></h4>
<ol>
<li><strong>Can’t free individual objects</strong>: All or nothing</li>
<li><strong>Memory usage</strong>: Can’t reclaim until reset/destroy</li>
<li><strong>Requires discipline</strong>: Must reset/destroy appropriately</li>
<li><strong>Not general-purpose</strong>: Specific use patterns</li>
</ol>
<h4 id="use-cases-2"><a class="header" href="#use-cases-2">Use Cases</a></h4>
<ol>
<li><strong>Per-request processing</strong></li>
</ol>
<pre><code class="language-c">void handle_http_request(struct Request* req) {
    struct Arena* arena = arena_create(1024 * 1024);

    // Parse headers (allocates from arena)
    struct Headers* headers = parse_headers(arena, req);

    // Parse body (allocates from arena)
    struct Body* body = parse_body(arena, req);

    // Generate response (allocates from arena)
    struct Response* response = generate_response(arena, headers, body);

    // Send response
    send_response(response);

    // Free everything!
    arena_destroy(arena);
}
</code></pre>
<ol start="2">
<li><strong>Compiler phases</strong></li>
</ol>
<pre><code class="language-c">void compile(const char* source) {
    // Lexing phase
    struct Arena* lex_arena = arena_create(1024 * 1024);
    struct Token* tokens = lex(lex_arena, source);

    // Parsing phase
    struct Arena* parse_arena = arena_create(1024 * 1024);
    struct AST* ast = parse(parse_arena, tokens);
    arena_destroy(lex_arena);  // Don't need tokens anymore

    // Code generation
    struct Arena* codegen_arena = arena_create(1024 * 1024);
    struct Code* code = codegen(codegen_arena, ast);
    arena_destroy(parse_arena);  // Don't need AST anymore

    // Emit code
    emit(code);
    arena_destroy(codegen_arena);
}
</code></pre>
<ol start="3">
<li><strong>Game frames</strong></li>
</ol>
<pre><code class="language-c">void game_loop() {
    struct Arena* frame_arena = arena_create(10 * 1024 * 1024);

    while (running) {
        // Allocate temporary data for this frame
        struct RenderList* render_list = arena_alloc(frame_arena, sizeof(*render_list));
        struct Input* input = arena_alloc(frame_arena, sizeof(*input));

        // Process frame
        process_input(input);
        update_game_state(input);
        build_render_list(render_list);
        render(render_list);

        // Reset arena for next frame
        arena_reset(frame_arena);
    }

    arena_destroy(frame_arena);
}
</code></pre>
<h4 id="temporary-allocations-pattern"><a class="header" href="#temporary-allocations-pattern">Temporary Allocations Pattern</a></h4>
<pre><code class="language-c">struct ArenaSave {
    size_t used;
};

// Save arena state
struct ArenaSave arena_save(struct Arena* arena) {
    return (struct ArenaSave){ .used = arena-&gt;used };
}

// Restore arena state (free allocations since save)
void arena_restore(struct Arena* arena, struct ArenaSave save) {
    arena-&gt;used = save.used;
}

// Usage:
void function() {
    struct ArenaSave save = arena_save(arena);

    // Make temporary allocations
    char* temp1 = arena_alloc(arena, 100);
    char* temp2 = arena_alloc(arena, 200);

    // Use temporaries...

    // Restore (free temp1 and temp2)
    arena_restore(arena, save);
}
</code></pre>
<hr>
<h2 id="garbage-collection"><a class="header" href="#garbage-collection">Garbage Collection</a></h2>
<p>Automatic memory management where the runtime system reclaims unused memory.</p>
<h3 id="reference-counting"><a class="header" href="#reference-counting">Reference Counting</a></h3>
<p>Track how many references point to each object; free when count reaches zero.</p>
<h4 id="basic-concept-2"><a class="header" href="#basic-concept-2">Basic Concept</a></h4>
<pre><code>Object: [data][ref_count=0]
         ↑
         |(create)
Object: [data][ref_count=1]
         ↑     ↑
         |     |(add reference)
Object: [data][ref_count=2]
         ↑
         |(remove reference)
Object: [data][ref_count=1]
         ↑
         |(remove reference)
Object: [data][ref_count=0] → FREE!
</code></pre>
<h4 id="simple-reference-counting-implementation"><a class="header" href="#simple-reference-counting-implementation">Simple Reference Counting Implementation</a></h4>
<pre><code class="language-c">struct RefCounted {
    void* data;
    size_t ref_count;
    void (*destructor)(void*);
};

// Create object with ref_count = 1
struct RefCounted* rc_create(void* data, void (*destructor)(void*)) {
    struct RefCounted* obj = malloc(sizeof(struct RefCounted));
    if (!obj) return NULL;

    obj-&gt;data = data;
    obj-&gt;ref_count = 1;
    obj-&gt;destructor = destructor;

    return obj;
}

// Increment reference count
void rc_retain(struct RefCounted* obj) {
    if (obj) {
        obj-&gt;ref_count++;
    }
}

// Decrement reference count; free if reaches 0
void rc_release(struct RefCounted* obj) {
    if (!obj) return;

    obj-&gt;ref_count--;

    if (obj-&gt;ref_count == 0) {
        if (obj-&gt;destructor) {
            obj-&gt;destructor(obj-&gt;data);
        }
        free(obj);
    }
}

// Usage example
void example() {
    // Create object (ref_count = 1)
    struct RefCounted* obj = rc_create(strdup("Hello"), free);

    // Share object (ref_count = 2)
    struct RefCounted* obj2 = obj;
    rc_retain(obj2);

    // Release first reference (ref_count = 1)
    rc_release(obj);

    // Release second reference (ref_count = 0, freed!)
    rc_release(obj2);
}
</code></pre>
<h4 id="pythons-reference-counting"><a class="header" href="#pythons-reference-counting">Python’s Reference Counting</a></h4>
<p>Python uses reference counting as its primary GC mechanism:</p>
<pre><code class="language-python">import sys

# Create object (ref_count = 1)
a = [1, 2, 3]
print(sys.getrefcount(a))  # 2 (1 + 1 for the argument to getrefcount)

# Add reference (ref_count = 2)
b = a
print(sys.getrefcount(a))  # 3

# Remove reference (ref_count = 1)
del b
print(sys.getrefcount(a))  # 2

# Remove last reference (ref_count = 0, freed!)
del a
</code></pre>
<p><strong>Python’s Implementation (CPython):</strong></p>
<pre><code class="language-c">// From Python's object.h (simplified)
typedef struct _object {
    Py_ssize_t ob_refcnt;  // Reference count
    struct _typeobject *ob_type;
} PyObject;

// Increment reference
#define Py_INCREF(op) ((void)(((PyObject*)(op))-&gt;ob_refcnt++))

// Decrement reference; free if 0
#define Py_DECREF(op) \
    do { \
        PyObject *_py_decref_tmp = (PyObject *)(op); \
        if (--(_py_decref_tmp)-&gt;ob_refcnt == 0) \
            _Py_Dealloc(_py_decref_tmp); \
    } while (0)
</code></pre>
<h4 id="swifts-automatic-reference-counting-arc"><a class="header" href="#swifts-automatic-reference-counting-arc">Swift’s Automatic Reference Counting (ARC)</a></h4>
<p>Swift automatically inserts retain/release calls at compile time:</p>
<pre><code class="language-swift">class Person {
    var name: String
    init(name: String) { self.name = name }
    deinit { print("\(name) is being deinitialized") }
}

do {
    let person1 = Person(name: "John")  // ref_count = 1
    let person2 = person1                // ref_count = 2
}  // Scope ends: ref_count = 0, deinit called
</code></pre>
<p><strong>Compiler transforms to (conceptually):</strong></p>
<pre><code class="language-swift">do {
    let person1 = Person(name: "John")
    swift_retain(person1)  // Inserted by compiler

    let person2 = person1
    swift_retain(person2)  // Inserted by compiler

    swift_release(person2)  // Inserted by compiler
    swift_release(person1)  // Inserted by compiler
}
</code></pre>
<h4 id="advantages-of-reference-counting"><a class="header" href="#advantages-of-reference-counting">Advantages of Reference Counting</a></h4>
<ol>
<li><strong>Deterministic</strong>: Objects freed immediately when unreferenced</li>
<li><strong>No pause times</strong>: No stop-the-world collection</li>
<li><strong>Simple</strong>: Easy to understand and implement</li>
<li><strong>Incremental</strong>: Work distributed over time</li>
</ol>
<h4 id="disadvantages-of-reference-counting"><a class="header" href="#disadvantages-of-reference-counting">Disadvantages of Reference Counting</a></h4>
<p><strong>1. Overhead</strong></p>
<pre><code class="language-c">// Every pointer assignment requires ref count update
obj-&gt;field = new_value;  // Becomes:
rc_release(obj-&gt;field);
obj-&gt;field = new_value;
rc_retain(obj-&gt;field);
</code></pre>
<p><strong>2. Performance</strong></p>
<pre><code class="language-c">// Cache pressure from updating ref counts
// False sharing in multithreaded code
</code></pre>
<p><strong>3. Cannot Handle Cycles</strong></p>
<pre><code class="language-c">struct Node {
    struct RefCounted* parent;
    struct RefCounted* child;
};

// Create cycle
struct RefCounted* node1 = rc_create(...);
struct RefCounted* node2 = rc_create(...);

((struct Node*)node1-&gt;data)-&gt;child = node2;
rc_retain(node2);  // node2 ref_count = 2

((struct Node*)node2-&gt;data)-&gt;parent = node1;
rc_retain(node1);  // node1 ref_count = 2

// Release external references
rc_release(node1);  // node1 ref_count = 1 (still referenced by node2)
rc_release(node2);  // node2 ref_count = 1 (still referenced by node1)

// MEMORY LEAK! Both objects keep each other alive
</code></pre>
<p><strong>Visualization:</strong></p>
<pre><code>node1 [ref_count=1] → node2 [ref_count=1]
  ↑                     ↓
  └─────────────────────┘

Cannot be freed because ref_count &gt; 0 for both!
</code></pre>
<h4 id="solving-cycle-problem"><a class="header" href="#solving-cycle-problem">Solving Cycle Problem</a></h4>
<p><strong>Solution 1: Weak References</strong></p>
<pre><code class="language-swift">class Node {
    var value: Int
    var children: [Node] = []
    weak var parent: Node?  // Weak reference doesn't increment ref_count
}

let parent = Node(value: 1)     // ref_count = 1
let child = Node(value: 2)      // ref_count = 1
child.parent = parent           // parent ref_count still 1 (weak!)
parent.children.append(child)   // child ref_count = 2

// When parent goes out of scope:
// parent ref_count = 0, freed
// child.parent automatically becomes nil
// child ref_count = 1
</code></pre>
<p><strong>Solution 2: Cycle Detection (Python)</strong></p>
<p>Python combines reference counting with cycle detection:</p>
<pre><code class="language-python"># Create cycle
class Node:
    pass

a = Node()
b = Node()
a.ref = b  # b ref_count = 2
b.ref = a  # a ref_count = 2

del a  # a ref_count = 1
del b  # b ref_count = 1

# Cycle detector (runs periodically) finds and breaks cycle
</code></pre>
<p><strong>Python’s Cycle Detector:</strong></p>
<pre><code class="language-c">// Simplified algorithm
1. Find all objects with ref_count &gt; 0
2. Subtract internal references (between tracked objects)
3. Objects with effective ref_count = 0 are in cycles
4. Free them
</code></pre>
<h4 id="reference-counting-in-practice"><a class="header" href="#reference-counting-in-practice">Reference Counting in Practice</a></h4>
<p><strong>Objective-C/Swift:</strong></p>
<ul>
<li>ARC automatically manages ref counts</li>
<li>Weak references for breaking cycles</li>
<li><code>@autoreleasepool</code> for optimization</li>
</ul>
<p><strong>C++ <code>std::shared_ptr</code>:</strong></p>
<pre><code class="language-cpp">{
    std::shared_ptr&lt;int&gt; ptr1 = std::make_shared&lt;int&gt;(42);  // ref_count = 1
    std::shared_ptr&lt;int&gt; ptr2 = ptr1;                        // ref_count = 2
    ptr1.reset();                                            // ref_count = 1
}  // ptr2 destroyed, ref_count = 0, memory freed
</code></pre>
<p><strong>COM (Component Object Model):</strong></p>
<pre><code class="language-cpp">interface IUnknown {
    virtual ULONG AddRef() = 0;
    virtual ULONG Release() = 0;
};

// Usage
IFoo* foo = CreateFoo();  // ref_count = 1
foo-&gt;AddRef();             // ref_count = 2
foo-&gt;Release();            // ref_count = 1
foo-&gt;Release();            // ref_count = 0, freed
</code></pre>
<h3 id="mark-and-sweep"><a class="header" href="#mark-and-sweep">Mark and Sweep</a></h3>
<p>Two-phase garbage collection: mark reachable objects, then sweep unreachable ones.</p>
<h4 id="algorithm"><a class="header" href="#algorithm">Algorithm</a></h4>
<p><strong>Phase 1: Mark</strong></p>
<pre><code>1. Start from root set (globals, stack variables, registers)
2. Traverse object graph, marking each reachable object
3. Use depth-first or breadth-first search
</code></pre>
<p><strong>Phase 2: Sweep</strong></p>
<pre><code>1. Scan entire heap
2. Free unmarked objects
3. Reset marks for next collection
</code></pre>
<h4 id="visual-example"><a class="header" href="#visual-example">Visual Example</a></h4>
<pre><code>Initial State:
Root → [A] → [B] → [C]
       ↓
      [D]     [E]  [F] → [G]

Objects: A, B, C, D reachable from root
Objects: E, F, G unreachable (garbage)

After Mark Phase:
Root → [A]* → [B]* → [C]*
       ↓
      [D]*    [E]  [F] → [G]

(*= marked)

After Sweep Phase:
Root → [A] → [B] → [C]
       ↓
      [D]

E, F, G freed
</code></pre>
<h4 id="simple-implementation"><a class="header" href="#simple-implementation">Simple Implementation</a></h4>
<pre><code class="language-c">#define MARK_BIT 0x1

struct Object {
    struct Object* next;      // For linking in heap list
    unsigned flags;           // MARK_BIT stored here
    void* data;
    size_t size;
    struct Object** refs;     // Pointers to other objects
    size_t num_refs;
};

struct GC {
    struct Object* heap;      // All allocated objects
    struct Object** roots;    // Root set
    size_t num_roots;
};

// Mark phase: recursively mark reachable objects
void gc_mark(struct Object* obj) {
    if (!obj || (obj-&gt;flags &amp; MARK_BIT)) {
        return;  // Already marked
    }

    obj-&gt;flags |= MARK_BIT;  // Mark this object

    // Recursively mark referenced objects
    for (size_t i = 0; i &lt; obj-&gt;num_refs; i++) {
        gc_mark(obj-&gt;refs[i]);
    }
}

// Sweep phase: free unmarked objects
void gc_sweep(struct GC* gc) {
    struct Object** obj_ptr = &amp;gc-&gt;heap;

    while (*obj_ptr) {
        struct Object* obj = *obj_ptr;

        if (!(obj-&gt;flags &amp; MARK_BIT)) {
            // Unmarked - remove from list and free
            *obj_ptr = obj-&gt;next;
            free(obj-&gt;data);
            free(obj-&gt;refs);
            free(obj);
        } else {
            // Marked - clear mark for next cycle
            obj-&gt;flags &amp;= ~MARK_BIT;
            obj_ptr = &amp;obj-&gt;next;
        }
    }
}

// Full garbage collection
void gc_collect(struct GC* gc) {
    // Mark phase
    for (size_t i = 0; i &lt; gc-&gt;num_roots; i++) {
        gc_mark(gc-&gt;roots[i]);
    }

    // Sweep phase
    gc_sweep(gc);
}
</code></pre>
<h4 id="iterative-marking-避免堆栈溢出"><a class="header" href="#iterative-marking-避免堆栈溢出">Iterative Marking (避免堆栈溢出)</a></h4>
<p>Recursive marking can overflow stack for deep object graphs. Use iterative approach:</p>
<pre><code class="language-c">void gc_mark_iterative(struct Object* root) {
    // Use explicit stack
    struct Object** stack = malloc(sizeof(struct Object*) * 1000);
    int top = 0;

    stack[top++] = root;

    while (top &gt; 0) {
        struct Object* obj = stack[--top];

        if (!obj || (obj-&gt;flags &amp; MARK_BIT)) {
            continue;
        }

        obj-&gt;flags |= MARK_BIT;

        // Push children onto stack
        for (size_t i = 0; i &lt; obj-&gt;num_refs; i++) {
            if (top &lt; 1000) {  // Prevent overflow
                stack[top++] = obj-&gt;refs[i];
            }
        }
    }

    free(stack);
}
</code></pre>
<h4 id="advantages-3"><a class="header" href="#advantages-3">Advantages</a></h4>
<ol>
<li><strong>Handles cycles</strong>: Unreachable cycles are collected</li>
<li><strong>No overhead per assignment</strong>: Unlike reference counting</li>
<li><strong>Simple conceptually</strong>: Mark reachable, free unreachable</li>
</ol>
<h4 id="disadvantages-3"><a class="header" href="#disadvantages-3">Disadvantages</a></h4>
<ol>
<li><strong>Stop-the-world pauses</strong>: Must pause program during collection</li>
<li><strong>Unpredictable timing</strong>: Collection happens when heap fills</li>
<li><strong>Memory overhead</strong>: Need mark bits</li>
<li><strong>Fragmentation</strong>: Freed objects leave gaps</li>
</ol>
<h4 id="optimizations"><a class="header" href="#optimizations">Optimizations</a></h4>
<p><strong>1. Tri-Color Marking</strong> (see next section)</p>
<p><strong>2. Lazy Sweeping</strong></p>
<pre><code class="language-c">// Don't sweep all at once
// Sweep incrementally during allocations
void* gc_alloc_with_lazy_sweep(struct GC* gc, size_t size) {
    // Sweep a few objects
    for (int i = 0; i &lt; 10; i++) {
        if (gc-&gt;sweep_pos) {
            // Sweep one object
            gc-&gt;sweep_pos = gc-&gt;sweep_pos-&gt;next;
        }
    }

    // Then allocate
    return allocate(size);
}
</code></pre>
<p><strong>3. Generational Collection</strong> (see later section)</p>
<h4 id="when-mark-sweep-runs"><a class="header" href="#when-mark-sweep-runs">When Mark-Sweep Runs</a></h4>
<p><strong>Trigger 1: Heap Full</strong></p>
<pre><code class="language-c">void* gc_alloc(struct GC* gc, size_t size) {
    void* ptr = try_allocate(size);

    if (!ptr) {
        // Out of memory - collect garbage
        gc_collect(gc);
        ptr = try_allocate(size);
    }

    return ptr;
}
</code></pre>
<p><strong>Trigger 2: Periodic</strong></p>
<pre><code class="language-c">void main_loop() {
    static int alloc_count = 0;

    while (1) {
        do_work();

        if (++alloc_count &gt; 1000) {
            gc_collect(&amp;gc);
            alloc_count = 0;
        }
    }
}
</code></pre>
<p><strong>Trigger 3: Manual</strong></p>
<pre><code class="language-c">// Explicit collection call
gc_collect(&amp;gc);
</code></pre>
<h3 id="tri-color-marking"><a class="header" href="#tri-color-marking">Tri-Color Marking</a></h3>
<p>An incremental marking algorithm that allows GC work to be interleaved with program execution.</p>
<h4 id="the-three-colors"><a class="header" href="#the-three-colors">The Three Colors</a></h4>
<p><strong>White</strong>: Not yet visited; candidates for collection
<strong>Gray</strong>: Visited but children not yet scanned
<strong>Black</strong>: Visited and all children scanned</p>
<h4 id="algorithm-1"><a class="header" href="#algorithm-1">Algorithm</a></h4>
<pre><code>Initial:
- All objects are WHITE
- Roots are GRAY

While GRAY objects exist:
1. Pick a GRAY object
2. Mark it BLACK
3. Mark its WHITE children GRAY

After marking:
- BLACK objects are reachable (keep)
- WHITE objects are unreachable (collect)
</code></pre>
<h4 id="visual-example-1"><a class="header" href="#visual-example-1">Visual Example</a></h4>
<pre><code>Initial State:
ROOT → [A] → [B] → [C]
       ↓
      [D]     [E]

All objects WHITE

Step 1: Mark roots GRAY
ROOT → [A:GRAY] → [B:WHITE] → [C:WHITE]
       ↓
      [D:WHITE]    [E:WHITE]

Step 2: Process A (GRAY → BLACK, mark children GRAY)
ROOT → [A:BLACK] → [B:GRAY] → [C:WHITE]
       ↓
      [D:GRAY]     [E:WHITE]

Step 3: Process B (GRAY → BLACK, mark children GRAY)
ROOT → [A:BLACK] → [B:BLACK] → [C:GRAY]
       ↓
      [D:GRAY]     [E:WHITE]

Step 4: Process D (GRAY → BLACK, no children)
ROOT → [A:BLACK] → [B:BLACK] → [C:GRAY]
       ↓
      [D:BLACK]    [E:WHITE]

Step 5: Process C (GRAY → BLACK, no children)
ROOT → [A:BLACK] → [B:BLACK] → [C:BLACK]
       ↓
      [D:BLACK]    [E:WHITE]

Done! E is WHITE → collect it
</code></pre>
<h4 id="implementation"><a class="header" href="#implementation">Implementation</a></h4>
<pre><code class="language-c">enum Color { WHITE, GRAY, BLACK };

struct Object {
    enum Color color;
    void* data;
    struct Object** refs;
    size_t num_refs;
    struct Object* next;
};

struct GC {
    struct Object* all_objects;
    struct Object* gray_list;  // Work list
    struct Object** roots;
    size_t num_roots;
};

// Initialize all objects to WHITE
void gc_init(struct GC* gc) {
    for (struct Object* obj = gc-&gt;all_objects; obj; obj = obj-&gt;next) {
        obj-&gt;color = WHITE;
    }
    gc-&gt;gray_list = NULL;
}

// Add object to gray list
void gc_mark_gray(struct GC* gc, struct Object* obj) {
    if (obj-&gt;color == WHITE) {
        obj-&gt;color = GRAY;
        obj-&gt;next_gray = gc-&gt;gray_list;
        gc-&gt;gray_list = obj;
    }
}

// Process one gray object (incremental step)
void gc_process_one_gray(struct GC* gc) {
    if (!gc-&gt;gray_list) {
        return;  // No work to do
    }

    // Remove from gray list
    struct Object* obj = gc-&gt;gray_list;
    gc-&gt;gray_list = obj-&gt;next_gray;

    // Mark black
    obj-&gt;color = BLACK;

    // Mark children gray
    for (size_t i = 0; i &lt; obj-&gt;num_refs; i++) {
        gc_mark_gray(gc, obj-&gt;refs[i]);
    }
}

// Full collection
void gc_collect(struct GC* gc) {
    // Initialize
    gc_init(gc);

    // Mark roots gray
    for (size_t i = 0; i &lt; gc-&gt;num_roots; i++) {
        gc_mark_gray(gc, gc-&gt;roots[i]);
    }

    // Process all gray objects
    while (gc-&gt;gray_list) {
        gc_process_one_gray(gc);
    }

    // Sweep: free all WHITE objects
    struct Object** obj_ptr = &amp;gc-&gt;all_objects;
    while (*obj_ptr) {
        if ((*obj_ptr)-&gt;color == WHITE) {
            struct Object* garbage = *obj_ptr;
            *obj_ptr = garbage-&gt;next;
            free(garbage);
        } else {
            obj_ptr = &amp;(*obj_ptr)-&gt;next;
        }
    }
}

// Incremental collection (process N objects)
void gc_incremental_collect(struct GC* gc, int steps) {
    for (int i = 0; i &lt; steps &amp;&amp; gc-&gt;gray_list; i++) {
        gc_process_one_gray(gc);
    }
}
</code></pre>
<h4 id="incremental-collection"><a class="header" href="#incremental-collection">Incremental Collection</a></h4>
<pre><code class="language-c">void* gc_alloc(struct GC* gc, size_t size) {
    // Do a little GC work on each allocation
    if (gc-&gt;gc_in_progress) {
        gc_incremental_collect(gc, 10);  // Process 10 objects
    }

    void* ptr = allocate(size);

    if (!ptr) {
        // Start new GC cycle
        gc_start_collection(gc);
        ptr = allocate(size);
    }

    return ptr;
}
</code></pre>
<h4 id="write-barrier-problem"><a class="header" href="#write-barrier-problem">Write Barrier Problem</a></h4>
<p>When program runs concurrently with incremental GC, need to track pointer updates:</p>
<pre><code>Scenario:
1. A is BLACK (fully scanned)
2. B is WHITE (not yet visited)
3. C is GRAY (in progress)

Program executes: A.field = B

Problem: B might never be marked!
- A is BLACK (won't be rescanned)
- B is WHITE (not in gray list)
- After marking completes, B is still WHITE → incorrectly collected!
</code></pre>
<p><strong>Solution: Write Barrier</strong></p>
<pre><code class="language-c">void object_set_field(struct Object* obj, size_t field, struct Object* value) {
    obj-&gt;refs[field] = value;

    // Write barrier
    if (obj-&gt;color == BLACK &amp;&amp; value-&gt;color == WHITE) {
        // Re-mark object as GRAY
        gc_mark_gray(&amp;gc, obj);
        // Or mark value GRAY:
        // gc_mark_gray(&amp;gc, value);
    }
}
</code></pre>
<h4 id="advantages-4"><a class="header" href="#advantages-4">Advantages</a></h4>
<ol>
<li><strong>Incremental</strong>: Can pause/resume marking</li>
<li><strong>Lower pause times</strong>: Spread work over time</li>
<li><strong>Handles cycles</strong>: Like regular mark-sweep</li>
</ol>
<h4 id="disadvantages-4"><a class="header" href="#disadvantages-4">Disadvantages</a></h4>
<ol>
<li><strong>Write barrier overhead</strong>: Every pointer update must be tracked</li>
<li><strong>Complexity</strong>: More complex than simple mark-sweep</li>
<li><strong>Floating garbage</strong>: Some garbage survives until next cycle</li>
</ol>
<h3 id="generational-gc"><a class="header" href="#generational-gc">Generational GC</a></h3>
<p>Exploit the generational hypothesis: “Most objects die young.”</p>
<h4 id="generational-hypothesis"><a class="header" href="#generational-hypothesis">Generational Hypothesis</a></h4>
<p><strong>Observation:</strong></p>
<ul>
<li>90%+ of objects die within a short time of allocation</li>
<li>Long-lived objects tend to stay long-lived</li>
</ul>
<p><strong>Implication:</strong></p>
<ul>
<li>Collect young objects frequently (fast)</li>
<li>Collect old objects infrequently (slow but rare)</li>
</ul>
<h4 id="multi-generation-heap"><a class="header" href="#multi-generation-heap">Multi-Generation Heap</a></h4>
<pre><code>┌─────────────────────────────────────────────────────┐
│                 Young Generation                     │
│  (Eden)  │  (Survivor 0)  │  (Survivor 1)           │
│  [new objects] [survived 1 GC] [survived 2+ GCs]    │
└─────────────────────────────────────────────────────┘
                    ↓ (promotion)
┌─────────────────────────────────────────────────────┐
│                 Old Generation                       │
│  [long-lived objects]                               │
└─────────────────────────────────────────────────────┘
                    ↓ (promotion)
┌─────────────────────────────────────────────────────┐
│            Permanent Generation (Java)               │
│  [class metadata, interned strings]                 │
└─────────────────────────────────────────────────────┘
</code></pre>
<h4 id="algorithm-2"><a class="header" href="#algorithm-2">Algorithm</a></h4>
<p><strong>Minor GC (Young Generation):</strong></p>
<pre><code>1. Mark live objects in young generation
2. Copy live objects to survivor space
3. Clear eden space
4. Promote old survivors to old generation
</code></pre>
<p><strong>Major GC (Old Generation):</strong></p>
<pre><code>1. Mark live objects in entire heap
2. Sweep/compact old generation
3. Much slower, but rare
</code></pre>
<h4 id="example-implementation-simplified"><a class="header" href="#example-implementation-simplified">Example Implementation (Simplified)</a></h4>
<pre><code class="language-c">#define YOUNG_GEN_SIZE (1024 * 1024)  // 1 MB
#define OLD_GEN_SIZE (10 * 1024 * 1024)  // 10 MB

struct Object {
    int generation;  // 0 = young, 1 = old
    int age;         // Survived GC count
    void* data;
    struct Object** refs;
    size_t num_refs;
};

struct GenerationalGC {
    struct Object* young_gen;
    struct Object* old_gen;
    size_t young_size;
    size_t old_size;
};

void minor_gc(struct GenerationalGC* gc) {
    // Mark live objects in young generation
    struct Object* survivors = NULL;

    for (struct Object* obj = gc-&gt;young_gen; obj; obj = obj-&gt;next) {
        if (is_reachable(obj)) {
            obj-&gt;age++;

            if (obj-&gt;age &gt; 3) {
                // Promote to old generation
                promote_to_old(gc, obj);
            } else {
                // Keep in young generation
                obj-&gt;next = survivors;
                survivors = obj;
            }
        } else {
            // Free
            free_object(obj);
        }
    }

    gc-&gt;young_gen = survivors;
}

void major_gc(struct GenerationalGC* gc) {
    // Full heap collection (slow)
    mark_and_sweep(gc-&gt;young_gen);
    mark_and_sweep(gc-&gt;old_gen);
}

void* gc_alloc(struct GenerationalGC* gc, size_t size) {
    // Try allocating in young generation
    void* ptr = allocate_in_young(gc, size);

    if (!ptr) {
        // Young generation full - minor GC
        minor_gc(gc);
        ptr = allocate_in_young(gc, size);
    }

    if (!ptr) {
        // Still no space - major GC
        major_gc(gc);
        ptr = allocate_in_young(gc, size);
    }

    return ptr;
}
</code></pre>
<h4 id="card-table-for-cross-generation-references"><a class="header" href="#card-table-for-cross-generation-references">Card Table for Cross-Generation References</a></h4>
<p>Problem: Old objects might reference young objects. How to find roots for minor GC without scanning old generation?</p>
<p><strong>Solution: Card Table</strong></p>
<pre><code>Old Generation divided into "cards" (e.g., 512-byte regions)

Card Table: [0][0][1][0][1][0][0][0]...
             ↑           ↑
        No refs    Has refs to young gen

When old object updated:
1. Mark corresponding card as "dirty"
2. During minor GC, only scan dirty cards
</code></pre>
<p><strong>Implementation:</strong></p>
<pre><code class="language-c">#define CARD_SIZE 512
#define NUM_CARDS (OLD_GEN_SIZE / CARD_SIZE)

struct GenerationalGC {
    // ...
    unsigned char card_table[NUM_CARDS];  // 0 = clean, 1 = dirty
};

void write_barrier(void* old_obj, struct Object* value) {
    if (value-&gt;generation == 0) {  // Young object
        // Mark card dirty
        size_t card_index = ((char*)old_obj - old_gen_start) / CARD_SIZE;
        gc.card_table[card_index] = 1;
    }
}

void minor_gc_with_card_table(struct GenerationalGC* gc) {
    // Scan stack roots
    mark_from_roots();

    // Scan dirty cards in old generation
    for (size_t i = 0; i &lt; NUM_CARDS; i++) {
        if (gc-&gt;card_table[i]) {
            void* card_start = old_gen_start + i * CARD_SIZE;
            scan_card_for_young_refs(card_start);
            gc-&gt;card_table[i] = 0;  // Clear dirty bit
        }
    }

    // Collect young generation
    collect_young_gen();
}
</code></pre>
<h4 id="performance-characteristics-1"><a class="header" href="#performance-characteristics-1">Performance Characteristics</a></h4>
<p><strong>Minor GC:</strong></p>
<ul>
<li>Frequency: Very high (every few seconds)</li>
<li>Pause time: Very low (&lt; 10 ms)</li>
<li>Throughput: High (most objects die young)</li>
</ul>
<p><strong>Major GC:</strong></p>
<ul>
<li>Frequency: Low (every few minutes/hours)</li>
<li>Pause time: High (100+ ms)</li>
<li>Throughput: Lower (must scan entire heap)</li>
</ul>
<p><strong>Example (JVM):</strong></p>
<pre><code>Minor GC: 2-5 ms pause, every 1-10 seconds
Major GC: 100-500 ms pause, every 10-60 minutes
</code></pre>
<h4 id="advantages-5"><a class="header" href="#advantages-5">Advantages</a></h4>
<ol>
<li><strong>Fast minor GCs</strong>: Only collect young generation</li>
<li><strong>Exploits generational hypothesis</strong>: Most work on short-lived objects</li>
<li><strong>Lower average pause times</strong>: Minor GCs are frequent but fast</li>
</ol>
<h4 id="disadvantages-5"><a class="header" href="#disadvantages-5">Disadvantages</a></h4>
<ol>
<li><strong>Write barrier overhead</strong>: Must track cross-generation pointers</li>
<li><strong>Complexity</strong>: More complex than single-generation</li>
<li><strong>Promotion failures</strong>: Can trigger full GC unexpectedly</li>
</ol>
<h3 id="gc-tuning"><a class="header" href="#gc-tuning">GC Tuning</a></h3>
<p>Adjusting garbage collector parameters for optimal performance.</p>
<h4 id="key-metrics"><a class="header" href="#key-metrics">Key Metrics</a></h4>
<p><strong>1. Throughput</strong></p>
<pre><code>Throughput = Application Time / (Application Time + GC Time)

Example:
- Application runs 90 seconds
- GC runs 10 seconds
- Throughput = 90 / 100 = 90%
</code></pre>
<p><strong>2. Latency (Pause Time)</strong></p>
<pre><code>Max pause time: Longest single GC pause
Average pause time: Mean of all GC pauses
99th percentile: 99% of pauses below this time
</code></pre>
<p><strong>3. Footprint (Memory Usage)</strong></p>
<pre><code>Heap size
Live set size (reachable objects)
Memory overhead (GC metadata)
</code></pre>
<p><strong>Trade-offs:</strong></p>
<ul>
<li>Larger heap → Higher throughput, longer pauses</li>
<li>Smaller heap → Lower throughput, shorter pauses, more frequent GC</li>
</ul>
<h4 id="java-gc-tuning"><a class="header" href="#java-gc-tuning">Java GC Tuning</a></h4>
<p><strong>Heap Size:</strong></p>
<pre><code class="language-bash"># Initial and maximum heap size
java -Xms2g -Xmx4g MyApp

# Young generation size
java -Xmn1g MyApp

# Or ratio of young/old
java -XX:NewRatio=2 MyApp  # Old = 2 * Young
</code></pre>
<p><strong>GC Algorithm Selection:</strong></p>
<pre><code class="language-bash"># Serial GC (single-threaded, low overhead)
java -XX:+UseSerialGC MyApp

# Parallel GC (multi-threaded, high throughput)
java -XX:+UseParallelGC MyApp

# CMS (Concurrent Mark Sweep, low latency)
java -XX:+UseConcMarkSweepGC MyApp

# G1 GC (Garbage First, balanced)
java -XX:+UseG1GC MyApp

# ZGC (ultra-low latency, JDK 11+)
java -XX:+UseZGC MyApp

# Shenandoah (low latency, JDK 12+)
java -XX:+UseShenandoahGC MyApp
</code></pre>
<p><strong>GC Logging:</strong></p>
<pre><code class="language-bash"># Enable GC logging (JDK 8)
java -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:gc.log MyApp

# Enable GC logging (JDK 9+)
java -Xlog:gc*:file=gc.log:time,uptime,level,tags MyApp
</code></pre>
<p><strong>Example GC Log Analysis:</strong></p>
<pre><code>[GC (Allocation Failure) 2021-01-01T10:00:00.123+0000: 1.234:
  [ParNew: 614400K-&gt;68068K(614400K), 0.0924544 secs]
  614400K-&gt;68068K(2063104K), 0.0925372 secs]
  [Times: user=0.15 sys=0.01, real=0.09 secs]

Interpretation:
- Type: Minor GC (ParNew)
- Reason: Allocation Failure (young gen full)
- Young gen: 614400K → 68068K (89% freed!)
- Total heap: 614400K → 68068K
- Pause time: 92.5 ms
- CPU time: user=150ms, sys=10ms, real=90ms (parallelism ~1.7x)
</code></pre>
<h4 id="python-gc-tuning"><a class="header" href="#python-gc-tuning">Python GC Tuning</a></h4>
<p><strong>Adjust Thresholds:</strong></p>
<pre><code class="language-python">import gc

# Get current thresholds
print(gc.get_threshold())  # (700, 10, 10)

# Set new thresholds (threshold0, threshold1, threshold2)
gc.set_threshold(1000, 15, 15)

# threshold0: # of allocations before gen0 collection
# threshold1: # of gen0 collections before gen1 collection
# threshold2: # of gen1 collections before gen2 collection
</code></pre>
<p><strong>Disable/Enable GC:</strong></p>
<pre><code class="language-python"># Disable automatic GC
gc.disable()

# Do work...

# Manually trigger collection
gc.collect()

# Re-enable automatic GC
gc.enable()
</code></pre>
<p><strong>Manual Collection Strategy:</strong></p>
<pre><code class="language-python">import gc

def batch_process(items):
    gc.disable()  # Disable during processing

    for item in items:
        process(item)

    gc.collect()  # Collect once at end
    gc.enable()
</code></pre>
<h4 id="go-gc-tuning"><a class="header" href="#go-gc-tuning">Go GC Tuning</a></h4>
<p><strong>GOGC Environment Variable:</strong></p>
<pre><code class="language-bash"># Default: GOGC=100 (run GC when heap doubles)
# GOGC=200 (run GC when heap triples)
# GOGC=50 (run GC when heap grows 50%)
# GOGC=off (disable GC)

GOGC=200 ./myapp  # Less frequent GC, more memory
</code></pre>
<p><strong>Set Target Memory:</strong></p>
<pre><code class="language-bash"># New in Go 1.19: set memory limit
GOMEMLIMIT=2GiB ./myapp
</code></pre>
<p><strong>Manual GC:</strong></p>
<pre><code class="language-go">import "runtime"

func cleanup() {
    runtime.GC()  // Force garbage collection
}
</code></pre>
<p><strong>GC Tracing:</strong></p>
<pre><code class="language-bash"># Print GC trace
GODEBUG=gctrace=1 ./myapp

# Example output:
# gc 1 @0.002s 5%: 0.015+0.85+0.003 ms clock, 0.12+0.12/0.70/0.015+0.025 ms cpu, 4-&gt;4-&gt;0 MB, 5 MB goal, 8 P
#
# Interpretation:
# - GC #1
# - At 0.002 seconds
# - 5% CPU time in GC
# - Heap: 4 MB → 4 MB → 0 MB (before GC, after mark, after sweep)
# - Goal: 5 MB (next GC trigger)
# - 8 P (processors)
</code></pre>
<h4 id="tuning-strategy"><a class="header" href="#tuning-strategy">Tuning Strategy</a></h4>
<p><strong>1. Measure First</strong></p>
<pre><code>- Profile application
- Identify GC overhead
- Measure pause times
- Check memory usage
</code></pre>
<p><strong>2. Set Goals</strong></p>
<pre><code>Throughput-oriented:
- Maximize application CPU time
- Accept longer pause times
- Use Parallel GC (Java) or larger heap

Latency-oriented:
- Minimize pause times
- Accept lower throughput
- Use CMS/G1/ZGC (Java) or smaller heap
</code></pre>
<p><strong>3. Tune Incrementally</strong></p>
<pre><code>- Change one parameter at a time
- Measure impact
- Iterate
</code></pre>
<p><strong>4. Common Tuning Patterns</strong></p>
<p><strong>Pattern 1: High Throughput</strong></p>
<pre><code class="language-bash"># Java
java -Xms8g -Xmx8g -XX:+UseParallelGC -XX:ParallelGCThreads=8 MyApp

# Large heap, parallel collection
</code></pre>
<p><strong>Pattern 2: Low Latency</strong></p>
<pre><code class="language-bash"># Java
java -Xmx4g -XX:+UseZGC -XX:MaxGCPauseMillis=10 MyApp

# ZGC for sub-10ms pauses
</code></pre>
<p><strong>Pattern 3: Batch Processing</strong></p>
<pre><code class="language-python"># Python: disable GC during batch, collect after
gc.disable()
process_large_dataset()
gc.collect()
gc.enable()
</code></pre>
<h3 id="gc-pauses"><a class="header" href="#gc-pauses">GC Pauses</a></h3>
<p>Understanding and minimizing garbage collection pauses.</p>
<h4 id="types-of-pauses"><a class="header" href="#types-of-pauses">Types of Pauses</a></h4>
<p><strong>1. Stop-the-World (STW)</strong></p>
<pre><code>Application threads:  ████░░░░░░░░████████
GC thread:            ░░░░████████░░░░░░░░
                          ↑
                      STW pause
</code></pre>
<p>All application threads stopped during GC.</p>
<p><strong>2. Concurrent</strong></p>
<pre><code>Application threads:  ████████████████████
GC thread:            ░░░░████████████░░░░
                          ↑
                      Running concurrently
</code></pre>
<p>GC runs while application continues (with write barriers).</p>
<p><strong>3. Incremental</strong></p>
<pre><code>Application threads:  ██░█░█░█░█░█░█░█░███
GC thread:            ░░█░█░█░█░█░█░█░█░░░
                        ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑
                      Short pauses
</code></pre>
<p>GC work interleaved with application.</p>
<h4 id="pause-time-analysis"><a class="header" href="#pause-time-analysis">Pause Time Analysis</a></h4>
<p><strong>Measuring Pauses (Java):</strong></p>
<pre><code class="language-bash"># GC log shows pause times
java -Xlog:gc:file=gc.log MyApp

# Analyze with GCViewer or similar tool
</code></pre>
<p><strong>Example GC Pause Distribution:</strong></p>
<pre><code>P50 (median): 10 ms
P90: 50 ms
P99: 200 ms
P99.9: 500 ms
Max: 2000 ms
</code></pre>
<p><strong>Interpreting:</strong></p>
<ul>
<li>50% of pauses ≤ 10 ms (good)</li>
<li>90% of pauses ≤ 50 ms (acceptable)</li>
<li>1% of pauses &gt; 200 ms (may be problematic)</li>
<li>Max pause of 2 seconds (bad for latency-sensitive apps)</li>
</ul>
<h4 id="reducing-pause-times"><a class="header" href="#reducing-pause-times">Reducing Pause Times</a></h4>
<p><strong>Strategy 1: Use Concurrent Collector</strong></p>
<p><strong>Java CMS (Concurrent Mark Sweep):</strong></p>
<pre><code class="language-bash">java -XX:+UseConcMarkSweepGC -XX:+CMSIncrementalMode MyApp
</code></pre>
<p><strong>Phases:</strong></p>
<pre><code>1. Initial Mark (STW, short): Mark roots
2. Concurrent Mark: Mark reachable objects
3. Remark (STW, short): Catch changes during concurrent mark
4. Concurrent Sweep: Free garbage

STW pauses are short (10-100 ms)
</code></pre>
<p><strong>Java G1 (Garbage First):</strong></p>
<pre><code class="language-bash">java -XX:+UseG1GC -XX:MaxGCPauseMillis=100 MyApp
</code></pre>
<p><strong>Characteristics:</strong></p>
<ul>
<li>Divides heap into regions</li>
<li>Collects regions with most garbage first</li>
<li>Predictable pause times</li>
<li>Target: ~100 ms pauses</li>
</ul>
<p><strong>Java ZGC:</strong></p>
<pre><code class="language-bash">java -XX:+UseZGC MyApp
</code></pre>
<p><strong>Characteristics:</strong></p>
<ul>
<li>Sub-10ms pause times (even for 1+ TB heaps!)</li>
<li>Concurrent compaction</li>
<li>Colored pointers for tracking</li>
</ul>
<p><strong>Strategy 2: Reduce Heap Size</strong></p>
<pre><code class="language-bash"># Smaller heap = shorter GC pauses
# But more frequent GC

# Before: 8 GB heap, 500 ms pauses
java -Xmx8g MyApp

# After: 2 GB heap, 100 ms pauses (but 4x more frequent)
java -Xmx2g MyApp
</code></pre>
<p><strong>Strategy 3: Increase Young Generation Size</strong></p>
<pre><code class="language-bash"># Larger young gen = less frequent minor GCs
java -Xmn2g MyApp

# But each minor GC takes longer
</code></pre>
<p><strong>Strategy 4: Tune GC Threads</strong></p>
<pre><code class="language-bash"># More threads = shorter pause (if CPU available)
java -XX:ParallelGCThreads=8 MyApp

# Balance: too many threads causes contention
</code></pre>
<p><strong>Strategy 5: Avoid Finalizers</strong></p>
<pre><code class="language-java">// BAD: Finalizers slow down GC
class BadResource {
    @Override
    protected void finalize() {  // Don't use!
        cleanup();
    }
}

// GOOD: Explicit cleanup
class GoodResource implements AutoCloseable {
    @Override
    public void close() {
        cleanup();
    }
}

try (GoodResource r = new GoodResource()) {
    // Use resource
}  // Automatically cleaned up
</code></pre>
<p><strong>Strategy 6: Object Pooling</strong></p>
<pre><code class="language-java">// Reuse objects instead of allocating new ones
class ObjectPool&lt;T&gt; {
    private Queue&lt;T&gt; pool = new ConcurrentLinkedQueue&lt;&gt;();

    public T acquire() {
        T obj = pool.poll();
        return obj != null ? obj : createNew();
    }

    public void release(T obj) {
        reset(obj);
        pool.offer(obj);
    }
}

// Reduces allocation rate → less GC pressure
</code></pre>
<h4 id="real-world-example"><a class="header" href="#real-world-example">Real-World Example</a></h4>
<p><strong>Before Tuning:</strong></p>
<pre><code>Application: Latency-sensitive web service
Heap: 4 GB
GC: Parallel GC
Pause times: P99 = 800 ms (too high!)
Throughput: 95%
</code></pre>
<p><strong>After Tuning:</strong></p>
<pre><code class="language-bash"># Switch to G1 with pause time goal
java -Xms4g -Xmx4g \
     -XX:+UseG1GC \
     -XX:MaxGCPauseMillis=50 \
     -XX:G1HeapRegionSize=16m \
     MyApp
</code></pre>
<p><strong>Results:</strong></p>
<pre><code>Pause times: P99 = 45 ms (improved!)
Throughput: 92% (slight decrease, acceptable)
</code></pre>
<hr>
<h2 id="manual-memory-management"><a class="header" href="#manual-memory-management">Manual Memory Management</a></h2>
<p>Explicit allocation and deallocation of memory by the programmer.</p>
<h3 id="mallocfree-in-c"><a class="header" href="#mallocfree-in-c">malloc/free in C</a></h3>
<h4 id="basic-usage"><a class="header" href="#basic-usage">Basic Usage</a></h4>
<pre><code class="language-c">#include &lt;stdlib.h&gt;

// Allocate memory
int* ptr = malloc(sizeof(int) * 10);
if (ptr == NULL) {
    // Handle allocation failure
    fprintf(stderr, "Out of memory\n");
    return -1;
}

// Use memory
for (int i = 0; i &lt; 10; i++) {
    ptr[i] = i * i;
}

// Free memory
free(ptr);
ptr = NULL;  // Best practice: nullify after free
</code></pre>
<h4 id="common-patterns"><a class="header" href="#common-patterns">Common Patterns</a></h4>
<p><strong>Pattern 1: Dynamic Strings</strong></p>
<pre><code class="language-c">char* create_greeting(const char* name) {
    size_t len = strlen(name) + strlen("Hello, ") + 2;  // +2 for "!\0"
    char* greeting = malloc(len);
    if (!greeting) return NULL;

    sprintf(greeting, "Hello, %s!", name);
    return greeting;  // Caller must free!
}

// Usage
char* msg = create_greeting("Alice");
if (msg) {
    printf("%s\n", msg);
    free(msg);
}
</code></pre>
<p><strong>Pattern 2: Dynamic Arrays</strong></p>
<pre><code class="language-c">struct DynArray {
    int* data;
    size_t size;
    size_t capacity;
};

void array_init(struct DynArray* arr) {
    arr-&gt;data = NULL;
    arr-&gt;size = 0;
    arr-&gt;capacity = 0;
}

int array_push(struct DynArray* arr, int value) {
    if (arr-&gt;size &gt;= arr-&gt;capacity) {
        size_t new_cap = arr-&gt;capacity ? arr-&gt;capacity * 2 : 4;
        int* new_data = realloc(arr-&gt;data, new_cap * sizeof(int));
        if (!new_data) return -1;  // Allocation failed

        arr-&gt;data = new_data;
        arr-&gt;capacity = new_cap;
    }

    arr-&gt;data[arr-&gt;size++] = value;
    return 0;
}

void array_destroy(struct DynArray* arr) {
    free(arr-&gt;data);
    arr-&gt;data = NULL;
    arr-&gt;size = arr-&gt;capacity = 0;
}
</code></pre>
<p><strong>Pattern 3: Structures with Pointers</strong></p>
<pre><code class="language-c">struct Person {
    char* name;
    char* email;
    int age;
};

struct Person* person_create(const char* name, const char* email, int age) {
    struct Person* p = malloc(sizeof(struct Person));
    if (!p) return NULL;

    p-&gt;name = strdup(name);    // strdup = malloc + strcpy
    p-&gt;email = strdup(email);

    if (!p-&gt;name || !p-&gt;email) {
        free(p-&gt;name);
        free(p-&gt;email);
        free(p);
        return NULL;
    }

    p-&gt;age = age;
    return p;
}

void person_destroy(struct Person* p) {
    if (p) {
        free(p-&gt;name);
        free(p-&gt;email);
        free(p);
    }
}
</code></pre>
<h4 id="memory-allocation-functions"><a class="header" href="#memory-allocation-functions">Memory Allocation Functions</a></h4>
<p><strong>malloc() vs calloc() vs realloc():</strong></p>
<pre><code class="language-c">// malloc: uninitialized memory
int* a = malloc(10 * sizeof(int));
// a[0] has garbage value

// calloc: zero-initialized memory
int* b = calloc(10, sizeof(int));
// b[0] == 0

// realloc: resize existing allocation
a = realloc(a, 20 * sizeof(int));
// First 10 elements preserved, next 10 uninitialized
</code></pre>
<p><strong>Performance:</strong></p>
<pre><code class="language-c">// Benchmark: malloc vs calloc
clock_t start;

start = clock();
for (int i = 0; i &lt; 100000; i++) {
    int* p = malloc(1000 * sizeof(int));
    free(p);
}
printf("malloc: %f s\n", (double)(clock() - start) / CLOCKS_PER_SEC);

start = clock();
for (int i = 0; i &lt; 100000; i++) {
    int* p = calloc(1000, sizeof(int));
    free(p);
}
printf("calloc: %f s\n", (double)(clock() - start) / CLOCKS_PER_SEC);

// calloc typically 2-3x slower due to zeroing
</code></pre>
<h4 id="common-mistakes"><a class="header" href="#common-mistakes">Common Mistakes</a></h4>
<p><strong>Mistake 1: Memory Leak</strong></p>
<pre><code class="language-c">// BAD: Memory leak
void bad_function() {
    char* ptr = malloc(1000);
    // Forgot to free!
}  // ptr goes out of scope, memory leaked

// GOOD
void good_function() {
    char* ptr = malloc(1000);
    // Use ptr...
    free(ptr);
}
</code></pre>
<p><strong>Mistake 2: Use After Free</strong></p>
<pre><code class="language-c">// BAD: Use after free
int* ptr = malloc(sizeof(int));
*ptr = 42;
free(ptr);
printf("%d\n", *ptr);  // Undefined behavior!

// GOOD
int* ptr = malloc(sizeof(int));
*ptr = 42;
printf("%d\n", *ptr);
free(ptr);
ptr = NULL;  // Nullify to catch errors
</code></pre>
<p><strong>Mistake 3: Double Free</strong></p>
<pre><code class="language-c">// BAD: Double free
int* ptr = malloc(sizeof(int));
free(ptr);
free(ptr);  // Undefined behavior!

// GOOD
int* ptr = malloc(sizeof(int));
free(ptr);
ptr = NULL;
// free(NULL) is safe (no-op)
</code></pre>
<p><strong>Mistake 4: Incorrect Size</strong></p>
<pre><code class="language-c">// BAD: Wrong size
int* arr = malloc(10);  // Only 10 bytes, not 10 ints!

// GOOD
int* arr = malloc(10 * sizeof(int));
// Or
int* arr = malloc(sizeof(int[10]));
</code></pre>
<p><strong>Mistake 5: Not Checking Return Value</strong></p>
<pre><code class="language-c">// BAD: No error checking
int* ptr = malloc(1000000000);
*ptr = 42;  // Crash if malloc failed!

// GOOD
int* ptr = malloc(1000000000);
if (!ptr) {
    fprintf(stderr, "Allocation failed\n");
    return -1;
}
*ptr = 42;
</code></pre>
<h3 id="newdelete-in-c"><a class="header" href="#newdelete-in-c">new/delete in C++</a></h3>
<h4 id="basic-usage-1"><a class="header" href="#basic-usage-1">Basic Usage</a></h4>
<pre><code class="language-cpp">// Single object
int* ptr = new int(42);
delete ptr;

// Array
int* arr = new int[10];
delete[] arr;  // Note: delete[], not delete!

// With constructor
class Person {
public:
    Person(std::string name) : name(name) {}
    ~Person() { std::cout &lt;&lt; "Destroying " &lt;&lt; name &lt;&lt; "\n"; }
private:
    std::string name;
};

Person* p = new Person("Alice");
delete p;  // Calls destructor automatically
</code></pre>
<h4 id="new-vs-malloc"><a class="header" href="#new-vs-malloc">new vs malloc</a></h4>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Feature</th><th>new</th><th>malloc</th></tr>
</thead>
<tbody>
<tr><td>Type</td><td>Operator</td><td>Function</td></tr>
<tr><td>Returns</td><td>Typed pointer</td><td>void*</td></tr>
<tr><td>Size</td><td>Automatic</td><td>Manual calculation</td></tr>
<tr><td>Initialization</td><td>Calls constructor</td><td>No initialization</td></tr>
<tr><td>Failure</td><td>Throws exception</td><td>Returns NULL</td></tr>
<tr><td>Overloadable</td><td>Yes</td><td>No</td></tr>
</tbody>
</table>
</div>
<pre><code class="language-cpp">// new: type-safe, calls constructor
std::string* s1 = new std::string("Hello");

// malloc: type-unsafe, no constructor
std::string* s2 = (std::string*)malloc(sizeof(std::string));
// BUG: s2 not initialized! (no constructor called)
</code></pre>
<h4 id="placement-new"><a class="header" href="#placement-new">Placement new</a></h4>
<p>Construct object at specific memory address:</p>
<pre><code class="language-cpp">#include &lt;new&gt;

// Allocate raw memory
void* buffer = malloc(sizeof(std::string));

// Construct object in that memory
std::string* s = new (buffer) std::string("Hello");

// Use object
std::cout &lt;&lt; *s &lt;&lt; "\n";

// Manually call destructor
s-&gt;~string();

// Free memory
free(buffer);
</code></pre>
<p><strong>Use case: Memory pools</strong></p>
<pre><code class="language-cpp">class ObjectPool {
    char buffer[1000 * sizeof(MyClass)];

public:
    MyClass* allocate() {
        void* ptr = get_free_slot();
        return new (ptr) MyClass();  // Placement new
    }

    void deallocate(MyClass* obj) {
        obj-&gt;~MyClass();  // Manual destructor call
        mark_slot_free(obj);
    }
};
</code></pre>
<h4 id="array-newdelete"><a class="header" href="#array-newdelete">Array new/delete</a></h4>
<pre><code class="language-cpp">// Allocate array
int* arr = new int[10];

// MUST use delete[]
delete[] arr;  // Correct

// BAD: Using delete instead of delete[]
delete arr;  // Undefined behavior! Memory corruption!
</code></pre>
<p><strong>Why separate delete[]?</strong></p>
<pre><code class="language-cpp">class MyClass {
public:
    MyClass() { std::cout &lt;&lt; "Constructor\n"; }
    ~MyClass() { std::cout &lt;&lt; "Destructor\n"; }
};

MyClass* arr = new MyClass[3];
// Calls constructor 3 times

delete[] arr;
// Calls destructor 3 times

delete arr;
// Only calls destructor once! Other 2 objects not destroyed!
</code></pre>
<h4 id="nothrow-new"><a class="header" href="#nothrow-new">nothrow new</a></h4>
<pre><code class="language-cpp">// Default: throws std::bad_alloc on failure
try {
    int* ptr = new int[1000000000000];  // Huge allocation
} catch (std::bad_alloc&amp; e) {
    std::cerr &lt;&lt; "Allocation failed: " &lt;&lt; e.what() &lt;&lt; "\n";
}

// nothrow: returns nullptr on failure (like malloc)
int* ptr = new (std::nothrow) int[1000000000000];
if (!ptr) {
    std::cerr &lt;&lt; "Allocation failed\n";
}
</code></pre>
<h4 id="custom-newdelete-operators"><a class="header" href="#custom-newdelete-operators">Custom new/delete Operators</a></h4>
<pre><code class="language-cpp">class MyClass {
public:
    // Custom new operator
    void* operator new(size_t size) {
        std::cout &lt;&lt; "Custom new: " &lt;&lt; size &lt;&lt; " bytes\n";
        void* ptr = ::operator new(size);  // Call global new
        return ptr;
    }

    // Custom delete operator
    void operator delete(void* ptr) {
        std::cout &lt;&lt; "Custom delete\n";
        ::operator delete(ptr);  // Call global delete
    }
};

MyClass* obj = new MyClass();  // Calls MyClass::operator new
delete obj;                     // Calls MyClass::operator delete
</code></pre>
<p><strong>Use case: Tracking allocations</strong></p>
<pre><code class="language-cpp">class Tracked {
    static size_t allocation_count;

public:
    void* operator new(size_t size) {
        allocation_count++;
        return ::operator new(size);
    }

    void operator delete(void* ptr) {
        allocation_count--;
        ::operator delete(ptr);
    }

    static size_t get_allocation_count() {
        return allocation_count;
    }
};

size_t Tracked::allocation_count = 0;
</code></pre>
<h3 id="memory-leak-detection"><a class="header" href="#memory-leak-detection">Memory Leak Detection</a></h3>
<h4 id="valgrind-linux"><a class="header" href="#valgrind-linux">Valgrind (Linux)</a></h4>
<p><strong>Installation:</strong></p>
<pre><code class="language-bash">sudo apt-get install valgrind
</code></pre>
<p><strong>Basic Usage:</strong></p>
<pre><code class="language-bash"># Compile with debug symbols
gcc -g -o myapp myapp.c

# Run with Valgrind
valgrind --leak-check=full ./myapp
</code></pre>
<p><strong>Example Output:</strong></p>
<pre><code>==12345== HEAP SUMMARY:
==12345==     in use at exit: 1,000 bytes in 1 blocks
==12345==   total heap usage: 2 allocs, 1 frees, 2,000 bytes allocated
==12345==
==12345== 1,000 bytes in 1 blocks are definitely lost in loss record 1 of 1
==12345==    at 0x4C2DB8F: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==12345==    by 0x40057E: main (myapp.c:10)
==12345==
==12345== LEAK SUMMARY:
==12345==    definitely lost: 1,000 bytes in 1 blocks
==12345==    indirectly lost: 0 bytes in 0 blocks
==12345==      possibly lost: 0 bytes in 0 blocks
==12345==    still reachable: 0 bytes in 0 blocks
==12345==         suppressed: 0 bytes in 0 blocks
</code></pre>
<p><strong>Leak Categories:</strong></p>
<ul>
<li><strong>Definitely lost</strong>: No pointers to block</li>
<li><strong>Indirectly lost</strong>: Lost through lost container</li>
<li><strong>Possibly lost</strong>: Pointer exists but not to start of block</li>
<li><strong>Still reachable</strong>: Pointer still exists (not necessarily a leak)</li>
</ul>
<p><strong>Advanced Options:</strong></p>
<pre><code class="language-bash"># Track all allocations (slow but thorough)
valgrind --leak-check=full --show-leak-kinds=all ./myapp

# Generate suppression file for false positives
valgrind --gen-suppressions=all ./myapp 2&gt;supp.txt

# Use suppression file
valgrind --suppressions=supp.txt ./myapp
</code></pre>
<h4 id="addresssanitizer-asan"><a class="header" href="#addresssanitizer-asan">AddressSanitizer (ASan)</a></h4>
<p>Compiler-based tool for detecting memory errors.</p>
<p><strong>Compilation:</strong></p>
<pre><code class="language-bash"># GCC/Clang
gcc -fsanitize=address -g -o myapp myapp.c

# Run normally (no special tool needed)
./myapp
</code></pre>
<p><strong>Detects:</strong></p>
<ul>
<li>Heap buffer overflow</li>
<li>Stack buffer overflow</li>
<li>Use-after-free</li>
<li>Use-after-return</li>
<li>Use-after-scope</li>
<li>Double-free</li>
<li>Memory leaks</li>
</ul>
<p><strong>Example Output:</strong></p>
<pre><code>=================================================================
==12345==ERROR: AddressSanitizer: heap-use-after-free on address 0x60300000eff0
READ of size 4 at 0x60300000eff0 thread T0
    #0 0x400b42 in main myapp.c:15
    #1 0x7f8b7c8c3b96 in __libc_start_main
    #2 0x400a09 in _start

0x60300000eff0 is located 0 bytes inside of 4-byte region [0x60300000eff0,0x60300000eff4)
freed by thread T0 here:
    #0 0x7f8b7cc63537 in __interceptor_free
    #1 0x400b2d in main myapp.c:14
</code></pre>
<p><strong>Advantages over Valgrind:</strong></p>
<ul>
<li>Much faster (2-3x slowdown vs 20-50x)</li>
<li>Catches more types of errors</li>
<li>Works with multithreaded code better</li>
</ul>
<p><strong>Disadvantages:</strong></p>
<ul>
<li>Requires recompilation</li>
<li>Increases binary size</li>
<li>May not catch all leaks</li>
</ul>
<h4 id="leaksanitizer-lsan"><a class="header" href="#leaksanitizer-lsan">LeakSanitizer (LSan)</a></h4>
<p>Part of AddressSanitizer, focused on leak detection.</p>
<pre><code class="language-bash"># Enable leak detection (included with ASan)
gcc -fsanitize=address -g -o myapp myapp.c

# Or use LeakSanitizer standalone
gcc -fsanitize=leak -g -o myapp myapp.c

./myapp
</code></pre>
<p><strong>Suppress false positives:</strong></p>
<pre><code class="language-c">// In code
const char* __lsan_default_suppressions() {
    return "leak:some_function\n";
}

// Or via environment variable
LSAN_OPTIONS=suppressions=supp.txt ./myapp
</code></pre>
<h4 id="manual-leak-tracking"><a class="header" href="#manual-leak-tracking">Manual Leak Tracking</a></h4>
<p><strong>Simple Reference Counting:</strong></p>
<pre><code class="language-c">#ifdef DEBUG_MEMORY
static size_t alloc_count = 0;
static size_t free_count = 0;

void* debug_malloc(size_t size, const char* file, int line) {
    void* ptr = malloc(size);
    if (ptr) {
        alloc_count++;
        printf("[ALLOC] %p (%zu bytes) at %s:%d\n", ptr, size, file, line);
    }
    return ptr;
}

void debug_free(void* ptr, const char* file, int line) {
    if (ptr) {
        free_count++;
        printf("[FREE] %p at %s:%d\n", ptr, file, line);
    }
    free(ptr);
}

#define malloc(size) debug_malloc(size, __FILE__, __LINE__)
#define free(ptr) debug_free(ptr, __FILE__, __LINE__)

void print_leak_summary() {
    printf("Allocations: %zu\n", alloc_count);
    printf("Frees: %zu\n", free_count);
    printf("Leaks: %zu\n", alloc_count - free_count);
}
#endif
</code></pre>
<p><strong>Allocation Tracking Table:</strong></p>
<pre><code class="language-c">#define MAX_ALLOCATIONS 10000

struct Allocation {
    void* ptr;
    size_t size;
    const char* file;
    int line;
};

static struct Allocation allocations[MAX_ALLOCATIONS];
static size_t num_allocations = 0;

void track_allocation(void* ptr, size_t size, const char* file, int line) {
    if (num_allocations &lt; MAX_ALLOCATIONS) {
        allocations[num_allocations++] = (struct Allocation){
            .ptr = ptr,
            .size = size,
            .file = file,
            .line = line
        };
    }
}

void untrack_allocation(void* ptr) {
    for (size_t i = 0; i &lt; num_allocations; i++) {
        if (allocations[i].ptr == ptr) {
            allocations[i] = allocations[--num_allocations];
            return;
        }
    }
    fprintf(stderr, "ERROR: Free of untracked pointer %p\n", ptr);
}

void print_leaks() {
    printf("=== Memory Leaks ===\n");
    for (size_t i = 0; i &lt; num_allocations; i++) {
        printf("Leak: %zu bytes at %s:%d\n",
               allocations[i].size,
               allocations[i].file,
               allocations[i].line);
    }
}
</code></pre>
<h3 id="use-after-free-bugs"><a class="header" href="#use-after-free-bugs">Use-After-Free Bugs</a></h3>
<p>Accessing memory after it has been freed.</p>
<h4 id="example"><a class="header" href="#example">Example</a></h4>
<pre><code class="language-c">int* ptr = malloc(sizeof(int));
*ptr = 42;
free(ptr);

// Use-after-free!
printf("%d\n", *ptr);  // Undefined behavior
*ptr = 100;            // Undefined behavior (likely crash)
</code></pre>
<h4 id="why-its-dangerous"><a class="header" href="#why-its-dangerous">Why It’s Dangerous</a></h4>
<p><strong>Scenario 1: Memory Reused</strong></p>
<pre><code class="language-c">int* ptr1 = malloc(sizeof(int));
*ptr1 = 42;
free(ptr1);

// Another allocation reuses the same memory
char* ptr2 = malloc(sizeof(char) * 100);
strcpy(ptr2, "Hello");

// Use-after-free: corrupts ptr2!
*ptr1 = 100;

printf("%s\n", ptr2);  // Might print garbage
</code></pre>
<p><strong>Scenario 2: Security Vulnerability</strong></p>
<pre><code class="language-c">struct User {
    char name[32];
    int is_admin;
};

struct User* user = malloc(sizeof(struct User));
strcpy(user-&gt;name, "Alice");
user-&gt;is_admin = 0;
free(user);

// Attacker allocates at same address
char* exploit = malloc(sizeof(struct User));
memset(exploit, 1, sizeof(struct User));  // Set is_admin = 1

// Use-after-free: treats exploit as user
if (user-&gt;is_admin) {
    printf("Admin access granted!\n");  // Security breach!
}
</code></pre>
<h4 id="detection-with-addresssanitizer"><a class="header" href="#detection-with-addresssanitizer">Detection with AddressSanitizer</a></h4>
<pre><code class="language-c">#include &lt;stdlib.h&gt;

int main() {
    int* ptr = malloc(sizeof(int));
    *ptr = 42;
    free(ptr);

    *ptr = 100;  // Use-after-free

    return 0;
}
</code></pre>
<pre><code class="language-bash">$ gcc -fsanitize=address -g -o test test.c
$ ./test

=================================================================
==12345==ERROR: AddressSanitizer: heap-use-after-free on address 0x60300000eff0
WRITE of size 4 at 0x60300000eff0 thread T0
    #0 0x400b95 in main test.c:8

0x60300000eff0 is located 0 bytes inside of 4-byte region
freed by thread T0 here:
    #0 0x7f0b7cc63537 in __interceptor_free
    #1 0x400b80 in main test.c:7
</code></pre>
<h4 id="prevention"><a class="header" href="#prevention">Prevention</a></h4>
<p><strong>1. Nullify After Free</strong></p>
<pre><code class="language-c">int* ptr = malloc(sizeof(int));
// Use ptr...
free(ptr);
ptr = NULL;  // Further access will crash (better than corruption)

if (ptr) {
    *ptr = 100;  // Won't execute
}
</code></pre>
<p><strong>2. Use Wrapper Functions</strong></p>
<pre><code class="language-c">#define SAFE_FREE(ptr) do { free(ptr); (ptr) = NULL; } while(0)

int* ptr = malloc(sizeof(int));
SAFE_FREE(ptr);  // Frees and nullifies

*ptr = 100;  // Crash (detectable) instead of corruption
</code></pre>
<p><strong>3. Smart Pointers (C++)</strong></p>
<pre><code class="language-cpp">{
    std::unique_ptr&lt;int&gt; ptr = std::make_unique&lt;int&gt;(42);
    // Use ptr...
}  // Automatically freed, ptr no longer accessible
</code></pre>
<p><strong>4. Ownership Tracking</strong></p>
<pre><code class="language-c">enum State { VALID, FREED };

struct TrackedPointer {
    void* ptr;
    enum State state;
};

struct TrackedPointer* create_tracked(size_t size) {
    struct TrackedPointer* tp = malloc(sizeof(struct TrackedPointer));
    tp-&gt;ptr = malloc(size);
    tp-&gt;state = VALID;
    return tp;
}

void* get_ptr(struct TrackedPointer* tp) {
    assert(tp-&gt;state == VALID &amp;&amp; "Use-after-free detected!");
    return tp-&gt;ptr;
}

void free_tracked(struct TrackedPointer* tp) {
    assert(tp-&gt;state == VALID &amp;&amp; "Double-free detected!");
    free(tp-&gt;ptr);
    tp-&gt;state = FREED;
}
</code></pre>
<h3 id="double-free-errors"><a class="header" href="#double-free-errors">Double-Free Errors</a></h3>
<p>Calling <code>free()</code> twice on the same pointer.</p>
<h4 id="example-1"><a class="header" href="#example-1">Example</a></h4>
<pre><code class="language-c">int* ptr = malloc(sizeof(int));
free(ptr);
free(ptr);  // Double-free! Undefined behavior
</code></pre>
<h4 id="why-its-dangerous-1"><a class="header" href="#why-its-dangerous-1">Why It’s Dangerous</a></h4>
<p><strong>Heap Corruption:</strong></p>
<pre><code class="language-c">int* a = malloc(100);
int* b = malloc(100);
free(a);
free(a);  // Double-free corrupts heap metadata

int* c = malloc(100);  // May crash or return invalid pointer
</code></pre>
<p><strong>Exploitability:</strong></p>
<ul>
<li>Attackers can trigger double-free to corrupt heap</li>
<li>Can lead to arbitrary code execution</li>
<li>Common in security vulnerabilities</li>
</ul>
<h4 id="detection"><a class="header" href="#detection">Detection</a></h4>
<p><strong>AddressSanitizer:</strong></p>
<pre><code class="language-c">int main() {
    int* ptr = malloc(sizeof(int));
    free(ptr);
    free(ptr);  // Double-free
    return 0;
}
</code></pre>
<pre><code class="language-bash">$ gcc -fsanitize=address -g -o test test.c
$ ./test

=================================================================
==12345==ERROR: AddressSanitizer: attempting double-free on 0x60300000eff0
    #0 0x7f0b7cc63537 in __interceptor_free
    #1 0x400b95 in main test.c:5
</code></pre>
<p><strong>Valgrind:</strong></p>
<pre><code class="language-bash">$ valgrind ./test

==12345== Invalid free() / delete / delete[] / realloc()
==12345==    at 0x4C2EDEB: free (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==12345==    by 0x40057E: main (test.c:5)
==12345==  Address 0x5203040 is 0 bytes inside a block of size 4 free'd
</code></pre>
<h4 id="prevention-1"><a class="header" href="#prevention-1">Prevention</a></h4>
<p><strong>1. Nullify After Free</strong></p>
<pre><code class="language-c">int* ptr = malloc(sizeof(int));
free(ptr);
ptr = NULL;

free(ptr);  // Safe: free(NULL) is a no-op
</code></pre>
<p><strong>2. Safe Free Macro</strong></p>
<pre><code class="language-c">#define SAFE_FREE(ptr) do { \
    free(ptr); \
    (ptr) = NULL; \
} while(0)

int* ptr = malloc(sizeof(int));
SAFE_FREE(ptr);
SAFE_FREE(ptr);  // Safe (second call frees NULL)
</code></pre>
<p><strong>3. Ownership Pattern</strong></p>
<pre><code class="language-c">struct Resource {
    void* data;
    int owned;  // 1 if we own it, 0 if transferred
};

void resource_free(struct Resource* r) {
    if (r-&gt;owned) {
        free(r-&gt;data);
        r-&gt;owned = 0;
    }
}

// Transfer ownership
void resource_transfer(struct Resource* from, struct Resource* to) {
    to-&gt;data = from-&gt;data;
    to-&gt;owned = 1;
    from-&gt;owned = 0;  // No longer owns it
}
</code></pre>
<p><strong>4. RAII in C++</strong></p>
<pre><code class="language-cpp">{
    std::unique_ptr&lt;int&gt; ptr = std::make_unique&lt;int&gt;(42);
    // Impossible to double-free with unique_ptr
}  // Automatically freed once
</code></pre>
<hr>
<h2 id="smart-pointers-c"><a class="header" href="#smart-pointers-c">Smart Pointers (C++)</a></h2>
<p>Automatic memory management through RAII (Resource Acquisition Is Initialization).</p>
<h3 id="unique_ptr"><a class="header" href="#unique_ptr">unique_ptr</a></h3>
<p>Exclusive ownership smart pointer - only one unique_ptr can own a resource.</p>
<h4 id="basic-usage-2"><a class="header" href="#basic-usage-2">Basic Usage</a></h4>
<pre><code class="language-cpp">#include &lt;memory&gt;

// Create unique_ptr
std::unique_ptr&lt;int&gt; ptr1(new int(42));
// Or (preferred):
std::unique_ptr&lt;int&gt; ptr2 = std::make_unique&lt;int&gt;(42);

// Access
*ptr2 = 100;
std::cout &lt;&lt; *ptr2 &lt;&lt; "\n";  // 100

// Automatic cleanup when ptr2 goes out of scope
</code></pre>
<h4 id="arrays"><a class="header" href="#arrays">Arrays</a></h4>
<pre><code class="language-cpp">// Array unique_ptr
std::unique_ptr&lt;int[]&gt; arr = std::make_unique&lt;int[]&gt;(10);

// Access elements
arr[0] = 1;
arr[1] = 2;

// Automatically deletes with delete[], not delete
</code></pre>
<h4 id="move-semantics"><a class="header" href="#move-semantics">Move Semantics</a></h4>
<pre><code class="language-cpp">// unique_ptr cannot be copied (deleted copy constructor)
std::unique_ptr&lt;int&gt; ptr1 = std::make_unique&lt;int&gt;(42);
// std::unique_ptr&lt;int&gt; ptr2 = ptr1;  // ERROR: cannot copy

// But can be moved (transfers ownership)
std::unique_ptr&lt;int&gt; ptr2 = std::move(ptr1);
// Now ptr1 is nullptr, ptr2 owns the resource
</code></pre>
<h4 id="return-from-function"><a class="header" href="#return-from-function">Return from Function</a></h4>
<pre><code class="language-cpp">std::unique_ptr&lt;MyClass&gt; create_object() {
    std::unique_ptr&lt;MyClass&gt; ptr = std::make_unique&lt;MyClass&gt;();
    // ...
    return ptr;  // Move semantics (no copy)
}

// Caller receives ownership
std::unique_ptr&lt;MyClass&gt; obj = create_object();
</code></pre>
<h4 id="custom-deleters"><a class="header" href="#custom-deleters">Custom Deleters</a></h4>
<pre><code class="language-cpp">// Custom deleter for FILE*
auto file_deleter = [](FILE* f) {
    if (f) fclose(f);
};

std::unique_ptr&lt;FILE, decltype(file_deleter)&gt; file(
    fopen("test.txt", "r"),
    file_deleter
);

// file automatically closed when unique_ptr destroyed

// Or with function pointer
void close_file(FILE* f) {
    if (f) fclose(f);
}

std::unique_ptr&lt;FILE, void(*)(FILE*)&gt; file2(
    fopen("test.txt", "r"),
    close_file
);
</code></pre>
<h4 id="release-ownership"><a class="header" href="#release-ownership">Release Ownership</a></h4>
<pre><code class="language-cpp">std::unique_ptr&lt;int&gt; ptr = std::make_unique&lt;int&gt;(42);

// Release ownership (returns raw pointer, unique_ptr becomes null)
int* raw = ptr.release();

// Now we're responsible for deletion
delete raw;
</code></pre>
<h4 id="reset"><a class="header" href="#reset">Reset</a></h4>
<pre><code class="language-cpp">std::unique_ptr&lt;int&gt; ptr = std::make_unique&lt;int&gt;(42);

// Delete current and manage new object
ptr.reset(new int(100));

// Or delete current and become null
ptr.reset();
// ptr is now nullptr
</code></pre>
<h4 id="advantages-6"><a class="header" href="#advantages-6">Advantages</a></h4>
<ul>
<li>Zero overhead (same size as raw pointer)</li>
<li>Automatic cleanup</li>
<li>Move-only (clear ownership semantics)</li>
<li>Type-safe</li>
<li>Works with arrays</li>
</ul>
<h4 id="use-cases-3"><a class="header" href="#use-cases-3">Use Cases</a></h4>
<pre><code class="language-cpp">// 1. Function-local resources
void process_file(const std::string&amp; filename) {
    std::unique_ptr&lt;File&gt; file = open_file(filename);
    // Process file...
    // Automatic cleanup even if exception thrown
}

// 2. Class members (exclusive ownership)
class Widget {
    std::unique_ptr&lt;Impl&gt; pImpl;  // Pimpl idiom
public:
    Widget() : pImpl(std::make_unique&lt;Impl&gt;()) {}
    // Compiler-generated destructor automatically deletes pImpl
};

// 3. Factory functions
std::unique_ptr&lt;Shape&gt; create_shape(ShapeType type) {
    switch (type) {
        case CIRCLE: return std::make_unique&lt;Circle&gt;();
        case SQUARE: return std::make_unique&lt;Square&gt;();
    }
}
</code></pre>
<h3 id="shared_ptr"><a class="header" href="#shared_ptr">shared_ptr</a></h3>
<p>Shared ownership smart pointer - multiple shared_ptrs can own the same resource.</p>
<h4 id="basic-usage-3"><a class="header" href="#basic-usage-3">Basic Usage</a></h4>
<pre><code class="language-cpp">std::shared_ptr&lt;int&gt; ptr1 = std::make_shared&lt;int&gt;(42);
std::cout &lt;&lt; "Count: " &lt;&lt; ptr1.use_count() &lt;&lt; "\n";  // 1

{
    std::shared_ptr&lt;int&gt; ptr2 = ptr1;  // Copying allowed
    std::cout &lt;&lt; "Count: " &lt;&lt; ptr1.use_count() &lt;&lt; "\n";  // 2
    *ptr2 = 100;
}  // ptr2 destroyed, count decrements

std::cout &lt;&lt; "Count: " &lt;&lt; ptr1.use_count() &lt;&lt; "\n";  // 1
std::cout &lt;&lt; "*ptr1: " &lt;&lt; *ptr1 &lt;&lt; "\n";  // 100

// When last shared_ptr destroyed, resource deleted
</code></pre>
<h4 id="reference-counting-1"><a class="header" href="#reference-counting-1">Reference Counting</a></h4>
<pre><code class="language-cpp">std::shared_ptr&lt;int&gt; ptr1 = std::make_shared&lt;int&gt;(42);  // ref_count = 1

std::shared_ptr&lt;int&gt; ptr2 = ptr1;                        // ref_count = 2
std::shared_ptr&lt;int&gt; ptr3 = ptr2;                        // ref_count = 3

ptr1.reset();                                            // ref_count = 2
ptr2 = nullptr;                                          // ref_count = 1
// Resource still alive (ptr3 still owns it)

ptr3.reset();                                            // ref_count = 0, deleted!
</code></pre>
<h4 id="make_shared-vs-constructor"><a class="header" href="#make_shared-vs-constructor">make_shared vs Constructor</a></h4>
<pre><code class="language-cpp">// Preferred: make_shared (one allocation)
std::shared_ptr&lt;MyClass&gt; ptr1 = std::make_shared&lt;MyClass&gt;(args);
// Allocates: [control block][MyClass object] in one block

// Not preferred: constructor (two allocations)
std::shared_ptr&lt;MyClass&gt; ptr2(new MyClass(args));
// Allocates: [MyClass object] and separately [control block]
</code></pre>
<p><strong>Performance difference:</strong></p>
<ul>
<li><code>make_shared</code>: 1 allocation, better cache locality</li>
<li>Constructor: 2 allocations, extra overhead</li>
</ul>
<h4 id="circular-reference-problem"><a class="header" href="#circular-reference-problem">Circular Reference Problem</a></h4>
<pre><code class="language-cpp">struct Node {
    std::shared_ptr&lt;Node&gt; next;
    ~Node() { std::cout &lt;&lt; "Destructor called\n"; }
};

{
    std::shared_ptr&lt;Node&gt; node1 = std::make_shared&lt;Node&gt;();
    std::shared_ptr&lt;Node&gt; node2 = std::make_shared&lt;Node&gt;();

    node1-&gt;next = node2;  // node2 ref_count = 2
    node2-&gt;next = node1;  // node1 ref_count = 2
}
// Both go out of scope, but ref_count still &gt; 0!
// MEMORY LEAK! Destructors never called!
</code></pre>
<p><strong>Solution: Use weak_ptr (see next section)</strong></p>
<h4 id="thread-safety"><a class="header" href="#thread-safety">Thread-Safety</a></h4>
<pre><code class="language-cpp">// Reference count is thread-safe
std::shared_ptr&lt;int&gt; global_ptr = std::make_shared&lt;int&gt;(42);

void thread1() {
    std::shared_ptr&lt;int&gt; local = global_ptr;  // Thread-safe increment
}

void thread2() {
    std::shared_ptr&lt;int&gt; local = global_ptr;  // Thread-safe increment
}

// But the pointed-to object is NOT automatically thread-safe
void thread3() {
    *global_ptr = 100;  // Data race if thread4 runs concurrently!
}

void thread4() {
    *global_ptr = 200;  // Data race!
}
</code></pre>
<h4 id="custom-deleters-1"><a class="header" href="#custom-deleters-1">Custom Deleters</a></h4>
<pre><code class="language-cpp">std::shared_ptr&lt;FILE&gt; file(
    fopen("test.txt", "r"),
    [](FILE* f) { if (f) fclose(f); }
);

// Or with std::function
std::shared_ptr&lt;Connection&gt; conn(
    connect_to_server(),
    [](Connection* c) { disconnect(c); }
);
</code></pre>
<h4 id="aliasing-constructor"><a class="header" href="#aliasing-constructor">Aliasing Constructor</a></h4>
<pre><code class="language-cpp">struct Foo {
    int x;
    int y;
};

std::shared_ptr&lt;Foo&gt; foo = std::make_shared&lt;Foo&gt;();

// Aliasing: share ownership of foo, but point to foo-&gt;x
std::shared_ptr&lt;int&gt; x_ptr(foo, &amp;foo-&gt;x);

// x_ptr.use_count() == 2
// foo won't be deleted until both foo and x_ptr are destroyed
</code></pre>
<h4 id="use-cases-4"><a class="header" href="#use-cases-4">Use Cases</a></h4>
<pre><code class="language-cpp">// 1. Shared resources
class ResourceManager {
    std::shared_ptr&lt;Database&gt; db;
public:
    std::shared_ptr&lt;Database&gt; get_database() {
        return db;  // Share ownership
    }
};

// 2. Observer pattern
class Subject {
    std::vector&lt;std::shared_ptr&lt;Observer&gt;&gt; observers;
public:
    void attach(std::shared_ptr&lt;Observer&gt; obs) {
        observers.push_back(obs);
    }
};

// 3. Cache with shared ownership
class Cache {
    std::map&lt;std::string, std::shared_ptr&lt;Data&gt;&gt; cache;
public:
    std::shared_ptr&lt;Data&gt; get(const std::string&amp; key) {
        auto it = cache.find(key);
        if (it != cache.end()) {
            return it-&gt;second;  // Share cached data
        }
        return nullptr;
    }
};
</code></pre>
<h3 id="weak_ptr"><a class="header" href="#weak_ptr">weak_ptr</a></h3>
<p>Non-owning smart pointer that observes a shared_ptr without increasing ref count.</p>
<h4 id="basic-usage-4"><a class="header" href="#basic-usage-4">Basic Usage</a></h4>
<pre><code class="language-cpp">std::shared_ptr&lt;int&gt; sp = std::make_shared&lt;int&gt;(42);
std::weak_ptr&lt;int&gt; wp = sp;  // Doesn't increase ref count

std::cout &lt;&lt; sp.use_count() &lt;&lt; "\n";  // 1 (weak_ptr doesn't count)

// weak_ptr cannot access object directly
// *wp; // ERROR

// Must convert to shared_ptr first
if (std::shared_ptr&lt;int&gt; sp2 = wp.lock()) {
    // Object still alive
    std::cout &lt;&lt; *sp2 &lt;&lt; "\n";  // 42
    std::cout &lt;&lt; sp.use_count() &lt;&lt; "\n";  // 2
} else {
    // Object was deleted
    std::cout &lt;&lt; "Object expired\n";
}
</code></pre>
<h4 id="breaking-circular-references"><a class="header" href="#breaking-circular-references">Breaking Circular References</a></h4>
<pre><code class="language-cpp">struct Node {
    std::shared_ptr&lt;Node&gt; next;     // Strong reference
    std::weak_ptr&lt;Node&gt; prev;       // Weak reference (breaks cycle)

    ~Node() { std::cout &lt;&lt; "Destructor called\n"; }
};

{
    std::shared_ptr&lt;Node&gt; node1 = std::make_shared&lt;Node&gt;();
    std::shared_ptr&lt;Node&gt; node2 = std::make_shared&lt;Node&gt;();

    node1-&gt;next = node2;  // node2 ref_count = 2
    node2-&gt;prev = node1;  // node1 ref_count still 1 (weak_ptr doesn't count)
}
// node1 ref_count = 0 → deleted
// node2 ref_count = 1 → 0 → deleted
// Both destructors called! No leak!
</code></pre>
<h4 id="observer-pattern"><a class="header" href="#observer-pattern">Observer Pattern</a></h4>
<pre><code class="language-cpp">class Subject;

class Observer {
public:
    void notify(std::shared_ptr&lt;Subject&gt; subject) {
        std::cout &lt;&lt; "Notified\n";
    }
};

class Subject {
    std::vector&lt;std::weak_ptr&lt;Observer&gt;&gt; observers;

public:
    void attach(std::shared_ptr&lt;Observer&gt; obs) {
        observers.push_back(obs);
    }

    void notify_all() {
        for (auto&amp; weak_obs : observers) {
            if (std::shared_ptr&lt;Observer&gt; obs = weak_obs.lock()) {
                obs-&gt;notify(shared_from_this());
            }
        }
    }
};

// If observer is deleted, weak_ptr.lock() returns nullptr
// No dangling pointers!
</code></pre>
<h4 id="cache-with-weak-references"><a class="header" href="#cache-with-weak-references">Cache with Weak References</a></h4>
<pre><code class="language-cpp">class ImageCache {
    std::map&lt;std::string, std::weak_ptr&lt;Image&gt;&gt; cache;

public:
    std::shared_ptr&lt;Image&gt; load(const std::string&amp; filename) {
        // Check cache
        auto it = cache.find(filename);
        if (it != cache.end()) {
            if (std::shared_ptr&lt;Image&gt; img = it-&gt;second.lock()) {
                return img;  // Image still in memory
            }
        }

        // Load image
        std::shared_ptr&lt;Image&gt; img = std::make_shared&lt;Image&gt;(filename);
        cache[filename] = img;  // Store weak reference
        return img;
    }
};

// When all shared_ptrs to image are destroyed, image is deleted
// Cache automatically updated (weak_ptr expires)
</code></pre>
<h4 id="checking-expiration"><a class="header" href="#checking-expiration">Checking Expiration</a></h4>
<pre><code class="language-cpp">std::shared_ptr&lt;int&gt; sp = std::make_shared&lt;int&gt;(42);
std::weak_ptr&lt;int&gt; wp = sp;

std::cout &lt;&lt; wp.expired() &lt;&lt; "\n";  // false (object alive)
std::cout &lt;&lt; wp.use_count() &lt;&lt; "\n";  // 1

sp.reset();  // Delete object

std::cout &lt;&lt; wp.expired() &lt;&lt; "\n";  // true (object deleted)
std::cout &lt;&lt; wp.use_count() &lt;&lt; "\n";  // 0
</code></pre>
<h3 id="raii-pattern"><a class="header" href="#raii-pattern">RAII Pattern</a></h3>
<p>Resource Acquisition Is Initialization - tie resource lifetime to object lifetime.</p>
<h4 id="principle"><a class="header" href="#principle">Principle</a></h4>
<pre><code class="language-cpp">// RAII:
// 1. Acquire resource in constructor
// 2. Release resource in destructor
// 3. Resource lifetime tied to object lifetime

class FileHandle {
    FILE* file;

public:
    FileHandle(const char* filename, const char* mode)
        : file(fopen(filename, mode))
    {
        if (!file) {
            throw std::runtime_error("Failed to open file");
        }
    }

    ~FileHandle() {
        if (file) {
            fclose(file);
        }
    }

    // Prevent copying (file handle shouldn't be copied)
    FileHandle(const FileHandle&amp;) = delete;
    FileHandle&amp; operator=(const FileHandle&amp;) = delete;

    FILE* get() { return file; }
};

// Usage
void process_file() {
    FileHandle file("data.txt", "r");
    // Use file.get()...
    // Automatic cleanup even if exception thrown!
}
</code></pre>
<h4 id="lock-guard"><a class="header" href="#lock-guard">Lock Guard</a></h4>
<pre><code class="language-cpp">#include &lt;mutex&gt;

std::mutex mtx;
int shared_data = 0;

void bad_example() {
    mtx.lock();

    shared_data++;

    if (error_condition) {
        return;  // BUG: Forgot to unlock!
    }

    mtx.unlock();
}

void good_example() {
    std::lock_guard&lt;std::mutex&gt; lock(mtx);  // RAII

    shared_data++;

    if (error_condition) {
        return;  // Automatic unlock
    }

    // Automatic unlock
}
</code></pre>
<h4 id="resource-manager-examples"><a class="header" href="#resource-manager-examples">Resource Manager Examples</a></h4>
<p><strong>Socket:</strong></p>
<pre><code class="language-cpp">class SocketHandle {
    int sockfd;

public:
    SocketHandle(const char* host, int port) {
        sockfd = socket(AF_INET, SOCK_STREAM, 0);
        if (sockfd &lt; 0) throw std::runtime_error("Socket creation failed");

        // Connect...
    }

    ~SocketHandle() {
        if (sockfd &gt;= 0) {
            close(sockfd);
        }
    }

    int get() { return sockfd; }
};
</code></pre>
<p><strong>Database Connection:</strong></p>
<pre><code class="language-cpp">class DatabaseConnection {
    Connection* conn;

public:
    DatabaseConnection(const char* connstr) {
        conn = db_connect(connstr);
        if (!conn) throw std::runtime_error("Connection failed");
    }

    ~DatabaseConnection() {
        if (conn) {
            db_disconnect(conn);
        }
    }

    Connection* get() { return conn; }
};
</code></pre>
<p><strong>Memory Buffer:</strong></p>
<pre><code class="language-cpp">class Buffer {
    char* data;
    size_t size;

public:
    Buffer(size_t n) : size(n) {
        data = new char[n];
    }

    ~Buffer() {
        delete[] data;
    }

    char* get() { return data; }
    size_t length() { return size; }
};
</code></pre>
<h4 id="advantages-7"><a class="header" href="#advantages-7">Advantages</a></h4>
<ol>
<li><strong>Automatic cleanup</strong>: Resources always released</li>
<li><strong>Exception-safe</strong>: Cleanup happens even if exception thrown</li>
<li><strong>Clear ownership</strong>: Resource lifetime tied to scope</li>
<li><strong>No manual cleanup</strong>: Can’t forget to free</li>
</ol>
<h4 id="best-practices-1"><a class="header" href="#best-practices-1">Best Practices</a></h4>
<pre><code class="language-cpp">// 1. Acquire in constructor, release in destructor
// 2. Delete copy operations if resource shouldn't be copied
// 3. Use unique_ptr/shared_ptr for dynamic allocations
// 4. Custom deleters for non-memory resources

// Example: Combining RAII with smart pointers
class Resource {
public:
    Resource() { std::cout &lt;&lt; "Acquired\n"; }
    ~Resource() { std::cout &lt;&lt; "Released\n"; }
};

void function() {
    std::unique_ptr&lt;Resource&gt; res = std::make_unique&lt;Resource&gt;();
    // Use resource...
    // Automatic cleanup
}
</code></pre>
<hr>
<h2 id="language-specific-memory-management"><a class="header" href="#language-specific-memory-management">Language-Specific Memory Management</a></h2>
<h3 id="python"><a class="header" href="#python">Python</a></h3>
<h4 id="memory-model"><a class="header" href="#memory-model">Memory Model</a></h4>
<p>Python uses:</p>
<ol>
<li><strong>Reference Counting</strong>: Primary mechanism</li>
<li><strong>Cycle Detector</strong>: For circular references</li>
<li><strong>Memory Pools</strong>: For small objects</li>
</ol>
<h4 id="reference-counting-2"><a class="header" href="#reference-counting-2">Reference Counting</a></h4>
<pre><code class="language-python">import sys

# Create object (ref_count = 1)
a = [1, 2, 3]
print(sys.getrefcount(a))  # 2 (1 + temporary reference from getrefcount)

# Add reference
b = a
print(sys.getrefcount(a))  # 3

# Remove reference
del b
print(sys.getrefcount(a))  # 2

# Remove last reference → object deleted
del a
</code></pre>
<h4 id="memory-management-with-__del__"><a class="header" href="#memory-management-with-__del__">Memory Management with <code>__del__</code></a></h4>
<pre><code class="language-python">class Resource:
    def __init__(self, name):
        self.name = name
        print(f"Acquiring {name}")

    def __del__(self):
        print(f"Releasing {name}")

# Create object
r = Resource("File")  # "Acquiring File"

# Delete object
del r  # "Releasing File" (if no other references)

# Warning: __del__ timing is unpredictable with cycles
</code></pre>
<h4 id="garbage-collection-1"><a class="header" href="#garbage-collection-1">Garbage Collection</a></h4>
<pre><code class="language-python">import gc

# Get garbage collector stats
print(gc.get_count())  # (threshold0, threshold1, threshold2)

# Manual collection
gc.collect()  # Force collection, returns # of objects collected

# Disable/enable automatic collection
gc.disable()
# ... do work ...
gc.enable()

# Find uncollectable objects (usually due to __del__ in cycles)
gc.set_debug(gc.DEBUG_SAVEALL)
gc.collect()
print(gc.garbage)  # List of uncollectable objects
</code></pre>
<h4 id="circular-reference-example"><a class="header" href="#circular-reference-example">Circular Reference Example</a></h4>
<pre><code class="language-python">class Node:
    def __init__(self):
        self.ref = None

# Create cycle
node1 = Node()
node2 = Node()
node1.ref = node2
node2.ref = node1

# Delete external references
del node1
del node2

# Objects not immediately freed (circular reference)
# Cycle detector will eventually collect them

gc.collect()  # Force collection
</code></pre>
<h4 id="memory-optimization"><a class="header" href="#memory-optimization">Memory Optimization</a></h4>
<p><strong>1. <code>__slots__</code></strong> (reduce memory overhead):</p>
<pre><code class="language-python"># Without __slots__: each instance has a __dict__
class NormalClass:
    def __init__(self, x, y):
        self.x = x
        self.y = y

import sys
obj = NormalClass(1, 2)
print(sys.getsizeof(obj))  # e.g., 56 bytes

# With __slots__: no __dict__, fixed attributes
class OptimizedClass:
    __slots__ = ['x', 'y']

    def __init__(self, x, y):
        self.x = x
        self.y = y

obj2 = OptimizedClass(1, 2)
print(sys.getsizeof(obj2))  # e.g., 48 bytes

# obj2.z = 3  # AttributeError: no __dict__!
</code></pre>
<p><strong>Memory savings for many objects:</strong></p>
<pre><code class="language-python">import sys

# 1 million objects without __slots__
objects1 = [NormalClass(i, i*2) for i in range(1000000)]
size1 = sum(sys.getsizeof(obj) for obj in objects1)

# 1 million objects with __slots__
objects2 = [OptimizedClass(i, i*2) for i in range(1000000)]
size2 = sum(sys.getsizeof(obj) for obj in objects2)

print(f"Normal: {size1/1024/1024:.2f} MB")
print(f"Optimized: {size2/1024/1024:.2f} MB")
print(f"Savings: {(1 - size2/size1)*100:.1f}%")

# Typical result: 30-50% memory savings
</code></pre>
<p><strong>2. Interning</strong> (reuse immutable objects):</p>
<pre><code class="language-python"># Small integers (-5 to 256) are interned
a = 100
b = 100
print(a is b)  # True (same object)

a = 1000
b = 1000
print(a is b)  # False (different objects)

# String interning
s1 = "hello"
s2 = "hello"
print(s1 is s2)  # True (interned)

# Force interning
import sys
s3 = sys.intern("unique_string")
s4 = sys.intern("unique_string")
print(s3 is s4)  # True
</code></pre>
<h4 id="memory-profiling"><a class="header" href="#memory-profiling">Memory Profiling</a></h4>
<pre><code class="language-python">import tracemalloc

# Start tracing
tracemalloc.start()

# Allocate memory
data = [i for i in range(1000000)]

# Get memory usage
current, peak = tracemalloc.get_traced_memory()
print(f"Current: {current / 1024 / 1024:.2f} MB")
print(f"Peak: {peak / 1024 / 1024:.2f} MB")

# Get top memory allocations
snapshot = tracemalloc.take_snapshot()
top_stats = snapshot.statistics('lineno')

for stat in top_stats[:5]:
    print(stat)

tracemalloc.stop()
</code></pre>
<h3 id="javascript"><a class="header" href="#javascript">JavaScript</a></h3>
<h4 id="v8-memory-management"><a class="header" href="#v8-memory-management">V8 Memory Management</a></h4>
<p>JavaScript (V8 engine) uses generational garbage collection.</p>
<p><strong>Heap Structure:</strong></p>
<pre><code>New Space (Young Generation):
  - New objects allocated here
  - Small (1-8 MB)
  - Fast, frequent GC (Scavenge)

Old Space (Old Generation):
  - Objects that survived multiple GCs
  - Larger (hundreds of MB)
  - Slower, less frequent GC (Mark-Sweep-Compact)

Large Object Space:
  - Objects &gt; ~512 KB
  - Never moved
</code></pre>
<h4 id="memory-leaks-in-javascript"><a class="header" href="#memory-leaks-in-javascript">Memory Leaks in JavaScript</a></h4>
<p><strong>Leak 1: Global Variables</strong></p>
<pre><code class="language-javascript">// BAD: Creates global variable
function leak() {
    leakyVar = new Array(1000000);  // No var/let/const!
}

// GOOD: Use const/let
function noLeak() {
    const localVar = new Array(1000000);
}
</code></pre>
<p><strong>Leak 2: Event Listeners</strong></p>
<pre><code class="language-javascript">// BAD: Event listener prevents GC
function setupElement() {
    const bigData = new Array(1000000);
    const element = document.getElementById('button');

    element.addEventListener('click', function() {
        console.log(bigData.length);  // Closes over bigData
    });
}

// GOOD: Remove listener when done
function setupElementCorrectly() {
    const bigData = new Array(1000000);
    const element = document.getElementById('button');

    const handler = function() {
        console.log(bigData.length);
    };

    element.addEventListener('click', handler);

    // Later:
    element.removeEventListener('click', handler);
}

// BETTER: Use AbortController
function setupElementBest() {
    const bigData = new Array(1000000);
    const element = document.getElementById('button');
    const controller = new AbortController();

    element.addEventListener('click', function() {
        console.log(bigData.length);
    }, { signal: controller.signal });

    // Later:
    controller.abort();  // Removes all listeners
}
</code></pre>
<p><strong>Leak 3: Timers</strong></p>
<pre><code class="language-javascript">// BAD: setInterval keeps running
function startTimer() {
    const bigData = new Array(1000000);

    setInterval(() =&gt; {
        console.log(bigData.length);
    }, 1000);
}

// GOOD: Clear timer
function startTimerCorrectly() {
    const bigData = new Array(1000000);

    const timer = setInterval(() =&gt; {
        console.log(bigData.length);
    }, 1000);

    // Later:
    clearInterval(timer);
}
</code></pre>
<p><strong>Leak 4: Closures</strong></p>
<pre><code class="language-javascript">// BAD: Closures retain entire scope
function createClosure() {
    const bigData = new Array(1000000);
    const smallData = [1, 2, 3];

    return function() {
        return smallData.length;  // Only uses smallData
    };
    // But bigData is still retained!
}

// GOOD: Minimize closure scope
function createClosureCorrectly() {
    const smallData = [1, 2, 3];

    return function() {
        return smallData.length;
    };
    // bigData not in closure scope
}
</code></pre>
<h4 id="weakmap-and-weakref"><a class="header" href="#weakmap-and-weakref">WeakMap and WeakRef</a></h4>
<p><strong>WeakMap</strong> (weak references to keys):</p>
<pre><code class="language-javascript">const cache = new WeakMap();

let obj = { data: 'value' };
cache.set(obj, 'cached data');

console.log(cache.get(obj));  // 'cached data'

obj = null;  // Object can be GC'd
// cache entry automatically removed
</code></pre>
<p><strong>WeakRef</strong> (ES2021):</p>
<pre><code class="language-javascript">let obj = { data: 'value' };
const weakRef = new WeakRef(obj);

console.log(weakRef.deref());  // { data: 'value' }

obj = null;  // Object can be GC'd

// Later:
console.log(weakRef.deref());  // undefined (if GC'd)
</code></pre>
<h4 id="memory-profiling-chrome-devtools"><a class="header" href="#memory-profiling-chrome-devtools">Memory Profiling (Chrome DevTools)</a></h4>
<pre><code class="language-javascript">// 1. Take heap snapshot
// DevTools → Memory → Take snapshot

// 2. Compare snapshots
// Take snapshot before
const leak = [];
function allocate() {
    leak.push(new Array(1000000));
}

allocate();
// Take snapshot after

// 3. Allocation timeline
// DevTools → Memory → Allocation instrumentation on timeline

// 4. Force GC
// DevTools → Performance → Collect garbage
</code></pre>
<h3 id="go"><a class="header" href="#go">Go</a></h3>
<h4 id="garbage-collector"><a class="header" href="#garbage-collector">Garbage Collector</a></h4>
<p>Go uses concurrent mark-sweep GC with tri-color marking.</p>
<p><strong>Characteristics:</strong></p>
<ul>
<li>Concurrent: Runs alongside application</li>
<li>Low latency: Pause times &lt; 1 ms (typically)</li>
<li>Non-generational: Single heap (no young/old split)</li>
</ul>
<h4 id="memory-allocation"><a class="header" href="#memory-allocation">Memory Allocation</a></h4>
<pre><code class="language-go">// Stack allocation (automatic)
func stackAlloc() {
    x := 42              // On stack
    arr := [10]int{}     // On stack
}

// Heap allocation (escapes to heap)
func heapAlloc() *int {
    x := 42
    return &amp;x  // Escapes to heap
}

// Slice (heap allocation)
func sliceAlloc() {
    s := make([]int, 1000)  // On heap
    _ = s
}
</code></pre>
<p><strong>Escape Analysis:</strong></p>
<pre><code class="language-go">// Check what escapes to heap
// go build -gcflags='-m'

func example() {
    x := 42         // stack
    y := &amp;x         // x escapes to heap (address taken and returned)
    _ = y
}
</code></pre>
<h4 id="manual-gc-control"><a class="header" href="#manual-gc-control">Manual GC Control</a></h4>
<pre><code class="language-go">import "runtime"

func main() {
    // Force GC
    runtime.GC()

    // Set GC percentage (default: 100)
    // GOGC=50: GC when heap grows 50%
    // GOGC=200: GC when heap triples
    runtime.SetGCPercent(200)

    // Get memory stats
    var m runtime.MemStats
    runtime.ReadMemStats(&amp;m)

    fmt.Printf("Alloc: %d MB\n", m.Alloc / 1024 / 1024)
    fmt.Printf("TotalAlloc: %d MB\n", m.TotalAlloc / 1024 / 1024)
    fmt.Printf("Sys: %d MB\n", m.Sys / 1024 / 1024)
    fmt.Printf("NumGC: %d\n", m.NumGC)
}
</code></pre>
<h4 id="memory-optimization-1"><a class="header" href="#memory-optimization-1">Memory Optimization</a></h4>
<p><strong>1. Sync.Pool</strong> (object reuse):</p>
<pre><code class="language-go">var bufferPool = sync.Pool{
    New: func() interface{} {
        return new(bytes.Buffer)
    },
}

func processData(data []byte) {
    // Get buffer from pool
    buf := bufferPool.Get().(*bytes.Buffer)
    buf.Reset()

    // Use buffer
    buf.Write(data)
    processBuffer(buf)

    // Return to pool
    bufferPool.Put(buf)
}
</code></pre>
<p><strong>2. Avoid allocations</strong>:</p>
<pre><code class="language-go">// BAD: Allocates on every call
func bad(n int) []int {
    return make([]int, n)
}

// GOOD: Reuse buffer
type Processor struct {
    buffer []int
}

func (p *Processor) process(n int) []int {
    if cap(p.buffer) &lt; n {
        p.buffer = make([]int, n)
    }
    return p.buffer[:n]
}
</code></pre>
<p><strong>3. Preallocate slices</strong>:</p>
<pre><code class="language-go">// BAD: Many reallocations
func bad() []int {
    var result []int
    for i := 0; i &lt; 1000000; i++ {
        result = append(result, i)  // Reallocates many times
    }
    return result
}

// GOOD: Preallocate
func good() []int {
    result := make([]int, 0, 1000000)
    for i := 0; i &lt; 1000000; i++ {
        result = append(result, i)  // No reallocations
    }
    return result
}
</code></pre>
<h3 id="rust"><a class="header" href="#rust">Rust</a></h3>
<h4 id="ownership-system"><a class="header" href="#ownership-system">Ownership System</a></h4>
<p>Rust uses compile-time ownership tracking instead of garbage collection.</p>
<p><strong>Rules:</strong></p>
<ol>
<li>Each value has a single owner</li>
<li>When owner goes out of scope, value is dropped</li>
<li>Only one mutable reference OR multiple immutable references</li>
</ol>
<pre class="playground"><code class="language-rust">fn main() {
    let s = String::from("hello");  // s owns the string

    takes_ownership(s);  // s moved, no longer valid

    // println!("{}", s);  // ERROR: s was moved
}

fn takes_ownership(s: String) {
    println!("{}", s);
}  // s dropped here</code></pre>
<h4 id="borrowing"><a class="header" href="#borrowing">Borrowing</a></h4>
<pre class="playground"><code class="language-rust">fn main() {
    let s = String::from("hello");

    // Immutable borrow
    let len = calculate_length(&amp;s);  // Borrow, don't move

    println!("Length of '{}' is {}", s, len);  // s still valid
}

fn calculate_length(s: &amp;String) -&gt; usize {
    s.len()
}  // s goes out of scope, but doesn't drop (just a reference)</code></pre>
<p><strong>Mutable borrows:</strong></p>
<pre class="playground"><code class="language-rust">fn main() {
    let mut s = String::from("hello");

    change(&amp;mut s);

    println!("{}", s);  // "hello, world"
}

fn change(s: &amp;mut String) {
    s.push_str(", world");
}</code></pre>
<p><strong>Borrow rules enforced at compile time:</strong></p>
<pre class="playground"><code class="language-rust">fn main() {
    let mut s = String::from("hello");

    let r1 = &amp;s;
    let r2 = &amp;s;  // OK: multiple immutable borrows
    // let r3 = &amp;mut s;  // ERROR: can't borrow as mutable while immutable borrows exist

    println!("{} {}", r1, r2);
}</code></pre>
<h4 id="lifetimes"><a class="header" href="#lifetimes">Lifetimes</a></h4>
<pre class="playground"><code class="language-rust">// Lifetime annotations
fn longest&lt;'a&gt;(x: &amp;'a str, y: &amp;'a str) -&gt; &amp;'a str {
    if x.len() &gt; y.len() {
        x
    } else {
        y
    }
}

fn main() {
    let string1 = String::from("long string");
    let string2 = String::from("short");

    let result = longest(&amp;string1, &amp;string2);
    println!("Longest: {}", result);
}

// Compiler ensures returned reference doesn't outlive inputs</code></pre>
<h4 id="smart-pointers"><a class="header" href="#smart-pointers">Smart Pointers</a></h4>
<p><strong>Box</strong> (heap allocation):</p>
<pre class="playground"><code class="language-rust">fn main() {
    let b = Box::new(5);  // Allocate on heap
    println!("b = {}", b);
}  // b dropped, heap memory freed</code></pre>
<p><strong>Rc</strong> (reference counting):</p>
<pre class="playground"><code class="language-rust">use std::rc::Rc;

fn main() {
    let a = Rc::new(5);           // ref_count = 1
    let b = Rc::clone(&amp;a);        // ref_count = 2
    let c = Rc::clone(&amp;a);        // ref_count = 3

    println!("count: {}", Rc::strong_count(&amp;a));  // 3
}  // All dropped, memory freed when count reaches 0</code></pre>
<p><strong>RefCell</strong> (interior mutability):</p>
<pre class="playground"><code class="language-rust">use std::cell::RefCell;

fn main() {
    let value = RefCell::new(5);

    *value.borrow_mut() = 10;  // Runtime borrow checking

    println!("{}", value.borrow());
}</code></pre>
<h4 id="zero-cost-abstractions"><a class="header" href="#zero-cost-abstractions">Zero-Cost Abstractions</a></h4>
<pre class="playground"><code class="language-rust">// No runtime overhead!
fn main() {
    let v = vec![1, 2, 3];

    // Iterator: compiled to same code as manual loop
    let sum: i32 = v.iter().map(|x| x * 2).sum();

    println!("{}", sum);
}

// Equivalent to:
fn manual() {
    let v = vec![1, 2, 3];
    let mut sum = 0;
    for x in &amp;v {
        sum += x * 2;
    }
    println!("{}", sum);
}

// Both compile to identical assembly!</code></pre>
<h3 id="java"><a class="header" href="#java">Java</a></h3>
<h4 id="heap-structure"><a class="header" href="#heap-structure">Heap Structure</a></h4>
<pre><code>Heap:
+---------------------------+
| Young Generation          |
|  - Eden Space             |
|  - Survivor Space 0       |
|  - Survivor Space 1       |
+---------------------------+
| Old Generation (Tenured)  |
+---------------------------+
| Metaspace (JDK 8+)        |
| (Class metadata)          |
+---------------------------+
</code></pre>
<h4 id="object-lifecycle"><a class="header" href="#object-lifecycle">Object Lifecycle</a></h4>
<pre><code class="language-java">public class ObjectLifecycle {
    public static void main(String[] args) {
        // 1. Allocation in Eden space
        MyObject obj = new MyObject();  // Allocated in Eden

        // 2. Minor GC moves survivors to Survivor space
        // (happens automatically when Eden fills)

        // 3. After several GCs, promoted to Old Generation

        // 4. When obj = null, object becomes eligible for GC
        obj = null;

        // 5. Major GC (when old gen fills) reclaims object
    }
}
</code></pre>
<h4 id="garbage-collectors"><a class="header" href="#garbage-collectors">Garbage Collectors</a></h4>
<p><strong>1. Serial GC</strong> (single-threaded):</p>
<pre><code class="language-bash">java -XX:+UseSerialGC MyApp
# Good for: Small heaps, single-CPU systems
</code></pre>
<p><strong>2. Parallel GC</strong> (multi-threaded):</p>
<pre><code class="language-bash">java -XX:+UseParallelGC MyApp
# Good for: High throughput, batch processing
</code></pre>
<p><strong>3. CMS (Concurrent Mark Sweep)</strong>:</p>
<pre><code class="language-bash">java -XX:+UseConcMarkSweepGC MyApp
# Good for: Low latency (deprecated in JDK 9)
</code></pre>
<p><strong>4. G1 (Garbage First)</strong>:</p>
<pre><code class="language-bash">java -XX:+UseG1GC -XX:MaxGCPauseMillis=200 MyApp
# Good for: Balanced throughput/latency, large heaps
</code></pre>
<p><strong>5. ZGC</strong> (ultra-low latency):</p>
<pre><code class="language-bash">java -XX:+UseZGC MyApp
# Good for: Sub-10ms pauses, very large heaps (TB+)
</code></pre>
<p><strong>6. Shenandoah</strong>:</p>
<pre><code class="language-bash">java -XX:+UseShenandoahGC MyApp
# Good for: Low latency, concurrent compaction
</code></pre>
<h4 id="memory-tuning"><a class="header" href="#memory-tuning">Memory Tuning</a></h4>
<pre><code class="language-bash"># Heap size
java -Xms2g -Xmx4g MyApp  # Initial 2GB, max 4GB

# Young generation size
java -Xmn1g MyApp  # 1GB young gen

# Metaspace size (class metadata)
java -XX:MetaspaceSize=256m -XX:MaxMetaspaceSize=512m MyApp

# GC logging
java -Xlog:gc*:file=gc.log:time,uptime:filecount=5,filesize=100m MyApp
</code></pre>
<h4 id="weakreference-softreference-phantomreference"><a class="header" href="#weakreference-softreference-phantomreference">WeakReference, SoftReference, PhantomReference</a></h4>
<pre><code class="language-java">import java.lang.ref.*;

public class References {
    public static void main(String[] args) {
        Object obj = new Object();

        // Strong reference: never GC'd while reachable
        Object strong = obj;

        // Weak reference: GC'd even if memory available
        WeakReference&lt;Object&gt; weak = new WeakReference&lt;&gt;(obj);
        System.out.println(weak.get());  // Returns object
        obj = null;
        System.gc();
        System.out.println(weak.get());  // null (GC'd)

        // Soft reference: GC'd only when memory low
        obj = new Object();
        SoftReference&lt;Object&gt; soft = new SoftReference&lt;&gt;(obj);
        obj = null;
        // soft.get() returns object until memory pressure

        // Phantom reference: for cleanup actions
        obj = new Object();
        ReferenceQueue&lt;Object&gt; queue = new ReferenceQueue&lt;&gt;();
        PhantomReference&lt;Object&gt; phantom = new PhantomReference&lt;&gt;(obj, queue);
        obj = null;
        System.gc();
        // phantom.get() always returns null
        // Used for post-finalization cleanup
    }
}
</code></pre>
<hr>
<h2 id="memory-profiling-1"><a class="header" href="#memory-profiling-1">Memory Profiling</a></h2>
<h3 id="profiling-tools"><a class="header" href="#profiling-tools">Profiling Tools</a></h3>
<h4 id="valgrind-linux-1"><a class="header" href="#valgrind-linux-1">Valgrind (Linux)</a></h4>
<p><strong>Memcheck</strong> (memory error detector):</p>
<pre><code class="language-bash">valgrind --tool=memcheck --leak-check=full --show-leak-kinds=all ./myapp
</code></pre>
<p>Detects:</p>
<ul>
<li>Memory leaks</li>
<li>Use-after-free</li>
<li>Double-free</li>
<li>Invalid reads/writes</li>
<li>Uninitialized memory usage</li>
</ul>
<p><strong>Massif</strong> (heap profiler):</p>
<pre><code class="language-bash">valgrind --tool=massif ./myapp
ms_print massif.out.&lt;pid&gt;
</code></pre>
<p>Output:</p>
<pre><code>KB
1.000^
     |
     |
     |                           @@@@@@@@
     |                      @@@@@        @@@@@
     |                 @@@@@                  @@@@@
     |            @@@@@                            @@@@@
     |       @@@@@                                      @@@@@
     |  @@@@@                                                @@@@@
0    +-----------------------------------------------------------------------&gt;
     0                                                                   100 s
</code></pre>
<p><strong>Cachegrind</strong> (cache profiler):</p>
<pre><code class="language-bash">valgrind --tool=cachegrind ./myapp
cg_annotate cachegrind.out.&lt;pid&gt;
</code></pre>
<h4 id="heaptrack-linux"><a class="header" href="#heaptrack-linux">Heaptrack (Linux)</a></h4>
<p>Modern heap profiler with GUI:</p>
<pre><code class="language-bash">heaptrack ./myapp
heaptrack_gui heaptrack.myapp.&lt;pid&gt;.gz
</code></pre>
<p>Shows:</p>
<ul>
<li>Allocation flamegraphs</li>
<li>Memory timeline</li>
<li>Top allocators</li>
<li>Leak detection</li>
</ul>
<h4 id="instruments-macos"><a class="header" href="#instruments-macos">Instruments (macOS)</a></h4>
<p>Xcode profiling tool:</p>
<pre><code class="language-bash"># Launch Instruments
instruments -t Leaks ./myapp

# Or from Xcode: Product → Profile (⌘I)
</code></pre>
<p>Templates:</p>
<ul>
<li><strong>Leaks</strong>: Detect memory leaks</li>
<li><strong>Allocations</strong>: Track all allocations</li>
<li><strong>VM Tracker</strong>: Virtual memory usage</li>
</ul>
<h4 id="windows-memory-diagnostic"><a class="header" href="#windows-memory-diagnostic">Windows Memory Diagnostic</a></h4>
<p><strong>Visual Studio Diagnostic Tools:</strong></p>
<ul>
<li>Debug → Windows → Show Diagnostic Tools</li>
<li>Shows memory usage timeline</li>
<li>Snapshot heap for analysis</li>
</ul>
<p><strong>Performance Profiler:</strong></p>
<ul>
<li>Debug → Performance Profiler</li>
<li>Select “.NET Object Allocation Tracking”</li>
<li>Analyze allocation flamegraphs</li>
</ul>
<h3 id="memory-leak-detection-tools"><a class="header" href="#memory-leak-detection-tools">Memory Leak Detection Tools</a></h3>
<h4 id="leaksanitizer"><a class="header" href="#leaksanitizer">LeakSanitizer</a></h4>
<pre><code class="language-bash"># Standalone
gcc -fsanitize=leak -g -o myapp myapp.c
./myapp

# Or included with AddressSanitizer
gcc -fsanitize=address -g -o myapp myapp.c
./myapp
</code></pre>
<p>Example output:</p>
<pre><code>=================================================================
==12345==ERROR: LeakSanitizer: detected memory leaks

Direct leak of 100 byte(s) in 1 object(s) allocated from:
    #0 0x7f8b7cc63537 in malloc
    #1 0x400b95 in main myapp.c:10

SUMMARY: LeakSanitizer: 100 byte(s) leaked in 1 allocation(s).
</code></pre>
<h4 id="mtrace-glibc"><a class="header" href="#mtrace-glibc">mtrace (glibc)</a></h4>
<p>GNU C library’s malloc tracer:</p>
<pre><code class="language-c">#include &lt;mcheck.h&gt;

int main() {
    mtrace();  // Start tracing

    char* leak = malloc(100);
    // Forgot to free!

    muntrace();  // Stop tracing
    return 0;
}
</code></pre>
<pre><code class="language-bash">gcc -g -o myapp myapp.c
export MALLOC_TRACE=mtrace.log
./myapp
mtrace myapp mtrace.log
</code></pre>
<p>Output:</p>
<pre><code>Memory not freed:
-----------------
   Address     Size     Caller
0x55e4d789ef00   0x64  at /path/to/myapp.c:10
</code></pre>
<h4 id="python-memory_profiler"><a class="header" href="#python-memory_profiler">Python memory_profiler</a></h4>
<pre><code class="language-python">from memory_profiler import profile

@profile
def my_function():
    a = [1] * (10 ** 6)
    b = [2] * (2 * 10 ** 7)
    del b
    return a

if __name__ == '__main__':
    my_function()
</code></pre>
<pre><code class="language-bash">python -m memory_profiler myapp.py
</code></pre>
<p>Output:</p>
<pre><code>Line #    Mem usage    Increment  Line Contents
================================================
     3   38.816 MiB   38.816 MiB  @profile
     4                             def my_function():
     5   46.492 MiB    7.676 MiB      a = [1] * (10 ** 6)
     6  199.344 MiB  152.852 MiB      b = [2] * (2 * 10 ** 7)
     7   46.492 MiB -152.852 MiB      del b
     8   46.492 MiB    0.000 MiB      return a
</code></pre>
<h3 id="heap-profiling"><a class="header" href="#heap-profiling">Heap Profiling</a></h3>
<h4 id="jemalloc-profiling"><a class="header" href="#jemalloc-profiling">jemalloc Profiling</a></h4>
<pre><code class="language-bash"># Compile with jemalloc
gcc -o myapp myapp.c -ljemalloc

# Enable profiling
export MALLOC_CONF=prof:true,prof_prefix:jeprof.out
./myapp

# Analyze profile
jeprof --pdf myapp jeprof.out.&lt;pid&gt;.heap &gt; profile.pdf
</code></pre>
<h4 id="gperftools-google-performance-tools"><a class="header" href="#gperftools-google-performance-tools">gperftools (Google Performance Tools)</a></h4>
<pre><code class="language-c">#include &lt;gperftools/heap-profiler.h&gt;

int main() {
    HeapProfilerStart("myapp");

    // Your code here
    for (int i = 0; i &lt; 1000000; i++) {
        char* ptr = malloc(100);
        // ...
    }

    HeapProfilerStop();
    return 0;
}
</code></pre>
<pre><code class="language-bash">gcc -o myapp myapp.c -ltcmalloc
./myapp
pprof --pdf myapp myapp.0001.heap &gt; heap_profile.pdf
</code></pre>
<h4 id="java-flight-recorder-jfr"><a class="header" href="#java-flight-recorder-jfr">Java Flight Recorder (JFR)</a></h4>
<pre><code class="language-bash"># Start recording
java -XX:StartFlightRecording=duration=60s,filename=recording.jfr MyApp

# Or attach to running process
jcmd &lt;pid&gt; JFR.start duration=60s filename=recording.jfr

# Analyze with JDK Mission Control
jmc recording.jfr
</code></pre>
<h4 id="chrome-devtools-javascript"><a class="header" href="#chrome-devtools-javascript">Chrome DevTools (JavaScript)</a></h4>
<pre><code class="language-javascript">// Heap snapshot
// DevTools → Memory → Take snapshot

// Example: Find detached DOM nodes
function createLeak() {
    const div = document.createElement('div');
    div.innerHTML = '&lt;p&gt;Content&lt;/p&gt;';

    window.leakedNode = div;  // Prevents GC
}

// 1. Take snapshot
// 2. Run createLeak()
// 3. Take snapshot
// 4. Compare snapshots → find "Detached DOM tree"
</code></pre>
<hr>
<h2 id="performance-optimization"><a class="header" href="#performance-optimization">Performance Optimization</a></h2>
<h3 id="cache-friendly-data-structures"><a class="header" href="#cache-friendly-data-structures">Cache-Friendly Data Structures</a></h3>
<h4 id="cache-hierarchy"><a class="header" href="#cache-hierarchy">Cache Hierarchy</a></h4>
<pre><code>CPU Registers: ~1 cycle (~0.3 ns)
L1 Cache: ~4 cycles (~1 ns), 32-64 KB per core
L2 Cache: ~12 cycles (~3 ns), 256-512 KB per core
L3 Cache: ~40 cycles (~10 ns), 8-64 MB shared
RAM: ~200 cycles (~60 ns), GB+
</code></pre>
<h4 id="cache-lines"><a class="header" href="#cache-lines">Cache Lines</a></h4>
<p>Modern CPUs fetch memory in cache lines (typically 64 bytes).</p>
<pre><code class="language-c">// BAD: False sharing
struct {
    int counter1;  // Offset 0
    int counter2;  // Offset 4
} shared;

// Thread 1
shared.counter1++;  // Invalidates entire cache line

// Thread 2
shared.counter2++;  // Must reload cache line (slow!)
</code></pre>
<pre><code class="language-c">// GOOD: Padding to separate cache lines
struct {
    int counter1;
    char padding[60];  // Pad to 64 bytes
    int counter2;
} shared;

// Thread 1 and 2 now use different cache lines
</code></pre>
<h4 id="array-of-structures-vs-structure-of-arrays"><a class="header" href="#array-of-structures-vs-structure-of-arrays">Array of Structures vs Structure of Arrays</a></h4>
<p><strong>Array of Structures (AoS):</strong></p>
<pre><code class="language-c">struct Particle {
    float x, y, z;
    float vx, vy, vz;
    float mass;
};

struct Particle particles[1000];

// Update positions
for (int i = 0; i &lt; 1000; i++) {
    particles[i].x += particles[i].vx;
    particles[i].y += particles[i].vy;
    particles[i].z += particles[i].vz;
}

// Cache-unfriendly: loads entire struct, wastes bandwidth
</code></pre>
<p><strong>Structure of Arrays (SoA):</strong></p>
<pre><code class="language-c">struct Particles {
    float x[1000];
    float y[1000];
    float z[1000];
    float vx[1000];
    float vy[1000];
    float vz[1000];
    float mass[1000];
};

struct Particles particles;

// Update positions
for (int i = 0; i &lt; 1000; i++) {
    particles.x[i] += particles.vx[i];
    particles.y[i] += particles.vy[i];
    particles.z[i] += particles.vz[i];
}

// Cache-friendly: sequential access, full cache line utilization
</code></pre>
<p><strong>Performance Comparison:</strong></p>
<pre><code>AoS: ~100 ms (many cache misses)
SoA: ~20 ms (few cache misses)
Speedup: 5x!
</code></pre>
<h4 id="prefetching"><a class="header" href="#prefetching">Prefetching</a></h4>
<pre><code class="language-c">// Manual prefetching
#include &lt;xmmintrin.h&gt;

void process_array(int* arr, size_t n) {
    for (size_t i = 0; i &lt; n; i++) {
        // Prefetch next iteration
        if (i + 8 &lt; n) {
            _mm_prefetch(&amp;arr[i + 8], _MM_HINT_T0);
        }

        // Process current
        arr[i] = arr[i] * 2 + 1;
    }
}
</code></pre>
<h3 id="memory-access-patterns"><a class="header" href="#memory-access-patterns">Memory Access Patterns</a></h3>
<h4 id="sequential-vs-random-access"><a class="header" href="#sequential-vs-random-access">Sequential vs Random Access</a></h4>
<pre><code class="language-c">#define SIZE (1024 * 1024 * 100)  // 100M ints

int* arr = malloc(SIZE * sizeof(int));

// Sequential access (cache-friendly)
clock_t start = clock();
for (int i = 0; i &lt; SIZE; i++) {
    arr[i] = i;
}
double seq_time = (double)(clock() - start) / CLOCKS_PER_SEC;

// Random access (cache-unfriendly)
start = clock();
for (int i = 0; i &lt; SIZE; i++) {
    int index = rand() % SIZE;
    arr[index] = i;
}
double rand_time = (double)(clock() - start) / CLOCKS_PER_SEC;

printf("Sequential: %.3f s\n", seq_time);
printf("Random: %.3f s\n", rand_time);
printf("Ratio: %.2fx\n", rand_time / seq_time);

// Typical result: Random is 10-50x slower!
</code></pre>
<h4 id="loop-tiling-blocking"><a class="header" href="#loop-tiling-blocking">Loop Tiling (Blocking)</a></h4>
<p>Improve cache locality by processing data in blocks:</p>
<pre><code class="language-c">// Matrix multiplication: Naive (cache-unfriendly)
void matmul_naive(double* A, double* B, double* C, int N) {
    for (int i = 0; i &lt; N; i++) {
        for (int j = 0; j &lt; N; j++) {
            double sum = 0;
            for (int k = 0; k &lt; N; k++) {
                sum += A[i*N + k] * B[k*N + j];
                // B accessed with stride N (cache miss!)
            }
            C[i*N + j] = sum;
        }
    }
}

// Matrix multiplication: Tiled (cache-friendly)
#define BLOCK_SIZE 32

void matmul_tiled(double* A, double* B, double* C, int N) {
    for (int i = 0; i &lt; N; i += BLOCK_SIZE) {
        for (int j = 0; j &lt; N; j += BLOCK_SIZE) {
            for (int k = 0; k &lt; N; k += BLOCK_SIZE) {
                // Process BLOCK_SIZE x BLOCK_SIZE sub-matrix
                for (int ii = i; ii &lt; i + BLOCK_SIZE &amp;&amp; ii &lt; N; ii++) {
                    for (int jj = j; jj &lt; j + BLOCK_SIZE &amp;&amp; jj &lt; N; jj++) {
                        double sum = C[ii*N + jj];
                        for (int kk = k; kk &lt; k + BLOCK_SIZE &amp;&amp; kk &lt; N; kk++) {
                            sum += A[ii*N + kk] * B[kk*N + jj];
                        }
                        C[ii*N + jj] = sum;
                    }
                }
            }
        }
    }
}

// Performance (N=1024):
// Naive: 10.5 seconds
// Tiled: 1.2 seconds
// Speedup: 8.75x!
</code></pre>
<h3 id="copy-on-write"><a class="header" href="#copy-on-write">Copy-on-Write</a></h3>
<p>Share memory until modification, then copy.</p>
<h4 id="fork-example-unix"><a class="header" href="#fork-example-unix">Fork Example (Unix)</a></h4>
<pre><code class="language-c">#include &lt;unistd.h&gt;
#include &lt;sys/wait.h&gt;

int main() {
    char* data = malloc(1000000000);  // 1 GB
    memset(data, 0, 1000000000);

    pid_t pid = fork();

    if (pid == 0) {
        // Child process
        // Shares parent's memory (COW)
        sleep(1);

        // Write triggers COW (copy page)
        data[0] = 42;

        exit(0);
    } else {
        // Parent process
        wait(NULL);
    }

    free(data);
    return 0;
}

// fork() is instant (doesn't copy 1 GB)
// Only modified pages are copied
</code></pre>
<h4 id="string-implementation"><a class="header" href="#string-implementation">String Implementation</a></h4>
<pre><code class="language-cpp">class CowString {
    struct Data {
        char* str;
        size_t len;
        std::atomic&lt;int&gt; ref_count;
    };

    Data* data;

    void detach() {
        if (data-&gt;ref_count &gt; 1) {
            // Copy string (COW)
            Data* new_data = new Data{
                new char[data-&gt;len + 1],
                data-&gt;len,
                1
            };
            memcpy(new_data-&gt;str, data-&gt;str, data-&gt;len + 1);

            data-&gt;ref_count--;
            data = new_data;
        }
    }

public:
    CowString(const char* s) {
        data = new Data{
            new char[strlen(s) + 1],
            strlen(s),
            1
        };
        strcpy(data-&gt;str, s);
    }

    // Copy constructor (shares data)
    CowString(const CowString&amp; other) : data(other.data) {
        data-&gt;ref_count++;
    }

    // Modify: triggers COW
    void set_char(size_t i, char c) {
        detach();  // Copy if shared
        data-&gt;str[i] = c;
    }

    // Read: no COW
    char get_char(size_t i) const {
        return data-&gt;str[i];
    }

    ~CowString() {
        if (--data-&gt;ref_count == 0) {
            delete[] data-&gt;str;
            delete data;
        }
    }
};
</code></pre>
<h3 id="memory-mapped-files"><a class="header" href="#memory-mapped-files">Memory-Mapped Files</a></h3>
<p>Map files directly into virtual memory.</p>
<h4 id="basic-usage-posix"><a class="header" href="#basic-usage-posix">Basic Usage (POSIX)</a></h4>
<pre><code class="language-c">#include &lt;sys/mman.h&gt;
#include &lt;fcntl.h&gt;
#include &lt;unistd.h&gt;

int main() {
    // Open file
    int fd = open("data.bin", O_RDWR);
    if (fd &lt; 0) {
        perror("open");
        return 1;
    }

    // Get file size
    struct stat sb;
    fstat(fd, &amp;sb);
    size_t size = sb.st_size;

    // Memory-map file
    char* data = mmap(NULL, size, PROT_READ | PROT_WRITE, MAP_SHARED, fd, 0);
    if (data == MAP_FAILED) {
        perror("mmap");
        close(fd);
        return 1;
    }

    // Access file as memory
    data[0] = 'H';
    data[1] = 'i';

    // Changes written back to file (eventually)
    msync(data, size, MS_SYNC);  // Force write

    // Unmap
    munmap(data, size);
    close(fd);

    return 0;
}
</code></pre>
<h4 id="advantages-8"><a class="header" href="#advantages-8">Advantages</a></h4>
<ol>
<li><strong>Lazy loading</strong>: Pages loaded on demand</li>
<li><strong>Shared memory</strong>: Multiple processes can map same file</li>
<li><strong>No explicit I/O</strong>: OS handles reads/writes</li>
<li><strong>Large files</strong>: Don’t need to fit in RAM</li>
</ol>
<h4 id="example-large-file-processing"><a class="header" href="#example-large-file-processing">Example: Large File Processing</a></h4>
<pre><code class="language-c">void process_large_file(const char* filename) {
    int fd = open(filename, O_RDONLY);
    struct stat sb;
    fstat(fd, &amp;sb);

    size_t size = sb.st_size;
    char* data = mmap(NULL, size, PROT_READ, MAP_PRIVATE, fd, 0);

    // Process file in chunks
    size_t chunk_size = 1024 * 1024;  // 1 MB
    for (size_t offset = 0; offset &lt; size; offset += chunk_size) {
        size_t len = (offset + chunk_size &lt; size) ? chunk_size : (size - offset);
        process_chunk(data + offset, len);
    }

    munmap(data, size);
    close(fd);
}

// OS loads only accessed pages (efficient!)
</code></pre>
<h4 id="example-shared-memory-ipc"><a class="header" href="#example-shared-memory-ipc">Example: Shared Memory IPC</a></h4>
<pre><code class="language-c">// Process 1: Create shared memory
int fd = shm_open("/my_shm", O_CREAT | O_RDWR, 0666);
ftruncate(fd, 4096);

int* shared = mmap(NULL, 4096, PROT_READ | PROT_WRITE, MAP_SHARED, fd, 0);
*shared = 42;

// Process 2: Attach to shared memory
int fd = shm_open("/my_shm", O_RDWR, 0666);
int* shared = mmap(NULL, 4096, PROT_READ | PROT_WRITE, MAP_SHARED, fd, 0);

printf("Value: %d\n", *shared);  // 42
</code></pre>
<hr>
<h2 id="common-pitfalls-and-best-practices"><a class="header" href="#common-pitfalls-and-best-practices">Common Pitfalls and Best Practices</a></h2>
<h3 id="common-pitfalls"><a class="header" href="#common-pitfalls">Common Pitfalls</a></h3>
<p><strong>1. Memory Leaks</strong></p>
<pre><code class="language-c">// BAD
char* get_string() {
    char* str = malloc(100);
    strcpy(str, "Hello");
    return str;  // Caller must remember to free!
}

// GOOD: Document ownership
char* get_string() {
    // Caller owns returned pointer and must free it
    char* str = malloc(100);
    strcpy(str, "Hello");
    return str;
}

// BETTER: Use output parameter
void get_string(char* buffer, size_t size) {
    strncpy(buffer, "Hello", size - 1);
    buffer[size - 1] = '\0';
}
</code></pre>
<p><strong>2. Dangling Pointers</strong></p>
<pre><code class="language-c">// BAD
int* get_local() {
    int x = 42;
    return &amp;x;  // Returns address of stack variable!
}

// GOOD
int* get_heap() {
    int* x = malloc(sizeof(int));
    *x = 42;
    return x;
}
</code></pre>
<p><strong>3. Buffer Overflows</strong></p>
<pre><code class="language-c">// BAD
char buffer[10];
strcpy(buffer, user_input);  // What if user_input is longer?

// GOOD
char buffer[10];
strncpy(buffer, user_input, sizeof(buffer) - 1);
buffer[sizeof(buffer) - 1] = '\0';

// BETTER (C11)
strncpy_s(buffer, sizeof(buffer), user_input, _TRUNCATE);
</code></pre>
<p><strong>4. Uninitialized Memory</strong></p>
<pre><code class="language-c">// BAD
int* arr = malloc(10 * sizeof(int));
printf("%d\n", arr[0]);  // Undefined value!

// GOOD
int* arr = calloc(10, sizeof(int));  // Zero-initialized
printf("%d\n", arr[0]);  // 0
</code></pre>
<p><strong>5. Memory Alignment Issues</strong></p>
<pre><code class="language-c">// BAD (may crash on some architectures)
char buffer[100];
int* ptr = (int*)&amp;buffer[1];  // Unaligned!
*ptr = 42;  // May crash or be slow

// GOOD
int* ptr = (int*)&amp;buffer[0];  // Aligned to int boundary
*ptr = 42;
</code></pre>
<h3 id="best-practices-2"><a class="header" href="#best-practices-2">Best Practices</a></h3>
<p><strong>1. Ownership Clarity</strong></p>
<pre><code class="language-cpp">// Clear ownership with unique_ptr
std::unique_ptr&lt;Resource&gt; create_resource() {
    return std::make_unique&lt;Resource&gt;();
}

void use_resource() {
    auto res = create_resource();  // Ownership transferred
    // Use res...
    // Automatic cleanup
}
</code></pre>
<p><strong>2. RAII Pattern</strong></p>
<pre><code class="language-cpp">class FileHandle {
    FILE* file;
public:
    FileHandle(const char* name, const char* mode)
        : file(fopen(name, mode))
    {
        if (!file) throw std::runtime_error("Failed to open file");
    }

    ~FileHandle() {
        if (file) fclose(file);
    }

    FILE* get() { return file; }
};

// Usage
void process_file() {
    FileHandle file("data.txt", "r");
    // Use file.get()...
    // Automatic cleanup even if exception thrown
}
</code></pre>
<p><strong>3. Bounds Checking</strong></p>
<pre><code class="language-c">void safe_copy(char* dest, size_t dest_size, const char* src) {
    if (strlen(src) &gt;= dest_size) {
        // Handle error
        return;
    }
    strcpy(dest, src);
}
</code></pre>
<p><strong>4. Null Pointer Checks</strong></p>
<pre><code class="language-c">void process(int* ptr) {
    if (!ptr) {
        // Handle null pointer
        return;
    }

    *ptr = 42;
}
</code></pre>
<p><strong>5. Use Static Analysis</strong></p>
<pre><code class="language-bash"># Clang static analyzer
scan-build gcc -o myapp myapp.c

# Cppcheck
cppcheck --enable=all myapp.c

# Valgrind
valgrind --leak-check=full ./myapp
</code></pre>
<p><strong>6. Memory Profiling in Development</strong></p>
<pre><code class="language-bash"># Compile with sanitizers during development
gcc -fsanitize=address -fsanitize=undefined -g -o myapp myapp.c

# Run tests
./myapp
</code></pre>
<p><strong>7. Documentation</strong></p>
<pre><code class="language-c">/**
 * Creates a new string.
 * @return Newly allocated string. Caller must free with free().
 */
char* create_string(const char* src);

/**
 * Processes data in buffer.
 * @param buffer Buffer to process (not owned, not modified).
 */
void process_data(const char* buffer);

/**
 * Takes ownership of resource.
 * @param resource Resource to take ownership of. Will be freed.
 */
void take_resource(Resource* resource);
</code></pre>
<p><strong>8. Defensive Programming</strong></p>
<pre><code class="language-c">void safe_free(void** ptr) {
    if (ptr &amp;&amp; *ptr) {
        free(*ptr);
        *ptr = NULL;
    }
}

// Usage
char* str = malloc(100);
safe_free((void**)&amp;str);
safe_free((void**)&amp;str);  // Safe to call twice
</code></pre>
<p><strong>9. Memory Budgets</strong></p>
<pre><code class="language-c">#define MAX_MEMORY_MB 100

static size_t allocated_memory = 0;

void* tracked_malloc(size_t size) {
    if (allocated_memory + size &gt; MAX_MEMORY_MB * 1024 * 1024) {
        fprintf(stderr, "Memory budget exceeded\n");
        return NULL;
    }

    void* ptr = malloc(size);
    if (ptr) {
        allocated_memory += size;
    }
    return ptr;
}

void tracked_free(void* ptr, size_t size) {
    if (ptr) {
        free(ptr);
        allocated_memory -= size;
    }
}
</code></pre>
<p><strong>10. Testing for Leaks</strong></p>
<pre><code class="language-bash">#!/bin/bash
# run_tests.sh

# Compile with sanitizers
gcc -fsanitize=address -g -o test test.c

# Run tests
./test

# Check exit code
if [ $? -ne 0 ]; then
    echo "Tests failed or memory errors detected"
    exit 1
fi

echo "All tests passed"
</code></pre>
<hr>
<h2 id="summary"><a class="header" href="#summary">Summary</a></h2>
<p>Memory management is a fundamental aspect of systems programming. Key takeaways:</p>
<ol>
<li><strong>Understand your memory model</strong>: Stack, heap, static allocation</li>
<li><strong>Choose appropriate strategies</strong>: Manual, GC, smart pointers, arenas</li>
<li><strong>Profile before optimizing</strong>: Measure, don’t guess</li>
<li><strong>Use tools</strong>: Valgrind, AddressSanitizer, profilers</li>
<li><strong>Follow best practices</strong>: RAII, ownership clarity, bounds checking</li>
<li><strong>Test thoroughly</strong>: Static analysis, dynamic analysis, leak detection</li>
</ol>
<p>Different languages and use cases require different approaches:</p>
<ul>
<li><strong>C/C++</strong>: Manual management or smart pointers</li>
<li><strong>Python/Java/Go</strong>: Garbage collection</li>
<li><strong>Rust</strong>: Compile-time ownership</li>
<li><strong>Games/Real-time</strong>: Arenas, pools, manual control</li>
</ul>
<p>The right choice depends on your performance requirements, development time constraints, and correctness guarantees needed.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../programming/concurrency.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256 246.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"/></svg></span>
                            </a>

                            <a rel="next prefetch" href="../programming/compilers.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M278.6 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L210.7 256 73.4 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"/></svg></span>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../programming/concurrency.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256 246.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"/></svg></span>
                    </a>

                    <a rel="next prefetch" href="../programming/compilers.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M278.6 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L210.7 256 73.4 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"/></svg></span>
                    </a>
            </nav>

        </div>

        <template id=fa-eye><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M288 32c-80.8 0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7 0 24.6C17.3 304 48.6 356 95.4 399.4C142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144s64.5-144 144-144s144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64c-11.5 0-22.3-3-31.6-8.4c-.2 2.8-.4 5.5-.4 8.4c0 53 43 96 96 96s96-43 96-96s-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6z"/></svg></span></template>
        <template id=fa-eye-slash><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M38.8 5.1C28.4-3.1 13.3-1.2 5.1 9.2S-1.2 34.7 9.2 42.9l592 464c10.4 8.2 25.5 6.3 33.7-4.1s6.3-25.5-4.1-33.7L525.6 386.7c39.6-40.6 66.4-86.1 79.9-118.4c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C465.5 68.8 400.8 32 320 32c-68.2 0-125 26.3-169.3 60.8L38.8 5.1zM223.1 149.5C248.6 126.2 282.7 112 320 112c79.5 0 144 64.5 144 144c0 24.9-6.3 48.3-17.4 68.7L408 294.5c5.2-11.8 8-24.8 8-38.5c0-53-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6c0 10.2-2.4 19.8-6.6 28.3l-90.3-70.8zm223.1 298L373 389.9c-16.4 6.5-34.3 10.1-53 10.1c-79.5 0-144-64.5-144-144c0-6.9 .5-13.6 1.4-20.2L83.1 161.5C60.3 191.2 44 220.8 34.5 243.7c-3.3 7.9-3.3 16.7 0 24.6c14.9 35.7 46.2 87.7 93 131.1C174.5 443.2 239.2 480 320 480c47.8 0 89.9-12.9 126.2-32.5z"/></svg></span></template>
        <template id=fa-copy><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M502.6 70.63l-61.25-61.25C435.4 3.371 427.2 0 418.7 0H255.1c-35.35 0-64 28.66-64 64l.0195 256C192 355.4 220.7 384 256 384h192c35.2 0 64-28.8 64-64V93.25C512 84.77 508.6 76.63 502.6 70.63zM464 320c0 8.836-7.164 16-16 16H255.1c-8.838 0-16-7.164-16-16L239.1 64.13c0-8.836 7.164-16 16-16h128L384 96c0 17.67 14.33 32 32 32h47.1V320zM272 448c0 8.836-7.164 16-16 16H63.1c-8.838 0-16-7.164-16-16L47.98 192.1c0-8.836 7.164-16 16-16H160V128H63.99c-35.35 0-64 28.65-64 64l.0098 256C.002 483.3 28.66 512 64 512h192c35.2 0 64-28.8 64-64v-32h-47.1L272 448z"/></svg></span></template>
        <template id=fa-play><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M73 39c-14.8-9.1-33.4-9.4-48.5-.9S0 62.6 0 80V432c0 17.4 9.4 33.4 24.5 41.9s33.7 8.1 48.5-.9L361 297c14.3-8.7 23-24.2 23-41s-8.7-32.2-23-41L73 39z"/></svg></span></template>
        <template id=fa-clock-rotate-left><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M75 75L41 41C25.9 25.9 0 36.6 0 57.9V168c0 13.3 10.7 24 24 24H134.1c21.4 0 32.1-25.9 17-41l-30.8-30.8C155 85.5 203 64 256 64c106 0 192 86 192 192s-86 192-192 192c-40.8 0-78.6-12.7-109.7-34.4c-14.5-10.1-34.4-6.6-44.6 7.9s-6.6 34.4 7.9 44.6C151.2 495 201.7 512 256 512c141.4 0 256-114.6 256-256S397.4 0 256 0C185.3 0 121.3 28.7 75 75zm181 53c-13.3 0-24 10.7-24 24V256c0 6.4 2.5 12.5 7 17l72 72c9.4 9.4 24.6 9.4 33.9 0s9.4-24.6 0-33.9l-65-65V152c0-13.3-10.7-24-24-24z"/></svg></span></template>



        <script>
            window.playground_copyable = true;
        </script>


        <script src="../elasticlunr-ef4e11c1.min.js"></script>
        <script src="../mark-09e88c2c.min.js"></script>
        <script src="../searcher-c2a407aa.js"></script>

        <script src="../clipboard-1626706a.min.js"></script>
        <script src="../highlight-abc7f01d.js"></script>
        <script src="../book-a0b12cfe.js"></script>

        <!-- Custom JS scripts -->



    </div>
    </body>
</html>
