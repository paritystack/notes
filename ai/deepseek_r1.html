<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Deepseek R1 - My Notes</title>


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon-de23e50b.svg">
        <link rel="shortcut icon" href="../favicon-8114d1fc.png">
        <link rel="stylesheet" href="../css/variables-8adf115d.css">
        <link rel="stylesheet" href="../css/general-2459343d.css">
        <link rel="stylesheet" href="../css/chrome-9dfbd86b.css">
        <link rel="stylesheet" href="../css/print-9e4910d8.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../fonts/fonts-9644e21d.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="mdbook-highlight-css" href="../highlight-493f70e1.css">
        <link rel="stylesheet" id="mdbook-tomorrow-night-css" href="../tomorrow-night-4c0ae647.css">
        <link rel="stylesheet" id="mdbook-ayu-highlight-css" href="../ayu-highlight-3fdfc3ac.css">

        <!-- Custom theme stylesheets -->

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "../";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "../searchindex-b2dc8d76.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../toc-728509bc.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="mdbook-body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="mdbook-sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("mdbook-sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="mdbook-sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="../toc.html"></iframe>
            </noscript>
            <div id="mdbook-sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="mdbook-page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="mdbook-menu-bar-hover-placeholder"></div>
                <div id="mdbook-menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="mdbook-sidebar-toggle" class="icon-button" for="mdbook-sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="mdbook-sidebar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"/></svg></span>
                        </label>
                        <button id="mdbook-theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="mdbook-theme-list">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M371.3 367.1c27.3-3.9 51.9-19.4 67.2-42.9L600.2 74.1c12.6-19.5 9.4-45.3-7.6-61.2S549.7-4.4 531.1 9.6L294.4 187.2c-24 18-38.2 46.1-38.4 76.1L371.3 367.1zm-19.6 25.4l-116-104.4C175.9 290.3 128 339.6 128 400c0 3.9 .2 7.8 .6 11.6c1.8 17.5-10.2 36.4-27.8 36.4H96c-17.7 0-32 14.3-32 32s14.3 32 32 32H240c61.9 0 112-50.1 112-112c0-2.5-.1-5-.2-7.5z"/></svg></span>
                        </button>
                        <ul id="mdbook-theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-ayu">Ayu</button></li>
                        </ul>
                        <button id="mdbook-search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="mdbook-searchbar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M416 208c0 45.9-14.9 88.3-40 122.7L502.6 457.4c12.5 12.5 12.5 32.8 0 45.3s-32.8 12.5-45.3 0L330.7 376c-34.4 25.2-76.8 40-122.7 40C93.1 416 0 322.9 0 208S93.1 0 208 0S416 93.1 416 208zM208 352c79.5 0 144-64.5 144-144s-64.5-144-144-144S64 128.5 64 208s64.5 144 144 144z"/></svg></span>
                        </button>
                    </div>

                    <h1 class="menu-title">My Notes</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <span class=fa-svg id="print-button"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M128 0C92.7 0 64 28.7 64 64v96h64V64H354.7L384 93.3V160h64V93.3c0-17-6.7-33.3-18.7-45.3L400 18.7C388 6.7 371.7 0 354.7 0H128zM384 352v32 64H128V384 368 352H384zm64 32h32c17.7 0 32-14.3 32-32V256c0-35.3-28.7-64-64-64H64c-35.3 0-64 28.7-64 64v96c0 17.7 14.3 32 32 32H64v64c0 35.3 28.7 64 64 64H384c35.3 0 64-28.7 64-64V384zm-16-88c-13.3 0-24-10.7-24-24s10.7-24 24-24s24 10.7 24 24s-10.7 24-24 24z"/></svg></span>
                        </a>

                    </div>
                </div>

                <div id="mdbook-search-wrapper" class="hidden">
                    <form id="mdbook-searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="mdbook-searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="mdbook-searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <span class=fa-svg id="fa-spin"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M304 48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zm0 416c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM48 304c26.5 0 48-21.5 48-48s-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48zm464-48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM142.9 437c18.7-18.7 18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zm0-294.2c18.7-18.7 18.7-49.1 0-67.9S93.7 56.2 75 75s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zM369.1 437c18.7 18.7 49.1 18.7 67.9 0s18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9z"/></svg></span>
                            </div>
                        </div>
                    </form>
                    <div id="mdbook-searchresults-outer" class="searchresults-outer hidden">
                        <div id="mdbook-searchresults-header" class="searchresults-header"></div>
                        <ul id="mdbook-searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('mdbook-sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('mdbook-sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#mdbook-sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="mdbook-content" class="content">
                    <main>
                        <h1 id="deepseek-r1---open-source-reasoning-model"><a class="header" href="#deepseek-r1---open-source-reasoning-model">DeepSeek R1 - Open Source Reasoning Model</a></h1>
<p>Complete guide to DeepSeek R1, the open-source reasoning model that rivals OpenAI’s o1, from setup to fine-tuning and deployment.</p>
<h2 id="table-of-contents"><a class="header" href="#table-of-contents">Table of Contents</a></h2>
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#model-versions">Model Versions</a></li>
<li><a href="#architecture">Architecture</a></li>
<li><a href="#installation--setup">Installation &amp; Setup</a></li>
<li><a href="#basic-usage">Basic Usage</a></li>
<li><a href="#prompt-engineering">Prompt Engineering</a></li>
<li><a href="#fine-tuning">Fine-tuning</a></li>
<li><a href="#deployment">Deployment</a></li>
<li><a href="#common-patterns--operations">Common Patterns &amp; Operations</a></li>
<li><a href="#advanced-techniques">Advanced Techniques</a></li>
<li><a href="#best-practices">Best Practices</a></li>
</ul>
<h2 id="introduction"><a class="header" href="#introduction">Introduction</a></h2>
<p>DeepSeek R1, released on January 20, 2025, represents a breakthrough in open-source reasoning models. Built on reinforcement learning (RL), it achieves performance comparable to OpenAI’s o1 on complex reasoning tasks while being fully open-source under an MIT license.</p>
<h3 id="key-features"><a class="header" href="#key-features">Key Features</a></h3>
<ul>
<li><strong>Advanced Reasoning</strong>: Native chain-of-thought reasoning capabilities</li>
<li><strong>Open Source</strong>: MIT licensed for commercial and academic use</li>
<li><strong>Competitive Performance</strong>: Matches or exceeds o1 on mathematical and coding benchmarks</li>
<li><strong>Multiple Sizes</strong>: From 1.5B to 671B parameters (distilled and full models)</li>
<li><strong>Long Context</strong>: 128K token context window</li>
<li><strong>Self-Verification</strong>: Built-in reasoning verification and error correction</li>
<li><strong>Efficient Architecture</strong>: Mixture of Experts (MoE) design</li>
</ul>
<h3 id="benchmark-performance"><a class="header" href="#benchmark-performance">Benchmark Performance</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Task</th><th>DeepSeek R1</th><th>OpenAI o1</th></tr>
</thead>
<tbody>
<tr><td>AIME 2024 (Math)</td><td>79.8%</td><td>79.2%</td></tr>
<tr><td>MATH-500</td><td>97.3%</td><td>97.3%</td></tr>
<tr><td>Codeforces</td><td>96.3 (2,029 Elo)</td><td>Similar</td></tr>
<tr><td>GPQA Diamond</td><td>71.5%</td><td>Comparable</td></tr>
<tr><td>MMLU</td><td>Superior to V3</td><td>-</td></tr>
</tbody>
</table>
</div>
<h3 id="architecture-highlights"><a class="header" href="#architecture-highlights">Architecture Highlights</a></h3>
<ul>
<li><strong>671B Total Parameters</strong> (37B activated per forward pass)</li>
<li><strong>Mixture of Experts (MoE)</strong>: Efficient routing to specialized expert networks</li>
<li><strong>Multi-head Latent Attention (MLA)</strong>: Reduces KV-cache to 5-13% of traditional methods</li>
<li><strong>Rotary Position Embeddings (RoPE)</strong>: Enhanced position encoding</li>
<li><strong>61 Hidden Layers</strong>: Deep architecture for complex reasoning</li>
</ul>
<h2 id="model-versions"><a class="header" href="#model-versions">Model Versions</a></h2>
<h3 id="deepseek-r1-full-model"><a class="header" href="#deepseek-r1-full-model">DeepSeek R1 (Full Model)</a></h3>
<p><strong>Released</strong>: January 2025</p>
<pre><code class="language-python"># Note: Direct Transformers support not yet available
# Use vLLM or refer to DeepSeek-V3 repo
</code></pre>
<p><strong>Specifications:</strong></p>
<ul>
<li>Total Parameters: 671B</li>
<li>Activated Parameters: 37B per forward pass</li>
<li>Context Length: 128K tokens</li>
<li>Architecture: MoE</li>
<li>License: MIT</li>
</ul>
<p><strong>Use Cases:</strong></p>
<ul>
<li>Complex mathematical reasoning</li>
<li>Advanced coding challenges</li>
<li>Multi-step logical problems</li>
<li>Research applications</li>
</ul>
<h3 id="deepseek-r1-zero"><a class="header" href="#deepseek-r1-zero">DeepSeek R1 Zero</a></h3>
<p><strong>Training Approach</strong>: Pure RL without supervised fine-tuning</p>
<pre><code class="language-python"># Same infrastructure as DeepSeek R1
# Demonstrates RL-only training effectiveness
</code></pre>
<p><strong>Key Differences:</strong></p>
<ul>
<li>No SFT phase (pure RL)</li>
<li>Emerged reasoning behaviors autonomously</li>
<li>Research-focused variant</li>
</ul>
<h3 id="distilled-models-qwen-based"><a class="header" href="#distilled-models-qwen-based">Distilled Models (Qwen-based)</a></h3>
<h4 id="deepseek-r1-distill-qwen-15b"><a class="header" href="#deepseek-r1-distill-qwen-15b">DeepSeek-R1-Distill-Qwen-1.5B</a></h4>
<pre><code class="language-bash"># Ollama
ollama pull deepseek-r1:1.5b
ollama run deepseek-r1:1.5b

# vLLM
vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B
</code></pre>
<p><strong>Features:</strong></p>
<ul>
<li>Smallest variant for edge deployment</li>
<li>Fast inference on consumer hardware</li>
<li>Suitable for resource-constrained environments</li>
</ul>
<h4 id="deepseek-r1-distill-qwen-7b"><a class="header" href="#deepseek-r1-distill-qwen-7b">DeepSeek-R1-Distill-Qwen-7B</a></h4>
<pre><code class="language-bash">ollama pull deepseek-r1:7b
ollama run deepseek-r1:7b
</code></pre>
<p><strong>Performance:</strong></p>
<ul>
<li>AIME 2024: 55.5%</li>
<li>Good balance of size and capability</li>
<li>Runs on 6GB+ VRAM GPUs</li>
</ul>
<h4 id="deepseek-r1-distill-qwen-14b"><a class="header" href="#deepseek-r1-distill-qwen-14b">DeepSeek-R1-Distill-Qwen-14B</a></h4>
<pre><code class="language-bash">ollama pull deepseek-r1:14b
ollama run deepseek-r1:14b
</code></pre>
<p><strong>Features:</strong></p>
<ul>
<li>Enhanced reasoning over 7B</li>
<li>Mid-range deployment option</li>
<li>Excellent for local development</li>
</ul>
<h4 id="deepseek-r1-distill-qwen-32b"><a class="header" href="#deepseek-r1-distill-qwen-32b">DeepSeek-R1-Distill-Qwen-32B</a></h4>
<pre><code class="language-bash"># vLLM with tensor parallelism
vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B \
    --tensor-parallel-size 2 \
    --max-model-len 32768 \
    --enforce-eager

# SGLang
python3 -m sglang.launch_server \
    --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B \
    --trust-remote-code \
    --tp 2
</code></pre>
<p><strong>Performance:</strong></p>
<ul>
<li>AIME 2024: 72.6%</li>
<li>MATH-500: 94.3%</li>
<li>LiveCodeBench: 57.2%</li>
<li>Outperforms OpenAI o1-mini on multiple benchmarks</li>
</ul>
<h3 id="distilled-models-llama-based"><a class="header" href="#distilled-models-llama-based">Distilled Models (Llama-based)</a></h3>
<h4 id="deepseek-r1-distill-llama-8b"><a class="header" href="#deepseek-r1-distill-llama-8b">DeepSeek-R1-Distill-Llama-8B</a></h4>
<pre><code class="language-bash">ollama pull deepseek-r1:8b
ollama run deepseek-r1:8b

# Hugging Face
from transformers import AutoTokenizer, AutoModelForCausalLM
model_name = "deepseek-ai/DeepSeek-R1-Distill-Llama-8B"
</code></pre>
<p><strong>Features:</strong></p>
<ul>
<li>Based on Llama architecture</li>
<li>Compatible with Llama ecosystem</li>
<li>Good for fine-tuning on custom tasks</li>
</ul>
<h4 id="deepseek-r1-distill-llama-70b"><a class="header" href="#deepseek-r1-distill-llama-70b">DeepSeek-R1-Distill-Llama-70B</a></h4>
<pre><code class="language-bash">ollama pull deepseek-r1:70b
ollama run deepseek-r1:70b
</code></pre>
<p><strong>Features:</strong></p>
<ul>
<li>Highest-capacity distilled model</li>
<li>Excellent reasoning capabilities</li>
<li>Production-ready performance</li>
</ul>
<h3 id="model-comparison"><a class="header" href="#model-comparison">Model Comparison</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Model</th><th>Parameters</th><th>AIME 2024</th><th>MATH-500</th><th>VRAM (FP16)</th><th>Use Case</th></tr>
</thead>
<tbody>
<tr><td>R1-Distill-Qwen-1.5B</td><td>1.5B</td><td>~20%</td><td>~60%</td><td>3GB</td><td>Edge/Mobile</td></tr>
<tr><td>R1-Distill-Qwen-7B</td><td>7B</td><td>55.5%</td><td>~85%</td><td>14GB</td><td>Desktop</td></tr>
<tr><td>R1-Distill-Llama-8B</td><td>8B</td><td>~57%</td><td>~86%</td><td>16GB</td><td>Standard</td></tr>
<tr><td>R1-Distill-Qwen-14B</td><td>14B</td><td>~65%</td><td>~90%</td><td>28GB</td><td>Mid-range</td></tr>
<tr><td>R1-Distill-Qwen-32B</td><td>32B</td><td>72.6%</td><td>94.3%</td><td>64GB</td><td>High-end</td></tr>
<tr><td>R1-Distill-Llama-70B</td><td>70B</td><td>~76%</td><td>~96%</td><td>140GB</td><td>Production</td></tr>
<tr><td>DeepSeek R1</td><td>671B (37B active)</td><td>79.8%</td><td>97.3%</td><td>140GB+ (MoE)</td><td>Research/Max quality</td></tr>
</tbody>
</table>
</div>
<h2 id="architecture"><a class="header" href="#architecture">Architecture</a></h2>
<h3 id="mixture-of-experts-moe"><a class="header" href="#mixture-of-experts-moe">Mixture of Experts (MoE)</a></h3>
<pre><code>Total Parameters: 671B
Activated per Forward Pass: 37B (~5.5%)

┌─────────────────────────────────────┐
│         Input Embedding             │
└──────────────┬──────────────────────┘
               │
        ┌──────▼──────┐
        │   Router    │
        └──────┬──────┘
               │
    ┌──────────┼──────────┐
    ▼          ▼          ▼
┌───────┐  ┌───────┐  ┌───────┐
│Expert1│  │Expert2│  │Expert3│ ... (multiple experts)
└───┬───┘  └───┬───┘  └───┬───┘
    └──────────┼──────────┘
               ▼
        ┌─────────────┐
        │   Output    │
        └─────────────┘
</code></pre>
<p><strong>Benefits:</strong></p>
<ul>
<li>Efficient compute: Only 37B params active per token</li>
<li>Specialized expertise: Routing to relevant expert clusters</li>
<li>Scalability: Add experts without linear compute increase</li>
</ul>
<h3 id="multi-head-latent-attention-mla"><a class="header" href="#multi-head-latent-attention-mla">Multi-head Latent Attention (MLA)</a></h3>
<pre><code class="language-python"># Traditional Attention KV-cache
traditional_kv_cache = num_heads * head_dim * sequence_length * 2 * bytes_per_param
# Example: 32 * 128 * 4096 * 2 * 2 = 64 MB per layer

# MLA Latent KV-cache (5-13% reduction)
mla_latent_cache = latent_dim * sequence_length * 2 * bytes_per_param
# Example: 512 * 4096 * 2 * 2 = 8 MB per layer (~87% reduction)
</code></pre>
<p><strong>Key Innovation:</strong></p>
<ol>
<li>Compress K and V into low-dimensional latent vectors during training</li>
<li>Store only latent representations in KV-cache</li>
<li>Decompress on-the-fly during inference</li>
<li>Dramatically reduces memory overhead</li>
</ol>
<h3 id="layer-structure-61-hidden-layers"><a class="header" href="#layer-structure-61-hidden-layers">Layer Structure (61 Hidden Layers)</a></h3>
<pre><code>Layer Pattern:
┌─────────────────────────┐
│  Input from prev layer  │
└────────────┬────────────┘
             │
        ┌────▼────┐
        │ RoPE    │  (Rotary Position Embeddings)
        └────┬────┘
             │
        ┌────▼────┐
        │   MLA   │  (Multi-head Latent Attention)
        └────┬────┘
             │
        ┌────▼────┐
        │ RMSNorm │
        └────┬────┘
             │
        ┌────▼────┐
        │ MoE FFN │  (Mixture of Experts Feed Forward)
        └────┬────┘
             │
        ┌────▼────┐
        │ RMSNorm │
        └────┬────┘
             │
    ┌────────▼────────┐
    │ Output to next  │
    │     layer       │
    └─────────────────┘
</code></pre>
<h3 id="training-methodology"><a class="header" href="#training-methodology">Training Methodology</a></h3>
<pre><code>Phase 1: Supervised Fine-Tuning (SFT)
├── Curated long chain-of-thought examples
├── 800K high-quality reasoning samples
└── Initial reasoning pattern formation

Phase 2: Reinforcement Learning (RL)
├── Policy gradient optimization
├── Self-verification rewards
├── Error correction incentivization
└── Emergent behaviors:
    ├── Chain-of-thought reasoning
    ├── Self-reflection
    ├── Verification steps
    └── Logical decomposition
</code></pre>
<p><strong>R1-Zero Variant</strong>: Skipped Phase 1 entirely, demonstrating RL can develop reasoning from scratch.</p>
<h2 id="installation--setup"><a class="header" href="#installation--setup">Installation &amp; Setup</a></h2>
<h3 id="via-ollama-easiest-for-local-use"><a class="header" href="#via-ollama-easiest-for-local-use">Via Ollama (Easiest for Local Use)</a></h3>
<pre><code class="language-bash"># Install Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Pull and run distilled models
ollama pull deepseek-r1:1.5b   # Smallest
ollama pull deepseek-r1:7b     # Balanced
ollama pull deepseek-r1:8b     # Llama-based
ollama pull deepseek-r1:14b    # Mid-range
ollama pull deepseek-r1:32b    # High-quality
ollama pull deepseek-r1:70b    # Best distilled

# Interactive chat
ollama run deepseek-r1:7b
</code></pre>
<p><strong>Python usage:</strong></p>
<pre><code class="language-python">import ollama

# Simple generation
response = ollama.generate(
    model='deepseek-r1:7b',
    prompt='Solve: If x^2 + 5x + 6 = 0, find x.',
)
print(response['response'])

# Chat interface
messages = [
    {
        'role': 'user',
        'content': 'Explain the time complexity of quicksort'
    }
]

response = ollama.chat(
    model='deepseek-r1:7b',
    messages=messages
)
print(response['message']['content'])
</code></pre>
<h3 id="via-vllm-production-inference"><a class="header" href="#via-vllm-production-inference">Via vLLM (Production Inference)</a></h3>
<pre><code class="language-bash"># Install vLLM
pip install vllm

# Serve distilled models
vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-7B

# For larger models with tensor parallelism
vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B \
    --tensor-parallel-size 2 \
    --max-model-len 32768 \
    --enforce-eager
</code></pre>
<p><strong>Python client:</strong></p>
<pre><code class="language-python">from vllm import LLM, SamplingParams

# Load model
llm = LLM(
    model="deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
    tensor_parallel_size=1
)

# Configure sampling
sampling_params = SamplingParams(
    temperature=0.6,  # Recommended: 0.5-0.7
    top_p=0.95,
    max_tokens=2048
)

# Generate
prompts = [
    "Write a Python function to find prime numbers up to n",
    "Explain the concept of gradient descent"
]

outputs = llm.generate(prompts, sampling_params)

for output in outputs:
    print(f"Prompt: {output.prompt}")
    print(f"Response: {output.outputs[0].text}\n")
</code></pre>
<h3 id="via-sglang-fast-inference-engine"><a class="header" href="#via-sglang-fast-inference-engine">Via SGLang (Fast Inference Engine)</a></h3>
<pre><code class="language-bash"># Install
pip install "sglang[all]"

# Serve model
python3 -m sglang.launch_server \
    --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B \
    --trust-remote-code \
    --tp 1
</code></pre>
<p><strong>Python usage:</strong></p>
<pre><code class="language-python">import sglang as sgl

@sgl.function
def reasoning_task(s, question):
    s += sgl.user(question)
    s += sgl.assistant(sgl.gen("answer", max_tokens=1024, temperature=0.6))

# Run
state = reasoning_task.run(
    question="What is the derivative of x^3 + 2x^2 + 5?",
    model="deepseek-ai/DeepSeek-R1-Distill-Qwen-14B"
)

print(state["answer"])
</code></pre>
<h3 id="via-hugging-face-transformers"><a class="header" href="#via-hugging-face-transformers">Via Hugging Face Transformers</a></h3>
<pre><code class="language-bash">pip install transformers torch accelerate
</code></pre>
<pre><code class="language-python">from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# Load distilled model
model_name = "deepseek-ai/DeepSeek-R1-Distill-Llama-8B"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True
)

# Generate
prompt = "Calculate the factorial of 10 step by step"
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

outputs = model.generate(
    **inputs,
    max_new_tokens=1024,
    temperature=0.6,
    top_p=0.95,
    do_sample=True
)

result = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(result)
</code></pre>
<h3 id="via-api-providers"><a class="header" href="#via-api-providers">Via API Providers</a></h3>
<h4 id="togetherai"><a class="header" href="#togetherai">Together.ai</a></h4>
<pre><code class="language-python">from openai import OpenAI

client = OpenAI(
    api_key="your-together-api-key",
    base_url="https://api.together.xyz/v1"
)

response = client.chat.completions.create(
    model="deepseek-ai/DeepSeek-R1",
    messages=[
        {"role": "user", "content": "Solve: 2x + 5 = 15"}
    ],
    temperature=0.6,
    max_tokens=2048
)

print(response.choices[0].message.content)
</code></pre>
<h4 id="fireworksai"><a class="header" href="#fireworksai">Fireworks.ai</a></h4>
<pre><code class="language-python">import fireworks.client

fireworks.client.api_key = "your-fireworks-api-key"

response = fireworks.client.ChatCompletion.create(
    model="accounts/fireworks/models/deepseek-r1",
    messages=[{
        "role": "user",
        "content": "Explain binary search algorithm"
    }],
    temperature=0.6
)

print(response.choices[0].message.content)
</code></pre>
<h2 id="basic-usage"><a class="header" href="#basic-usage">Basic Usage</a></h2>
<h3 id="simple-text-generation"><a class="header" href="#simple-text-generation">Simple Text Generation</a></h3>
<pre><code class="language-python">from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model_name = "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

# Direct generation
prompt = "What is the square root of 144?"
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

outputs = model.generate(
    **inputs,
    max_new_tokens=512,
    temperature=0.6,  # CRITICAL: 0.5-0.7 range
    top_p=0.95,
    do_sample=True
)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))
</code></pre>
<h3 id="chat-format"><a class="header" href="#chat-format">Chat Format</a></h3>
<p><strong>IMPORTANT</strong>: DeepSeek R1 works best WITHOUT system prompts. Put all instructions in user messages.</p>
<pre><code class="language-python"># ❌ AVOID: System prompts reduce effectiveness
messages_bad = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Solve this problem..."}
]

# ✅ RECOMMENDED: All instructions in user message
messages_good = [
    {"role": "user", "content": "Solve this problem step by step: ..."}
]

# Apply chat template
input_ids = tokenizer.apply_chat_template(
    messages_good,
    add_generation_prompt=True,
    return_tensors="pt"
).to(model.device)

# Generate
outputs = model.generate(
    input_ids,
    max_new_tokens=1024,
    temperature=0.6,
    top_p=0.95,
    pad_token_id=tokenizer.eos_token_id
)

response = tokenizer.decode(
    outputs[0][input_ids.shape[-1]:],
    skip_special_tokens=True
)
print(response)
</code></pre>
<h3 id="enforcing-reasoning-with-think-tags"><a class="header" href="#enforcing-reasoning-with-think-tags">Enforcing Reasoning with <code>&lt;think&gt;</code> Tags</a></h3>
<pre><code class="language-python"># Force model to show reasoning
prompt = """Solve the following problem. Begin your response with &lt;think&gt; to show your reasoning process.

Problem: A train travels 120 km in 2 hours. If it maintains the same speed, how far will it travel in 5 hours?"""

inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

outputs = model.generate(
    **inputs,
    max_new_tokens=2048,
    temperature=0.6,
    top_p=0.95
)

result = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(result)

# Output will include:
# &lt;think&gt;
# The train travels 120 km in 2 hours
# Speed = distance / time = 120 / 2 = 60 km/h
# For 5 hours: distance = speed × time = 60 × 5 = 300 km
# &lt;/think&gt;
# The train will travel 300 km in 5 hours.
</code></pre>
<h3 id="multi-turn-conversation"><a class="header" href="#multi-turn-conversation">Multi-turn Conversation</a></h3>
<pre><code class="language-python">conversation = []

def chat(user_message):
    # Add user message
    conversation.append({"role": "user", "content": user_message})

    # Generate response
    input_ids = tokenizer.apply_chat_template(
        conversation,
        add_generation_prompt=True,
        return_tensors="pt"
    ).to(model.device)

    outputs = model.generate(
        input_ids,
        max_new_tokens=1024,
        temperature=0.6,
        top_p=0.95,
        pad_token_id=tokenizer.eos_token_id
    )

    response = tokenizer.decode(
        outputs[0][input_ids.shape[-1]:],
        skip_special_tokens=True
    )

    # Add to conversation
    conversation.append({"role": "assistant", "content": response})

    return response

# Use
print(chat("What is 15 factorial?"))
print(chat("Now divide that by 120"))
print(chat("Express the result in scientific notation"))
</code></pre>
<h3 id="batch-processing"><a class="header" href="#batch-processing">Batch Processing</a></h3>
<pre><code class="language-python">prompts = [
    "What is the time complexity of merge sort?",
    "Explain the difference between TCP and UDP",
    "Calculate: (3x + 5)(2x - 1)"
]

# Tokenize with padding
inputs = tokenizer(
    prompts,
    return_tensors="pt",
    padding=True,
    truncation=True,
    max_length=512
).to(model.device)

# Generate
outputs = model.generate(
    **inputs,
    max_new_tokens=512,
    temperature=0.6,
    top_p=0.95,
    pad_token_id=tokenizer.pad_token_id
)

# Decode
results = tokenizer.batch_decode(outputs, skip_special_tokens=True)
for prompt, result in zip(prompts, results):
    print(f"Q: {prompt}")
    print(f"A: {result}\n")
</code></pre>
<h2 id="prompt-engineering"><a class="header" href="#prompt-engineering">Prompt Engineering</a></h2>
<p>DeepSeek R1 requires a <strong>fundamentally different</strong> prompting approach than traditional LLMs.</p>
<h3 id="critical-guidelines"><a class="header" href="#critical-guidelines">Critical Guidelines</a></h3>
<h4 id="-dont"><a class="header" href="#-dont">❌ DON’T:</a></h4>
<ol>
<li><strong>Don’t use few-shot examples</strong> - They degrade performance</li>
</ol>
<pre><code class="language-python"># ❌ AVOID
prompt = """
Q: What is 2+2?
A: 4

Q: What is 3+3?
A: 6

Q: What is 5+5?
A: """
</code></pre>
<ol start="2">
<li><strong>Don’t add explicit chain-of-thought instructions</strong> - R1 does this natively</li>
</ol>
<pre><code class="language-python"># ❌ AVOID
prompt = "Let's think step by step. First, ... Second, ... Third, ..."
</code></pre>
<ol start="3">
<li><strong>Don’t use system prompts</strong> - Put everything in user message</li>
</ol>
<pre><code class="language-python"># ❌ AVOID
messages = [
    {"role": "system", "content": "You are an expert mathematician..."},
    {"role": "user", "content": "Solve..."}
]
</code></pre>
<ol start="4">
<li><strong>Don’t overload with context</strong> - Be concise and clear</li>
</ol>
<pre><code class="language-python"># ❌ AVOID
prompt = "Given the following extensive background information... [5 paragraphs]... now solve..."
</code></pre>
<h4 id="-do"><a class="header" href="#-do">✅ DO:</a></h4>
<ol>
<li><strong>Use minimal, clear prompts</strong></li>
</ol>
<pre><code class="language-python"># ✅ GOOD
prompt = "Solve: If f(x) = 3x^2 + 2x - 5, find f(4)"
</code></pre>
<ol start="2">
<li><strong>State the problem directly</strong></li>
</ol>
<pre><code class="language-python"># ✅ GOOD
prompt = "Compare the advantages and disadvantages of SQL vs NoSQL databases"
</code></pre>
<ol start="3">
<li><strong>Use structured input when needed</strong></li>
</ol>
<pre><code class="language-python"># ✅ GOOD
prompt = """Analyze these three options:
A. Cloud deployment
B. On-premise servers
C. Hybrid approach

Evaluate cost, scalability, and security for each."""
</code></pre>
<ol start="4">
<li><strong>Request specific output formats</strong></li>
</ol>
<pre><code class="language-python"># ✅ GOOD
prompt = "List the prime numbers between 1 and 50. Format as a Python list."
</code></pre>
<h3 id="optimal-parameters"><a class="header" href="#optimal-parameters">Optimal Parameters</a></h3>
<pre><code class="language-python"># Recommended configuration
generation_config = {
    "temperature": 0.6,      # Range: 0.5-0.7 (prevents loops)
    "top_p": 0.95,          # Recommended value
    "max_new_tokens": 2048,  # Adjust based on task
    "do_sample": True,
    "repetition_penalty": 1.0  # Usually not needed
}

outputs = model.generate(**inputs, **generation_config)
</code></pre>
<h3 id="chain-of-draft-cod-technique"><a class="header" href="#chain-of-draft-cod-technique">Chain-of-Draft (CoD) Technique</a></h3>
<p>Reduce token usage by 80% while maintaining quality:</p>
<pre><code class="language-python"># Standard reasoning (verbose)
prompt = "Solve this complex calculus problem: ..."
# Output: 2000+ tokens with full reasoning

# Chain-of-Draft (efficient)
prompt = """Solve this complex calculus problem: ...

Think step by step, but only keep a minimum draft for each thinking step."""

# Output: ~400 tokens with condensed reasoning, same accuracy
</code></pre>
<h3 id="template-patterns"><a class="header" href="#template-patterns">Template Patterns</a></h3>
<h4 id="mathematical-problems"><a class="header" href="#mathematical-problems">Mathematical Problems</a></h4>
<pre><code class="language-python">template = """Solve the following problem:

Problem: {problem}

Show your work and provide the final answer."""

prompt = template.format(
    problem="Find the derivative of f(x) = x^3 * sin(x)"
)
</code></pre>
<h4 id="code-generation"><a class="header" href="#code-generation">Code Generation</a></h4>
<pre><code class="language-python">template = """Write a {language} function that {description}.

Requirements:
- {requirement1}
- {requirement2}
- Include error handling"""

prompt = template.format(
    language="Python",
    description="implements a binary search tree",
    requirement1="Support insert, search, and delete operations",
    requirement2="Maintain BST properties"
)
</code></pre>
<h4 id="analysis-tasks"><a class="header" href="#analysis-tasks">Analysis Tasks</a></h4>
<pre><code class="language-python">template = """Analyze the following scenario:

{scenario}

Provide:
1. Key insights
2. Potential risks
3. Recommended actions"""

prompt = template.format(
    scenario="A startup wants to migrate from monolith to microservices"
)
</code></pre>
<h4 id="comparison-tasks"><a class="header" href="#comparison-tasks">Comparison Tasks</a></h4>
<pre><code class="language-python">template = """Compare {option_a} vs {option_b}:

Evaluate:
- Performance
- Scalability
- Cost
- Ease of use

Provide a recommendation."""

prompt = template.format(
    option_a="PostgreSQL",
    option_b="MongoDB"
)
</code></pre>
<h3 id="advanced-prompting-techniques"><a class="header" href="#advanced-prompting-techniques">Advanced Prompting Techniques</a></h3>
<h4 id="self-verification-prompting"><a class="header" href="#self-verification-prompting">Self-Verification Prompting</a></h4>
<pre><code class="language-python">prompt = """Solve: x^2 - 7x + 12 = 0

After solving, verify your answer by substituting back into the original equation."""
</code></pre>
<h4 id="multi-part-problems"><a class="header" href="#multi-part-problems">Multi-Part Problems</a></h4>
<pre><code class="language-python">prompt = """Problem: A rectangle has a perimeter of 30 cm and an area of 50 cm².

Find:
1. The length
2. The width
3. The diagonal length"""
</code></pre>
<h4 id="constraint-based-prompting"><a class="header" href="#constraint-based-prompting">Constraint-Based Prompting</a></h4>
<pre><code class="language-python">prompt = """Generate a regex pattern that matches:
- Valid email addresses
- Must include @ symbol
- Domain must end in .com, .org, or .net
- No special characters except . and _

Provide the pattern and explain each component."""
</code></pre>
<h2 id="fine-tuning"><a class="header" href="#fine-tuning">Fine-tuning</a></h2>
<h3 id="lora-fine-tuning-recommended"><a class="header" href="#lora-fine-tuning-recommended">LoRA Fine-tuning (Recommended)</a></h3>
<pre><code class="language-bash">pip install transformers peft accelerate datasets torch
</code></pre>
<pre><code class="language-python">from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from datasets import load_dataset
import torch

# Load base model
model_name = "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# Prepare for training
model.gradient_checkpointing_enable()
model = prepare_model_for_kbit_training(model)

# LoRA configuration
lora_config = LoraConfig(
    r=16,                    # Rank (8, 16, 32)
    lora_alpha=32,          # Scaling factor (2*r typical)
    target_modules=[        # Target attention layers
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj"
    ],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
# trainable params: ~8M / 7B (~0.1%)

# Prepare dataset
dataset = load_dataset("your-dataset")

def format_prompt(example):
    # Format for reasoning tasks
    return {
        "text": f"Problem: {example['problem']}\n\nSolution: {example['solution']}"
    }

dataset = dataset.map(format_prompt)

# Tokenize
def tokenize(examples):
    return tokenizer(
        examples["text"],
        truncation=True,
        max_length=2048,
        padding="max_length"
    )

tokenized_dataset = dataset.map(tokenize, batched=True)

# Training arguments
training_args = TrainingArguments(
    output_dir="./deepseek-r1-lora",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    bf16=True,
    logging_steps=10,
    save_strategy="epoch",
    warmup_steps=100,
    optim="adamw_torch"
)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"] if "test" in tokenized_dataset else None
)

# Train
trainer.train()

# Save
model.save_pretrained("./deepseek-r1-lora-final")
tokenizer.save_pretrained("./deepseek-r1-lora-final")
</code></pre>
<h3 id="qlora-fine-tuning-4-bit-quantization"><a class="header" href="#qlora-fine-tuning-4-bit-quantization">QLoRA Fine-tuning (4-bit Quantization)</a></h3>
<pre><code class="language-bash">pip install bitsandbytes
</code></pre>
<pre><code class="language-python">from transformers import BitsAndBytesConfig

# 4-bit quantization config
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True
)

# Load model with quantization
model = AutoModelForCausalLM.from_pretrained(
    "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True
)

# Prepare for k-bit training
model = prepare_model_for_kbit_training(model)

# LoRA config (same as above)
lora_config = LoraConfig(...)
model = get_peft_model(model, lora_config)

# Training args with 8-bit optimizer
training_args = TrainingArguments(
    output_dir="./deepseek-r1-qlora",
    optim="paged_adamw_8bit",  # 8-bit optimizer
    fp16=True,  # or bf16
    # ... rest of args
)

# Train
trainer = Trainer(model=model, args=training_args, ...)
trainer.train()
</code></pre>
<h3 id="using-fine-tuned-model"><a class="header" href="#using-fine-tuned-model">Using Fine-tuned Model</a></h3>
<pre><code class="language-python">from peft import PeftModel

# Load base model
base_model = AutoModelForCausalLM.from_pretrained(
    "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

# Load LoRA weights
model = PeftModel.from_pretrained(
    base_model,
    "./deepseek-r1-lora-final"
)

# Merge for faster inference (optional)
model = model.merge_and_unload()

# Use normally
tokenizer = AutoTokenizer.from_pretrained("./deepseek-r1-lora-final")
inputs = tokenizer("Your prompt", return_tensors="pt").to(model.device)
outputs = model.generate(**inputs, max_new_tokens=512, temperature=0.6)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
</code></pre>
<h3 id="dataset-preparation-for-reasoning"><a class="header" href="#dataset-preparation-for-reasoning">Dataset Preparation for Reasoning</a></h3>
<pre><code class="language-python"># Format data for reasoning tasks
dataset_dict = {
    "train": [
        {
            "problem": "Find the area of a circle with radius 5",
            "solution": "Area = πr² = π(5)² = 25π ≈ 78.54 square units"
        },
        {
            "problem": "What is 15! / 13!?",
            "solution": "15! / 13! = 15 × 14 × 13! / 13! = 15 × 14 = 210"
        },
        # ... more examples
    ]
}

from datasets import Dataset
dataset = Dataset.from_dict(dataset_dict)

# Or load from files
# JSON Lines format:
# {"problem": "...", "solution": "..."}
# {"problem": "...", "solution": "..."}

dataset = load_dataset("json", data_files="train.jsonl")
</code></pre>
<h3 id="hyperparameter-recommendations"><a class="header" href="#hyperparameter-recommendations">Hyperparameter Recommendations</a></h3>
<pre><code class="language-python"># Small models (1.5B-7B)
small_model_config = {
    "lora_r": 8,
    "lora_alpha": 16,
    "learning_rate": 3e-4,
    "batch_size": 8,
    "gradient_accumulation": 2
}

# Medium models (8B-14B)
medium_model_config = {
    "lora_r": 16,
    "lora_alpha": 32,
    "learning_rate": 2e-4,
    "batch_size": 4,
    "gradient_accumulation": 4
}

# Large models (32B-70B)
large_model_config = {
    "lora_r": 32,
    "lora_alpha": 64,
    "learning_rate": 1e-4,
    "batch_size": 2,
    "gradient_accumulation": 8
}
</code></pre>
<h3 id="using-axolotl-for-simplified-training"><a class="header" href="#using-axolotl-for-simplified-training">Using Axolotl for Simplified Training</a></h3>
<pre><code class="language-bash">git clone https://github.com/OpenAccess-AI-Collective/axolotl
cd axolotl
pip install -e .
</code></pre>
<p>Create <code>deepseek_r1_config.yml</code>:</p>
<pre><code class="language-yaml">base_model: deepseek-ai/DeepSeek-R1-Distill-Qwen-7B
model_type: AutoModelForCausalLM
tokenizer_type: AutoTokenizer
trust_remote_code: true

load_in_4bit: true
adapter: qlora
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules:
  - q_proj
  - k_proj
  - v_proj
  - o_proj

datasets:
  - path: your-dataset.jsonl
    type: alpaca

num_epochs: 3
micro_batch_size: 4
gradient_accumulation_steps: 4
learning_rate: 0.0002

warmup_steps: 100
optimizer: paged_adamw_8bit
lr_scheduler: cosine

output_dir: ./deepseek-r1-tuned

bf16: true
tf32: true
gradient_checkpointing: true
</code></pre>
<p>Train:</p>
<pre><code class="language-bash">accelerate launch -m axolotl.cli.train deepseek_r1_config.yml
</code></pre>
<h2 id="deployment"><a class="header" href="#deployment">Deployment</a></h2>
<h3 id="fastapi-server"><a class="header" href="#fastapi-server">FastAPI Server</a></h3>
<pre><code class="language-python">from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import uvicorn

app = FastAPI()

# Load model at startup
model_name = "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

class GenerateRequest(BaseModel):
    prompt: str
    max_tokens: int = 1024
    temperature: float = 0.6
    top_p: float = 0.95

class GenerateResponse(BaseModel):
    response: str
    tokens_used: int

@app.post("/generate", response_model=GenerateResponse)
async def generate(request: GenerateRequest):
    try:
        inputs = tokenizer(request.prompt, return_tensors="pt").to(model.device)

        outputs = model.generate(
            **inputs,
            max_new_tokens=request.max_tokens,
            temperature=request.temperature,
            top_p=request.top_p,
            do_sample=True
        )

        result = tokenizer.decode(outputs[0], skip_special_tokens=True)
        tokens_used = len(outputs[0])

        return GenerateResponse(
            response=result,
            tokens_used=tokens_used
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health():
    return {"status": "healthy"}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
</code></pre>
<p>Usage:</p>
<pre><code class="language-bash"># Run server
python server.py

# Test
curl -X POST http://localhost:8000/generate \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "What is recursion?",
    "max_tokens": 512,
    "temperature": 0.6
  }'
</code></pre>
<h3 id="vllm-production-server"><a class="header" href="#vllm-production-server">vLLM Production Server</a></h3>
<pre><code class="language-bash"># Start server
vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-14B \
    --host 0.0.0.0 \
    --port 8000 \
    --tensor-parallel-size 2 \
    --max-model-len 8192

# With GPU specification
CUDA_VISIBLE_DEVICES=0,1 vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B \
    --tensor-parallel-size 2
</code></pre>
<p><strong>Client usage:</strong></p>
<pre><code class="language-python">from openai import OpenAI

# vLLM provides OpenAI-compatible API
client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="dummy"  # vLLM doesn't require auth by default
)

response = client.chat.completions.create(
    model="deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",
    messages=[
        {"role": "user", "content": "Explain binary trees"}
    ],
    temperature=0.6,
    max_tokens=1024
)

print(response.choices[0].message.content)
</code></pre>
<h3 id="docker-deployment"><a class="header" href="#docker-deployment">Docker Deployment</a></h3>
<pre><code class="language-dockerfile">FROM nvidia/cuda:12.1.0-runtime-ubuntu22.04

# Install Python
RUN apt-get update &amp;&amp; apt-get install -y python3 python3-pip

# Install dependencies
RUN pip3 install vllm transformers torch

# Download model (or mount as volume)
RUN python3 -c "from transformers import AutoModel; \
    AutoModel.from_pretrained('deepseek-ai/DeepSeek-R1-Distill-Qwen-7B')"

# Expose port
EXPOSE 8000

# Run server
CMD ["vllm", "serve", "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B", \
     "--host", "0.0.0.0", "--port", "8000"]
</code></pre>
<p>Build and run:</p>
<pre><code class="language-bash">docker build -t deepseek-r1-server .
docker run --gpus all -p 8000:8000 deepseek-r1-server
</code></pre>
<h3 id="kubernetes-deployment"><a class="header" href="#kubernetes-deployment">Kubernetes Deployment</a></h3>
<pre><code class="language-yaml">apiVersion: apps/v1
kind: Deployment
metadata:
  name: deepseek-r1
spec:
  replicas: 2
  selector:
    matchLabels:
      app: deepseek-r1
  template:
    metadata:
      labels:
        app: deepseek-r1
    spec:
      containers:
      - name: deepseek-r1
        image: deepseek-r1-server:latest
        ports:
        - containerPort: 8000
        resources:
          limits:
            nvidia.com/gpu: 1
            memory: "32Gi"
          requests:
            nvidia.com/gpu: 1
            memory: "16Gi"
        env:
        - name: CUDA_VISIBLE_DEVICES
          value: "0"
---
apiVersion: v1
kind: Service
metadata:
  name: deepseek-r1-service
spec:
  selector:
    app: deepseek-r1
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8000
  type: LoadBalancer
</code></pre>
<h3 id="aws-sagemaker-deployment"><a class="header" href="#aws-sagemaker-deployment">AWS SageMaker Deployment</a></h3>
<pre><code class="language-python">import sagemaker
from sagemaker.huggingface import HuggingFaceModel

# HuggingFace model configuration
huggingface_model = HuggingFaceModel(
    model_data="s3://your-bucket/model.tar.gz",  # Or use hub
    transformers_version='4.37',
    pytorch_version='2.1',
    py_version='py310',
    role=sagemaker.get_execution_role(),
    env={
        'HF_MODEL_ID': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-14B',
        'HF_TASK': 'text-generation'
    }
)

# Deploy
predictor = huggingface_model.deploy(
    initial_instance_count=1,
    instance_type='ml.g5.2xlarge'
)

# Use
response = predictor.predict({
    'inputs': 'What is machine learning?',
    'parameters': {
        'max_new_tokens': 512,
        'temperature': 0.6
    }
})

print(response[0]['generated_text'])
</code></pre>
<h3 id="langchain-integration"><a class="header" href="#langchain-integration">LangChain Integration</a></h3>
<pre><code class="language-bash">pip install langchain langchain-community
</code></pre>
<pre><code class="language-python">from langchain_community.llms import HuggingFacePipeline
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch

# Load model
model = AutoModelForCausalLM.from_pretrained(
    "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
    torch_dtype=torch.bfloat16,
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("deepseek-ai/DeepSeek-R1-Distill-Qwen-7B")

# Create pipeline
pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=1024,
    temperature=0.6,
    top_p=0.95
)

# LangChain LLM
llm = HuggingFacePipeline(pipeline=pipe)

# Create chain
template = """Problem: {problem}

Solve this step by step."""

prompt = PromptTemplate(template=template, input_variables=["problem"])
chain = LLMChain(llm=llm, prompt=prompt)

# Use
result = chain.run("Find the roots of x^2 - 5x + 6 = 0")
print(result)
</code></pre>
<h2 id="common-patterns--operations"><a class="header" href="#common-patterns--operations">Common Patterns &amp; Operations</a></h2>
<h3 id="mathematical-problem-solving"><a class="header" href="#mathematical-problem-solving">Mathematical Problem Solving</a></h3>
<pre><code class="language-python">def solve_math_problem(problem: str) -&gt; str:
    """Solve mathematical problems with reasoning"""
    prompt = f"""Solve the following mathematical problem. Show your reasoning.

Problem: {problem}

Begin with &lt;think&gt; to show your work."""

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(
        **inputs,
        max_new_tokens=2048,
        temperature=0.6,
        top_p=0.95
    )

    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# Examples
print(solve_math_problem("What is the derivative of x^3 + 2x^2 - 5x + 3?"))
print(solve_math_problem("Solve the system: 2x + y = 7, x - y = 2"))
print(solve_math_problem("Find the area under y=x^2 from x=0 to x=3"))
</code></pre>
<h3 id="code-generation-1"><a class="header" href="#code-generation-1">Code Generation</a></h3>
<pre><code class="language-python">def generate_code(description: str, language: str = "Python") -&gt; str:
    """Generate code with explanation"""
    prompt = f"""Write a {language} function that {description}.

Requirements:
- Include docstring
- Add error handling
- Use type hints (if applicable)
- Provide usage example"""

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(
        **inputs,
        max_new_tokens=1024,
        temperature=0.6
    )

    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# Example
code = generate_code(
    "implements a binary search algorithm on a sorted list",
    "Python"
)
print(code)
</code></pre>
<h3 id="code-review--debugging"><a class="header" href="#code-review--debugging">Code Review &amp; Debugging</a></h3>
<pre><code class="language-python">def review_code(code: str) -&gt; str:
    """Review code for issues and improvements"""
    prompt = f"""Review the following code. Identify:
1. Potential bugs
2. Performance issues
3. Security concerns
4. Suggested improvements

Code:
</code></pre>
<p>{code}</p>
<pre><code>
Provide detailed analysis."""

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=2048, temperature=0.6)

    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# Example
buggy_code = """
def divide(a, b):
    return a / b

result = divide(10, 0)
"""

print(review_code(buggy_code))
</code></pre>
<h3 id="logical-reasoning"><a class="header" href="#logical-reasoning">Logical Reasoning</a></h3>
<pre><code class="language-python">def logical_reasoning(premise: str, question: str) -&gt; str:
    """Perform logical reasoning on given premises"""
    prompt = f"""Given the following information:

{premise}

Question: {question}

Think through this logically and provide a reasoned answer."""

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=1536, temperature=0.6)

    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# Example
premise = """
- All programmers know at least one language
- Alice is a programmer
- Bob knows Python
- Python is a programming language
"""

question = "Does Alice necessarily know Python?"
print(logical_reasoning(premise, question))
</code></pre>
<h3 id="data-analysis"><a class="header" href="#data-analysis">Data Analysis</a></h3>
<pre><code class="language-python">def analyze_data(data_description: str, question: str) -&gt; str:
    """Analyze data and answer questions"""
    prompt = f"""Dataset: {data_description}

Question: {question}

Analyze the data and provide insights with reasoning."""

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=1536, temperature=0.6)

    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# Example
data = """
Sales data for Q1 2025:
- January: $50,000 (100 customers)
- February: $65,000 (120 customers)
- March: $72,000 (130 customers)
"""

analysis = analyze_data(
    data,
    "What is the trend in average revenue per customer?"
)
print(analysis)
</code></pre>
<h3 id="comparative-analysis"><a class="header" href="#comparative-analysis">Comparative Analysis</a></h3>
<pre><code class="language-python">def compare_options(options: list, criteria: list) -&gt; str:
    """Compare multiple options across criteria"""
    options_text = "\n".join([f"{i+1}. {opt}" for i, opt in enumerate(options)])
    criteria_text = "\n".join([f"- {c}" for c in criteria])

    prompt = f"""Compare the following options:

{options_text}

Evaluation criteria:
{criteria_text}

Provide a detailed comparison and recommendation."""

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=2048, temperature=0.6)

    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# Example
result = compare_options(
    options=[
        "PostgreSQL",
        "MongoDB",
        "MySQL"
    ],
    criteria=[
        "Performance",
        "Scalability",
        "Ease of use",
        "ACID compliance"
    ]
)
print(result)
</code></pre>
<h3 id="question-answering-with-context"><a class="header" href="#question-answering-with-context">Question Answering with Context</a></h3>
<pre><code class="language-python">def qa_with_context(context: str, question: str) -&gt; str:
    """Answer questions based on provided context"""
    prompt = f"""Context:
{context}

Question: {question}

Answer based on the context provided."""

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=1024, temperature=0.6)

    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# Example
context = """
The Python programming language was created by Guido van Rossum and first
released in 1991. Python emphasizes code readability with significant whitespace.
It supports multiple programming paradigms including procedural, object-oriented,
and functional programming.
"""

answer = qa_with_context(context, "When was Python first released?")
print(answer)
</code></pre>
<h2 id="advanced-techniques"><a class="header" href="#advanced-techniques">Advanced Techniques</a></h2>
<h3 id="retrieval-augmented-generation-rag"><a class="header" href="#retrieval-augmented-generation-rag">Retrieval-Augmented Generation (RAG)</a></h3>
<pre><code class="language-bash">pip install langchain chromadb sentence-transformers
</code></pre>
<pre><code class="language-python">from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain_community.llms import HuggingFacePipeline

# Load documents
documents = [
    "DeepSeek R1 is an open-source reasoning model released in January 2025.",
    "It uses a Mixture of Experts architecture with 671B parameters.",
    "The model achieves 79.8% on AIME 2024 mathematics benchmark.",
    # ... more documents
]

# Split text
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50
)
texts = text_splitter.create_documents(documents)

# Create embeddings
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)

# Create vector store
vectorstore = Chroma.from_documents(texts, embeddings)

# Create retrieval chain
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,  # HuggingFacePipeline from earlier
    chain_type="stuff",
    retriever=vectorstore.as_retriever(search_kwargs={"k": 3})
)

# Query
question = "What is DeepSeek R1's performance on mathematics?"
result = qa_chain.run(question)
print(result)
</code></pre>
<h3 id="function-calling--tool-use"><a class="header" href="#function-calling--tool-use">Function Calling / Tool Use</a></h3>
<pre><code class="language-python">import json
import re

def execute_function_call(response: str, available_functions: dict):
    """Execute function calls from model responses"""
    # Extract function call from response
    pattern = r'\{"function":\s*"(\w+)",\s*"parameters":\s*(\{[^}]+\})\}'
    match = re.search(pattern, response)

    if match:
        func_name = match.group(1)
        params = json.loads(match.group(2))

        if func_name in available_functions:
            return available_functions[func_name](**params)

    return None

# Define tools
def calculate(expression: str) -&gt; float:
    """Safely evaluate mathematical expressions"""
    try:
        return eval(expression, {"__builtins__": {}}, {})
    except:
        return "Error in calculation"

def get_weather(location: str) -&gt; dict:
    """Get weather for location (simulated)"""
    return {
        "location": location,
        "temperature": 22,
        "condition": "sunny"
    }

available_functions = {
    "calculate": calculate,
    "get_weather": get_weather
}

# System prompt with tools
tools_description = """
Available functions:
1. calculate(expression) - Evaluate math expressions
2. get_weather(location) - Get weather for a location

To use a function, respond with:
{"function": "function_name", "parameters": {"param": "value"}}
"""

prompt = f"""{tools_description}

User: What's 15 * 23 + 45?

Respond with a function call."""

inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
outputs = model.generate(**inputs, max_new_tokens=256, temperature=0.6)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)

# Execute function
result = execute_function_call(response, available_functions)
print(f"Result: {result}")
</code></pre>
<h3 id="constrained-generation"><a class="header" href="#constrained-generation">Constrained Generation</a></h3>
<pre><code class="language-bash">pip install outlines
</code></pre>
<pre><code class="language-python">import outlines

# Load model for outlines
model = outlines.models.transformers(
    "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"
)

# JSON schema constraint
schema = {
    "type": "object",
    "properties": {
        "name": {"type": "string"},
        "age": {"type": "integer"},
        "skills": {
            "type": "array",
            "items": {"type": "string"}
        },
        "experience_years": {"type": "integer"}
    },
    "required": ["name", "age", "skills"]
}

generator = outlines.generate.json(model, schema)
result = generator("Generate a software engineer profile:")
print(json.dumps(result, indent=2))

# Regex constraint
email_pattern = r"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}"
generator = outlines.generate.regex(model, email_pattern)
email = generator("Generate a professional email address:")
print(email)

# Multiple choice
choices = ["Python", "JavaScript", "Java", "C++", "Go"]
generator = outlines.generate.choice(model, choices)
language = generator("What is the best language for web backends?")
print(language)
</code></pre>
<h3 id="streaming-generation"><a class="header" href="#streaming-generation">Streaming Generation</a></h3>
<pre><code class="language-python">from transformers import TextIteratorStreamer
from threading import Thread

def stream_response(prompt: str):
    """Generate response with streaming"""
    streamer = TextIteratorStreamer(
        tokenizer,
        skip_special_tokens=True,
        skip_prompt=True
    )

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    generation_kwargs = {
        "input_ids": inputs["input_ids"],
        "attention_mask": inputs["attention_mask"],
        "max_new_tokens": 1024,
        "temperature": 0.6,
        "top_p": 0.95,
        "streamer": streamer
    }

    # Generate in thread
    thread = Thread(target=model.generate, kwargs=generation_kwargs)
    thread.start()

    # Stream output
    print("Response: ", end="", flush=True)
    for text in streamer:
        print(text, end="", flush=True)
    print()

    thread.join()

# Use
stream_response("Explain how recursion works in programming")
</code></pre>
<h3 id="multi-step-reasoning"><a class="header" href="#multi-step-reasoning">Multi-Step Reasoning</a></h3>
<pre><code class="language-python">def multi_step_solver(problem: str, max_steps: int = 5) -&gt; str:
    """Solve problems through iterative reasoning"""
    conversation = []

    # Initial problem
    conversation.append({
        "role": "user",
        "content": f"""Solve this problem step by step:

{problem}

Provide one reasoning step at a time."""
    })

    for step in range(max_steps):
        input_ids = tokenizer.apply_chat_template(
            conversation,
            add_generation_prompt=True,
            return_tensors="pt"
        ).to(model.device)

        outputs = model.generate(
            input_ids,
            max_new_tokens=512,
            temperature=0.6,
            pad_token_id=tokenizer.eos_token_id
        )

        response = tokenizer.decode(
            outputs[0][input_ids.shape[-1]:],
            skip_special_tokens=True
        )

        conversation.append({"role": "assistant", "content": response})

        # Check if solution is complete
        if "final answer" in response.lower() or "conclusion" in response.lower():
            break

        # Ask for next step
        conversation.append({
            "role": "user",
            "content": "Continue with the next step."
        })

    return "\n\n".join([msg["content"] for msg in conversation if msg["role"] == "assistant"])

# Example
solution = multi_step_solver("""
A company's revenue grows by 20% each year. If the revenue in 2023 was $100,000,
what will be the total revenue over 5 years (2023-2027)?
""")
print(solution)
</code></pre>
<h2 id="best-practices"><a class="header" href="#best-practices">Best Practices</a></h2>
<h3 id="1-temperature-settings"><a class="header" href="#1-temperature-settings">1. Temperature Settings</a></h3>
<pre><code class="language-python"># Mathematical/coding tasks - Lower temperature
generation_config_precise = {
    "temperature": 0.5,
    "top_p": 0.95,
    "do_sample": True
}

# Creative/open-ended tasks - Medium temperature
generation_config_balanced = {
    "temperature": 0.6,  # Recommended
    "top_p": 0.95,
    "do_sample": True
}

# Brainstorming/diverse outputs - Higher temperature
generation_config_creative = {
    "temperature": 0.7,  # Max recommended
    "top_p": 0.95,
    "do_sample": True
}

# ❌ AVOID: Temperature &gt; 0.7 causes repetition loops
generation_config_bad = {
    "temperature": 0.9,  # Too high!
    "top_p": 0.95
}
</code></pre>
<h3 id="2-memory-management"><a class="header" href="#2-memory-management">2. Memory Management</a></h3>
<pre><code class="language-python">import torch
import gc

def clear_gpu_memory():
    """Clear GPU cache"""
    gc.collect()
    torch.cuda.empty_cache()

# After inference
outputs = model.generate(...)
result = tokenizer.decode(outputs[0])
del outputs
clear_gpu_memory()

# Use context manager for automatic cleanup
class ModelInference:
    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        clear_gpu_memory()

    def generate(self, prompt):
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        outputs = model.generate(**inputs, max_new_tokens=512, temperature=0.6)
        return tokenizer.decode(outputs[0], skip_special_tokens=True)

# Use
with ModelInference() as inference:
    result = inference.generate("Your prompt")
</code></pre>
<h3 id="3-batch-processing"><a class="header" href="#3-batch-processing">3. Batch Processing</a></h3>
<pre><code class="language-python">def batch_inference(prompts: list, batch_size: int = 4) -&gt; list:
    """Process prompts in batches for efficiency"""
    results = []

    for i in range(0, len(prompts), batch_size):
        batch = prompts[i:i + batch_size]

        inputs = tokenizer(
            batch,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=512
        ).to(model.device)

        outputs = model.generate(
            **inputs,
            max_new_tokens=512,
            temperature=0.6,
            pad_token_id=tokenizer.pad_token_id
        )

        batch_results = tokenizer.batch_decode(outputs, skip_special_tokens=True)
        results.extend(batch_results)

        # Clear memory after each batch
        del inputs, outputs
        torch.cuda.empty_cache()

    return results
</code></pre>
<h3 id="4-error-handling"><a class="header" href="#4-error-handling">4. Error Handling</a></h3>
<pre><code class="language-python">def safe_generate(prompt: str, max_retries: int = 3) -&gt; str:
    """Generate with error handling and retries"""
    for attempt in range(max_retries):
        try:
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

            outputs = model.generate(
                **inputs,
                max_new_tokens=1024,
                temperature=0.6,
                top_p=0.95
            )

            result = tokenizer.decode(outputs[0], skip_special_tokens=True)

            # Cleanup
            del inputs, outputs
            torch.cuda.empty_cache()

            return result

        except RuntimeError as e:
            if "out of memory" in str(e):
                if attempt &lt; max_retries - 1:
                    print(f"OOM error, retrying... (attempt {attempt + 1})")
                    torch.cuda.empty_cache()
                    continue
                else:
                    raise Exception("Persistent OOM error after retries")
            else:
                raise

        except Exception as e:
            print(f"Error during generation: {e}")
            if attempt &lt; max_retries - 1:
                continue
            else:
                raise

    return "Error: Could not generate response"
</code></pre>
<h3 id="5-prompt-validation"><a class="header" href="#5-prompt-validation">5. Prompt Validation</a></h3>
<pre><code class="language-python">def validate_and_format_prompt(prompt: str, max_length: int = 4096) -&gt; str:
    """Validate and format prompts before generation"""
    # Remove excessive whitespace
    prompt = " ".join(prompt.split())

    # Check length
    tokens = tokenizer.encode(prompt)
    if len(tokens) &gt; max_length:
        print(f"Warning: Prompt too long ({len(tokens)} tokens), truncating...")
        tokens = tokens[:max_length]
        prompt = tokenizer.decode(tokens)

    # Ensure no system prompt patterns
    if prompt.strip().startswith("System:"):
        print("Warning: Removing system prompt prefix")
        prompt = prompt.replace("System:", "").strip()

    return prompt

# Use
prompt = validate_and_format_prompt("Your very long prompt here...")
</code></pre>
<h3 id="6-model-selection-guide"><a class="header" href="#6-model-selection-guide">6. Model Selection Guide</a></h3>
<pre><code class="language-python">def select_model(task_type: str, hardware: dict) -&gt; str:
    """Recommend model based on task and hardware"""
    vram_gb = hardware.get("vram_gb", 0)

    task_recommendations = {
        "math": {
            "min_quality": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
            "high_quality": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"
        },
        "coding": {
            "min_quality": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
            "high_quality": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B"
        },
        "reasoning": {
            "min_quality": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",
            "high_quality": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"
        }
    }

    # Select based on VRAM
    if vram_gb &lt; 8:
        return "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"
    elif vram_gb &lt; 16:
        return task_recommendations.get(task_type, {}).get("min_quality", "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B")
    elif vram_gb &lt; 80:
        return task_recommendations.get(task_type, {}).get("high_quality", "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B")
    else:
        return "deepseek-ai/DeepSeek-R1-Distill-Llama-70B"

# Example
recommended = select_model("math", {"vram_gb": 24})
print(f"Recommended model: {recommended}")
</code></pre>
<h3 id="7-monitoring--logging"><a class="header" href="#7-monitoring--logging">7. Monitoring &amp; Logging</a></h3>
<pre><code class="language-python">import time
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def generate_with_metrics(prompt: str) -&gt; dict:
    """Generate with performance metrics"""
    start_time = time.time()

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    input_tokens = len(inputs["input_ids"][0])

    logger.info(f"Input tokens: {input_tokens}")

    gen_start = time.time()
    outputs = model.generate(
        **inputs,
        max_new_tokens=1024,
        temperature=0.6
    )
    gen_time = time.time() - gen_start

    result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    output_tokens = len(outputs[0])

    total_time = time.time() - start_time
    tokens_per_second = output_tokens / gen_time if gen_time &gt; 0 else 0

    metrics = {
        "response": result,
        "input_tokens": input_tokens,
        "output_tokens": output_tokens,
        "generation_time": gen_time,
        "total_time": total_time,
        "tokens_per_second": tokens_per_second
    }

    logger.info(f"Generation metrics: {metrics}")

    return metrics
</code></pre>
<h2 id="resources"><a class="header" href="#resources">Resources</a></h2>
<h3 id="official"><a class="header" href="#official">Official</a></h3>
<ul>
<li><a href="https://github.com/deepseek-ai/DeepSeek-R1">DeepSeek GitHub</a></li>
<li><a href="https://huggingface.co/deepseek-ai">Hugging Face Models</a></li>
<li><a href="https://www.deepseek.com/">DeepSeek Website</a></li>
<li><a href="https://arxiv.org/abs/2501.12948">Research Paper</a></li>
</ul>
<h3 id="model-hubs"><a class="header" href="#model-hubs">Model Hubs</a></h3>
<ul>
<li><a href="https://ollama.com/library/deepseek-r1">Ollama Library</a></li>
<li><a href="https://www.together.ai/">Together.ai</a></li>
<li><a href="https://fireworks.ai/">Fireworks.ai</a></li>
</ul>
<h3 id="tools--libraries"><a class="header" href="#tools--libraries">Tools &amp; Libraries</a></h3>
<ul>
<li><a href="https://github.com/vllm-project/vllm">vLLM</a> - Fast inference</li>
<li><a href="https://github.com/sgl-project/sglang">SGLang</a> - Efficient serving</li>
<li><a href="https://github.com/OpenAccess-AI-Collective/axolotl">Axolotl</a> - Fine-tuning</li>
<li><a href="https://github.com/huggingface/peft">PEFT</a> - Parameter-efficient training</li>
<li><a href="https://github.com/outlines-dev/outlines">Outlines</a> - Structured generation</li>
</ul>
<h3 id="tutorials--guides"><a class="header" href="#tutorials--guides">Tutorials &amp; Guides</a></h3>
<ul>
<li><a href="https://docs.together.ai/docs/prompting-deepseek-r1">Together.ai Prompting Guide</a></li>
<li><a href="https://www.datacamp.com/tutorial/deepseek-r1-ollama">DataCamp R1 Guide</a></li>
<li><a href="https://techcommunity.microsoft.com/blog/machinelearningblog/fine-tuning-deepseek-r1-distill-llama-8b-with-pytorch-fsdp-qlora-on-azure-machin/4377965">Microsoft Azure Fine-tuning</a></li>
</ul>
<h3 id="community"><a class="header" href="#community">Community</a></h3>
<ul>
<li><a href="https://huggingface.co/deepseek-ai">Hugging Face Forums</a></li>
<li>r/LocalLLaMA</li>
<li><a href="https://github.com/deepseek-ai/DeepSeek-R1/discussions">GitHub Discussions</a></li>
</ul>
<h2 id="conclusion"><a class="header" href="#conclusion">Conclusion</a></h2>
<p>DeepSeek R1 represents a milestone in open-source AI, bringing advanced reasoning capabilities to the community. Its MIT license, competitive performance, and range of model sizes make it suitable for everything from edge deployment to production-scale applications.</p>
<p><strong>Key Takeaways:</strong></p>
<ul>
<li><strong>Start Small</strong>: Test with 1.5B-7B distilled models first</li>
<li><strong>Use Ollama</strong>: Easiest way to get started locally</li>
<li><strong>Simple Prompts</strong>: Avoid few-shot examples and explicit CoT</li>
<li><strong>Temperature 0.6</strong>: Critical for preventing repetition loops</li>
<li><strong>No System Prompts</strong>: Put all instructions in user messages</li>
<li><strong>LoRA for Fine-tuning</strong>: Parameter-efficient customization</li>
<li><strong>vLLM for Production</strong>: Fast, scalable inference serving</li>
<li><strong>Monitor Performance</strong>: Track tokens/sec and memory usage</li>
</ul>
<p>The model’s native reasoning capabilities, combined with its open-source nature, make it an excellent choice for applications requiring complex problem-solving, mathematical reasoning, code generation, and logical analysis.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../ai/fine_tuning.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256 246.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"/></svg></span>
                            </a>

                            <a rel="next prefetch" href="../ai/whisper.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M278.6 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L210.7 256 73.4 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"/></svg></span>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../ai/fine_tuning.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256 246.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"/></svg></span>
                    </a>

                    <a rel="next prefetch" href="../ai/whisper.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M278.6 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L210.7 256 73.4 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"/></svg></span>
                    </a>
            </nav>

        </div>

        <template id=fa-eye><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M288 32c-80.8 0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7 0 24.6C17.3 304 48.6 356 95.4 399.4C142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144s64.5-144 144-144s144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64c-11.5 0-22.3-3-31.6-8.4c-.2 2.8-.4 5.5-.4 8.4c0 53 43 96 96 96s96-43 96-96s-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6z"/></svg></span></template>
        <template id=fa-eye-slash><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M38.8 5.1C28.4-3.1 13.3-1.2 5.1 9.2S-1.2 34.7 9.2 42.9l592 464c10.4 8.2 25.5 6.3 33.7-4.1s6.3-25.5-4.1-33.7L525.6 386.7c39.6-40.6 66.4-86.1 79.9-118.4c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C465.5 68.8 400.8 32 320 32c-68.2 0-125 26.3-169.3 60.8L38.8 5.1zM223.1 149.5C248.6 126.2 282.7 112 320 112c79.5 0 144 64.5 144 144c0 24.9-6.3 48.3-17.4 68.7L408 294.5c5.2-11.8 8-24.8 8-38.5c0-53-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6c0 10.2-2.4 19.8-6.6 28.3l-90.3-70.8zm223.1 298L373 389.9c-16.4 6.5-34.3 10.1-53 10.1c-79.5 0-144-64.5-144-144c0-6.9 .5-13.6 1.4-20.2L83.1 161.5C60.3 191.2 44 220.8 34.5 243.7c-3.3 7.9-3.3 16.7 0 24.6c14.9 35.7 46.2 87.7 93 131.1C174.5 443.2 239.2 480 320 480c47.8 0 89.9-12.9 126.2-32.5z"/></svg></span></template>
        <template id=fa-copy><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M502.6 70.63l-61.25-61.25C435.4 3.371 427.2 0 418.7 0H255.1c-35.35 0-64 28.66-64 64l.0195 256C192 355.4 220.7 384 256 384h192c35.2 0 64-28.8 64-64V93.25C512 84.77 508.6 76.63 502.6 70.63zM464 320c0 8.836-7.164 16-16 16H255.1c-8.838 0-16-7.164-16-16L239.1 64.13c0-8.836 7.164-16 16-16h128L384 96c0 17.67 14.33 32 32 32h47.1V320zM272 448c0 8.836-7.164 16-16 16H63.1c-8.838 0-16-7.164-16-16L47.98 192.1c0-8.836 7.164-16 16-16H160V128H63.99c-35.35 0-64 28.65-64 64l.0098 256C.002 483.3 28.66 512 64 512h192c35.2 0 64-28.8 64-64v-32h-47.1L272 448z"/></svg></span></template>
        <template id=fa-play><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M73 39c-14.8-9.1-33.4-9.4-48.5-.9S0 62.6 0 80V432c0 17.4 9.4 33.4 24.5 41.9s33.7 8.1 48.5-.9L361 297c14.3-8.7 23-24.2 23-41s-8.7-32.2-23-41L73 39z"/></svg></span></template>
        <template id=fa-clock-rotate-left><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M75 75L41 41C25.9 25.9 0 36.6 0 57.9V168c0 13.3 10.7 24 24 24H134.1c21.4 0 32.1-25.9 17-41l-30.8-30.8C155 85.5 203 64 256 64c106 0 192 86 192 192s-86 192-192 192c-40.8 0-78.6-12.7-109.7-34.4c-14.5-10.1-34.4-6.6-44.6 7.9s-6.6 34.4 7.9 44.6C151.2 495 201.7 512 256 512c141.4 0 256-114.6 256-256S397.4 0 256 0C185.3 0 121.3 28.7 75 75zm181 53c-13.3 0-24 10.7-24 24V256c0 6.4 2.5 12.5 7 17l72 72c9.4 9.4 24.6 9.4 33.9 0s9.4-24.6 0-33.9l-65-65V152c0-13.3-10.7-24-24-24z"/></svg></span></template>



        <script>
            window.playground_copyable = true;
        </script>


        <script src="../elasticlunr-ef4e11c1.min.js"></script>
        <script src="../mark-09e88c2c.min.js"></script>
        <script src="../searcher-c2a407aa.js"></script>

        <script src="../clipboard-1626706a.min.js"></script>
        <script src="../highlight-abc7f01d.js"></script>
        <script src="../book-a0b12cfe.js"></script>

        <!-- Custom JS scripts -->



    </div>
    </body>
</html>
