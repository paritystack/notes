<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Pandas - My Notes</title>


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon-de23e50b.svg">
        <link rel="shortcut icon" href="../favicon-8114d1fc.png">
        <link rel="stylesheet" href="../css/variables-8adf115d.css">
        <link rel="stylesheet" href="../css/general-2459343d.css">
        <link rel="stylesheet" href="../css/chrome-ae938929.css">
        <link rel="stylesheet" href="../css/print-9e4910d8.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../fonts/fonts-9644e21d.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="mdbook-highlight-css" href="../highlight-493f70e1.css">
        <link rel="stylesheet" id="mdbook-tomorrow-night-css" href="../tomorrow-night-4c0ae647.css">
        <link rel="stylesheet" id="mdbook-ayu-highlight-css" href="../ayu-highlight-3fdfc3ac.css">

        <!-- Custom theme stylesheets -->

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "../";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "../searchindex-93ad08a6.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../toc-f9b1048b.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="mdbook-body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="mdbook-sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("mdbook-sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="mdbook-sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="../toc.html"></iframe>
            </noscript>
            <div id="mdbook-sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="mdbook-page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="mdbook-menu-bar-hover-placeholder"></div>
                <div id="mdbook-menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="mdbook-sidebar-toggle" class="icon-button" for="mdbook-sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="mdbook-sidebar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"/></svg></span>
                        </label>
                        <button id="mdbook-theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="mdbook-theme-list">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M371.3 367.1c27.3-3.9 51.9-19.4 67.2-42.9L600.2 74.1c12.6-19.5 9.4-45.3-7.6-61.2S549.7-4.4 531.1 9.6L294.4 187.2c-24 18-38.2 46.1-38.4 76.1L371.3 367.1zm-19.6 25.4l-116-104.4C175.9 290.3 128 339.6 128 400c0 3.9 .2 7.8 .6 11.6c1.8 17.5-10.2 36.4-27.8 36.4H96c-17.7 0-32 14.3-32 32s14.3 32 32 32H240c61.9 0 112-50.1 112-112c0-2.5-.1-5-.2-7.5z"/></svg></span>
                        </button>
                        <ul id="mdbook-theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-ayu">Ayu</button></li>
                        </ul>
                        <button id="mdbook-search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="mdbook-searchbar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M416 208c0 45.9-14.9 88.3-40 122.7L502.6 457.4c12.5 12.5 12.5 32.8 0 45.3s-32.8 12.5-45.3 0L330.7 376c-34.4 25.2-76.8 40-122.7 40C93.1 416 0 322.9 0 208S93.1 0 208 0S416 93.1 416 208zM208 352c79.5 0 144-64.5 144-144s-64.5-144-144-144S64 128.5 64 208s64.5 144 144 144z"/></svg></span>
                        </button>
                    </div>

                    <h1 class="menu-title">My Notes</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <span class=fa-svg id="print-button"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M128 0C92.7 0 64 28.7 64 64v96h64V64H354.7L384 93.3V160h64V93.3c0-17-6.7-33.3-18.7-45.3L400 18.7C388 6.7 371.7 0 354.7 0H128zM384 352v32 64H128V384 368 352H384zm64 32h32c17.7 0 32-14.3 32-32V256c0-35.3-28.7-64-64-64H64c-35.3 0-64 28.7-64 64v96c0 17.7 14.3 32 32 32H64v64c0 35.3 28.7 64 64 64H384c35.3 0 64-28.7 64-64V384zm-16-88c-13.3 0-24-10.7-24-24s10.7-24 24-24s24 10.7 24 24s-10.7 24-24 24z"/></svg></span>
                        </a>

                    </div>
                </div>

                <div id="mdbook-search-wrapper" class="hidden">
                    <form id="mdbook-searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="mdbook-searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="mdbook-searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <span class=fa-svg id="fa-spin"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M304 48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zm0 416c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM48 304c26.5 0 48-21.5 48-48s-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48zm464-48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM142.9 437c18.7-18.7 18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zm0-294.2c18.7-18.7 18.7-49.1 0-67.9S93.7 56.2 75 75s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zM369.1 437c18.7 18.7 49.1 18.7 67.9 0s18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9z"/></svg></span>
                            </div>
                        </div>
                    </form>
                    <div id="mdbook-searchresults-outer" class="searchresults-outer hidden">
                        <div id="mdbook-searchresults-header" class="searchresults-header"></div>
                        <ul id="mdbook-searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('mdbook-sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('mdbook-sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#mdbook-sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="mdbook-content" class="content">
                    <main>
                        <h1 id="pandas-complete-guide-for-data-analysis"><a class="header" href="#pandas-complete-guide-for-data-analysis">Pandas: Complete Guide for Data Analysis</a></h1>
<p>Pandas is the essential library for data manipulation and analysis in Python. Built on top of NumPy, it provides powerful, flexible data structures and data analysis tools for working with structured data.</p>
<h2 id="table-of-contents"><a class="header" href="#table-of-contents">Table of Contents</a></h2>
<ul>
<li><a href="#core-concepts">Core Concepts</a></li>
<li><a href="#data-structures-series--dataframe">Data Structures: Series &amp; DataFrame</a></li>
<li><a href="#data-creation--io">Data Creation &amp; I/O</a></li>
<li><a href="#indexing-and-selection">Indexing and Selection</a></li>
<li><a href="#data-cleaning">Data Cleaning</a></li>
<li><a href="#data-transformation">Data Transformation</a></li>
<li><a href="#groupby-operations">GroupBy Operations</a></li>
<li><a href="#merging-joining--concatenation">Merging, Joining &amp; Concatenation</a></li>
<li><a href="#reshaping-data">Reshaping Data</a></li>
<li><a href="#time-series">Time Series</a></li>
<li><a href="#string-operations">String Operations</a></li>
<li><a href="#categorical-data">Categorical Data</a></li>
<li><a href="#window-functions">Window Functions</a></li>
<li><a href="#performance-optimization">Performance Optimization</a></li>
<li><a href="#mldata-science-patterns">ML/Data Science Patterns</a></li>
<li><a href="#integration-patterns">Integration Patterns</a></li>
<li><a href="#best-practices">Best Practices</a></li>
</ul>
<hr>
<h2 id="core-concepts"><a class="header" href="#core-concepts">Core Concepts</a></h2>
<h3 id="why-pandas"><a class="header" href="#why-pandas">Why Pandas?</a></h3>
<p><strong>Labeled Data</strong>: Unlike NumPy, pandas provides explicit index/column labels, making data self-documenting and easier to manipulate.</p>
<p><strong>Heterogeneous Data</strong>: DataFrames can hold different data types in different columns (strings, integers, floats, dates, etc.).</p>
<p><strong>Missing Data Handling</strong>: Built-in support for missing data (NaN, None) with powerful tools for detection and handling.</p>
<p><strong>Relational Operations</strong>: SQL-like operations (join, merge, group by) built into the API.</p>
<p><strong>Time Series</strong>: Powerful date/time functionality with frequency-based indexing.</p>
<pre><code class="language-python">import pandas as pd
import numpy as np

# The pandas way
df = pd.DataFrame({
    'name': ['Alice', 'Bob', 'Charlie'],
    'age': [25, 30, 35],
    'salary': [50000, 60000, 70000]
})

# Labeled access
df['name']  # Get column by name
df.loc[0]   # Get row by label

# vs NumPy (unlabeled)
arr = np.array([[25, 50000], [30, 60000], [35, 70000]])
arr[:, 0]   # Get column by position (what does column 0 mean?)
</code></pre>
<h3 id="key-design-principles"><a class="header" href="#key-design-principles">Key Design Principles</a></h3>
<ol>
<li><strong>Explicit is better than implicit</strong>: Use <code>.loc[]</code> and <code>.iloc[]</code> for clarity</li>
<li><strong>Chaining</strong>: Operations can be chained for readable data pipelines</li>
<li><strong>Copy vs View</strong>: Be aware of when operations create copies vs views</li>
<li><strong>Vectorization</strong>: Avoid loops, use vectorized operations</li>
<li><strong>Index matters</strong>: The index is a first-class citizen in pandas</li>
</ol>
<hr>
<h2 id="data-structures-series--dataframe"><a class="header" href="#data-structures-series--dataframe">Data Structures: Series &amp; DataFrame</a></h2>
<h3 id="series-1d-labeled-array"><a class="header" href="#series-1d-labeled-array">Series: 1D Labeled Array</a></h3>
<pre><code class="language-python"># Creating Series
s = pd.Series([1, 2, 3, 4, 5])
print(s)
# 0    1
# 1    2
# 2    3
# 3    4
# 4    5
# dtype: int64

# Custom index
s = pd.Series([1, 2, 3], index=['a', 'b', 'c'])
print(s['a'])  # 1

# From dictionary
data = {'a': 1, 'b': 2, 'c': 3}
s = pd.Series(data)

# Series attributes
s.values    # Underlying NumPy array
s.index     # Index object
s.dtype     # Data type
s.shape     # Shape tuple
s.size      # Number of elements
s.name      # Series name
</code></pre>
<h3 id="series-operations"><a class="header" href="#series-operations">Series Operations</a></h3>
<pre><code class="language-python">s = pd.Series([1, 2, 3, 4, 5])

# Vectorized operations
s + 10      # Add 10 to all elements
s * 2       # Multiply by 2
s ** 2      # Square all elements
np.sqrt(s)  # NumPy functions work

# Statistical methods
s.mean()
s.std()
s.median()
s.quantile(0.75)
s.sum()
s.cumsum()  # Cumulative sum
s.min(), s.max()

# Boolean operations
s[s &gt; 2]         # Filter
s.between(2, 4)  # Values between 2 and 4
s.isin([1, 3, 5])  # Check membership
</code></pre>
<h3 id="dataframe-2d-labeled-array"><a class="header" href="#dataframe-2d-labeled-array">DataFrame: 2D Labeled Array</a></h3>
<pre><code class="language-python"># Creating DataFrames
df = pd.DataFrame({
    'A': [1, 2, 3, 4],
    'B': [10, 20, 30, 40],
    'C': ['w', 'x', 'y', 'z']
})

# From list of dictionaries
data = [
    {'name': 'Alice', 'age': 25, 'city': 'NYC'},
    {'name': 'Bob', 'age': 30, 'city': 'LA'}
]
df = pd.DataFrame(data)

# From NumPy array
arr = np.random.randn(4, 3)
df = pd.DataFrame(arr,
                  columns=['A', 'B', 'C'],
                  index=['row1', 'row2', 'row3', 'row4'])

# From dict of Series
df = pd.DataFrame({
    'A': pd.Series([1, 2, 3]),
    'B': pd.Series([4, 5, 6])
})
</code></pre>
<h3 id="dataframe-attributes-and-methods"><a class="header" href="#dataframe-attributes-and-methods">DataFrame Attributes and Methods</a></h3>
<pre><code class="language-python"># Basic information
df.shape        # (rows, columns)
df.size         # Total elements
df.columns      # Column names
df.index        # Row index
df.dtypes       # Data types of each column
df.info()       # Summary information
df.describe()   # Statistical summary

# Quick views
df.head(n=5)    # First n rows
df.tail(n=5)    # Last n rows
df.sample(n=5)  # Random n rows

# Column access
df['A']         # Returns Series
df[['A', 'B']]  # Returns DataFrame
df.A            # Attribute access (only if valid Python identifier)
</code></pre>
<h3 id="index-object"><a class="header" href="#index-object">Index Object</a></h3>
<pre><code class="language-python"># Creating custom index
df = pd.DataFrame({'A': [1, 2, 3]},
                  index=['row1', 'row2', 'row3'])

# Index operations
df.index = ['a', 'b', 'c']  # Set new index
df.reset_index()             # Reset to default integer index
df.reset_index(drop=True)    # Reset without keeping old index
df.set_index('column_name')  # Set column as index

# Multi-level index (hierarchical)
arrays = [
    ['A', 'A', 'B', 'B'],
    [1, 2, 1, 2]
]
index = pd.MultiIndex.from_arrays(arrays, names=['letter', 'number'])
df = pd.DataFrame({'value': [10, 20, 30, 40]}, index=index)

# Accessing multi-index
df.loc['A']           # All rows where first level is 'A'
df.loc[('A', 1)]      # Specific row
df.xs('A', level=0)   # Cross-section
</code></pre>
<hr>
<h2 id="data-creation--io"><a class="header" href="#data-creation--io">Data Creation &amp; I/O</a></h2>
<h3 id="reading-data-from-files"><a class="header" href="#reading-data-from-files">Reading Data from Files</a></h3>
<pre><code class="language-python"># CSV
df = pd.read_csv('data.csv')
df = pd.read_csv('data.csv',
                 sep=';',              # Delimiter
                 header=0,             # Row for column names
                 index_col=0,          # Column to use as index
                 usecols=['A', 'B'],   # Columns to read
                 dtype={'A': int},     # Specify dtypes
                 parse_dates=['date'], # Parse dates
                 na_values=['?', 'N/A'],  # Additional NA values
                 encoding='utf-8',     # Character encoding
                 nrows=1000)           # Read first 1000 rows

# Excel
df = pd.read_excel('data.xlsx', sheet_name='Sheet1')
df = pd.read_excel('data.xlsx', sheet_name=0)  # First sheet

# JSON
df = pd.read_json('data.json')
df = pd.read_json('data.json', orient='records')  # List of dicts
df = pd.read_json('data.json', orient='index')    # Dict of dicts

# SQL
import sqlite3
conn = sqlite3.connect('database.db')
df = pd.read_sql('SELECT * FROM table', conn)
df = pd.read_sql_query('SELECT * FROM table WHERE id &gt; 10', conn)
df = pd.read_sql_table('table_name', conn)

# Parquet (efficient columnar format)
df = pd.read_parquet('data.parquet')

# HTML tables
dfs = pd.read_html('https://example.com/page.html')  # Returns list

# Clipboard
df = pd.read_clipboard()  # Read from clipboard
</code></pre>
<h3 id="writing-data-to-files"><a class="header" href="#writing-data-to-files">Writing Data to Files</a></h3>
<pre><code class="language-python"># CSV
df.to_csv('output.csv', index=False)
df.to_csv('output.csv',
          sep='\t',              # Tab-separated
          columns=['A', 'B'],    # Select columns
          header=True,           # Include header
          index=True,            # Include index
          encoding='utf-8')

# Excel
df.to_excel('output.xlsx', sheet_name='Sheet1', index=False)

# Multiple sheets
with pd.ExcelWriter('output.xlsx') as writer:
    df1.to_excel(writer, sheet_name='Sheet1')
    df2.to_excel(writer, sheet_name='Sheet2')

# JSON
df.to_json('output.json', orient='records', indent=2)

# SQL
df.to_sql('table_name', conn, if_exists='replace', index=False)
# if_exists: 'fail', 'replace', 'append'

# Parquet
df.to_parquet('output.parquet', compression='gzip')

# HTML
df.to_html('output.html', index=False)

# Pickle (preserves all pandas types)
df.to_pickle('data.pkl')
df = pd.read_pickle('data.pkl')
</code></pre>
<h3 id="creating-dataframes-from-scratch"><a class="header" href="#creating-dataframes-from-scratch">Creating DataFrames from Scratch</a></h3>
<pre><code class="language-python"># From dictionary
df = pd.DataFrame({
    'A': [1, 2, 3],
    'B': [4, 5, 6]
})

# From list of lists
data = [[1, 4], [2, 5], [3, 6]]
df = pd.DataFrame(data, columns=['A', 'B'])

# From NumPy array
arr = np.random.randn(100, 5)
df = pd.DataFrame(arr, columns=list('ABCDE'))

# Empty DataFrame with schema
df = pd.DataFrame(columns=['name', 'age', 'city'])

# Date range DataFrame
dates = pd.date_range('2023-01-01', periods=100, freq='D')
df = pd.DataFrame({'date': dates, 'value': np.random.randn(100)})

# From records
records = [
    (1, 'Alice', 25),
    (2, 'Bob', 30),
    (3, 'Charlie', 35)
]
df = pd.DataFrame.from_records(records, columns=['id', 'name', 'age'])
</code></pre>
<hr>
<h2 id="indexing-and-selection"><a class="header" href="#indexing-and-selection">Indexing and Selection</a></h2>
<h3 id="column-selection"><a class="header" href="#column-selection">Column Selection</a></h3>
<pre><code class="language-python">df = pd.DataFrame({
    'A': [1, 2, 3, 4],
    'B': [10, 20, 30, 40],
    'C': [100, 200, 300, 400]
})

# Single column (returns Series)
df['A']
df.A  # Only if column name is valid Python identifier

# Multiple columns (returns DataFrame)
df[['A', 'B']]

# Column slicing (by position, not label!)
df.iloc[:, 0:2]  # First two columns

# Select columns by type
df.select_dtypes(include=['int64'])
df.select_dtypes(exclude=['object'])
df.select_dtypes(include=[np.number])  # All numeric

# Filter columns by name pattern
df.filter(like='col')       # Contains 'col'
df.filter(regex='^col')     # Starts with 'col'
df.filter(items=['A', 'B']) # Exact names
</code></pre>
<h3 id="row-selection"><a class="header" href="#row-selection">Row Selection</a></h3>
<pre><code class="language-python"># By position (integer-location based)
df.iloc[0]        # First row
df.iloc[-1]       # Last row
df.iloc[0:3]      # First three rows
df.iloc[[0, 2]]   # First and third rows

# By label (label-based)
df.loc[0]         # Row with index label 0
df.loc[0:2]       # INCLUSIVE slicing!
df.loc[['a', 'c']]  # Specific labels

# Boolean indexing
df[df['A'] &gt; 2]
df[df['B'].isin([10, 30])]
df[(df['A'] &gt; 2) &amp; (df['B'] &lt; 40)]  # Multiple conditions
df[df['A'].between(2, 3)]

# Query method (SQL-like)
df.query('A &gt; 2')
df.query('A &gt; 2 and B &lt; 40')
df.query('A in [1, 3]')
</code></pre>
<h3 id="combined-selection-loc-and-iloc"><a class="header" href="#combined-selection-loc-and-iloc">Combined Selection: loc and iloc</a></h3>
<pre><code class="language-python"># loc: [rows, columns] by label
df.loc[0, 'A']              # Single value
df.loc[0:2, ['A', 'B']]     # Rows 0-2, columns A and B
df.loc[:, 'A':'C']          # All rows, columns A through C
df.loc[df['A'] &gt; 2, 'B']    # Boolean row selection, column B

# iloc: [rows, columns] by integer position
df.iloc[0, 0]               # First row, first column
df.iloc[0:2, 0:2]           # First 2 rows, first 2 columns
df.iloc[:, [0, 2]]          # All rows, first and third columns
df.iloc[df['A'].values &gt; 2, 1]  # Boolean with iloc (convert to bool array)

# at and iat: Fast scalar access
df.at[0, 'A']    # By label (faster than loc for scalars)
df.iat[0, 0]     # By position (faster than iloc for scalars)
</code></pre>
<h3 id="boolean-indexing-deep-dive"><a class="header" href="#boolean-indexing-deep-dive">Boolean Indexing Deep Dive</a></h3>
<pre><code class="language-python"># Simple boolean masks
mask = df['A'] &gt; 2
df[mask]

# Compound conditions (use &amp; | ~ not 'and' 'or' 'not')
df[(df['A'] &gt; 2) &amp; (df['B'] &lt; 40)]   # AND
df[(df['A'] &gt; 2) | (df['B'] &lt; 40)]   # OR
df[~(df['A'] &gt; 2)]                    # NOT

# Using isin for membership
df[df['A'].isin([1, 3, 5])]

# String methods
df[df['name'].str.contains('Alice')]
df[df['name'].str.startswith('A')]

# Null checks
df[df['A'].isna()]
df[df['A'].notna()]

# Multiple column conditions
df[df[['A', 'B']].apply(lambda x: x.sum() &gt; 50, axis=1)]

# Query with variables
threshold = 2
df.query('A &gt; @threshold')  # @ for external variables
</code></pre>
<h3 id="advanced-indexing"><a class="header" href="#advanced-indexing">Advanced Indexing</a></h3>
<pre><code class="language-python"># Fancy indexing with lists
rows = [0, 2, 4]
cols = ['A', 'C']
df.loc[rows, cols]

# Boolean indexing with assignment
df.loc[df['A'] &gt; 2, 'B'] = 999  # Set values where condition is True

# MultiIndex selection
df.loc[('A', 1), :]  # Tuple for multi-level index
df.xs('A', level=0)   # Cross-section

# IndexSlice for complex multi-index selection
idx = pd.IndexSlice
df.loc[idx[:, 'value'], :]  # All first level, 'value' in second level
</code></pre>
<hr>
<h2 id="data-cleaning"><a class="header" href="#data-cleaning">Data Cleaning</a></h2>
<h3 id="handling-missing-data"><a class="header" href="#handling-missing-data">Handling Missing Data</a></h3>
<pre><code class="language-python"># Detecting missing values
df.isna()         # Boolean DataFrame
df.isna().sum()   # Count per column
df.isna().sum().sum()  # Total missing values
df.isnull()       # Alias for isna()

# Visualize missing patterns
df.isna().sum().plot(kind='bar')

# Check if any/all missing
df.isna().any()   # Any missing per column
df.isna().all()   # All missing per column

# Drop missing values
df.dropna()                    # Drop rows with any NA
df.dropna(how='all')          # Drop rows where all values are NA
df.dropna(subset=['A', 'B'])  # Drop rows with NA in specific columns
df.dropna(axis=1)             # Drop columns with any NA
df.dropna(thresh=2)           # Keep rows with at least 2 non-NA values

# Fill missing values
df.fillna(0)                  # Fill with constant
df.fillna({'A': 0, 'B': 99})  # Different values per column
df.fillna(method='ffill')     # Forward fill
df.fillna(method='bfill')     # Backward fill
df.fillna(df.mean())          # Fill with column mean
df.fillna(df.median())        # Fill with column median
df.fillna(df.mode().iloc[0])  # Fill with mode

# Interpolate missing values
df.interpolate()                    # Linear interpolation
df.interpolate(method='polynomial', order=2)  # Polynomial
df.interpolate(method='time')       # Time-based interpolation

# Replace specific values
df.replace(0, np.nan)         # Replace 0 with NaN
df.replace([0, 1], [100, 200])  # Multiple replacements
df.replace({'A': 0}, 100)     # Column-specific replacement
</code></pre>
<h3 id="handling-duplicates"><a class="header" href="#handling-duplicates">Handling Duplicates</a></h3>
<pre><code class="language-python"># Detect duplicates
df.duplicated()                    # Boolean Series
df.duplicated().sum()              # Count duplicates
df.duplicated(subset=['A'])        # Check specific columns
df.duplicated(keep='first')        # Mark all but first as duplicate
df.duplicated(keep='last')         # Mark all but last as duplicate
df.duplicated(keep=False)          # Mark all duplicates (including first)

# Get duplicate rows
df[df.duplicated()]
df[df.duplicated(subset=['name'], keep=False)]

# Remove duplicates
df.drop_duplicates()
df.drop_duplicates(subset=['name'])  # Based on specific columns
df.drop_duplicates(keep='last')      # Keep last occurrence
df.drop_duplicates(inplace=True)     # Modify in place
</code></pre>
<h3 id="data-type-conversion"><a class="header" href="#data-type-conversion">Data Type Conversion</a></h3>
<pre><code class="language-python"># Check types
df.dtypes
df['A'].dtype

# Convert types
df['A'] = df['A'].astype(int)
df['B'] = df['B'].astype(float)
df['C'] = df['C'].astype(str)

# Convert to categorical
df['category'] = df['category'].astype('category')

# Convert to datetime
df['date'] = pd.to_datetime(df['date'])
df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')
df['date'] = pd.to_datetime(df['date'], errors='coerce')  # Invalid -&gt; NaT

# Convert to numeric (handles errors)
df['number'] = pd.to_numeric(df['number'], errors='coerce')  # Invalid -&gt; NaN
df['number'] = pd.to_numeric(df['number'], errors='ignore')  # Leave as-is

# Infer better dtypes
df = df.infer_objects()
df = df.convert_dtypes()  # Use nullable dtypes (pd.Int64, pd.StringDtype)
</code></pre>
<h3 id="string-cleaning"><a class="header" href="#string-cleaning">String Cleaning</a></h3>
<pre><code class="language-python"># Strip whitespace
df['name'] = df['name'].str.strip()
df['name'] = df['name'].str.lstrip()
df['name'] = df['name'].str.rstrip()

# Case conversion
df['name'] = df['name'].str.lower()
df['name'] = df['name'].str.upper()
df['name'] = df['name'].str.title()

# Replace patterns
df['text'] = df['text'].str.replace('old', 'new')
df['text'] = df['text'].str.replace(r'\d+', '', regex=True)  # Remove digits

# Remove special characters
df['clean'] = df['text'].str.replace(r'[^a-zA-Z0-9\s]', '', regex=True)

# Extract patterns
df['number'] = df['text'].str.extract(r'(\d+)')
df[['area', 'phone']] = df['full_phone'].str.extract(r'(\d{3})-(\d{7})')
</code></pre>
<h3 id="outlier-detection-and-handling"><a class="header" href="#outlier-detection-and-handling">Outlier Detection and Handling</a></h3>
<pre><code class="language-python"># Z-score method
from scipy import stats
z_scores = np.abs(stats.zscore(df['value']))
df_no_outliers = df[z_scores &lt; 3]

# IQR method
Q1 = df['value'].quantile(0.25)
Q3 = df['value'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
df_no_outliers = df[(df['value'] &gt;= lower_bound) &amp; (df['value'] &lt;= upper_bound)]

# Clip outliers
df['value_clipped'] = df['value'].clip(lower_bound, upper_bound)

# Winsorize
from scipy.stats.mstats import winsorize
df['value_winsorized'] = winsorize(df['value'], limits=[0.05, 0.05])
</code></pre>
<hr>
<h2 id="data-transformation"><a class="header" href="#data-transformation">Data Transformation</a></h2>
<h3 id="sorting"><a class="header" href="#sorting">Sorting</a></h3>
<pre><code class="language-python"># Sort by values
df.sort_values('A')                          # Ascending
df.sort_values('A', ascending=False)         # Descending
df.sort_values(['A', 'B'])                   # Multiple columns
df.sort_values(['A', 'B'], ascending=[True, False])  # Mixed order

# Sort by index
df.sort_index()
df.sort_index(ascending=False)

# Sort with custom key
df.sort_values('name', key=lambda x: x.str.lower())
</code></pre>
<h3 id="filtering"><a class="header" href="#filtering">Filtering</a></h3>
<pre><code class="language-python"># Boolean filtering
df[df['age'] &gt; 25]
df[df['name'].str.contains('Alice')]

# Query method
df.query('age &gt; 25')
df.query('age &gt; 25 and city == "NYC"')
df.query('name.str.contains("Alice")', engine='python')

# Using where (keeps shape, fills non-matching with NaN)
df.where(df['age'] &gt; 25)

# Using mask (opposite of where)
df.mask(df['age'] &lt;= 25)

# Filter by callable
df[lambda x: x['age'] &gt; 25]
</code></pre>
<h3 id="adding-and-removing-columns"><a class="header" href="#adding-and-removing-columns">Adding and Removing Columns</a></h3>
<pre><code class="language-python"># Add new column
df['new_col'] = 0
df['sum'] = df['A'] + df['B']
df['ratio'] = df['A'] / df['B']

# Add column from function
df['squared'] = df['A'].apply(lambda x: x ** 2)

# Add column with assign (returns new DataFrame)
df = df.assign(
    new1=lambda x: x['A'] * 2,
    new2=lambda x: x['new1'] + 10
)

# Insert column at specific position
df.insert(1, 'new_col', [1, 2, 3, 4])

# Remove columns
df.drop('col_name', axis=1)
df.drop(['col1', 'col2'], axis=1)
df.drop(columns=['col1', 'col2'])

# Remove columns in place
df.drop('col_name', axis=1, inplace=True)

# Select all except certain columns
df.drop(columns=df.columns.difference(['keep1', 'keep2']))
</code></pre>
<h3 id="renaming"><a class="header" href="#renaming">Renaming</a></h3>
<pre><code class="language-python"># Rename columns
df.rename(columns={'old': 'new'})
df.rename(columns={'A': 'col_A', 'B': 'col_B'})

# Rename with function
df.rename(columns=str.lower)
df.rename(columns=lambda x: x.replace(' ', '_'))

# Set column names directly
df.columns = ['col1', 'col2', 'col3']

# Rename index
df.rename(index={0: 'row1', 1: 'row2'})

# Add prefix/suffix
df.add_prefix('col_')
df.add_suffix('_x')
</code></pre>
<h3 id="apply-map-and-applymap"><a class="header" href="#apply-map-and-applymap">Apply, Map, and Applymap</a></h3>
<pre><code class="language-python"># apply: Apply function along axis
df['A'].apply(lambda x: x ** 2)              # Series apply
df.apply(lambda x: x.max() - x.min())        # Column-wise (axis=0, default)
df.apply(lambda x: x.max() - x.min(), axis=1)  # Row-wise

# apply with multiple return values
df.apply(lambda x: pd.Series([x.min(), x.max()]), axis=1)

# map: Element-wise transformation (Series only)
df['category'].map({'A': 1, 'B': 2, 'C': 3})
df['value'].map(lambda x: x * 2)

# applymap: Element-wise transformation (entire DataFrame) - DEPRECATED
# Use .map() instead
df.map(lambda x: x * 2)

# replace: Value replacement
df.replace({'A': {'old': 'new'}})
df['status'].replace({'active': 1, 'inactive': 0})

# Vectorized string operations (preferred over apply)
df['text'].str.upper()  # Better than df['text'].apply(str.upper)

# Vectorized operations (always prefer these)
df['A'] * 2  # Better than df['A'].apply(lambda x: x * 2)
</code></pre>
<h3 id="binning-and-discretization"><a class="header" href="#binning-and-discretization">Binning and Discretization</a></h3>
<pre><code class="language-python"># Cut: Bin continuous data into discrete intervals
ages = [1, 5, 10, 15, 20, 25, 30, 35, 40]
bins = [0, 12, 18, 60, 100]
labels = ['Child', 'Teen', 'Adult', 'Senior']
pd.cut(ages, bins=bins, labels=labels)

# Equal-width bins
pd.cut(df['value'], bins=5)  # 5 equal-width bins

# qcut: Quantile-based discretization
pd.qcut(df['value'], q=4)  # Quartiles
pd.qcut(df['value'], q=4, labels=['Q1', 'Q2', 'Q3', 'Q4'])

# Custom function for binning
def age_group(age):
    if age &lt; 18: return 'Minor'
    elif age &lt; 65: return 'Adult'
    else: return 'Senior'

df['age_group'] = df['age'].apply(age_group)

# np.select for multiple conditions
conditions = [
    df['score'] &gt;= 90,
    df['score'] &gt;= 80,
    df['score'] &gt;= 70,
    df['score'] &gt;= 60
]
choices = ['A', 'B', 'C', 'D']
df['grade'] = np.select(conditions, choices, default='F')
</code></pre>
<h3 id="rank-and-quantile"><a class="header" href="#rank-and-quantile">Rank and Quantile</a></h3>
<pre><code class="language-python"># Rank
df['rank'] = df['score'].rank()
df['rank'] = df['score'].rank(ascending=False)  # Higher score = rank 1
df['rank'] = df['score'].rank(method='dense')    # No gaps in ranking
df['rank'] = df['score'].rank(method='min')      # Ties get minimum rank
df['rank'] = df['score'].rank(pct=True)          # Percentile ranks

# Quantile
df['score'].quantile(0.25)  # 25th percentile
df['score'].quantile([0.25, 0.5, 0.75])  # Multiple quantiles
df.quantile(0.5)  # Median of all numeric columns
</code></pre>
<hr>
<h2 id="groupby-operations"><a class="header" href="#groupby-operations">GroupBy Operations</a></h2>
<h3 id="basic-groupby"><a class="header" href="#basic-groupby">Basic GroupBy</a></h3>
<pre><code class="language-python"># Group by single column
grouped = df.groupby('category')

# Group by multiple columns
grouped = df.groupby(['category', 'region'])

# Aggregation
df.groupby('category')['value'].sum()
df.groupby('category')['value'].mean()
df.groupby('category')['value'].count()
df.groupby('category').size()  # Count including NaN

# Multiple aggregations
df.groupby('category').agg({
    'value': 'sum',
    'quantity': 'mean',
    'price': ['min', 'max']
})

# Named aggregations (pandas 0.25+)
df.groupby('category').agg(
    total_value=('value', 'sum'),
    avg_quantity=('quantity', 'mean'),
    max_price=('price', 'max')
)

# Apply same aggregation to all columns
df.groupby('category').sum()
df.groupby('category').mean()
</code></pre>
<h3 id="advanced-groupby"><a class="header" href="#advanced-groupby">Advanced GroupBy</a></h3>
<pre><code class="language-python"># Custom aggregation functions
def range_func(x):
    return x.max() - x.min()

df.groupby('category')['value'].agg(range_func)
df.groupby('category')['value'].agg(['sum', 'mean', range_func])

# agg with lambda
df.groupby('category')['value'].agg(lambda x: x.max() - x.min())

# Multiple columns, multiple functions
df.groupby('category').agg({
    'value': ['sum', 'mean', 'std'],
    'quantity': ['min', 'max'],
    'price': lambda x: x.median()
})

# transform: Return same-shaped object with group-wise operations
df['value_norm'] = df.groupby('category')['value'].transform(
    lambda x: (x - x.mean()) / x.std()
)

# Group-wise centering
df['centered'] = df.groupby('category')['value'].transform(lambda x: x - x.mean())

# filter: Filter groups based on group properties
df.groupby('category').filter(lambda x: x['value'].sum() &gt; 100)
df.groupby('category').filter(lambda x: len(x) &gt;= 5)  # Groups with &gt;=5 members
</code></pre>
<h3 id="iteration-and-group-wise-operations"><a class="header" href="#iteration-and-group-wise-operations">Iteration and Group-wise Operations</a></h3>
<pre><code class="language-python"># Iterate over groups
for name, group in df.groupby('category'):
    print(f"Group: {name}")
    print(group)
    print()

# Get specific group
df.groupby('category').get_group('A')

# Apply custom function to each group
def process_group(group):
    group['normalized'] = (group['value'] - group['value'].mean()) / group['value'].std()
    return group

df = df.groupby('category').apply(process_group)

# apply vs transform
# apply: Can change shape, returns DataFrame
# transform: Must return same shape, returns Series/DataFrame aligned with input

# Cumulative operations within groups
df['cumsum'] = df.groupby('category')['value'].cumsum()
df['cummax'] = df.groupby('category')['value'].cummax()
df['rank'] = df.groupby('category')['value'].rank()
</code></pre>
<h3 id="grouping-with-bins-and-time"><a class="header" href="#grouping-with-bins-and-time">Grouping with Bins and Time</a></h3>
<pre><code class="language-python"># Group by bins
df['age_group'] = pd.cut(df['age'], bins=[0, 18, 35, 50, 100])
df.groupby('age_group')['income'].mean()

# Group by time period
df['date'] = pd.to_datetime(df['date'])
df.set_index('date').groupby(pd.Grouper(freq='M')).sum()  # Monthly
df.set_index('date').groupby(pd.Grouper(freq='W')).sum()  # Weekly
df.set_index('date').groupby(pd.Grouper(freq='Q')).sum()  # Quarterly
</code></pre>
<h3 id="pivot-table-groupby--reshape"><a class="header" href="#pivot-table-groupby--reshape">Pivot Table (GroupBy + Reshape)</a></h3>
<pre><code class="language-python"># Pivot table: GroupBy + Reshape in one operation
df.pivot_table(
    values='sales',
    index='region',
    columns='product',
    aggfunc='sum'
)

# Multiple aggregations
df.pivot_table(
    values='sales',
    index='region',
    columns='product',
    aggfunc=['sum', 'mean', 'count']
)

# Multiple values
df.pivot_table(
    values=['sales', 'profit'],
    index='region',
    columns='product',
    aggfunc='sum'
)

# With margins (totals)
df.pivot_table(
    values='sales',
    index='region',
    columns='product',
    aggfunc='sum',
    margins=True,
    margins_name='Total'
)

# Fill missing values
df.pivot_table(
    values='sales',
    index='region',
    columns='product',
    aggfunc='sum',
    fill_value=0
)
</code></pre>
<hr>
<h2 id="merging-joining--concatenation"><a class="header" href="#merging-joining--concatenation">Merging, Joining &amp; Concatenation</a></h2>
<h3 id="concatenation"><a class="header" href="#concatenation">Concatenation</a></h3>
<pre><code class="language-python"># Vertical concatenation (stacking rows)
df1 = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})
df2 = pd.DataFrame({'A': [5, 6], 'B': [7, 8]})
pd.concat([df1, df2])  # axis=0 is default

# Reset index after concat
pd.concat([df1, df2], ignore_index=True)

# Horizontal concatenation (side by side)
pd.concat([df1, df2], axis=1)

# Concatenate with keys (multi-level index)
pd.concat([df1, df2], keys=['first', 'second'])

# Only keep matching columns
pd.concat([df1, df2], join='inner')

# Keep all columns, fill with NaN
pd.concat([df1, df2], join='outer')  # Default
</code></pre>
<h3 id="merge-sql-like-joins"><a class="header" href="#merge-sql-like-joins">Merge (SQL-like joins)</a></h3>
<pre><code class="language-python">df1 = pd.DataFrame({'key': ['A', 'B', 'C'], 'value1': [1, 2, 3]})
df2 = pd.DataFrame({'key': ['B', 'C', 'D'], 'value2': [4, 5, 6]})

# Inner join (intersection)
pd.merge(df1, df2, on='key', how='inner')
# Result: B, C

# Left join (keep all from left)
pd.merge(df1, df2, on='key', how='left')
# Result: A, B, C (value2 is NaN for A)

# Right join (keep all from right)
pd.merge(df1, df2, on='key', how='right')
# Result: B, C, D (value1 is NaN for D)

# Outer join (union)
pd.merge(df1, df2, on='key', how='outer')
# Result: A, B, C, D (with NaN where no match)

# Merge on multiple columns
pd.merge(df1, df2, on=['key1', 'key2'])

# Merge with different column names
pd.merge(df1, df2, left_on='key1', right_on='key2')

# Merge on index
pd.merge(df1, df2, left_index=True, right_index=True)

# Suffixes for overlapping columns
pd.merge(df1, df2, on='key', suffixes=('_left', '_right'))

# Validate merge type
pd.merge(df1, df2, on='key', validate='one_to_one')
# Options: 'one_to_one', 'one_to_many', 'many_to_one', 'many_to_many'
</code></pre>
<h3 id="join-index-based-merge"><a class="header" href="#join-index-based-merge">Join (Index-based merge)</a></h3>
<pre><code class="language-python"># Join using index
df1.join(df2)  # Left join by default
df1.join(df2, how='inner')
df1.join(df2, how='outer')

# Join on column
df1.join(df2, on='key')

# Join with suffix
df1.join(df2, lsuffix='_left', rsuffix='_right')
</code></pre>
<h3 id="merge-strategies-for-large-data"><a class="header" href="#merge-strategies-for-large-data">Merge Strategies for Large Data</a></h3>
<pre><code class="language-python"># Indicator column (shows merge source)
result = pd.merge(df1, df2, on='key', how='outer', indicator=True)
# _merge column: 'left_only', 'right_only', 'both'

# Check for merge issues
result[result['_merge'] == 'left_only']  # Rows only in left
result[result['_merge'] == 'right_only']  # Rows only in right

# Merge with validation
try:
    pd.merge(df1, df2, on='key', validate='one_to_one')
except ValueError as e:
    print(f"Merge validation failed: {e}")
</code></pre>
<hr>
<h2 id="reshaping-data"><a class="header" href="#reshaping-data">Reshaping Data</a></h2>
<h3 id="pivot-and-melt"><a class="header" href="#pivot-and-melt">Pivot and Melt</a></h3>
<pre><code class="language-python"># Wide to long: melt
df_wide = pd.DataFrame({
    'id': [1, 2, 3],
    '2020': [100, 200, 300],
    '2021': [110, 220, 330],
    '2022': [120, 240, 360]
})

df_long = df_wide.melt(
    id_vars=['id'],
    value_vars=['2020', '2021', '2022'],
    var_name='year',
    value_name='value'
)

# Long to wide: pivot
df_wide_again = df_long.pivot(
    index='id',
    columns='year',
    values='value'
)

# pivot_table (handles duplicates with aggregation)
df_long.pivot_table(
    index='id',
    columns='year',
    values='value',
    aggfunc='mean'
)
</code></pre>
<h3 id="stack-and-unstack"><a class="header" href="#stack-and-unstack">Stack and Unstack</a></h3>
<pre><code class="language-python"># Stack: Column labels → innermost row index
df = pd.DataFrame({
    'A': [1, 2],
    'B': [3, 4]
}, index=['row1', 'row2'])

stacked = df.stack()
# Multi-index Series:
# row1  A    1
#       B    3
# row2  A    2
#       B    4

# Unstack: Row index → column labels
unstacked = stacked.unstack()  # Back to original

# Unstack specific level
multi_index_df.unstack(level=0)
multi_index_df.unstack(level='level_name')

# Fill NaN after unstack
df.unstack(fill_value=0)
</code></pre>
<h3 id="crosstab"><a class="header" href="#crosstab">Crosstab</a></h3>
<pre><code class="language-python"># Frequency table
df = pd.DataFrame({
    'gender': ['M', 'F', 'M', 'F', 'M'],
    'smoker': ['Y', 'N', 'Y', 'Y', 'N'],
    'age_group': ['Adult', 'Adult', 'Senior', 'Adult', 'Senior']
})

# Simple cross-tabulation
pd.crosstab(df['gender'], df['smoker'])

# With margins
pd.crosstab(df['gender'], df['smoker'], margins=True)

# Normalize (proportions)
pd.crosstab(df['gender'], df['smoker'], normalize='index')  # Row percentages
pd.crosstab(df['gender'], df['smoker'], normalize='columns')  # Column percentages
pd.crosstab(df['gender'], df['smoker'], normalize='all')  # Overall percentages

# With aggregation
pd.crosstab(df['gender'], df['smoker'],
            values=df['age'], aggfunc='mean')

# Multiple row/column variables
pd.crosstab([df['gender'], df['age_group']], df['smoker'])
</code></pre>
<h3 id="transpose"><a class="header" href="#transpose">Transpose</a></h3>
<pre><code class="language-python"># Swap rows and columns
df.T
df.transpose()
</code></pre>
<h3 id="explode-unnest-lists"><a class="header" href="#explode-unnest-lists">Explode (Unnest lists)</a></h3>
<pre><code class="language-python"># Expand lists in column to separate rows
df = pd.DataFrame({
    'name': ['Alice', 'Bob'],
    'hobbies': [['reading', 'swimming'], ['gaming', 'cooking', 'travel']]
})

df.explode('hobbies')
# name    hobbies
# Alice   reading
# Alice   swimming
# Bob     gaming
# Bob     cooking
# Bob     travel

# Explode multiple columns (pandas 1.3+)
df.explode(['hobbies', 'other_col'])
</code></pre>
<hr>
<h2 id="time-series"><a class="header" href="#time-series">Time Series</a></h2>
<h3 id="datetime-basics"><a class="header" href="#datetime-basics">Datetime Basics</a></h3>
<pre><code class="language-python"># Create datetime objects
pd.Timestamp('2023-01-01')
pd.Timestamp('2023-01-01 12:30:45')
pd.Timestamp(2023, 1, 1)

# Date range
dates = pd.date_range('2023-01-01', '2023-12-31', freq='D')  # Daily
dates = pd.date_range('2023-01-01', periods=100, freq='H')   # 100 hours
dates = pd.date_range('2023-01-01', periods=12, freq='MS')   # Month start

# Frequencies: 'D' (day), 'H' (hour), 'T' or 'min' (minute),
#              'S' (second), 'W' (week), 'M' (month end),
#              'MS' (month start), 'Q' (quarter), 'Y' (year)

# Business day frequencies
pd.date_range('2023-01-01', periods=10, freq='B')   # Business days
pd.date_range('2023-01-01', periods=10, freq='BM')  # Business month end

# Convert to datetime
df['date'] = pd.to_datetime(df['date'])
df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')
df['date'] = pd.to_datetime(df['date'], errors='coerce')  # Invalid → NaT
</code></pre>
<h3 id="datetime-properties-and-methods"><a class="header" href="#datetime-properties-and-methods">DateTime Properties and Methods</a></h3>
<pre><code class="language-python">df['date'] = pd.to_datetime(df['date'])

# Extract components
df['year'] = df['date'].dt.year
df['month'] = df['date'].dt.month
df['day'] = df['date'].dt.day
df['hour'] = df['date'].dt.hour
df['dayofweek'] = df['date'].dt.dayofweek  # Monday=0, Sunday=6
df['dayofyear'] = df['date'].dt.dayofyear
df['week'] = df['date'].dt.isocalendar().week
df['quarter'] = df['date'].dt.quarter

# Day name and month name
df['day_name'] = df['date'].dt.day_name()  # 'Monday', etc.
df['month_name'] = df['date'].dt.month_name()

# Boolean checks
df['is_month_start'] = df['date'].dt.is_month_start
df['is_month_end'] = df['date'].dt.is_month_end
df['is_quarter_start'] = df['date'].dt.is_quarter_start
df['is_year_start'] = df['date'].dt.is_year_start
df['is_leap_year'] = df['date'].dt.is_leap_year

# Time deltas
df['days_since'] = (pd.Timestamp.now() - df['date']).dt.days
</code></pre>
<h3 id="time-series-indexing"><a class="header" href="#time-series-indexing">Time Series Indexing</a></h3>
<pre><code class="language-python"># Set datetime as index
df = df.set_index('date')

# Sort by datetime index
df = df.sort_index()

# Select by date
df.loc['2023-01-01']
df.loc['2023-01']  # Entire month
df.loc['2023']     # Entire year
df.loc['2023-01':'2023-06']  # Date range

# Boolean filtering
df[df.index.year == 2023]
df[df.index.month == 1]
df[(df.index &gt;= '2023-01-01') &amp; (df.index &lt; '2023-02-01')]

# Truncate
df.truncate(before='2023-01-01', after='2023-12-31')
</code></pre>
<h3 id="resampling"><a class="header" href="#resampling">Resampling</a></h3>
<pre><code class="language-python"># Downsampling (high freq → low freq)
df_daily = df.resample('D').sum()     # Daily sum
df_weekly = df.resample('W').mean()   # Weekly mean
df_monthly = df.resample('M').agg({
    'value': 'sum',
    'quantity': 'mean'
})

# Upsampling (low freq → high freq)
df_hourly = df.resample('H').ffill()  # Forward fill
df_hourly = df.resample('H').interpolate()  # Interpolate

# Custom aggregation
df.resample('M').agg({
    'open': 'first',
    'high': 'max',
    'low': 'min',
    'close': 'last',
    'volume': 'sum'
})

# OHLC resampling (financial data)
df.resample('D').ohlc()
</code></pre>
<h3 id="rolling-windows"><a class="header" href="#rolling-windows">Rolling Windows</a></h3>
<pre><code class="language-python"># Simple moving average
df['MA_7'] = df['value'].rolling(window=7).mean()

# Multiple aggregations
df['rolling_stats'] = df['value'].rolling(window=7).agg(['mean', 'std', 'min', 'max'])

# Centered window
df['MA_centered'] = df['value'].rolling(window=7, center=True).mean()

# Minimum periods (requires at least N non-NaN values)
df['MA'] = df['value'].rolling(window=7, min_periods=1).mean()

# Custom function
df['custom'] = df['value'].rolling(window=7).apply(lambda x: x.max() - x.min())

# Exponentially weighted moving average
df['EMA'] = df['value'].ewm(span=7).mean()
df['EMA'] = df['value'].ewm(alpha=0.3).mean()  # Decay factor
</code></pre>
<h3 id="shifting-and-lagging"><a class="header" href="#shifting-and-lagging">Shifting and Lagging</a></h3>
<pre><code class="language-python"># Shift forward (lag)
df['value_lag1'] = df['value'].shift(1)
df['value_lag7'] = df['value'].shift(7)

# Shift backward (lead)
df['value_lead1'] = df['value'].shift(-1)

# Shift with custom frequency (datetime index)
df['value_prev_month'] = df['value'].shift(1, freq='M')

# Percent change
df['pct_change'] = df['value'].pct_change()
df['pct_change_7d'] = df['value'].pct_change(periods=7)

# Difference
df['diff'] = df['value'].diff()
df['diff_7d'] = df['value'].diff(periods=7)
</code></pre>
<h3 id="time-zones"><a class="header" href="#time-zones">Time Zones</a></h3>
<pre><code class="language-python"># Localize (add timezone to naive datetime)
df.index = df.index.tz_localize('UTC')
df.index = df.index.tz_localize('US/Eastern')

# Convert timezone
df.index = df.index.tz_convert('Asia/Tokyo')

# Check timezone
df.index.tz

# Remove timezone
df.index = df.index.tz_localize(None)
</code></pre>
<hr>
<h2 id="string-operations"><a class="header" href="#string-operations">String Operations</a></h2>
<h3 id="basic-string-methods"><a class="header" href="#basic-string-methods">Basic String Methods</a></h3>
<pre><code class="language-python"># All string methods accessed via .str
df['text'].str.lower()
df['text'].str.upper()
df['text'].str.title()
df['text'].str.capitalize()
df['text'].str.swapcase()

# Strip whitespace
df['text'].str.strip()
df['text'].str.lstrip()
df['text'].str.rstrip()

# Length
df['text'].str.len()

# Concatenation
df['full_name'] = df['first_name'].str.cat(df['last_name'], sep=' ')

# Repeat
df['repeated'] = df['text'].str.repeat(3)
</code></pre>
<h3 id="string-searching-and-matching"><a class="header" href="#string-searching-and-matching">String Searching and Matching</a></h3>
<pre><code class="language-python"># Contains
df[df['text'].str.contains('keyword')]
df[df['text'].str.contains('keyword', case=False)]  # Case-insensitive
df[df['text'].str.contains('key|word', regex=True)]  # OR pattern

# Startswith / Endswith
df[df['text'].str.startswith('prefix')]
df[df['text'].str.endswith('suffix')]

# Match (requires pattern at start)
df['text'].str.match(r'^\d+')  # Starts with digits

# Find position
df['text'].str.find('substring')  # Returns index or -1
df['text'].str.rfind('substring')  # Find from right

# Count occurrences
df['text'].str.count('pattern')
</code></pre>
<h3 id="string-replacement"><a class="header" href="#string-replacement">String Replacement</a></h3>
<pre><code class="language-python"># Simple replacement
df['text'].str.replace('old', 'new')
df['text'].str.replace('old', 'new', case=False)

# Regex replacement
df['text'].str.replace(r'\d+', '', regex=True)  # Remove all digits
df['text'].str.replace(r'\s+', ' ', regex=True)  # Normalize whitespace

# Multiple replacements
df['text'].str.replace({'old1': 'new1', 'old2': 'new2'})
</code></pre>
<h3 id="string-extraction"><a class="header" href="#string-extraction">String Extraction</a></h3>
<pre><code class="language-python"># Extract with regex groups
df['text'].str.extract(r'(\d+)')  # First group
df[['area', 'phone']] = df['phone'].str.extract(r'(\d{3})-(\d{7})')

# Extract all occurrences
df['text'].str.extractall(r'(\d+)')

# Split
df['text'].str.split()  # Returns list
df['text'].str.split(expand=True)  # Returns DataFrame
df[['first', 'last']] = df['name'].str.split(' ', n=1, expand=True)

# Split from right
df['text'].str.rsplit(n=1, expand=True)

# Partition (split into 3 parts)
df['email'].str.partition('@')  # [before, separator, after]
</code></pre>
<h3 id="string-transformation"><a class="header" href="#string-transformation">String Transformation</a></h3>
<pre><code class="language-python"># Pad
df['text'].str.pad(width=10, side='left', fillchar='0')  # Zero-padding
df['text'].str.pad(width=10, side='right')
df['text'].str.pad(width=10, side='both')

# Center
df['text'].str.center(width=20, fillchar='*')

# Slice
df['text'].str.slice(start=0, stop=5)
df['text'].str[:5]  # First 5 characters

# Get specific character
df['text'].str.get(0)  # First character
df['text'].str[0]     # Alternative

# Wrap text
df['text'].str.wrap(width=40)

# Normalize (Unicode)
df['text'].str.normalize('NFC')
</code></pre>
<h3 id="advanced-string-operations"><a class="header" href="#advanced-string-operations">Advanced String Operations</a></h3>
<pre><code class="language-python"># Regular expression methods
df['text'].str.findall(r'\b\w+\b')  # All words
df['text'].str.replace(r'(\w+) (\w+)', r'\2 \1', regex=True)  # Swap words

# String contains any/all
keywords = ['python', 'pandas', 'numpy']
df['has_keyword'] = df['text'].str.contains('|'.join(keywords), case=False)

# Remove special characters
df['clean'] = df['text'].str.replace(r'[^a-zA-Z0-9\s]', '', regex=True)

# Decode/Encode
df['text'].str.encode('utf-8')
df['bytes'].str.decode('utf-8')

# String formatting
df['formatted'] = df['value'].apply(lambda x: f'{x:.2f}')
</code></pre>
<hr>
<h2 id="categorical-data"><a class="header" href="#categorical-data">Categorical Data</a></h2>
<h3 id="creating-categorical"><a class="header" href="#creating-categorical">Creating Categorical</a></h3>
<pre><code class="language-python"># Convert to categorical
df['category'] = df['category'].astype('category')
df['category'] = pd.Categorical(df['category'])

# Create with specific categories and order
df['size'] = pd.Categorical(
    df['size'],
    categories=['small', 'medium', 'large'],
    ordered=True
)

# Check if categorical
df['category'].dtype == 'category'
</code></pre>
<h3 id="categorical-properties"><a class="header" href="#categorical-properties">Categorical Properties</a></h3>
<pre><code class="language-python"># Get categories
df['category'].cat.categories

# Get codes (integer representation)
df['category'].cat.codes

# Check if ordered
df['category'].cat.ordered
</code></pre>
<h3 id="categorical-operations"><a class="header" href="#categorical-operations">Categorical Operations</a></h3>
<pre><code class="language-python"># Add categories
df['category'] = df['category'].cat.add_categories(['new_cat'])

# Remove categories
df['category'] = df['category'].cat.remove_categories(['old_cat'])

# Rename categories
df['category'] = df['category'].cat.rename_categories({
    'old': 'new'
})

# Reorder categories
df['size'] = df['size'].cat.reorder_categories(
    ['small', 'medium', 'large', 'xl'],
    ordered=True
)

# Set ordered
df['category'] = df['category'].cat.as_ordered()
df['category'] = df['category'].cat.as_unordered()

# Remove unused categories
df['category'] = df['category'].cat.remove_unused_categories()
</code></pre>
<h3 id="benefits-of-categorical"><a class="header" href="#benefits-of-categorical">Benefits of Categorical</a></h3>
<pre><code class="language-python"># Memory savings
df['category'] = df['category'].astype('category')
# Can reduce memory usage by 50-90% for low-cardinality columns

# Faster groupby
df.groupby('category').sum()  # Faster with categorical

# Ordered comparisons
df['size'] = pd.Categorical(
    df['size'],
    categories=['S', 'M', 'L', 'XL'],
    ordered=True
)
df[df['size'] &gt; 'M']  # Returns L and XL

# Preserved category order in plots
df['size'].value_counts().plot(kind='bar')  # Bars in category order
</code></pre>
<hr>
<h2 id="window-functions"><a class="header" href="#window-functions">Window Functions</a></h2>
<h3 id="rolling-windows-1"><a class="header" href="#rolling-windows-1">Rolling Windows</a></h3>
<pre><code class="language-python"># Simple rolling
df['rolling_mean'] = df['value'].rolling(window=7).mean()
df['rolling_sum'] = df['value'].rolling(window=7).sum()
df['rolling_std'] = df['value'].rolling(window=7).std()

# Multiple aggregations
df[['mean', 'std', 'min', 'max']] = df['value'].rolling(window=7).agg(
    ['mean', 'std', 'min', 'max']
)

# Custom function
df['range'] = df['value'].rolling(window=7).apply(
    lambda x: x.max() - x.min()
)

# With min_periods
df['rolling_mean'] = df['value'].rolling(window=7, min_periods=1).mean()

# Centered window
df['centered_ma'] = df['value'].rolling(window=7, center=True).mean()
</code></pre>
<h3 id="expanding-windows"><a class="header" href="#expanding-windows">Expanding Windows</a></h3>
<pre><code class="language-python"># Cumulative operations
df['expanding_mean'] = df['value'].expanding().mean()
df['expanding_sum'] = df['value'].expanding().sum()
df['expanding_max'] = df['value'].expanding().max()

# Same as cumsum, cummax, etc.
df['cumsum'] = df['value'].cumsum()
df['cummax'] = df['value'].cummax()
df['cummin'] = df['value'].cummin()
df['cumprod'] = df['value'].cumprod()
</code></pre>
<h3 id="exponentially-weighted-windows"><a class="header" href="#exponentially-weighted-windows">Exponentially Weighted Windows</a></h3>
<pre><code class="language-python"># EWMA - Exponentially Weighted Moving Average
df['ewma'] = df['value'].ewm(span=7).mean()
df['ewma'] = df['value'].ewm(alpha=0.3).mean()
df['ewma'] = df['value'].ewm(halflife=7).mean()

# Other EW functions
df['ewm_std'] = df['value'].ewm(span=7).std()
df['ewm_var'] = df['value'].ewm(span=7).var()

# Adjust parameter
df['ewma'] = df['value'].ewm(span=7, adjust=False).mean()
</code></pre>
<h3 id="window-functions-for-finance"><a class="header" href="#window-functions-for-finance">Window Functions for Finance</a></h3>
<pre><code class="language-python"># Bollinger Bands
window = 20
df['MA'] = df['close'].rolling(window).mean()
df['std'] = df['close'].rolling(window).std()
df['upper_band'] = df['MA'] + 2 * df['std']
df['lower_band'] = df['MA'] - 2 * df['std']

# RSI (Relative Strength Index)
delta = df['close'].diff()
gain = delta.where(delta &gt; 0, 0)
loss = -delta.where(delta &lt; 0, 0)
avg_gain = gain.rolling(window=14).mean()
avg_loss = loss.rolling(window=14).mean()
rs = avg_gain / avg_loss
df['RSI'] = 100 - (100 / (1 + rs))

# MACD (Moving Average Convergence Divergence)
df['ema12'] = df['close'].ewm(span=12).mean()
df['ema26'] = df['close'].ewm(span=26).mean()
df['MACD'] = df['ema12'] - df['ema26']
df['signal'] = df['MACD'].ewm(span=9).mean()
</code></pre>
<hr>
<h2 id="performance-optimization"><a class="header" href="#performance-optimization">Performance Optimization</a></h2>
<h3 id="memory-optimization"><a class="header" href="#memory-optimization">Memory Optimization</a></h3>
<pre><code class="language-python"># Check memory usage
df.info(memory_usage='deep')
df.memory_usage(deep=True)

# Optimize dtypes
def optimize_dtypes(df):
    for col in df.select_dtypes(include=['int']).columns:
        df[col] = pd.to_numeric(df[col], downcast='integer')

    for col in df.select_dtypes(include=['float']).columns:
        df[col] = pd.to_numeric(df[col], downcast='float')

    for col in df.select_dtypes(include=['object']).columns:
        if df[col].nunique() / len(df) &lt; 0.5:  # Low cardinality
            df[col] = df[col].astype('category')

    return df

df = optimize_dtypes(df)

# Use specific dtypes when reading
df = pd.read_csv('data.csv', dtype={
    'id': 'int32',
    'value': 'float32',
    'category': 'category'
})

# Chunking for large files
chunk_size = 10000
chunks = []
for chunk in pd.read_csv('large_file.csv', chunksize=chunk_size):
    # Process chunk
    processed_chunk = process(chunk)
    chunks.append(processed_chunk)

df = pd.concat(chunks, ignore_index=True)
</code></pre>
<h3 id="vectorization"><a class="header" href="#vectorization">Vectorization</a></h3>
<pre><code class="language-python"># BAD: Iterating rows
result = []
for idx, row in df.iterrows():  # SLOW!
    result.append(row['A'] + row['B'])
df['sum'] = result

# GOOD: Vectorized operation
df['sum'] = df['A'] + df['B']  # FAST!

# BAD: Apply with simple operation
df['squared'] = df['value'].apply(lambda x: x ** 2)

# GOOD: Direct operation
df['squared'] = df['value'] ** 2

# When apply is needed
df['complex'] = df.apply(lambda row: complex_function(row['A'], row['B']), axis=1)

# Try vectorization with numpy
df['result'] = np.where(df['value'] &gt; 0, df['value'], 0)  # Vectorized ReLU
</code></pre>
<h3 id="efficient-operations"><a class="header" href="#efficient-operations">Efficient Operations</a></h3>
<pre><code class="language-python"># Use query for complex filtering
df.query('A &gt; 10 and B &lt; 20')  # Can be faster than boolean indexing

# Use eval for expressions
df.eval('C = A + B')  # More memory efficient

# Avoid chained indexing
# BAD
df[df['A'] &gt; 0]['B'] = 999  # SettingWithCopyWarning

# GOOD
df.loc[df['A'] &gt; 0, 'B'] = 999

# Use categories for low-cardinality strings
df['category'] = df['category'].astype('category')

# Avoid loops, use groupby + transform
# BAD
for group in df['category'].unique():
    mask = df['category'] == group
    df.loc[mask, 'normalized'] = (
        df.loc[mask, 'value'] - df.loc[mask, 'value'].mean()
    )

# GOOD
df['normalized'] = df.groupby('category')['value'].transform(
    lambda x: x - x.mean()
)
</code></pre>
<h3 id="index-optimization"><a class="header" href="#index-optimization">Index Optimization</a></h3>
<pre><code class="language-python"># Set index for frequent lookups
df = df.set_index('id')  # O(1) lookups

# Sort index for range queries
df = df.sort_index()

# Multi-index for hierarchical data
df = df.set_index(['category', 'subcategory'])

# Reset when not needed
df = df.reset_index(drop=True)
</code></pre>
<h3 id="parallel-processing"><a class="header" href="#parallel-processing">Parallel Processing</a></h3>
<pre><code class="language-python"># Use swifter for automatic parallelization
# pip install swifter
import swifter
df['result'] = df['value'].swifter.apply(complex_function)

# Manual multiprocessing
from multiprocessing import Pool
import numpy as np

def process_chunk(chunk):
    return chunk.apply(complex_function)

# Split dataframe
chunks = np.array_split(df, 4)  # 4 chunks

# Process in parallel
with Pool(4) as pool:
    results = pool.map(process_chunk, chunks)

df = pd.concat(results)

# Dask for out-of-core computation
import dask.dataframe as dd
ddf = dd.from_pandas(df, npartitions=4)
result = ddf.groupby('category').value.mean().compute()
</code></pre>
<hr>
<h2 id="mldata-science-patterns"><a class="header" href="#mldata-science-patterns">ML/Data Science Patterns</a></h2>
<h3 id="train-test-split"><a class="header" href="#train-test-split">Train-Test Split</a></h3>
<pre><code class="language-python">from sklearn.model_selection import train_test_split

# Basic split
X = df[['feature1', 'feature2', 'feature3']]
y = df['target']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Stratified split (for classification)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# Time series split (no shuffle)
split_point = int(len(df) * 0.8)
train = df[:split_point]
test = df[split_point:]
</code></pre>
<h3 id="feature-engineering"><a class="header" href="#feature-engineering">Feature Engineering</a></h3>
<pre><code class="language-python"># One-hot encoding
df_encoded = pd.get_dummies(df, columns=['category'], prefix='cat')

# Or with scikit-learn
from sklearn.preprocessing import OneHotEncoder
encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')
encoded = encoder.fit_transform(df[['category']])
df_encoded = pd.DataFrame(encoded, columns=encoder.get_feature_names_out())

# Label encoding (ordinal)
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df['category_encoded'] = le.fit_transform(df['category'])

# Binning
df['age_group'] = pd.cut(df['age'], bins=[0, 18, 35, 50, 100],
                         labels=['young', 'adult', 'middle', 'senior'])

# Interaction features
df['interaction'] = df['feature1'] * df['feature2']

# Polynomial features
from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(degree=2, include_bias=False)
poly_features = poly.fit_transform(df[['feature1', 'feature2']])
df_poly = pd.DataFrame(poly_features, columns=poly.get_feature_names_out())

# Log transform (for skewed data)
df['log_value'] = np.log1p(df['value'])  # log(1 + x)

# Date features
df['date'] = pd.to_datetime(df['date'])
df['year'] = df['date'].dt.year
df['month'] = df['date'].dt.month
df['day_of_week'] = df['date'].dt.dayofweek
df['is_weekend'] = df['date'].dt.dayofweek.isin([5, 6]).astype(int)
df['quarter'] = df['date'].dt.quarter
</code></pre>
<h3 id="feature-scaling"><a class="header" href="#feature-scaling">Feature Scaling</a></h3>
<pre><code class="language-python">from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler

# Standardization (z-score normalization)
scaler = StandardScaler()
df[['feature1', 'feature2']] = scaler.fit_transform(df[['feature1', 'feature2']])

# Min-Max scaling (0-1 range)
scaler = MinMaxScaler()
df[['feature1', 'feature2']] = scaler.fit_transform(df[['feature1', 'feature2']])

# Robust scaling (less sensitive to outliers)
scaler = RobustScaler()
df[['feature1', 'feature2']] = scaler.fit_transform(df[['feature1', 'feature2']])

# Manual standardization
df['normalized'] = (df['value'] - df['value'].mean()) / df['value'].std()

# Group-wise scaling
df['scaled'] = df.groupby('category')['value'].transform(
    lambda x: (x - x.mean()) / x.std()
)
</code></pre>
<h3 id="handling-imbalanced-data"><a class="header" href="#handling-imbalanced-data">Handling Imbalanced Data</a></h3>
<pre><code class="language-python"># Check class distribution
df['target'].value_counts()
df['target'].value_counts(normalize=True)

# Undersampling majority class
from sklearn.utils import resample

# Separate classes
df_majority = df[df['target'] == 0]
df_minority = df[df['target'] == 1]

# Downsample majority
df_majority_downsampled = resample(
    df_majority,
    replace=False,
    n_samples=len(df_minority),
    random_state=42
)

df_balanced = pd.concat([df_majority_downsampled, df_minority])

# Oversampling minority class
df_minority_upsampled = resample(
    df_minority,
    replace=True,
    n_samples=len(df_majority),
    random_state=42
)

df_balanced = pd.concat([df_majority, df_minority_upsampled])

# SMOTE (Synthetic Minority Over-sampling)
from imblearn.over_sampling import SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)
</code></pre>
<h3 id="cross-validation-folds"><a class="header" href="#cross-validation-folds">Cross-Validation Folds</a></h3>
<pre><code class="language-python">from sklearn.model_selection import KFold, StratifiedKFold, TimeSeriesSplit

# K-Fold
kf = KFold(n_splits=5, shuffle=True, random_state=42)
for train_idx, val_idx in kf.split(df):
    train = df.iloc[train_idx]
    val = df.iloc[val_idx]
    # Train and evaluate

# Stratified K-Fold (for classification)
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
for train_idx, val_idx in skf.split(X, y):
    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

# Time Series Split
tscv = TimeSeriesSplit(n_splits=5)
for train_idx, val_idx in tscv.split(df):
    train = df.iloc[train_idx]
    val = df.iloc[val_idx]
</code></pre>
<h3 id="feature-selection"><a class="header" href="#feature-selection">Feature Selection</a></h3>
<pre><code class="language-python"># Correlation with target
correlations = df.corr()['target'].sort_values(ascending=False)
top_features = correlations[1:11].index.tolist()  # Top 10

# Variance threshold
from sklearn.feature_selection import VarianceThreshold
selector = VarianceThreshold(threshold=0.1)
X_selected = selector.fit_transform(X)

# Univariate feature selection
from sklearn.feature_selection import SelectKBest, f_classif
selector = SelectKBest(f_classif, k=10)
X_selected = selector.fit_transform(X, y)
selected_features = X.columns[selector.get_support()]

# Recursive Feature Elimination
from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier()
rfe = RFE(model, n_features_to_select=10)
rfe.fit(X, y)
selected_features = X.columns[rfe.support_]

# Feature importance from tree models
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier()
model.fit(X, y)
feature_importance = pd.DataFrame({
    'feature': X.columns,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=False)
</code></pre>
<hr>
<h2 id="integration-patterns"><a class="header" href="#integration-patterns">Integration Patterns</a></h2>
<h3 id="with-numpy"><a class="header" href="#with-numpy">With NumPy</a></h3>
<pre><code class="language-python"># DataFrame to NumPy array
array = df.values
array = df.to_numpy()  # Preferred

# Specific columns
array = df[['A', 'B']].values

# NumPy array to DataFrame
df = pd.DataFrame(array, columns=['A', 'B', 'C'])

# Apply NumPy functions
df['result'] = np.sqrt(df['value'])
df['log'] = np.log1p(df['value'])

# NumPy operations on DataFrames
df_normalized = (df - df.mean()) / df.std()
</code></pre>
<h3 id="with-scikit-learn"><a class="header" href="#with-scikit-learn">With Scikit-learn</a></h3>
<pre><code class="language-python">from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier

# Preprocessing
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df[features])
X_scaled = pd.DataFrame(X_scaled, columns=features, index=df.index)

# PCA
pca = PCA(n_components=2)
principal_components = pca.fit_transform(X_scaled)
df_pca = pd.DataFrame(principal_components, columns=['PC1', 'PC2'])

# Model training
model = RandomForestClassifier()
model.fit(X_train, y_train)

# Predictions
predictions = model.predict(X_test)
df['predictions'] = predictions

# Probabilities
probabilities = model.predict_proba(X_test)
df_probs = pd.DataFrame(probabilities, columns=model.classes_)
</code></pre>
<h3 id="with-matplotlibseaborn"><a class="header" href="#with-matplotlibseaborn">With Matplotlib/Seaborn</a></h3>
<pre><code class="language-python">import matplotlib.pyplot as plt
import seaborn as sns

# Pandas built-in plotting
df['value'].plot(kind='hist', bins=30)
df.plot(x='date', y='value', kind='line')
df.plot(kind='scatter', x='A', y='B', c='C', colormap='viridis')

# Bar plot
df.groupby('category')['value'].sum().plot(kind='bar')

# Box plot
df.boxplot(column='value', by='category')

# Correlation heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')

# Pairplot
sns.pairplot(df, hue='category')

# Distribution plot
sns.histplot(data=df, x='value', hue='category', kde=True)

# Time series
df.set_index('date')['value'].plot(figsize=(12, 6))
</code></pre>
<h3 id="with-sql"><a class="header" href="#with-sql">With SQL</a></h3>
<pre><code class="language-python">import sqlite3
from sqlalchemy import create_engine

# SQLite
conn = sqlite3.connect('database.db')

# Read
df = pd.read_sql('SELECT * FROM table', conn)
df = pd.read_sql_query('SELECT * FROM table WHERE id &gt; 100', conn)

# Write
df.to_sql('table_name', conn, if_exists='replace', index=False)

# SQLAlchemy (more databases)
engine = create_engine('postgresql://user:pass@localhost/dbname')
df = pd.read_sql('SELECT * FROM table', engine)
df.to_sql('table_name', engine, if_exists='append', index=False)

# Chunked reading for large tables
for chunk in pd.read_sql('SELECT * FROM large_table', conn, chunksize=10000):
    process(chunk)
</code></pre>
<h3 id="with-spark"><a class="header" href="#with-spark">With Spark</a></h3>
<pre><code class="language-python">from pyspark.sql import SparkSession

spark = SparkSession.builder.appName('example').getOrCreate()

# Pandas to Spark
spark_df = spark.createDataFrame(df)

# Spark to Pandas
df = spark_df.toPandas()

# Distributed operations
spark_df.groupBy('category').agg({'value': 'sum'}).show()
</code></pre>
<hr>
<h2 id="best-practices"><a class="header" href="#best-practices">Best Practices</a></h2>
<h3 id="code-style"><a class="header" href="#code-style">Code Style</a></h3>
<pre><code class="language-python"># Use meaningful variable names
user_data = pd.read_csv('users.csv')  # Good
df = pd.read_csv('users.csv')        # Acceptable for exploratory

# Method chaining (when appropriate)
result = (df
    .query('age &gt; 18')
    .groupby('category')
    ['value'].sum()
    .sort_values(ascending=False)
    .head(10)
)

# Avoid chained assignment
# BAD
df[df['A'] &gt; 0]['B'] = 999  # SettingWithCopyWarning

# GOOD
df.loc[df['A'] &gt; 0, 'B'] = 999

# Use .copy() when needed
df_subset = df[df['A'] &gt; 0].copy()
df_subset['B'] = 999  # Safe
</code></pre>
<h3 id="error-handling"><a class="header" href="#error-handling">Error Handling</a></h3>
<pre><code class="language-python"># Check for missing values before operations
if df['column'].isna().any():
    df['column'].fillna(0, inplace=True)

# Validate data types
assert df['age'].dtype == 'int64', "Age must be integer"

# Handle divisions by zero
df['ratio'] = df['A'] / df['B'].replace(0, np.nan)

# Try-except for robust code
try:
    df['date'] = pd.to_datetime(df['date'])
except ValueError:
    df['date'] = pd.to_datetime(df['date'], errors='coerce')
</code></pre>
<h3 id="performance-tips"><a class="header" href="#performance-tips">Performance Tips</a></h3>
<pre><code class="language-python"># 1. Use vectorized operations
df['result'] = df['A'] + df['B']  # Fast

# 2. Avoid iterrows()
# for idx, row in df.iterrows():  # SLOW!

# 3. Use categorical for low-cardinality strings
df['category'] = df['category'].astype('category')

# 4. Read only needed columns
df = pd.read_csv('file.csv', usecols=['A', 'B', 'C'])

# 5. Use appropriate dtypes
df = pd.read_csv('file.csv', dtype={'id': 'int32', 'value': 'float32'})

# 6. Filter early, aggregate late
result = df[df['year'] == 2023].groupby('category').sum()  # Good

# 7. Use query() for complex conditions
df.query('A &gt; 10 and B &lt; 20')

# 8. Profile your code
import pandas as pd
pd.set_option('mode.chained_assignment', 'warn')  # Catch issues
</code></pre>
<h3 id="data-validation"><a class="header" href="#data-validation">Data Validation</a></h3>
<pre><code class="language-python"># Schema validation
expected_columns = ['id', 'name', 'age', 'email']
assert set(expected_columns).issubset(df.columns), "Missing columns"

# Type validation
assert df['age'].dtype in ['int64', 'int32'], "Age must be integer"

# Range validation
assert df['age'].between(0, 120).all(), "Invalid age values"

# No duplicates
assert not df['id'].duplicated().any(), "Duplicate IDs found"

# No missing values in required columns
required = ['id', 'name']
assert df[required].notna().all().all(), "Missing required values"

# Using pandera (schema validation library)
# import pandera as pa
# schema = pa.DataFrameSchema({
#     'id': pa.Column(int, pa.Check.greater_than(0)),
#     'age': pa.Column(int, pa.Check.in_range(0, 120)),
#     'name': pa.Column(str, nullable=False)
# })
# validated_df = schema.validate(df)
</code></pre>
<h3 id="documentation"><a class="header" href="#documentation">Documentation</a></h3>
<pre><code class="language-python">def process_sales_data(df: pd.DataFrame) -&gt; pd.DataFrame:
    """
    Process raw sales data.

    Parameters:
    -----------
    df : pd.DataFrame
        Raw sales data with columns: date, product, quantity, price

    Returns:
    --------
    pd.DataFrame
        Processed data with additional columns: revenue, month, year

    Examples:
    ---------
    &gt;&gt;&gt; df = pd.DataFrame({
    ...     'date': ['2023-01-01', '2023-01-02'],
    ...     'product': ['A', 'B'],
    ...     'quantity': [10, 20],
    ...     'price': [100, 200]
    ... })
    &gt;&gt;&gt; processed = process_sales_data(df)
    """
    df = df.copy()
    df['date'] = pd.to_datetime(df['date'])
    df['revenue'] = df['quantity'] * df['price']
    df['month'] = df['date'].dt.month
    df['year'] = df['date'].dt.year
    return df
</code></pre>
<hr>
<h2 id="summary"><a class="header" href="#summary">Summary</a></h2>
<p>Pandas is the cornerstone of data analysis in Python. Key takeaways:</p>
<ol>
<li><strong>Master the fundamentals</strong>: Series, DataFrame, indexing (loc/iloc)</li>
<li><strong>Think vectorized</strong>: Avoid loops, use pandas operations</li>
<li><strong>Leverage groupby</strong>: Most data analysis involves split-apply-combine</li>
<li><strong>Clean your data</strong>: Handle missing values, duplicates, and types properly</li>
<li><strong>Optimize for performance</strong>: Use appropriate dtypes, categories, and chunking</li>
<li><strong>Chain operations</strong>: Build readable data pipelines</li>
<li><strong>Integrate well</strong>: Pandas works seamlessly with NumPy, scikit-learn, matplotlib</li>
<li><strong>Validate your data</strong>: Check assumptions, handle errors gracefully</li>
</ol>
<p><strong>Common Patterns to Remember:</strong></p>
<ul>
<li>Filter: <code>df[df['col'] &gt; value]</code> or <code>df.query('col &gt; value')</code></li>
<li>Group &amp; Aggregate: <code>df.groupby('col').agg({'val': 'sum'})</code></li>
<li>Join: <code>pd.merge(df1, df2, on='key', how='inner')</code></li>
<li>Reshape: <code>df.pivot()</code>, <code>df.melt()</code>, <code>df.stack()</code></li>
<li>Time Series: <code>df.resample('D').sum()</code>, <code>df.rolling(7).mean()</code></li>
</ul>
<p><strong>Resources:</strong></p>
<ul>
<li><a href="https://pandas.pydata.org/docs/">Pandas Documentation</a></li>
<li><a href="https://pandas.pydata.org/docs/user_guide/10min.html">10 Minutes to Pandas</a></li>
<li><a href="https://pandas.pydata.org/docs/user_guide/cookbook.html">Pandas Cookbook</a></li>
<li><a href="https://tomaugspurger.github.io/modern-1-intro.html">Modern Pandas</a></li>
<li><a href="https://github.com/TomAugspurger/effective-pandas">Effective Pandas</a></li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../misc/matplotlib.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256 246.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"/></svg></span>
                            </a>

                            <a rel="next prefetch" href="../misc/blockchain.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M278.6 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L210.7 256 73.4 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"/></svg></span>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../misc/matplotlib.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256 246.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"/></svg></span>
                    </a>

                    <a rel="next prefetch" href="../misc/blockchain.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M278.6 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L210.7 256 73.4 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"/></svg></span>
                    </a>
            </nav>

        </div>

        <template id=fa-eye><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M288 32c-80.8 0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7 0 24.6C17.3 304 48.6 356 95.4 399.4C142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144s64.5-144 144-144s144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64c-11.5 0-22.3-3-31.6-8.4c-.2 2.8-.4 5.5-.4 8.4c0 53 43 96 96 96s96-43 96-96s-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6z"/></svg></span></template>
        <template id=fa-eye-slash><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M38.8 5.1C28.4-3.1 13.3-1.2 5.1 9.2S-1.2 34.7 9.2 42.9l592 464c10.4 8.2 25.5 6.3 33.7-4.1s6.3-25.5-4.1-33.7L525.6 386.7c39.6-40.6 66.4-86.1 79.9-118.4c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C465.5 68.8 400.8 32 320 32c-68.2 0-125 26.3-169.3 60.8L38.8 5.1zM223.1 149.5C248.6 126.2 282.7 112 320 112c79.5 0 144 64.5 144 144c0 24.9-6.3 48.3-17.4 68.7L408 294.5c5.2-11.8 8-24.8 8-38.5c0-53-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6c0 10.2-2.4 19.8-6.6 28.3l-90.3-70.8zm223.1 298L373 389.9c-16.4 6.5-34.3 10.1-53 10.1c-79.5 0-144-64.5-144-144c0-6.9 .5-13.6 1.4-20.2L83.1 161.5C60.3 191.2 44 220.8 34.5 243.7c-3.3 7.9-3.3 16.7 0 24.6c14.9 35.7 46.2 87.7 93 131.1C174.5 443.2 239.2 480 320 480c47.8 0 89.9-12.9 126.2-32.5z"/></svg></span></template>
        <template id=fa-copy><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M502.6 70.63l-61.25-61.25C435.4 3.371 427.2 0 418.7 0H255.1c-35.35 0-64 28.66-64 64l.0195 256C192 355.4 220.7 384 256 384h192c35.2 0 64-28.8 64-64V93.25C512 84.77 508.6 76.63 502.6 70.63zM464 320c0 8.836-7.164 16-16 16H255.1c-8.838 0-16-7.164-16-16L239.1 64.13c0-8.836 7.164-16 16-16h128L384 96c0 17.67 14.33 32 32 32h47.1V320zM272 448c0 8.836-7.164 16-16 16H63.1c-8.838 0-16-7.164-16-16L47.98 192.1c0-8.836 7.164-16 16-16H160V128H63.99c-35.35 0-64 28.65-64 64l.0098 256C.002 483.3 28.66 512 64 512h192c35.2 0 64-28.8 64-64v-32h-47.1L272 448z"/></svg></span></template>
        <template id=fa-play><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M73 39c-14.8-9.1-33.4-9.4-48.5-.9S0 62.6 0 80V432c0 17.4 9.4 33.4 24.5 41.9s33.7 8.1 48.5-.9L361 297c14.3-8.7 23-24.2 23-41s-8.7-32.2-23-41L73 39z"/></svg></span></template>
        <template id=fa-clock-rotate-left><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M75 75L41 41C25.9 25.9 0 36.6 0 57.9V168c0 13.3 10.7 24 24 24H134.1c21.4 0 32.1-25.9 17-41l-30.8-30.8C155 85.5 203 64 256 64c106 0 192 86 192 192s-86 192-192 192c-40.8 0-78.6-12.7-109.7-34.4c-14.5-10.1-34.4-6.6-44.6 7.9s-6.6 34.4 7.9 44.6C151.2 495 201.7 512 256 512c141.4 0 256-114.6 256-256S397.4 0 256 0C185.3 0 121.3 28.7 75 75zm181 53c-13.3 0-24 10.7-24 24V256c0 6.4 2.5 12.5 7 17l72 72c9.4 9.4 24.6 9.4 33.9 0s9.4-24.6 0-33.9l-65-65V152c0-13.3-10.7-24-24-24z"/></svg></span></template>



        <script>
            window.playground_copyable = true;
        </script>


        <script src="../elasticlunr-ef4e11c1.min.js"></script>
        <script src="../mark-09e88c2c.min.js"></script>
        <script src="../searcher-c2a407aa.js"></script>

        <script src="../clipboard-1626706a.min.js"></script>
        <script src="../highlight-abc7f01d.js"></script>
        <script src="../book-a0b12cfe.js"></script>

        <!-- Custom JS scripts -->



    </div>
    </body>
</html>
