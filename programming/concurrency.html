<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Concurrency - My Notes</title>


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon-de23e50b.svg">
        <link rel="shortcut icon" href="../favicon-8114d1fc.png">
        <link rel="stylesheet" href="../css/variables-8adf115d.css">
        <link rel="stylesheet" href="../css/general-2459343d.css">
        <link rel="stylesheet" href="../css/chrome-ae938929.css">
        <link rel="stylesheet" href="../css/print-9e4910d8.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../fonts/fonts-9644e21d.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="mdbook-highlight-css" href="../highlight-493f70e1.css">
        <link rel="stylesheet" id="mdbook-tomorrow-night-css" href="../tomorrow-night-4c0ae647.css">
        <link rel="stylesheet" id="mdbook-ayu-highlight-css" href="../ayu-highlight-3fdfc3ac.css">

        <!-- Custom theme stylesheets -->

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "../";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "../searchindex-91648d44.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../toc-a4fa5267.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="mdbook-body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="mdbook-sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("mdbook-sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="mdbook-sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="../toc.html"></iframe>
            </noscript>
            <div id="mdbook-sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="mdbook-page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="mdbook-menu-bar-hover-placeholder"></div>
                <div id="mdbook-menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="mdbook-sidebar-toggle" class="icon-button" for="mdbook-sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="mdbook-sidebar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"/></svg></span>
                        </label>
                        <button id="mdbook-theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="mdbook-theme-list">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M371.3 367.1c27.3-3.9 51.9-19.4 67.2-42.9L600.2 74.1c12.6-19.5 9.4-45.3-7.6-61.2S549.7-4.4 531.1 9.6L294.4 187.2c-24 18-38.2 46.1-38.4 76.1L371.3 367.1zm-19.6 25.4l-116-104.4C175.9 290.3 128 339.6 128 400c0 3.9 .2 7.8 .6 11.6c1.8 17.5-10.2 36.4-27.8 36.4H96c-17.7 0-32 14.3-32 32s14.3 32 32 32H240c61.9 0 112-50.1 112-112c0-2.5-.1-5-.2-7.5z"/></svg></span>
                        </button>
                        <ul id="mdbook-theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-ayu">Ayu</button></li>
                        </ul>
                        <button id="mdbook-search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="mdbook-searchbar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M416 208c0 45.9-14.9 88.3-40 122.7L502.6 457.4c12.5 12.5 12.5 32.8 0 45.3s-32.8 12.5-45.3 0L330.7 376c-34.4 25.2-76.8 40-122.7 40C93.1 416 0 322.9 0 208S93.1 0 208 0S416 93.1 416 208zM208 352c79.5 0 144-64.5 144-144s-64.5-144-144-144S64 128.5 64 208s64.5 144 144 144z"/></svg></span>
                        </button>
                    </div>

                    <h1 class="menu-title">My Notes</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <span class=fa-svg id="print-button"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M128 0C92.7 0 64 28.7 64 64v96h64V64H354.7L384 93.3V160h64V93.3c0-17-6.7-33.3-18.7-45.3L400 18.7C388 6.7 371.7 0 354.7 0H128zM384 352v32 64H128V384 368 352H384zm64 32h32c17.7 0 32-14.3 32-32V256c0-35.3-28.7-64-64-64H64c-35.3 0-64 28.7-64 64v96c0 17.7 14.3 32 32 32H64v64c0 35.3 28.7 64 64 64H384c35.3 0 64-28.7 64-64V384zm-16-88c-13.3 0-24-10.7-24-24s10.7-24 24-24s24 10.7 24 24s-10.7 24-24 24z"/></svg></span>
                        </a>

                    </div>
                </div>

                <div id="mdbook-search-wrapper" class="hidden">
                    <form id="mdbook-searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="mdbook-searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="mdbook-searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <span class=fa-svg id="fa-spin"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M304 48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zm0 416c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM48 304c26.5 0 48-21.5 48-48s-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48zm464-48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM142.9 437c18.7-18.7 18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zm0-294.2c18.7-18.7 18.7-49.1 0-67.9S93.7 56.2 75 75s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zM369.1 437c18.7 18.7 49.1 18.7 67.9 0s18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9z"/></svg></span>
                            </div>
                        </div>
                    </form>
                    <div id="mdbook-searchresults-outer" class="searchresults-outer hidden">
                        <div id="mdbook-searchresults-header" class="searchresults-header"></div>
                        <ul id="mdbook-searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('mdbook-sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('mdbook-sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#mdbook-sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="mdbook-content" class="content">
                    <main>
                        <h1 id="concurrency-programming-guide"><a class="header" href="#concurrency-programming-guide">Concurrency Programming Guide</a></h1>
<p>A comprehensive guide to concurrent programming, covering fundamentals, synchronization primitives, patterns, and practical implementations across multiple programming languages.</p>
<h2 id="table-of-contents"><a class="header" href="#table-of-contents">Table of Contents</a></h2>
<ol>
<li><a href="#fundamentals">Fundamentals</a></li>
<li><a href="#synchronization-primitives">Synchronization Primitives</a></li>
<li><a href="#concurrency-patterns">Concurrency Patterns</a></li>
<li><a href="#language-specific-implementations">Language-Specific Implementations</a></li>
<li><a href="#deadlock-prevention">Deadlock Prevention</a></li>
<li><a href="#performance-considerations">Performance Considerations</a></li>
<li><a href="#real-world-applications">Real-World Applications</a></li>
<li><a href="#best-practices">Best Practices</a></li>
<li><a href="#anti-patterns">Anti-Patterns</a></li>
<li><a href="#debugging-concurrent-programs">Debugging Concurrent Programs</a></li>
<li><a href="#testing-concurrent-code">Testing Concurrent Code</a></li>
</ol>
<hr>
<h2 id="fundamentals"><a class="header" href="#fundamentals">Fundamentals</a></h2>
<h3 id="concurrency-vs-parallelism"><a class="header" href="#concurrency-vs-parallelism">Concurrency vs Parallelism</a></h3>
<p><strong>Concurrency</strong> and <strong>parallelism</strong> are related but distinct concepts:</p>
<p><strong>Concurrency:</strong></p>
<ul>
<li>Dealing with multiple tasks at once</li>
<li>Tasks make progress by interleaving execution</li>
<li>Can occur on a single-core processor through time-slicing</li>
<li>About the <em>structure</em> of the program</li>
<li>Example: A single person juggling multiple tasks by switching between them</li>
</ul>
<p><strong>Parallelism:</strong></p>
<ul>
<li>Executing multiple tasks simultaneously</li>
<li>Requires multiple processing units (cores)</li>
<li>Tasks execute at the exact same time</li>
<li>About the <em>execution</em> of the program</li>
<li>Example: Multiple people each working on different tasks simultaneously</li>
</ul>
<pre><code>Concurrency:        |----Task A----|----Task A----|
                         |----Task B----|----Task B----|
                    (Interleaved on single core)

Parallelism:        |----Task A----|----Task A----|
                    |----Task B----|----Task B----|
                    (Simultaneous on multiple cores)
</code></pre>
<p><strong>Key Insight:</strong> A program can be concurrent but not parallel (one core, multiple tasks interleaved), parallel but not concurrent (two independent single-threaded programs), or both concurrent and parallel (multi-threaded program on multi-core system).</p>
<h3 id="processes-vs-threads"><a class="header" href="#processes-vs-threads">Processes vs Threads</a></h3>
<h4 id="processes"><a class="header" href="#processes">Processes</a></h4>
<p><strong>Definition:</strong> A process is an instance of a running program with its own memory space.</p>
<p><strong>Characteristics:</strong></p>
<ul>
<li><strong>Isolated memory:</strong> Each process has its own address space</li>
<li><strong>Heavy-weight:</strong> High overhead to create and context switch</li>
<li><strong>Independent:</strong> Crash in one process doesn’t affect others</li>
<li><strong>Communication:</strong> Inter-Process Communication (IPC) required (pipes, sockets, shared memory)</li>
<li><strong>Security:</strong> Strong isolation boundaries</li>
</ul>
<p><strong>When to use:</strong></p>
<ul>
<li>Need strong isolation</li>
<li>Running untrusted code</li>
<li>Want to leverage multiple cores without shared memory complexity</li>
<li>Fault tolerance is critical</li>
</ul>
<pre><code class="language-python"># Python multiprocessing example
import multiprocessing
import os

def worker(num):
    print(f"Worker {num}, PID: {os.getpid()}")
    return num * num

if __name__ == '__main__':
    # Create separate processes
    with multiprocessing.Pool(processes=4) as pool:
        results = pool.map(worker, range(10))
    print(results)
</code></pre>
<h4 id="threads"><a class="header" href="#threads">Threads</a></h4>
<p><strong>Definition:</strong> A thread is a lightweight execution unit within a process that shares the process’s memory.</p>
<p><strong>Characteristics:</strong></p>
<ul>
<li><strong>Shared memory:</strong> All threads share the same address space</li>
<li><strong>Light-weight:</strong> Lower overhead to create and context switch</li>
<li><strong>Dependent:</strong> Crash in one thread can crash the entire process</li>
<li><strong>Communication:</strong> Direct memory sharing (requires synchronization)</li>
<li><strong>Speed:</strong> Faster context switching than processes</li>
</ul>
<p><strong>When to use:</strong></p>
<ul>
<li>Need fast communication between concurrent tasks</li>
<li>Sharing large amounts of data</li>
<li>I/O-bound operations</li>
<li>GUI applications (event handling)</li>
</ul>
<pre><code class="language-python"># Python threading example
import threading

def worker(num):
    print(f"Worker {num}, Thread: {threading.current_thread().name}")
    return num * num

threads = []
for i in range(5):
    t = threading.Thread(target=worker, args=(i,))
    threads.append(t)
    t.start()

for t in threads:
    t.join()
</code></pre>
<p><strong>Process vs Thread Comparison:</strong></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Aspect</th><th>Process</th><th>Thread</th></tr>
</thead>
<tbody>
<tr><td>Memory</td><td>Separate address space</td><td>Shared address space</td></tr>
<tr><td>Creation overhead</td><td>High (~1-10ms)</td><td>Low (~10-100μs)</td></tr>
<tr><td>Context switch</td><td>Expensive</td><td>Cheap</td></tr>
<tr><td>Communication</td><td>IPC (slow)</td><td>Direct (fast)</td></tr>
<tr><td>Isolation</td><td>Strong</td><td>Weak</td></tr>
<tr><td>Crash impact</td><td>Isolated</td><td>Affects all threads</td></tr>
</tbody>
</table>
</div>
<h3 id="context-switching"><a class="header" href="#context-switching">Context Switching</a></h3>
<p><strong>Definition:</strong> Context switching is the process of storing and restoring the state of a thread or process so execution can resume from the same point later.</p>
<p><strong>What gets saved/restored:</strong></p>
<ul>
<li>Program counter (PC)</li>
<li>CPU registers</li>
<li>Stack pointer</li>
<li>Memory management information</li>
<li>I/O status</li>
</ul>
<p><strong>Cost of context switching:</strong></p>
<ol>
<li>
<p><strong>Direct costs:</strong></p>
<ul>
<li>Saving/restoring registers</li>
<li>Updating kernel data structures</li>
<li>Time: ~1-10 microseconds</li>
</ul>
</li>
<li>
<p><strong>Indirect costs:</strong></p>
<ul>
<li>Cache pollution (cold cache after switch)</li>
<li>TLB (Translation Lookaside Buffer) misses</li>
<li>Pipeline stalls</li>
<li>Can be 10-100x the direct cost</li>
</ul>
</li>
</ol>
<p><strong>Example scenario:</strong></p>
<pre><code>Thread A running -&gt; Interrupt/yield -&gt; Save Thread A state
                                    -&gt; Load Thread B state
Thread B running -&gt; Interrupt/yield -&gt; Save Thread B state
                                    -&gt; Load Thread A state
Thread A resumes
</code></pre>
<p><strong>Minimizing context switches:</strong></p>
<ul>
<li>Reduce number of threads (use thread pools)</li>
<li>Minimize lock contention</li>
<li>Use asynchronous I/O</li>
<li>Batch operations</li>
<li>Set appropriate thread affinity</li>
</ul>
<h3 id="race-conditions"><a class="header" href="#race-conditions">Race Conditions</a></h3>
<p><strong>Definition:</strong> A race condition occurs when the program’s behavior depends on the relative timing or interleaving of multiple threads or processes.</p>
<p><strong>Classic example - Bank account:</strong></p>
<pre><code class="language-python"># UNSAFE: Race condition
class BankAccount:
    def __init__(self):
        self.balance = 0

    def deposit(self, amount):
        # This is NOT atomic!
        current = self.balance  # Read
        current += amount       # Modify
        self.balance = current  # Write

# Two threads depositing simultaneously
account = BankAccount()

# Thread 1: deposit(100)
# Thread 2: deposit(50)

# Possible execution:
# T1: current = balance  (reads 0)
# T2: current = balance  (reads 0)
# T1: current += 100     (current = 100)
# T2: current += 50      (current = 50)
# T1: balance = current  (balance = 100)
# T2: balance = current  (balance = 50)
# Final balance: 50 (WRONG! Should be 150)
</code></pre>
<p><strong>Types of race conditions:</strong></p>
<ol>
<li><strong>Data race:</strong> Multiple threads access shared data, at least one writes, without synchronization</li>
<li><strong>Read-modify-write:</strong> Classic race (shown above)</li>
<li><strong>Check-then-act:</strong> Checking a condition then acting on it</li>
</ol>
<pre><code class="language-python"># Check-then-act race condition
if file_exists("config.txt"):  # Check
    data = read_file("config.txt")  # Act (file might be deleted between check and read)
</code></pre>
<p><strong>Detecting race conditions:</strong></p>
<ul>
<li>Dynamic analysis tools (ThreadSanitizer, Helgrind, Intel Inspector)</li>
<li>Static analysis</li>
<li>Stress testing with many threads</li>
<li>Code review focusing on shared mutable state</li>
</ul>
<p><strong>Fixing race conditions:</strong></p>
<ul>
<li>Use synchronization primitives (locks, atomics)</li>
<li>Eliminate shared mutable state</li>
<li>Use immutable data structures</li>
<li>Message passing instead of shared memory</li>
</ul>
<h3 id="deadlocks"><a class="header" href="#deadlocks">Deadlocks</a></h3>
<p><strong>Definition:</strong> A deadlock is a situation where two or more threads are blocked forever, each waiting for resources held by the other.</p>
<p><strong>Classic example - Dining Philosophers:</strong></p>
<pre><code class="language-python">import threading

fork1 = threading.Lock()
fork2 = threading.Lock()

def philosopher1():
    fork1.acquire()  # Got fork 1
    # Context switch!
    fork2.acquire()  # Waiting for fork 2... (held by philosopher2)
    print("Philosopher 1 eating")
    fork2.release()
    fork1.release()

def philosopher2():
    fork2.acquire()  # Got fork 2
    # Context switch!
    fork1.acquire()  # Waiting for fork 1... (held by philosopher1)
    print("Philosopher 2 eating")
    fork1.release()
    fork2.release()

# DEADLOCK: philosopher1 has fork1, wants fork2
#           philosopher2 has fork2, wants fork1
#           Both wait forever
</code></pre>
<p><strong>Resource deadlock example:</strong></p>
<pre><code>Thread A:          Thread B:
lock(mutex1)       lock(mutex2)
  lock(mutex2)       lock(mutex1)  &lt;-- DEADLOCK
    ...                ...
  unlock(mutex2)     unlock(mutex1)
unlock(mutex1)     unlock(mutex2)
</code></pre>
<p><strong>Four necessary conditions for deadlock (Coffman conditions):</strong></p>
<ol>
<li><strong>Mutual Exclusion:</strong> Resources cannot be shared</li>
<li><strong>Hold and Wait:</strong> Thread holds resources while waiting for others</li>
<li><strong>No Preemption:</strong> Resources cannot be forcibly taken away</li>
<li><strong>Circular Wait:</strong> Circular chain of threads, each waiting for a resource held by the next</li>
</ol>
<p><strong>All four conditions must be present for deadlock to occur.</strong></p>
<p><strong>Prevention strategies:</strong></p>
<ul>
<li>Break one of the four conditions</li>
<li>Lock ordering (always acquire locks in same order)</li>
<li>Lock timeout (try-lock with timeout)</li>
<li>Deadlock detection and recovery</li>
</ul>
<p><strong>Visualizing deadlock:</strong></p>
<pre><code>    Thread A              Thread B
       |                     |
   Lock(R1) ✓               |
       |                 Lock(R2) ✓
       |                     |
   Lock(R2) ⏸              |
   (waiting...)             |
       |                 Lock(R1) ⏸
       |                 (waiting...)
       ↓                     ↓
     DEADLOCK - Both threads waiting forever
</code></pre>
<hr>
<h2 id="synchronization-primitives"><a class="header" href="#synchronization-primitives">Synchronization Primitives</a></h2>
<p>Synchronization primitives are low-level constructs used to control access to shared resources and coordinate thread execution.</p>
<h3 id="mutexes-and-locks"><a class="header" href="#mutexes-and-locks">Mutexes and Locks</a></h3>
<p><strong>Mutex (Mutual Exclusion):</strong> Ensures that only one thread can access a critical section at a time.</p>
<p><strong>Basic operations:</strong></p>
<ul>
<li><code>lock()</code> / <code>acquire()</code>: Acquire the lock (block if held by another thread)</li>
<li><code>unlock()</code> / <code>release()</code>: Release the lock</li>
<li><code>trylock()</code>: Try to acquire without blocking (returns success/failure)</li>
</ul>
<p><strong>Python example:</strong></p>
<pre><code class="language-python">import threading

class Counter:
    def __init__(self):
        self.value = 0
        self.lock = threading.Lock()

    def increment(self):
        with self.lock:  # Acquire lock
            # Critical section
            current = self.value
            current += 1
            self.value = current
        # Lock released automatically

    def increment_manual(self):
        self.lock.acquire()
        try:
            self.value += 1
        finally:
            self.lock.release()  # Always release, even if exception

# Usage
counter = Counter()
threads = [threading.Thread(target=counter.increment) for _ in range(1000)]
for t in threads:
    t.start()
for t in threads:
    t.join()
print(f"Final value: {counter.value}")  # Correctly prints 1000
</code></pre>
<p><strong>C++ example:</strong></p>
<pre><code class="language-cpp">#include &lt;mutex&gt;
#include &lt;thread&gt;

class Counter {
private:
    int value = 0;
    std::mutex mtx;

public:
    void increment() {
        std::lock_guard&lt;std::mutex&gt; lock(mtx);  // RAII
        value++;
    }  // Lock released automatically

    void increment_manual() {
        mtx.lock();
        value++;
        mtx.unlock();
    }

    bool try_increment() {
        if (mtx.try_lock()) {
            value++;
            mtx.unlock();
            return true;
        }
        return false;
    }

    int get_value() {
        std::lock_guard&lt;std::mutex&gt; lock(mtx);
        return value;
    }
};
</code></pre>
<p><strong>Rust example (using Mutex from std::sync):</strong></p>
<pre class="playground"><code class="language-rust">use std::sync::{Arc, Mutex};
use std::thread;

fn main() {
    let counter = Arc::new(Mutex::new(0));
    let mut handles = vec![];

    for _ in 0..10 {
        let counter = Arc::clone(&amp;counter);
        let handle = thread::spawn(move || {
            let mut num = counter.lock().unwrap();
            *num += 1;
        }); // Lock released when `num` goes out of scope
        handles.push(handle);
    }

    for handle in handles {
        handle.join().unwrap();
    }

    println!("Result: {}", *counter.lock().unwrap());
}</code></pre>
<p><strong>Types of locks:</strong></p>
<ol>
<li>
<p><strong>Spinlock:</strong> Busy-waits in a loop checking the lock</p>
<ul>
<li>Low latency for short critical sections</li>
<li>Wastes CPU cycles</li>
<li>Good for kernel-level code or when lock is held very briefly</li>
</ul>
</li>
<li>
<p><strong>Mutex (blocking lock):</strong> Puts thread to sleep when waiting</p>
<ul>
<li>Higher latency (context switch overhead)</li>
<li>Doesn’t waste CPU</li>
<li>Good for longer critical sections</li>
</ul>
</li>
<li>
<p><strong>Recursive lock:</strong> Can be locked multiple times by the same thread</p>
<ul>
<li>Useful but can hide design issues</li>
<li>Higher overhead</li>
</ul>
</li>
</ol>
<pre><code class="language-python">import threading

# Recursive lock example
lock = threading.RLock()  # Recursive lock

def recursive_function(n):
    with lock:
        if n &gt; 0:
            print(n)
            recursive_function(n - 1)  # Can re-acquire same lock

recursive_function(5)
</code></pre>
<h3 id="semaphores"><a class="header" href="#semaphores">Semaphores</a></h3>
<p><strong>Semaphore:</strong> A synchronization primitive that maintains a count, allowing a fixed number of threads to access a resource.</p>
<p><strong>Operations:</strong></p>
<ul>
<li><code>wait()</code> / <code>P()</code> / <code>acquire()</code>: Decrement count (block if zero)</li>
<li><code>signal()</code> / <code>V()</code> / <code>release()</code>: Increment count</li>
</ul>
<p><strong>Types:</strong></p>
<ol>
<li><strong>Binary semaphore:</strong> Count of 0 or 1 (similar to mutex)</li>
<li><strong>Counting semaphore:</strong> Count can be any non-negative integer</li>
</ol>
<p><strong>Python example:</strong></p>
<pre><code class="language-python">import threading
import time

# Limit to 3 concurrent database connections
db_semaphore = threading.Semaphore(3)

def access_database(thread_id):
    print(f"Thread {thread_id} waiting for DB access")
    with db_semaphore:
        print(f"Thread {thread_id} accessing database")
        time.sleep(2)  # Simulate database operation
        print(f"Thread {thread_id} done with database")

threads = [threading.Thread(target=access_database, args=(i,)) for i in range(10)]
for t in threads:
    t.start()
for t in threads:
    t.join()
</code></pre>
<p><strong>C++ example:</strong></p>
<pre><code class="language-cpp">#include &lt;semaphore&gt;
#include &lt;thread&gt;
#include &lt;iostream&gt;

std::counting_semaphore&lt;3&gt; db_semaphore(3);  // Max 3 concurrent accesses

void access_database(int id) {
    db_semaphore.acquire();
    std::cout &lt;&lt; "Thread " &lt;&lt; id &lt;&lt; " accessing database\n";
    std::this_thread::sleep_for(std::chrono::seconds(2));
    std::cout &lt;&lt; "Thread " &lt;&lt; id &lt;&lt; " done\n";
    db_semaphore.release();
}
</code></pre>
<p><strong>Classic use case - Producer-Consumer:</strong></p>
<pre><code class="language-python">import threading
import queue
import time

# Using semaphores to implement producer-consumer
MAX_SIZE = 5
buffer = []
mutex = threading.Lock()
empty_slots = threading.Semaphore(MAX_SIZE)  # Initially MAX_SIZE
full_slots = threading.Semaphore(0)          # Initially 0

def producer(id):
    for i in range(10):
        item = f"Item-{id}-{i}"
        empty_slots.acquire()  # Wait for empty slot
        with mutex:
            buffer.append(item)
            print(f"Producer {id} produced {item}, buffer size: {len(buffer)}")
        full_slots.release()  # Signal item available
        time.sleep(0.1)

def consumer(id):
    for i in range(10):
        full_slots.acquire()  # Wait for item
        with mutex:
            item = buffer.pop(0)
            print(f"Consumer {id} consumed {item}, buffer size: {len(buffer)}")
        empty_slots.release()  # Signal empty slot
        time.sleep(0.15)

# Create producers and consumers
producers = [threading.Thread(target=producer, args=(i,)) for i in range(2)]
consumers = [threading.Thread(target=consumer, args=(i,)) for i in range(2)]

for t in producers + consumers:
    t.start()
for t in producers + consumers:
    t.join()
</code></pre>
<h3 id="condition-variables"><a class="header" href="#condition-variables">Condition Variables</a></h3>
<p><strong>Condition Variable:</strong> Allows threads to wait for a specific condition to become true, avoiding busy-waiting.</p>
<p><strong>Operations:</strong></p>
<ul>
<li><code>wait()</code>: Release lock and sleep until signaled</li>
<li><code>notify()</code> / <code>signal()</code>: Wake up one waiting thread</li>
<li><code>notify_all()</code> / <code>broadcast()</code>: Wake up all waiting threads</li>
</ul>
<p><strong>Python example:</strong></p>
<pre><code class="language-python">import threading
import time

class BoundedQueue:
    def __init__(self, max_size):
        self.queue = []
        self.max_size = max_size
        self.lock = threading.Lock()
        self.not_empty = threading.Condition(self.lock)
        self.not_full = threading.Condition(self.lock)

    def put(self, item):
        with self.not_full:  # Acquires lock
            while len(self.queue) &gt;= self.max_size:
                self.not_full.wait()  # Release lock and wait
            self.queue.append(item)
            self.not_empty.notify()  # Wake up a consumer

    def get(self):
        with self.not_empty:
            while len(self.queue) == 0:
                self.not_empty.wait()
            item = self.queue.pop(0)
            self.not_full.notify()  # Wake up a producer
            return item

# Usage
queue = BoundedQueue(5)

def producer(id):
    for i in range(10):
        item = f"P{id}-Item{i}"
        queue.put(item)
        print(f"Produced: {item}")
        time.sleep(0.1)

def consumer(id):
    for i in range(10):
        item = queue.get()
        print(f"Consumer {id} consumed: {item}")
        time.sleep(0.15)

threads = [
    threading.Thread(target=producer, args=(1,)),
    threading.Thread(target=consumer, args=(1,)),
]
for t in threads:
    t.start()
for t in threads:
    t.join()
</code></pre>
<p><strong>C++ example:</strong></p>
<pre><code class="language-cpp">#include &lt;condition_variable&gt;
#include &lt;mutex&gt;
#include &lt;queue&gt;
#include &lt;thread&gt;

template&lt;typename T&gt;
class BoundedQueue {
private:
    std::queue&lt;T&gt; queue;
    size_t max_size;
    std::mutex mtx;
    std::condition_variable not_empty;
    std::condition_variable not_full;

public:
    BoundedQueue(size_t size) : max_size(size) {}

    void put(T item) {
        std::unique_lock&lt;std::mutex&gt; lock(mtx);
        not_full.wait(lock, [this] { return queue.size() &lt; max_size; });
        queue.push(item);
        not_empty.notify_one();
    }

    T get() {
        std::unique_lock&lt;std::mutex&gt; lock(mtx);
        not_empty.wait(lock, [this] { return !queue.empty(); });
        T item = queue.front();
        queue.pop();
        not_full.notify_one();
        return item;
    }
};
</code></pre>
<p><strong>Important pattern - Wait in a loop:</strong></p>
<pre><code class="language-python"># WRONG: Don't do this
with condition:
    if not predicate():
        condition.wait()
    # Proceed

# RIGHT: Always wait in a loop
with condition:
    while not predicate():
        condition.wait()
    # Proceed
</code></pre>
<p><strong>Why?</strong> Spurious wakeups can occur (thread wakes up without being signaled), and multiple threads might be waiting.</p>
<h3 id="read-write-locks"><a class="header" href="#read-write-locks">Read-Write Locks</a></h3>
<p><strong>RWLock:</strong> Allows multiple readers OR one writer (but not both simultaneously).</p>
<p><strong>Benefits:</strong></p>
<ul>
<li>Better concurrency for read-heavy workloads</li>
<li>Multiple threads can read simultaneously</li>
<li>Writes still get exclusive access</li>
</ul>
<p><strong>Python example:</strong></p>
<pre><code class="language-python">import threading

class RWLock:
    def __init__(self):
        self.readers = 0
        self.writer = False
        self.lock = threading.Lock()
        self.can_read = threading.Condition(self.lock)
        self.can_write = threading.Condition(self.lock)

    def acquire_read(self):
        with self.lock:
            while self.writer:
                self.can_read.wait()
            self.readers += 1

    def release_read(self):
        with self.lock:
            self.readers -= 1
            if self.readers == 0:
                self.can_write.notify()

    def acquire_write(self):
        with self.lock:
            while self.writer or self.readers &gt; 0:
                self.can_write.wait()
            self.writer = True

    def release_write(self):
        with self.lock:
            self.writer = False
            self.can_write.notify()
            self.can_read.notify_all()

# Usage
class CachedData:
    def __init__(self):
        self.data = {}
        self.rwlock = RWLock()

    def read(self, key):
        self.rwlock.acquire_read()
        try:
            return self.data.get(key)
        finally:
            self.rwlock.release_read()

    def write(self, key, value):
        self.rwlock.acquire_write()
        try:
            self.data[key] = value
        finally:
            self.rwlock.release_write()
</code></pre>
<p><strong>Rust example (using std::sync::RwLock):</strong></p>
<pre class="playground"><code class="language-rust">use std::sync::{Arc, RwLock};
use std::thread;

fn main() {
    let data = Arc::new(RwLock::new(vec![1, 2, 3]));
    let mut handles = vec![];

    // Spawn readers
    for i in 0..5 {
        let data = Arc::clone(&amp;data);
        let handle = thread::spawn(move || {
            let r = data.read().unwrap();
            println!("Reader {}: {:?}", i, *r);
        });
        handles.push(handle);
    }

    // Spawn writer
    let data_clone = Arc::clone(&amp;data);
    handles.push(thread::spawn(move || {
        let mut w = data_clone.write().unwrap();
        w.push(4);
        println!("Writer added element");
    }));

    for handle in handles {
        handle.join().unwrap();
    }
}</code></pre>
<p><strong>C++ example:</strong></p>
<pre><code class="language-cpp">#include &lt;shared_mutex&gt;
#include &lt;thread&gt;
#include &lt;vector&gt;

class CachedData {
private:
    std::map&lt;std::string, int&gt; data;
    mutable std::shared_mutex rwlock;

public:
    int read(const std::string&amp; key) const {
        std::shared_lock lock(rwlock);  // Multiple readers OK
        auto it = data.find(key);
        return it != data.end() ? it-&gt;second : 0;
    }

    void write(const std::string&amp; key, int value) {
        std::unique_lock lock(rwlock);  // Exclusive access
        data[key] = value;
    }
};
</code></pre>
<p><strong>Performance characteristics:</strong></p>
<ul>
<li><strong>Uncontended read:</strong> Very fast (just increment counter)</li>
<li><strong>Contended read:</strong> Still fast (multiple readers allowed)</li>
<li><strong>Write:</strong> More expensive (must wait for all readers to finish)</li>
</ul>
<p><strong>Use when:</strong></p>
<ul>
<li>Read operations are much more frequent than writes</li>
<li>Read operations take significant time</li>
<li>Data is large enough that read-only access is valuable</li>
</ul>
<h3 id="spinlocks"><a class="header" href="#spinlocks">Spinlocks</a></h3>
<p><strong>Spinlock:</strong> A lock that causes threads to busy-wait (spin) in a loop checking if the lock is available.</p>
<p><strong>Characteristics:</strong></p>
<ul>
<li>No context switching overhead</li>
<li>Wastes CPU cycles while waiting</li>
<li>Only suitable for very short critical sections</li>
<li>Often used in kernel code</li>
</ul>
<p><strong>Python example (conceptual - not recommended for Python):</strong></p>
<pre><code class="language-python">import threading
import time

class Spinlock:
    def __init__(self):
        self.locked = False

    def acquire(self):
        while True:
            # Atomic test-and-set
            if not self.locked:
                self.locked = True
                break
            # Spin (busy-wait)

    def release(self):
        self.locked = False

# Note: Python's GIL makes spinlocks inefficient
# This is just for demonstration
</code></pre>
<p><strong>C++ example:</strong></p>
<pre><code class="language-cpp">#include &lt;atomic&gt;

class Spinlock {
private:
    std::atomic_flag flag = ATOMIC_FLAG_INIT;

public:
    void lock() {
        while (flag.test_and_set(std::memory_order_acquire)) {
            // Spin
        }
    }

    void unlock() {
        flag.clear(std::memory_order_release);
    }
};

// Usage
Spinlock spinlock;
int counter = 0;

void increment() {
    spinlock.lock();
    counter++;
    spinlock.unlock();
}
</code></pre>
<p><strong>Optimized spinlock with backoff:</strong></p>
<pre><code class="language-cpp">#include &lt;atomic&gt;
#include &lt;thread&gt;

class BackoffSpinlock {
private:
    std::atomic_flag flag = ATOMIC_FLAG_INIT;

public:
    void lock() {
        int backoff = 1;
        while (flag.test_and_set(std::memory_order_acquire)) {
            for (int i = 0; i &lt; backoff; i++) {
                // Pause instruction (hint to CPU)
                std::this_thread::yield();
            }
            backoff = std::min(backoff * 2, 1024);  // Exponential backoff
        }
    }

    void unlock() {
        flag.clear(std::memory_order_release);
    }
};
</code></pre>
<p><strong>When to use spinlocks:</strong></p>
<ul>
<li>Critical section is very short (&lt; 100 nanoseconds)</li>
<li>Number of threads ≤ number of cores</li>
<li>Real-time systems where latency is critical</li>
<li>Kernel-level code where sleeping is not allowed</li>
</ul>
<p><strong>When NOT to use spinlocks:</strong></p>
<ul>
<li>Critical section is long</li>
<li>More threads than cores (causes CPU waste)</li>
<li>User-space applications (use mutexes instead)</li>
</ul>
<h3 id="atomic-operations"><a class="header" href="#atomic-operations">Atomic Operations</a></h3>
<p><strong>Atomic operation:</strong> An operation that completes without interruption, appearing instantaneous to other threads.</p>
<p><strong>Common atomic operations:</strong></p>
<ul>
<li>Load</li>
<li>Store</li>
<li>Exchange (swap)</li>
<li>Compare-and-swap (CAS)</li>
<li>Fetch-and-add</li>
<li>Fetch-and-subtract</li>
</ul>
<p><strong>Python example:</strong></p>
<pre><code class="language-python">import threading

# Python's += is NOT atomic (even for integers)
counter = 0

def increment():
    global counter
    for _ in range(100000):
        counter += 1  # NOT ATOMIC!

threads = [threading.Thread(target=increment) for _ in range(10)]
for t in threads:
    t.start()
for t in threads:
    t.join()

print(f"Counter: {counter}")  # Will be &lt; 1000000 due to race conditions

# To fix: use threading.Lock or atomic operations
</code></pre>
<p><strong>C++ atomic example:</strong></p>
<pre><code class="language-cpp">#include &lt;atomic&gt;
#include &lt;thread&gt;
#include &lt;vector&gt;

std::atomic&lt;int&gt; counter(0);

void increment() {
    for (int i = 0; i &lt; 100000; i++) {
        counter.fetch_add(1, std::memory_order_relaxed);
        // Or simply: counter++;  (atomic increment)
    }
}

int main() {
    std::vector&lt;std::thread&gt; threads;
    for (int i = 0; i &lt; 10; i++) {
        threads.emplace_back(increment);
    }
    for (auto&amp; t : threads) {
        t.join();
    }
    std::cout &lt;&lt; "Counter: " &lt;&lt; counter &lt;&lt; std::endl;  // Correctly prints 1000000
}
</code></pre>
<p><strong>Compare-and-swap (CAS) example:</strong></p>
<pre><code class="language-cpp">#include &lt;atomic&gt;

std::atomic&lt;int&gt; value(0);

void increment_cas() {
    int expected = value.load();
    int desired;
    do {
        desired = expected + 1;
    } while (!value.compare_exchange_weak(expected, desired));
    // If value == expected, set value to desired and return true
    // Otherwise, load current value into expected and return false
}
</code></pre>
<p><strong>Rust atomic example:</strong></p>
<pre class="playground"><code class="language-rust">use std::sync::atomic::{AtomicUsize, Ordering};
use std::sync::Arc;
use std::thread;

fn main() {
    let counter = Arc::new(AtomicUsize::new(0));
    let mut handles = vec![];

    for _ in 0..10 {
        let counter = Arc::clone(&amp;counter);
        let handle = thread::spawn(move || {
            for _ in 0..100000 {
                counter.fetch_add(1, Ordering::Relaxed);
            }
        });
        handles.push(handle);
    }

    for handle in handles {
        handle.join().unwrap();
    }

    println!("Result: {}", counter.load(Ordering::Relaxed));
}</code></pre>
<p><strong>Memory ordering (important for atomics):</strong></p>
<ol>
<li><strong>Relaxed:</strong> No synchronization, only atomicity guaranteed</li>
<li><strong>Acquire:</strong> Prevents reordering of subsequent reads/writes before this operation</li>
<li><strong>Release:</strong> Prevents reordering of prior reads/writes after this operation</li>
<li><strong>AcqRel:</strong> Both acquire and release</li>
<li><strong>SeqCst:</strong> Sequential consistency (strongest, most expensive)</li>
</ol>
<pre><code class="language-cpp">// Example: Producer-consumer with atomics
std::atomic&lt;bool&gt; data_ready(false);
int data;

// Producer thread
void produce() {
    data = 42;  // Write data
    data_ready.store(true, std::memory_order_release);  // Signal
}

// Consumer thread
void consume() {
    while (!data_ready.load(std::memory_order_acquire)) {
        // Wait
    }
    // Now safe to read data
    std::cout &lt;&lt; data &lt;&lt; std::endl;
}
</code></pre>
<p><strong>Lock-free counter using atomics:</strong></p>
<pre><code class="language-cpp">template&lt;typename T&gt;
class LockFreeStack {
private:
    struct Node {
        T data;
        Node* next;
    };
    std::atomic&lt;Node*&gt; head;

public:
    LockFreeStack() : head(nullptr) {}

    void push(T value) {
        Node* new_node = new Node{value, nullptr};
        new_node-&gt;next = head.load();
        while (!head.compare_exchange_weak(new_node-&gt;next, new_node)) {
            // Retry if another thread modified head
        }
    }

    bool pop(T&amp; result) {
        Node* old_head = head.load();
        while (old_head &amp;&amp;
               !head.compare_exchange_weak(old_head, old_head-&gt;next)) {
            // Retry
        }
        if (old_head) {
            result = old_head-&gt;data;
            delete old_head;  // Note: ABA problem in production code!
            return true;
        }
        return false;
    }
};
</code></pre>
<hr>
<h2 id="concurrency-patterns"><a class="header" href="#concurrency-patterns">Concurrency Patterns</a></h2>
<p>Common patterns for structuring concurrent programs.</p>
<h3 id="producer-consumer-pattern"><a class="header" href="#producer-consumer-pattern">Producer-Consumer Pattern</a></h3>
<p><strong>Problem:</strong> Decouple production of data from its consumption.</p>
<p><strong>Components:</strong></p>
<ul>
<li>Producers: Generate data</li>
<li>Consumers: Process data</li>
<li>Buffer: Queue between producers and consumers</li>
</ul>
<p><strong>Python implementation:</strong></p>
<pre><code class="language-python">import threading
import queue
import time
import random

def producer(q, producer_id):
    for i in range(5):
        item = f"Item-{producer_id}-{i}"
        time.sleep(random.uniform(0.1, 0.5))
        q.put(item)
        print(f"Producer {producer_id} produced {item}")
    # Signal completion
    q.put(None)

def consumer(q, consumer_id):
    while True:
        item = q.get()
        if item is None:
            q.put(None)  # Pass signal to other consumers
            break
        print(f"Consumer {consumer_id} consumed {item}")
        time.sleep(random.uniform(0.1, 0.3))
        q.task_done()

# Create queue with max size
buffer = queue.Queue(maxsize=10)

# Create and start threads
producers = [threading.Thread(target=producer, args=(buffer, i)) for i in range(2)]
consumers = [threading.Thread(target=consumer, args=(buffer, i)) for i in range(3)]

for t in producers + consumers:
    t.start()

for t in producers + consumers:
    t.join()

print("All done!")
</code></pre>
<p><strong>Go implementation:</strong></p>
<pre><code class="language-go">package main

import (
    "fmt"
    "math/rand"
    "sync"
    "time"
)

func producer(ch chan&lt;- int, id int, wg *sync.WaitGroup) {
    defer wg.Done()
    for i := 0; i &lt; 5; i++ {
        item := id*100 + i
        time.Sleep(time.Duration(rand.Intn(500)) * time.Millisecond)
        ch &lt;- item
        fmt.Printf("Producer %d produced %d\n", id, item)
    }
}

func consumer(ch &lt;-chan int, id int, wg *sync.WaitGroup) {
    defer wg.Done()
    for item := range ch {
        fmt.Printf("Consumer %d consumed %d\n", id, item)
        time.Sleep(time.Duration(rand.Intn(300)) * time.Millisecond)
    }
}

func main() {
    buffer := make(chan int, 10)  // Buffered channel
    var producerWg, consumerWg sync.WaitGroup

    // Start producers
    for i := 0; i &lt; 2; i++ {
        producerWg.Add(1)
        go producer(buffer, i, &amp;producerWg)
    }

    // Start consumers
    for i := 0; i &lt; 3; i++ {
        consumerWg.Add(1)
        go consumer(buffer, i, &amp;consumerWg)
    }

    // Wait for producers to finish, then close channel
    go func() {
        producerWg.Wait()
        close(buffer)
    }()

    // Wait for consumers
    consumerWg.Wait()
    fmt.Println("All done!")
}
</code></pre>
<h3 id="reader-writer-pattern"><a class="header" href="#reader-writer-pattern">Reader-Writer Pattern</a></h3>
<p><strong>Problem:</strong> Multiple readers can access data simultaneously, but writers need exclusive access.</p>
<p><strong>Implementation using RWLock:</strong></p>
<pre><code class="language-python">import threading
import time

class SharedResource:
    def __init__(self):
        self.data = []
        self.rwlock = threading.Lock()  # Simple version
        # In production, use a proper RWLock implementation

    def read_data(self, reader_id):
        # Multiple readers can hold this
        print(f"Reader {reader_id} reading: {self.data}")
        time.sleep(0.1)

    def write_data(self, writer_id, value):
        with self.rwlock:
            print(f"Writer {writer_id} writing {value}")
            self.data.append(value)
            time.sleep(0.2)

# Usage
resource = SharedResource()

def reader(resource, id):
    for _ in range(3):
        resource.read_data(id)
        time.sleep(0.05)

def writer(resource, id):
    for i in range(2):
        resource.write_data(id, f"Data-{id}-{i}")
        time.sleep(0.1)

threads = []
threads.extend([threading.Thread(target=reader, args=(resource, i)) for i in range(5)])
threads.extend([threading.Thread(target=writer, args=(resource, i)) for i in range(2)])

for t in threads:
    t.start()
for t in threads:
    t.join()
</code></pre>
<h3 id="thread-pool-pattern"><a class="header" href="#thread-pool-pattern">Thread Pool Pattern</a></h3>
<p><strong>Problem:</strong> Creating threads is expensive; reuse a fixed pool of threads for tasks.</p>
<p><strong>Python implementation:</strong></p>
<pre><code class="language-python">from concurrent.futures import ThreadPoolExecutor
import time

def task(n):
    print(f"Processing task {n}")
    time.sleep(1)
    return n * n

# Create thread pool with 4 workers
with ThreadPoolExecutor(max_workers=4) as executor:
    # Submit tasks
    futures = [executor.submit(task, i) for i in range(10)]

    # Get results as they complete
    for future in futures:
        result = future.result()
        print(f"Result: {result}")

# Alternative: map operation
with ThreadPoolExecutor(max_workers=4) as executor:
    results = executor.map(task, range(10))
    for result in results:
        print(f"Result: {result}")
</code></pre>
<p><strong>Custom thread pool implementation:</strong></p>
<pre><code class="language-python">import threading
import queue

class ThreadPool:
    def __init__(self, num_threads):
        self.tasks = queue.Queue()
        self.threads = []
        for _ in range(num_threads):
            t = threading.Thread(target=self._worker)
            t.daemon = True
            t.start()
            self.threads.append(t)

    def _worker(self):
        while True:
            func, args, kwargs = self.tasks.get()
            if func is None:
                break
            try:
                func(*args, **kwargs)
            except Exception as e:
                print(f"Error in task: {e}")
            finally:
                self.tasks.task_done()

    def submit(self, func, *args, **kwargs):
        self.tasks.put((func, args, kwargs))

    def wait_completion(self):
        self.tasks.join()

    def shutdown(self):
        for _ in self.threads:
            self.tasks.put((None, None, None))
        for t in self.threads:
            t.join()

# Usage
pool = ThreadPool(4)
for i in range(10):
    pool.submit(task, i)
pool.wait_completion()
pool.shutdown()
</code></pre>
<p><strong>Java ExecutorService:</strong></p>
<pre><code class="language-java">import java.util.concurrent.*;

public class ThreadPoolExample {
    public static void main(String[] args) throws InterruptedException {
        // Create thread pool
        ExecutorService executor = Executors.newFixedThreadPool(4);

        // Submit tasks
        for (int i = 0; i &lt; 10; i++) {
            final int taskId = i;
            executor.submit(() -&gt; {
                System.out.println("Task " + taskId + " running");
                try {
                    Thread.sleep(1000);
                } catch (InterruptedException e) {
                    e.printStackTrace();
                }
                return taskId * taskId;
            });
        }

        // Shutdown
        executor.shutdown();
        executor.awaitTermination(1, TimeUnit.MINUTES);
    }
}
</code></pre>
<h3 id="futurepromise-pattern"><a class="header" href="#futurepromise-pattern">Future/Promise Pattern</a></h3>
<p><strong>Problem:</strong> Represent a value that will be available in the future, allowing asynchronous computation.</p>
<p><strong>Python Future example:</strong></p>
<pre><code class="language-python">from concurrent.futures import ThreadPoolExecutor
import time

def slow_computation(n):
    time.sleep(2)
    return n * n

executor = ThreadPoolExecutor(max_workers=4)

# Submit computation, get Future object immediately
future = executor.submit(slow_computation, 5)

print("Computation started, doing other work...")
time.sleep(1)
print("Still doing other work...")

# Block until result is ready
result = future.result()  # Blocks here
print(f"Result: {result}")

# Check if done without blocking
future2 = executor.submit(slow_computation, 10)
if future2.done():
    print("Already done!")
else:
    print("Still computing...")
    future2.add_done_callback(lambda f: print(f"Result: {f.result()}"))

executor.shutdown()
</code></pre>
<p><strong>JavaScript Promise:</strong></p>
<pre><code class="language-javascript">// Creating a Promise
function slowComputation(n) {
    return new Promise((resolve, reject) =&gt; {
        setTimeout(() =&gt; {
            if (n &lt; 0) {
                reject(new Error("Negative number"));
            } else {
                resolve(n * n);
            }
        }, 2000);
    });
}

// Using Promise
slowComputation(5)
    .then(result =&gt; {
        console.log("Result:", result);
        return slowComputation(result);
    })
    .then(result =&gt; {
        console.log("Second result:", result);
    })
    .catch(error =&gt; {
        console.error("Error:", error);
    });

// Multiple Promises
Promise.all([
    slowComputation(2),
    slowComputation(3),
    slowComputation(4)
]).then(results =&gt; {
    console.log("All results:", results);
});
</code></pre>
<p><strong>Rust Future:</strong></p>
<pre class="playground"><code class="language-rust">use std::future::Future;
use std::pin::Pin;
use std::task::{Context, Poll};

struct SlowComputation {
    value: i32,
}

impl Future for SlowComputation {
    type Output = i32;

    fn poll(self: Pin&lt;&amp;mut Self&gt;, cx: &amp;mut Context&lt;'_&gt;) -&gt; Poll&lt;Self::Output&gt; {
        // Simulate async work
        Poll::Ready(self.value * self.value)
    }
}

// Using async/await (built on Futures)
async fn compute(n: i32) -&gt; i32 {
    // Simulate slow computation
    n * n
}

#[tokio::main]
async fn main() {
    let result = compute(5).await;
    println!("Result: {}", result);

    // Multiple concurrent futures
    let (r1, r2, r3) = tokio::join!(
        compute(2),
        compute(3),
        compute(4)
    );
    println!("Results: {}, {}, {}", r1, r2, r3);
}</code></pre>
<h3 id="asyncawait-pattern"><a class="header" href="#asyncawait-pattern">Async/Await Pattern</a></h3>
<p><strong>Problem:</strong> Write asynchronous code that looks synchronous, avoiding callback hell.</p>
<p><strong>Python asyncio:</strong></p>
<pre><code class="language-python">import asyncio
import aiohttp

async def fetch_url(session, url):
    print(f"Fetching {url}")
    async with session.get(url) as response:
        data = await response.text()
        print(f"Got {len(data)} bytes from {url}")
        return data

async def main():
    urls = [
        'http://example.com',
        'http://example.org',
        'http://example.net'
    ]

    async with aiohttp.ClientSession() as session:
        # Concurrent execution
        tasks = [fetch_url(session, url) for url in urls]
        results = await asyncio.gather(*tasks)

    print(f"Fetched {len(results)} URLs")

# Run
asyncio.run(main())
</code></pre>
<p><strong>JavaScript async/await:</strong></p>
<pre><code class="language-javascript">// Async function
async function fetchUserData(userId) {
    try {
        const response = await fetch(`/api/users/${userId}`);
        const user = await response.json();

        // Sequential awaits
        const posts = await fetch(`/api/users/${userId}/posts`);
        const postsData = await posts.json();

        return { user, posts: postsData };
    } catch (error) {
        console.error("Error fetching user data:", error);
        throw error;
    }
}

// Concurrent execution
async function fetchMultipleUsers(userIds) {
    const promises = userIds.map(id =&gt; fetchUserData(id));
    const results = await Promise.all(promises);
    return results;
}

// Usage
fetchMultipleUsers([1, 2, 3])
    .then(users =&gt; console.log("Users:", users))
    .catch(error =&gt; console.error("Error:", error));
</code></pre>
<p><strong>C# async/await:</strong></p>
<pre><code class="language-csharp">using System;
using System.Net.Http;
using System.Threading.Tasks;

class Program {
    static async Task&lt;string&gt; FetchUrlAsync(string url) {
        using (HttpClient client = new HttpClient()) {
            Console.WriteLine($"Fetching {url}");
            string content = await client.GetStringAsync(url);
            Console.WriteLine($"Got {content.Length} bytes");
            return content;
        }
    }

    static async Task Main(string[] args) {
        var urls = new[] {
            "http://example.com",
            "http://example.org",
            "http://example.net"
        };

        // Concurrent execution
        var tasks = Array.ConvertAll(urls, url =&gt; FetchUrlAsync(url));
        var results = await Task.WhenAll(tasks);

        Console.WriteLine($"Fetched {results.Length} URLs");
    }
}
</code></pre>
<h3 id="pipeline-pattern"><a class="header" href="#pipeline-pattern">Pipeline Pattern</a></h3>
<p><strong>Problem:</strong> Process data through a series of stages, each running concurrently.</p>
<p><strong>Go pipeline:</strong></p>
<pre><code class="language-go">package main

import "fmt"

// Stage 1: Generate numbers
func generate(nums ...int) &lt;-chan int {
    out := make(chan int)
    go func() {
        for _, n := range nums {
            out &lt;- n
        }
        close(out)
    }()
    return out
}

// Stage 2: Square numbers
func square(in &lt;-chan int) &lt;-chan int {
    out := make(chan int)
    go func() {
        for n := range in {
            out &lt;- n * n
        }
        close(out)
    }()
    return out
}

// Stage 3: Filter even numbers
func filterEven(in &lt;-chan int) &lt;-chan int {
    out := make(chan int)
    go func() {
        for n := range in {
            if n%2 == 0 {
                out &lt;- n
            }
        }
        close(out)
    }()
    return out
}

func main() {
    // Build pipeline
    c := generate(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)
    c = square(c)
    c = filterEven(c)

    // Consume results
    for result := range c {
        fmt.Println(result)
    }
}
</code></pre>
<p><strong>Python pipeline:</strong></p>
<pre><code class="language-python">import queue
import threading

def pipeline_stage(input_queue, output_queue, transform):
    while True:
        item = input_queue.get()
        if item is None:
            output_queue.put(None)
            break
        result = transform(item)
        if result is not None:
            output_queue.put(result)
        input_queue.task_done()

# Create queues
q1 = queue.Queue()
q2 = queue.Queue()
q3 = queue.Queue()

# Create stages
stage1 = threading.Thread(target=pipeline_stage, args=(q1, q2, lambda x: x * x))
stage2 = threading.Thread(target=pipeline_stage, args=(q2, q3, lambda x: x if x % 2 == 0 else None))

stage1.start()
stage2.start()

# Feed input
for i in range(1, 11):
    q1.put(i)
q1.put(None)

# Consume output
while True:
    item = q3.get()
    if item is None:
        break
    print(item)

stage1.join()
stage2.join()
</code></pre>
<hr>
<h2 id="language-specific-implementations"><a class="header" href="#language-specific-implementations">Language-Specific Implementations</a></h2>
<h3 id="python"><a class="header" href="#python">Python</a></h3>
<p>Python’s concurrency model is unique due to the <strong>Global Interpreter Lock (GIL)</strong>.</p>
<h4 id="global-interpreter-lock-gil"><a class="header" href="#global-interpreter-lock-gil">Global Interpreter Lock (GIL)</a></h4>
<p><strong>What is it?</strong> A mutex that protects access to Python objects, preventing multiple threads from executing Python bytecode simultaneously.</p>
<p><strong>Implications:</strong></p>
<ul>
<li><strong>CPU-bound tasks:</strong> Multithreading doesn’t help (only one thread executes at a time)</li>
<li><strong>I/O-bound tasks:</strong> Multithreading works well (threads release GIL during I/O)</li>
<li><strong>Multiprocessing:</strong> Required for true parallelism in CPU-bound tasks</li>
</ul>
<p><strong>Example showing GIL impact:</strong></p>
<pre><code class="language-python">import threading
import time

def cpu_bound():
    count = 0
    for i in range(50_000_000):
        count += 1
    return count

# Single-threaded
start = time.time()
cpu_bound()
cpu_bound()
print(f"Single-threaded: {time.time() - start:.2f}s")

# Multi-threaded (doesn't help due to GIL)
start = time.time()
t1 = threading.Thread(target=cpu_bound)
t2 = threading.Thread(target=cpu_bound)
t1.start()
t2.start()
t1.join()
t2.join()
print(f"Multi-threaded: {time.time() - start:.2f}s")  # Similar time!

# Multi-processing (true parallelism)
import multiprocessing
start = time.time()
p1 = multiprocessing.Process(target=cpu_bound)
p2 = multiprocessing.Process(target=cpu_bound)
p1.start()
p2.start()
p1.join()
p2.join()
print(f"Multi-processing: {time.time() - start:.2f}s")  # Faster!
</code></pre>
<h4 id="threading-module"><a class="header" href="#threading-module">Threading Module</a></h4>
<pre><code class="language-python">import threading
import time

# Basic thread creation
def worker(name, delay):
    print(f"{name} starting")
    time.sleep(delay)
    print(f"{name} finished")

t = threading.Thread(target=worker, args=("Thread-1", 2))
t.start()
t.join()

# Thread with return value
from concurrent.futures import ThreadPoolExecutor

def compute(x):
    return x * x

with ThreadPoolExecutor() as executor:
    future = executor.submit(compute, 5)
    result = future.result()
    print(f"Result: {result}")

# Thread-local storage
thread_local = threading.local()

def process():
    if not hasattr(thread_local, 'value'):
        thread_local.value = threading.current_thread().name
    print(f"Thread {thread_local.value} processing")

threads = [threading.Thread(target=process) for _ in range(3)]
for t in threads:
    t.start()
for t in threads:
    t.join()
</code></pre>
<h4 id="multiprocessing-module"><a class="header" href="#multiprocessing-module">Multiprocessing Module</a></h4>
<pre><code class="language-python">import multiprocessing
import os

def worker(num):
    print(f"Worker {num}, PID: {os.getpid()}")
    return num * num

if __name__ == '__main__':
    # Process pool
    with multiprocessing.Pool(processes=4) as pool:
        results = pool.map(worker, range(10))
        print(results)

    # Shared memory
    shared_value = multiprocessing.Value('i', 0)
    shared_array = multiprocessing.Array('d', [1.0, 2.0, 3.0])

    def increment(val):
        with val.get_lock():
            val.value += 1

    processes = [multiprocessing.Process(target=increment, args=(shared_value,)) for _ in range(10)]
    for p in processes:
        p.start()
    for p in processes:
        p.join()

    print(f"Final value: {shared_value.value}")

    # Queue for communication
    queue = multiprocessing.Queue()

    def producer(q):
        for i in range(5):
            q.put(i)

    def consumer(q):
        while True:
            item = q.get()
            if item is None:
                break
            print(f"Consumed: {item}")

    p1 = multiprocessing.Process(target=producer, args=(queue,))
    p2 = multiprocessing.Process(target=consumer, args=(queue,))

    p1.start()
    p2.start()
    p1.join()
    queue.put(None)
    p2.join()
</code></pre>
<h4 id="asyncio"><a class="header" href="#asyncio">AsyncIO</a></h4>
<pre><code class="language-python">import asyncio
import time

# Basic async function
async def say_hello(name, delay):
    await asyncio.sleep(delay)
    print(f"Hello, {name}!")

# Run async function
asyncio.run(say_hello("World", 1))

# Multiple concurrent tasks
async def main():
    await asyncio.gather(
        say_hello("Alice", 1),
        say_hello("Bob", 2),
        say_hello("Charlie", 1.5)
    )

asyncio.run(main())

# Async context manager
class AsyncResource:
    async def __aenter__(self):
        print("Acquiring resource")
        await asyncio.sleep(0.1)
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        print("Releasing resource")
        await asyncio.sleep(0.1)

async def use_resource():
    async with AsyncResource() as resource:
        print("Using resource")

asyncio.run(use_resource())

# Async generator
async def async_range(count):
    for i in range(count):
        await asyncio.sleep(0.1)
        yield i

async def consume():
    async for i in async_range(5):
        print(i)

asyncio.run(consume())

# Running blocking code in executor
import concurrent.futures

def blocking_io():
    time.sleep(1)
    return "Done"

async def main():
    loop = asyncio.get_event_loop()
    result = await loop.run_in_executor(None, blocking_io)
    print(result)

asyncio.run(main())
</code></pre>
<h3 id="javascript"><a class="header" href="#javascript">JavaScript</a></h3>
<p>JavaScript uses an <strong>event loop</strong> for concurrency, running on a single thread.</p>
<h4 id="event-loop"><a class="header" href="#event-loop">Event Loop</a></h4>
<pre><code class="language-javascript">console.log("1");

setTimeout(() =&gt; {
    console.log("2");
}, 0);

Promise.resolve().then(() =&gt; {
    console.log("3");
});

console.log("4");

// Output: 1, 4, 3, 2
// Explanation:
// - Synchronous code runs first: 1, 4
// - Microtasks (Promises) run next: 3
// - Macrotasks (setTimeout) run last: 2
</code></pre>
<p><strong>Event loop phases:</strong></p>
<ol>
<li><strong>Call stack:</strong> Synchronous code</li>
<li><strong>Microtask queue:</strong> Promises, process.nextTick (Node.js)</li>
<li><strong>Macrotask queue:</strong> setTimeout, setInterval, I/O</li>
</ol>
<h4 id="asyncawait"><a class="header" href="#asyncawait">Async/Await</a></h4>
<pre><code class="language-javascript">// Async function always returns a Promise
async function fetchData() {
    const response = await fetch('https://api.example.com/data');
    const data = await response.json();
    return data;
}

// Error handling
async function fetchWithErrorHandling() {
    try {
        const data = await fetchData();
        console.log(data);
    } catch (error) {
        console.error("Error:", error);
    }
}

// Parallel execution
async function fetchMultiple() {
    const [user, posts, comments] = await Promise.all([
        fetch('/api/user').then(r =&gt; r.json()),
        fetch('/api/posts').then(r =&gt; r.json()),
        fetch('/api/comments').then(r =&gt; r.json())
    ]);

    return { user, posts, comments };
}

// Race condition
async function fetchWithTimeout(url, timeout) {
    const fetchPromise = fetch(url);
    const timeoutPromise = new Promise((_, reject) =&gt;
        setTimeout(() =&gt; reject(new Error('Timeout')), timeout)
    );

    return Promise.race([fetchPromise, timeoutPromise]);
}
</code></pre>
<h4 id="web-workers"><a class="header" href="#web-workers">Web Workers</a></h4>
<pre><code class="language-javascript">// main.js - Main thread
const worker = new Worker('worker.js');

// Send message to worker
worker.postMessage({ data: [1, 2, 3, 4, 5] });

// Receive message from worker
worker.onmessage = function(event) {
    console.log("Result from worker:", event.data);
};

worker.onerror = function(error) {
    console.error("Worker error:", error);
};

// Terminate worker
// worker.terminate();

// worker.js - Worker thread
self.onmessage = function(event) {
    const data = event.data.data;

    // Perform heavy computation
    const result = data.map(x =&gt; x * x);

    // Send result back
    self.postMessage(result);
};
</code></pre>
<p><strong>SharedArrayBuffer (advanced):</strong></p>
<pre><code class="language-javascript">// main.js
const shared = new SharedArrayBuffer(16);
const view = new Int32Array(shared);

const worker = new Worker('worker.js');
worker.postMessage(shared);

// Atomic operations
Atomics.store(view, 0, 123);
console.log(Atomics.load(view, 0));

// Wait/notify
Atomics.wait(view, 0, 123);  // Wait until value at index 0 is not 123

// worker.js
self.onmessage = function(event) {
    const shared = event.data;
    const view = new Int32Array(shared);

    Atomics.store(view, 0, 456);
    Atomics.notify(view, 0, 1);  // Wake up one waiter
};
</code></pre>
<h3 id="go"><a class="header" href="#go">Go</a></h3>
<p>Go’s concurrency is based on <strong>goroutines</strong> and <strong>channels</strong>.</p>
<h4 id="goroutines"><a class="header" href="#goroutines">Goroutines</a></h4>
<pre><code class="language-go">package main

import (
    "fmt"
    "time"
)

func say(s string) {
    for i := 0; i &lt; 3; i++ {
        time.Sleep(100 * time.Millisecond)
        fmt.Println(s)
    }
}

func main() {
    // Start goroutine
    go say("world")
    say("hello")
}

// Anonymous goroutine
go func() {
    fmt.Println("Anonymous goroutine")
}()

// Goroutines are very lightweight (~2KB stack)
for i := 0; i &lt; 1000; i++ {
    go func(id int) {
        fmt.Println("Goroutine", id)
    }(i)
}
</code></pre>
<h4 id="channels"><a class="header" href="#channels">Channels</a></h4>
<pre><code class="language-go">// Unbuffered channel
ch := make(chan int)

// Send (blocks until received)
go func() {
    ch &lt;- 42
}()

// Receive (blocks until sent)
value := &lt;-ch
fmt.Println(value)

// Buffered channel
ch := make(chan int, 3)
ch &lt;- 1  // Doesn't block
ch &lt;- 2
ch &lt;- 3
// ch &lt;- 4  // Would block (buffer full)

// Close channel
close(ch)

// Range over channel
ch := make(chan int, 5)
go func() {
    for i := 0; i &lt; 5; i++ {
        ch &lt;- i
    }
    close(ch)
}()

for value := range ch {
    fmt.Println(value)
}

// Check if closed
value, ok := &lt;-ch
if !ok {
    fmt.Println("Channel closed")
}
</code></pre>
<h4 id="select-statement"><a class="header" href="#select-statement">Select Statement</a></h4>
<pre><code class="language-go">package main

import (
    "fmt"
    "time"
)

func main() {
    ch1 := make(chan string)
    ch2 := make(chan string)

    go func() {
        time.Sleep(1 * time.Second)
        ch1 &lt;- "one"
    }()

    go func() {
        time.Sleep(2 * time.Second)
        ch2 &lt;- "two"
    }()

    // Wait for both
    for i := 0; i &lt; 2; i++ {
        select {
        case msg1 := &lt;-ch1:
            fmt.Println("Received", msg1)
        case msg2 := &lt;-ch2:
            fmt.Println("Received", msg2)
        case &lt;-time.After(3 * time.Second):
            fmt.Println("Timeout")
        }
    }

    // Non-blocking select
    select {
    case msg := &lt;-ch1:
        fmt.Println(msg)
    default:
        fmt.Println("No message ready")
    }
}
</code></pre>
<h4 id="sync-package"><a class="header" href="#sync-package">Sync Package</a></h4>
<pre><code class="language-go">package main

import (
    "fmt"
    "sync"
)

// Mutex
var (
    counter int
    mutex   sync.Mutex
)

func increment() {
    mutex.Lock()
    counter++
    mutex.Unlock()
}

// WaitGroup
func worker(id int, wg *sync.WaitGroup) {
    defer wg.Done()
    fmt.Printf("Worker %d starting\n", id)
    fmt.Printf("Worker %d done\n", id)
}

func main() {
    var wg sync.WaitGroup

    for i := 1; i &lt;= 5; i++ {
        wg.Add(1)
        go worker(i, &amp;wg)
    }

    wg.Wait()
    fmt.Println("All workers done")
}

// Once (execute exactly once)
var once sync.Once

func initialize() {
    fmt.Println("Initializing...")
}

func main() {
    for i := 0; i &lt; 10; i++ {
        once.Do(initialize)  // Only prints once
    }
}

// Atomic operations
import "sync/atomic"

var counter int64

func increment() {
    atomic.AddInt64(&amp;counter, 1)
}

func get() int64 {
    return atomic.LoadInt64(&amp;counter)
}
</code></pre>
<h3 id="rust"><a class="header" href="#rust">Rust</a></h3>
<p>Rust’s ownership system ensures memory safety and eliminates data races at compile time.</p>
<h4 id="send-and-sync-traits"><a class="header" href="#send-and-sync-traits">Send and Sync Traits</a></h4>
<p><strong>Send:</strong> Type can be transferred between threads
<strong>Sync:</strong> Type can be accessed from multiple threads simultaneously (T is Sync if &amp;T is Send)</p>
<pre class="playground"><code class="language-rust">// Most types are Send and Sync
// Exceptions: Rc, RefCell (not thread-safe)

use std::sync::Arc;
use std::thread;

fn main() {
    let data = Arc::new(vec![1, 2, 3]);  // Arc is Send + Sync

    let mut handles = vec![];
    for i in 0..3 {
        let data = Arc::clone(&amp;data);
        let handle = thread::spawn(move || {
            println!("Thread {} sees: {:?}", i, data);
        });
        handles.push(handle);
    }

    for handle in handles {
        handle.join().unwrap();
    }
}</code></pre>
<h4 id="threads-1"><a class="header" href="#threads-1">Threads</a></h4>
<pre class="playground"><code class="language-rust">use std::thread;
use std::time::Duration;

fn main() {
    // Spawn thread
    let handle = thread::spawn(|| {
        for i in 1..10 {
            println!("Thread: {}", i);
            thread::sleep(Duration::from_millis(1));
        }
    });

    for i in 1..5 {
        println!("Main: {}", i);
        thread::sleep(Duration::from_millis(1));
    }

    handle.join().unwrap();

    // Thread with return value
    let handle = thread::spawn(|| {
        42
    });
    let result = handle.join().unwrap();
    println!("Result: {}", result);

    // Moving data into thread
    let v = vec![1, 2, 3];
    let handle = thread::spawn(move || {
        println!("Vector: {:?}", v);
    });
    // v is moved, can't use here
    handle.join().unwrap();
}</code></pre>
<h4 id="mutex-and-arc"><a class="header" href="#mutex-and-arc">Mutex and Arc</a></h4>
<pre class="playground"><code class="language-rust">use std::sync::{Arc, Mutex};
use std::thread;

fn main() {
    let counter = Arc::new(Mutex::new(0));
    let mut handles = vec![];

    for _ in 0..10 {
        let counter = Arc::clone(&amp;counter);
        let handle = thread::spawn(move || {
            let mut num = counter.lock().unwrap();
            *num += 1;
        });
        handles.push(handle);
    }

    for handle in handles {
        handle.join().unwrap();
    }

    println!("Result: {}", *counter.lock().unwrap());
}</code></pre>
<h4 id="channels-1"><a class="header" href="#channels-1">Channels</a></h4>
<pre class="playground"><code class="language-rust">use std::sync::mpsc;
use std::thread;
use std::time::Duration;

fn main() {
    let (tx, rx) = mpsc::channel();

    thread::spawn(move || {
        let vals = vec![
            String::from("hi"),
            String::from("from"),
            String::from("thread"),
        ];

        for val in vals {
            tx.send(val).unwrap();
            thread::sleep(Duration::from_secs(1));
        }
    });

    for received in rx {
        println!("Got: {}", received);
    }

    // Multiple producers
    let (tx, rx) = mpsc::channel();
    let tx1 = tx.clone();

    thread::spawn(move || {
        tx.send("message from first").unwrap();
    });

    thread::spawn(move || {
        tx1.send("message from second").unwrap();
    });

    for _ in 0..2 {
        println!("{}", rx.recv().unwrap());
    }
}</code></pre>
<h3 id="java"><a class="header" href="#java">Java</a></h3>
<h4 id="threads-2"><a class="header" href="#threads-2">Threads</a></h4>
<pre><code class="language-java">// Extending Thread class
class MyThread extends Thread {
    public void run() {
        System.out.println("Thread running: " + getName());
    }
}

// Implementing Runnable
class MyRunnable implements Runnable {
    public void run() {
        System.out.println("Runnable running");
    }
}

public class Main {
    public static void main(String[] args) {
        // Start thread
        MyThread thread = new MyThread();
        thread.start();

        // Using Runnable
        Thread thread2 = new Thread(new MyRunnable());
        thread2.start();

        // Lambda expression
        Thread thread3 = new Thread(() -&gt; {
            System.out.println("Lambda thread");
        });
        thread3.start();

        // Join
        try {
            thread.join();
            thread2.join();
            thread3.join();
        } catch (InterruptedException e) {
            e.printStackTrace();
        }
    }
}
</code></pre>
<h4 id="synchronized"><a class="header" href="#synchronized">Synchronized</a></h4>
<pre><code class="language-java">class Counter {
    private int count = 0;

    // Synchronized method
    public synchronized void increment() {
        count++;
    }

    // Synchronized block
    public void increment2() {
        synchronized(this) {
            count++;
        }
    }

    public synchronized int getCount() {
        return count;
    }
}

// Static synchronized (class-level lock)
class MyClass {
    private static int count = 0;

    public static synchronized void increment() {
        count++;
    }
}
</code></pre>
<h4 id="executorservice"><a class="header" href="#executorservice">ExecutorService</a></h4>
<pre><code class="language-java">import java.util.concurrent.*;

public class ExecutorExample {
    public static void main(String[] args) throws InterruptedException, ExecutionException {
        // Fixed thread pool
        ExecutorService executor = Executors.newFixedThreadPool(4);

        // Submit Runnable
        executor.submit(() -&gt; {
            System.out.println("Task running");
        });

        // Submit Callable (returns value)
        Future&lt;Integer&gt; future = executor.submit(() -&gt; {
            Thread.sleep(1000);
            return 42;
        });

        System.out.println("Result: " + future.get());  // Blocks

        // Execute multiple tasks
        List&lt;Callable&lt;Integer&gt;&gt; tasks = new ArrayList&lt;&gt;();
        for (int i = 0; i &lt; 10; i++) {
            final int taskId = i;
            tasks.add(() -&gt; taskId * taskId);
        }

        List&lt;Future&lt;Integer&gt;&gt; results = executor.invokeAll(tasks);
        for (Future&lt;Integer&gt; result : results) {
            System.out.println(result.get());
        }

        // Shutdown
        executor.shutdown();
        executor.awaitTermination(1, TimeUnit.MINUTES);
    }
}
</code></pre>
<h4 id="concurrent-collections"><a class="header" href="#concurrent-collections">Concurrent Collections</a></h4>
<pre><code class="language-java">import java.util.concurrent.*;

// ConcurrentHashMap
ConcurrentHashMap&lt;String, Integer&gt; map = new ConcurrentHashMap&lt;&gt;();
map.put("key", 1);
map.putIfAbsent("key", 2);  // Atomic
int value = map.get("key");

// CopyOnWriteArrayList (good for read-heavy workloads)
CopyOnWriteArrayList&lt;String&gt; list = new CopyOnWriteArrayList&lt;&gt;();
list.add("item");

// BlockingQueue
BlockingQueue&lt;Integer&gt; queue = new ArrayBlockingQueue&lt;&gt;(10);

// Producer
new Thread(() -&gt; {
    try {
        for (int i = 0; i &lt; 10; i++) {
            queue.put(i);  // Blocks if full
        }
    } catch (InterruptedException e) {
        e.printStackTrace();
    }
}).start();

// Consumer
new Thread(() -&gt; {
    try {
        while (true) {
            Integer item = queue.take();  // Blocks if empty
            System.out.println(item);
        }
    } catch (InterruptedException e) {
        e.printStackTrace();
    }
}).start();
</code></pre>
<h3 id="c"><a class="header" href="#c">C++</a></h3>
<h4 id="stdthread"><a class="header" href="#stdthread">std::thread</a></h4>
<pre><code class="language-cpp">#include &lt;iostream&gt;
#include &lt;thread&gt;
#include &lt;vector&gt;

void hello() {
    std::cout &lt;&lt; "Hello from thread\n";
}

void count(int n) {
    for (int i = 0; i &lt; n; i++) {
        std::cout &lt;&lt; i &lt;&lt; " ";
    }
}

int main() {
    // Basic thread
    std::thread t1(hello);
    t1.join();

    // Thread with arguments
    std::thread t2(count, 10);
    t2.join();

    // Lambda
    std::thread t3([]() {
        std::cout &lt;&lt; "Lambda thread\n";
    });
    t3.join();

    // Multiple threads
    std::vector&lt;std::thread&gt; threads;
    for (int i = 0; i &lt; 4; i++) {
        threads.emplace_back([i]() {
            std::cout &lt;&lt; "Thread " &lt;&lt; i &lt;&lt; "\n";
        });
    }

    for (auto&amp; t : threads) {
        t.join();
    }

    return 0;
}
</code></pre>
<h4 id="mutex"><a class="header" href="#mutex">Mutex</a></h4>
<pre><code class="language-cpp">#include &lt;iostream&gt;
#include &lt;thread&gt;
#include &lt;mutex&gt;
#include &lt;vector&gt;

std::mutex mtx;
int counter = 0;

void increment() {
    for (int i = 0; i &lt; 100000; i++) {
        std::lock_guard&lt;std::mutex&gt; lock(mtx);  // RAII
        counter++;
    }
}

int main() {
    std::vector&lt;std::thread&gt; threads;
    for (int i = 0; i &lt; 10; i++) {
        threads.emplace_back(increment);
    }

    for (auto&amp; t : threads) {
        t.join();
    }

    std::cout &lt;&lt; "Counter: " &lt;&lt; counter &lt;&lt; "\n";
    return 0;
}
</code></pre>
<h4 id="stdasync"><a class="header" href="#stdasync">std::async</a></h4>
<pre><code class="language-cpp">#include &lt;iostream&gt;
#include &lt;future&gt;
#include &lt;chrono&gt;

int compute(int n) {
    std::this_thread::sleep_for(std::chrono::seconds(1));
    return n * n;
}

int main() {
    // Launch async task
    std::future&lt;int&gt; result = std::async(std::launch::async, compute, 5);

    std::cout &lt;&lt; "Doing other work...\n";

    // Get result (blocks if not ready)
    std::cout &lt;&lt; "Result: " &lt;&lt; result.get() &lt;&lt; "\n";

    // Multiple async tasks
    auto f1 = std::async(std::launch::async, compute, 2);
    auto f2 = std::async(std::launch::async, compute, 3);
    auto f3 = std::async(std::launch::async, compute, 4);

    std::cout &lt;&lt; f1.get() + f2.get() + f3.get() &lt;&lt; "\n";

    return 0;
}
</code></pre>
<hr>
<h2 id="deadlock-prevention"><a class="header" href="#deadlock-prevention">Deadlock Prevention</a></h2>
<p>Four necessary conditions for deadlock (Coffman conditions):</p>
<ol>
<li>Mutual Exclusion</li>
<li>Hold and Wait</li>
<li>No Preemption</li>
<li>Circular Wait</li>
</ol>
<p><strong>Prevent deadlock by breaking at least one condition.</strong></p>
<h3 id="lock-ordering"><a class="header" href="#lock-ordering">Lock Ordering</a></h3>
<p>Always acquire locks in the same global order.</p>
<pre><code class="language-python">import threading

class BankAccount:
    def __init__(self, id, balance):
        self.id = id
        self.balance = balance
        self.lock = threading.Lock()

def transfer(from_account, to_account, amount):
    # WRONG: Can deadlock
    # with from_account.lock:
    #     with to_account.lock:
    #         from_account.balance -= amount
    #         to_account.balance += amount

    # RIGHT: Lock in consistent order (by ID)
    first, second = (from_account, to_account) if from_account.id &lt; to_account.id else (to_account, from_account)

    with first.lock:
        with second.lock:
            from_account.balance -= amount
            to_account.balance += amount

# Now safe regardless of call order
account1 = BankAccount(1, 1000)
account2 = BankAccount(2, 1000)

# Both threads acquire locks in same order (lock1, then lock2)
t1 = threading.Thread(target=transfer, args=(account1, account2, 100))
t2 = threading.Thread(target=transfer, args=(account2, account1, 50))
t1.start()
t2.start()
t1.join()
t2.join()
</code></pre>
<p><strong>C++ example:</strong></p>
<pre><code class="language-cpp">#include &lt;mutex&gt;
#include &lt;algorithm&gt;

class BankAccount {
public:
    int id;
    int balance;
    std::mutex mtx;

    BankAccount(int id, int bal) : id(id), balance(bal) {}
};

void transfer(BankAccount&amp; from, BankAccount&amp; to, int amount) {
    // Lock in consistent order
    BankAccount* first = &amp;from;
    BankAccount* second = &amp;to;

    if (from.id &gt; to.id) {
        std::swap(first, second);
    }

    std::lock_guard&lt;std::mutex&gt; lock1(first-&gt;mtx);
    std::lock_guard&lt;std::mutex&gt; lock2(second-&gt;mtx);

    from.balance -= amount;
    to.balance += amount;
}

// Or use std::lock to acquire multiple locks atomically
void transfer_v2(BankAccount&amp; from, BankAccount&amp; to, int amount) {
    std::unique_lock&lt;std::mutex&gt; lock1(from.mtx, std::defer_lock);
    std::unique_lock&lt;std::mutex&gt; lock2(to.mtx, std::defer_lock);

    std::lock(lock1, lock2);  // Acquire both atomically

    from.balance -= amount;
    to.balance += amount;
}
</code></pre>
<h3 id="lock-timeout"><a class="header" href="#lock-timeout">Lock Timeout</a></h3>
<p>Try to acquire lock with timeout; if timeout, release all locks and retry.</p>
<pre><code class="language-python">import threading
import time

class TimedLock:
    def __init__(self):
        self.lock = threading.Lock()

    def acquire_with_timeout(self, timeout):
        end_time = time.time() + timeout
        while True:
            if self.lock.acquire(blocking=False):
                return True
            if time.time() &gt;= end_time:
                return False
            time.sleep(0.001)

    def release(self):
        self.lock.release()

lock1 = TimedLock()
lock2 = TimedLock()

def worker1():
    while True:
        if lock1.acquire_with_timeout(1):
            try:
                time.sleep(0.1)
                if lock2.acquire_with_timeout(1):
                    try:
                        print("Worker1 has both locks")
                        break
                    finally:
                        lock2.release()
                else:
                    print("Worker1 timeout on lock2, retrying")
            finally:
                lock1.release()
        else:
            print("Worker1 timeout on lock1, retrying")
        time.sleep(0.01)  # Backoff

def worker2():
    while True:
        if lock2.acquire_with_timeout(1):
            try:
                time.sleep(0.1)
                if lock1.acquire_with_timeout(1):
                    try:
                        print("Worker2 has both locks")
                        break
                    finally:
                        lock1.release()
                else:
                    print("Worker2 timeout on lock1, retrying")
            finally:
                lock2.release()
        else:
            print("Worker2 timeout on lock2, retrying")
        time.sleep(0.01)

t1 = threading.Thread(target=worker1)
t2 = threading.Thread(target=worker2)
t1.start()
t2.start()
t1.join()
t2.join()
</code></pre>
<h3 id="try-lock"><a class="header" href="#try-lock">Try-Lock</a></h3>
<p>Attempt to acquire lock without blocking.</p>
<pre><code class="language-cpp">#include &lt;mutex&gt;
#include &lt;thread&gt;
#include &lt;chrono&gt;

std::mutex mtx1, mtx2;

void worker() {
    while (true) {
        if (mtx1.try_lock()) {
            std::this_thread::sleep_for(std::chrono::milliseconds(10));
            if (mtx2.try_lock()) {
                // Got both locks
                std::cout &lt;&lt; "Worker has both locks\n";
                mtx2.unlock();
                mtx1.unlock();
                break;
            } else {
                // Couldn't get second lock, release first
                mtx1.unlock();
            }
        }
        // Backoff before retry
        std::this_thread::sleep_for(std::chrono::milliseconds(1));
    }
}
</code></pre>
<h3 id="deadlock-detection"><a class="header" href="#deadlock-detection">Deadlock Detection</a></h3>
<p>Build resource allocation graph and detect cycles.</p>
<pre><code class="language-python">class DeadlockDetector:
    def __init__(self):
        self.waiting_for = {}  # thread -&gt; resource
        self.held_by = {}      # resource -&gt; thread
        self.lock = threading.Lock()

    def acquire_intent(self, thread_id, resource_id):
        with self.lock:
            self.waiting_for[thread_id] = resource_id

            # Check for cycle
            if self._has_cycle(thread_id):
                del self.waiting_for[thread_id]
                raise Exception(f"Deadlock detected! Thread {thread_id} waiting for {resource_id}")

    def acquire_complete(self, thread_id, resource_id):
        with self.lock:
            if thread_id in self.waiting_for:
                del self.waiting_for[thread_id]
            self.held_by[resource_id] = thread_id

    def release(self, thread_id, resource_id):
        with self.lock:
            if resource_id in self.held_by:
                del self.held_by[resource_id]

    def _has_cycle(self, start_thread):
        visited = set()
        thread = start_thread

        while thread not in visited:
            visited.add(thread)

            if thread not in self.waiting_for:
                return False

            resource = self.waiting_for[thread]

            if resource not in self.held_by:
                return False

            thread = self.held_by[resource]

            if thread == start_thread:
                return True

        return False
</code></pre>
<h3 id="resource-hierarchy"><a class="header" href="#resource-hierarchy">Resource Hierarchy</a></h3>
<p>Assign hierarchy levels to resources; always acquire in increasing order.</p>
<pre><code class="language-python"># Define resource hierarchy
RESOURCE_LEVELS = {
    'database': 1,
    'cache': 2,
    'network': 3,
    'file': 4
}

class HierarchicalLock:
    def __init__(self, name):
        self.name = name
        self.level = RESOURCE_LEVELS[name]
        self.lock = threading.Lock()

thread_local = threading.local()

def acquire_hierarchical(lock):
    if not hasattr(thread_local, 'max_level'):
        thread_local.max_level = 0

    if lock.level &lt;= thread_local.max_level:
        raise Exception(f"Lock hierarchy violation! Trying to acquire {lock.name} (level {lock.level}) after level {thread_local.max_level}")

    lock.lock.acquire()
    thread_local.max_level = lock.level

def release_hierarchical(lock):
    lock.lock.release()
    thread_local.max_level = lock.level - 1

# Usage
db_lock = HierarchicalLock('database')
cache_lock = HierarchicalLock('cache')

# This is OK
acquire_hierarchical(db_lock)
acquire_hierarchical(cache_lock)
release_hierarchical(cache_lock)
release_hierarchical(db_lock)

# This would raise exception (wrong order)
# acquire_hierarchical(cache_lock)
# acquire_hierarchical(db_lock)  # Exception!
</code></pre>
<hr>
<h2 id="performance-considerations"><a class="header" href="#performance-considerations">Performance Considerations</a></h2>
<h3 id="lock-contention"><a class="header" href="#lock-contention">Lock Contention</a></h3>
<p><strong>Problem:</strong> Many threads competing for the same lock, causing serialization.</p>
<p><strong>Solutions:</strong></p>
<ol>
<li><strong>Reduce critical section size:</strong></li>
</ol>
<pre><code class="language-python"># BAD: Large critical section
with lock:
    data = read_from_database()
    result = expensive_computation(data)
    write_to_cache(result)

# GOOD: Minimize critical section
data = read_from_database()
result = expensive_computation(data)
with lock:
    write_to_cache(result)
</code></pre>
<ol start="2">
<li><strong>Lock striping:</strong> Use multiple locks for different parts of data structure</li>
</ol>
<pre><code class="language-python">class StripedHashMap:
    def __init__(self, num_stripes=16):
        self.num_stripes = num_stripes
        self.stripes = [{'lock': threading.Lock(), 'data': {}} for _ in range(num_stripes)]

    def _get_stripe(self, key):
        return hash(key) % self.num_stripes

    def get(self, key):
        stripe = self.stripes[self._get_stripe(key)]
        with stripe['lock']:
            return stripe['data'].get(key)

    def put(self, key, value):
        stripe = self.stripes[self._get_stripe(key)]
        with stripe['lock']:
            stripe['data'][key] = value
</code></pre>
<ol start="3">
<li><strong>Read-write locks:</strong> Allow concurrent readers</li>
</ol>
<pre><code class="language-cpp">#include &lt;shared_mutex&gt;

std::shared_mutex rwlock;
std::map&lt;std::string, int&gt; data;

int read(const std::string&amp; key) {
    std::shared_lock lock(rwlock);  // Concurrent reads
    return data[key];
}

void write(const std::string&amp; key, int value) {
    std::unique_lock lock(rwlock);  // Exclusive write
    data[key] = value;
}
</code></pre>
<h3 id="false-sharing"><a class="header" href="#false-sharing">False Sharing</a></h3>
<p><strong>Problem:</strong> Different threads access different variables on the same cache line, causing unnecessary cache invalidation.</p>
<p><strong>Cache line:</strong> Typically 64 bytes; when one thread modifies a byte, the entire cache line is invalidated in other cores.</p>
<p><strong>Example of false sharing:</strong></p>
<pre><code class="language-cpp">// BAD: False sharing
struct Counters {
    int counter1;  // Likely on same cache line
    int counter2;  // as counter1
};

Counters counters;

// Thread 1
void increment1() {
    for (int i = 0; i &lt; 1000000; i++) {
        counters.counter1++;  // Invalidates cache line
    }
}

// Thread 2
void increment2() {
    for (int i = 0; i &lt; 1000000; i++) {
        counters.counter2++;  // Invalidates cache line
    }
}

// GOOD: Padding to separate cache lines
struct alignas(64) PaddedCounters {
    int counter1;
    char padding1[60];  // Fill rest of cache line
    int counter2;
    char padding2[60];
};

// Or use C++17 hardware_destructive_interference_size
struct Counters {
    alignas(std::hardware_destructive_interference_size) int counter1;
    alignas(std::hardware_destructive_interference_size) int counter2;
};
</code></pre>
<p><strong>Java example:</strong></p>
<pre><code class="language-java">// Using @Contended annotation (requires -XX:-RestrictContended)
public class Counters {
    @jdk.internal.vm.annotation.Contended
    volatile long counter1;

    @jdk.internal.vm.annotation.Contended
    volatile long counter2;
}
</code></pre>
<h3 id="lock-free-data-structures"><a class="header" href="#lock-free-data-structures">Lock-Free Data Structures</a></h3>
<p>Use atomic operations instead of locks for better performance.</p>
<p><strong>Lock-free queue (simplified):</strong></p>
<pre><code class="language-cpp">#include &lt;atomic&gt;

template&lt;typename T&gt;
class LockFreeQueue {
private:
    struct Node {
        T data;
        std::atomic&lt;Node*&gt; next;
        Node(T val) : data(val), next(nullptr) {}
    };

    std::atomic&lt;Node*&gt; head;
    std::atomic&lt;Node*&gt; tail;

public:
    LockFreeQueue() {
        Node* dummy = new Node(T());
        head.store(dummy);
        tail.store(dummy);
    }

    void enqueue(T value) {
        Node* node = new Node(value);
        Node* prev_tail;

        while (true) {
            prev_tail = tail.load();
            Node* next = prev_tail-&gt;next.load();

            if (prev_tail == tail.load()) {
                if (next == nullptr) {
                    if (prev_tail-&gt;next.compare_exchange_weak(next, node)) {
                        break;
                    }
                } else {
                    tail.compare_exchange_weak(prev_tail, next);
                }
            }
        }
        tail.compare_exchange_weak(prev_tail, node);
    }

    bool dequeue(T&amp; result) {
        while (true) {
            Node* first = head.load();
            Node* last = tail.load();
            Node* next = first-&gt;next.load();

            if (first == head.load()) {
                if (first == last) {
                    if (next == nullptr) {
                        return false;  // Empty
                    }
                    tail.compare_exchange_weak(last, next);
                } else {
                    result = next-&gt;data;
                    if (head.compare_exchange_weak(first, next)) {
                        delete first;  // Caution: ABA problem
                        return true;
                    }
                }
            }
        }
    }
};
</code></pre>
<p><strong>Benefits:</strong></p>
<ul>
<li>No lock contention</li>
<li>No deadlocks</li>
<li>Better scalability</li>
</ul>
<p><strong>Drawbacks:</strong></p>
<ul>
<li>Complex to implement correctly</li>
<li>ABA problem</li>
<li>Memory reclamation challenges</li>
</ul>
<h3 id="memory-ordering"><a class="header" href="#memory-ordering">Memory Ordering</a></h3>
<p><strong>Sequential consistency (strongest):</strong></p>
<pre><code class="language-cpp">std::atomic&lt;int&gt; x(0);
x.store(1, std::memory_order_seq_cst);  // Default
</code></pre>
<p><strong>Relaxed (weakest, fastest):</strong></p>
<pre><code class="language-cpp">x.store(1, std::memory_order_relaxed);  // Only atomicity, no ordering
</code></pre>
<p><strong>Acquire-Release:</strong></p>
<pre><code class="language-cpp">// Producer
data = 42;
flag.store(true, std::memory_order_release);

// Consumer
while (!flag.load(std::memory_order_acquire));
assert(data == 42);  // Guaranteed
</code></pre>
<p><strong>Performance impact:</strong></p>
<ul>
<li><strong>SeqCst:</strong> Full memory fence (slowest)</li>
<li><strong>AcqRel:</strong> Partial fence</li>
<li><strong>Relaxed:</strong> No fence (fastest)</li>
</ul>
<hr>
<h2 id="real-world-applications"><a class="header" href="#real-world-applications">Real-World Applications</a></h2>
<h3 id="web-servers"><a class="header" href="#web-servers">Web Servers</a></h3>
<p><strong>Problem:</strong> Handle thousands of concurrent requests.</p>
<p><strong>Solutions:</strong></p>
<ol>
<li><strong>Thread-per-request (traditional):</strong></li>
</ol>
<pre><code class="language-python">import socket
import threading

def handle_client(client_socket):
    request = client_socket.recv(1024)
    # Process request
    response = b"HTTP/1.1 200 OK\r\n\r\nHello World"
    client_socket.send(response)
    client_socket.close()

server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
server.bind(('0.0.0.0', 8080))
server.listen(5)

while True:
    client, addr = server.accept()
    thread = threading.Thread(target=handle_client, args=(client,))
    thread.start()
</code></pre>
<ol start="2">
<li><strong>Thread pool:</strong></li>
</ol>
<pre><code class="language-python">from concurrent.futures import ThreadPoolExecutor
import socket

def handle_client(client_socket):
    # ... same as above

server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
server.bind(('0.0.0.0', 8080))
server.listen(5)

with ThreadPoolExecutor(max_workers=50) as executor:
    while True:
        client, addr = server.accept()
        executor.submit(handle_client, client)
</code></pre>
<ol start="3">
<li><strong>Async I/O (most scalable):</strong></li>
</ol>
<pre><code class="language-python">import asyncio

async def handle_client(reader, writer):
    data = await reader.read(1024)
    # Process request
    response = b"HTTP/1.1 200 OK\r\n\r\nHello World"
    writer.write(response)
    await writer.drain()
    writer.close()

async def main():
    server = await asyncio.start_server(handle_client, '0.0.0.0', 8080)
    async with server:
        await server.serve_forever()

asyncio.run(main())
</code></pre>
<h3 id="database-connection-pools"><a class="header" href="#database-connection-pools">Database Connection Pools</a></h3>
<p><strong>Problem:</strong> Database connections are expensive to create; reuse a pool.</p>
<pre><code class="language-python">import threading
import queue
import time

class ConnectionPool:
    def __init__(self, create_connection, max_connections=10):
        self.create_connection = create_connection
        self.max_connections = max_connections
        self.pool = queue.Queue(maxsize=max_connections)
        self.current_connections = 0
        self.lock = threading.Lock()

    def acquire(self, timeout=None):
        try:
            # Try to get from pool
            return self.pool.get(block=False)
        except queue.Empty:
            # Pool empty, maybe create new connection
            with self.lock:
                if self.current_connections &lt; self.max_connections:
                    self.current_connections += 1
                    return self.create_connection()

            # Wait for available connection
            return self.pool.get(timeout=timeout)

    def release(self, connection):
        try:
            self.pool.put(connection, block=False)
        except queue.Full:
            # Pool full, close connection
            connection.close()
            with self.lock:
                self.current_connections -= 1

    def __enter__(self):
        self.connection = self.acquire()
        return self.connection

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.release(self.connection)

# Usage
def create_db_connection():
    # Simulate creating connection
    print("Creating new connection")
    return {"connection": "db"}

pool = ConnectionPool(create_db_connection, max_connections=5)

def worker(id):
    with pool as conn:
        print(f"Worker {id} using {conn}")
        time.sleep(1)
    print(f"Worker {id} released connection")

threads = [threading.Thread(target=worker, args=(i,)) for i in range(20)]
for t in threads:
    t.start()
for t in threads:
    t.join()
</code></pre>
<h3 id="gui-event-handling"><a class="header" href="#gui-event-handling">GUI Event Handling</a></h3>
<p><strong>Problem:</strong> Keep UI responsive while doing background work.</p>
<pre><code class="language-python">import tkinter as tk
import threading
import time

class Application(tk.Tk):
    def __init__(self):
        super().__init__()
        self.title("Concurrent GUI")

        self.label = tk.Label(self, text="Ready")
        self.label.pack()

        self.button = tk.Button(self, text="Start Task", command=self.start_task)
        self.button.pack()

    def start_task(self):
        self.label.config(text="Working...")
        self.button.config(state='disabled')

        # Run in background thread
        thread = threading.Thread(target=self.background_task)
        thread.start()

    def background_task(self):
        # Simulate long-running task
        for i in range(5):
            time.sleep(1)
            # Update UI from background thread (use after)
            self.after(0, self.update_progress, i + 1)

        self.after(0, self.task_complete)

    def update_progress(self, count):
        self.label.config(text=f"Progress: {count}/5")

    def task_complete(self):
        self.label.config(text="Done!")
        self.button.config(state='normal')

app = Application()
app.mainloop()
</code></pre>
<h3 id="background-task-processing"><a class="header" href="#background-task-processing">Background Task Processing</a></h3>
<p><strong>Task queue with workers:</strong></p>
<pre><code class="language-python">import threading
import queue
import time

class TaskQueue:
    def __init__(self, num_workers=4):
        self.tasks = queue.Queue()
        self.workers = []
        self.shutdown_flag = False

        for i in range(num_workers):
            worker = threading.Thread(target=self._worker, args=(i,))
            worker.start()
            self.workers.append(worker)

    def _worker(self, worker_id):
        while not self.shutdown_flag:
            try:
                task, callback = self.tasks.get(timeout=1)
                print(f"Worker {worker_id} processing {task}")
                result = self._process_task(task)
                if callback:
                    callback(result)
                self.tasks.task_done()
            except queue.Empty:
                continue

    def _process_task(self, task):
        # Simulate task processing
        time.sleep(2)
        return f"Result of {task}"

    def submit(self, task, callback=None):
        self.tasks.put((task, callback))

    def shutdown(self):
        self.shutdown_flag = True
        for worker in self.workers:
            worker.join()

# Usage
def on_complete(result):
    print(f"Task completed: {result}")

task_queue = TaskQueue(num_workers=4)

for i in range(10):
    task_queue.submit(f"Task-{i}", on_complete)

task_queue.tasks.join()  # Wait for all tasks
task_queue.shutdown()
</code></pre>
<hr>
<h2 id="best-practices"><a class="header" href="#best-practices">Best Practices</a></h2>
<h3 id="1-prefer-immutability"><a class="header" href="#1-prefer-immutability">1. Prefer Immutability</a></h3>
<p>Immutable data can be shared without synchronization.</p>
<pre><code class="language-python"># BAD: Mutable shared state
class Counter:
    def __init__(self):
        self.count = 0
        self.lock = threading.Lock()

    def increment(self):
        with self.lock:
            self.count += 1

# GOOD: Immutable data
from dataclasses import dataclass

@dataclass(frozen=True)
class CounterState:
    count: int

def increment(state):
    return CounterState(state.count + 1)

# Use atomic reference for updates
import threading

class AtomicReference:
    def __init__(self, value):
        self.value = value
        self.lock = threading.Lock()

    def get(self):
        with self.lock:
            return self.value

    def set(self, new_value):
        with self.lock:
            self.value = new_value

    def update(self, func):
        with self.lock:
            self.value = func(self.value)

state = AtomicReference(CounterState(0))
state.update(increment)
</code></pre>
<h3 id="2-minimize-shared-state"><a class="header" href="#2-minimize-shared-state">2. Minimize Shared State</a></h3>
<p>Reduce the amount of data shared between threads.</p>
<pre><code class="language-python"># BAD: Everything shared
shared_data = {'counter': 0, 'results': [], 'status': 'running'}
lock = threading.Lock()

def worker():
    with lock:
        shared_data['counter'] += 1
        shared_data['results'].append(compute())

# GOOD: Minimize sharing, use message passing
result_queue = queue.Queue()

def worker(task_id):
    result = compute(task_id)  # No shared state
    result_queue.put(result)   # Communicate via queue
</code></pre>
<h3 id="3-use-thread-safe-data-structures"><a class="header" href="#3-use-thread-safe-data-structures">3. Use Thread-Safe Data Structures</a></h3>
<pre><code class="language-python">from queue import Queue
from collections import deque
import threading

# Thread-safe queue
q = Queue()

# Thread-safe deque (for most operations)
d = deque()

# NOT thread-safe without synchronization
lst = []
dct = {}
</code></pre>
<h3 id="4-always-release-locks"><a class="header" href="#4-always-release-locks">4. Always Release Locks</a></h3>
<p>Use RAII, context managers, or try-finally.</p>
<pre><code class="language-python"># BAD: Can leak lock on exception
lock.acquire()
do_something()  # Exception here leaks lock!
lock.release()

# GOOD: Context manager
with lock:
    do_something()

# GOOD: Try-finally
lock.acquire()
try:
    do_something()
finally:
    lock.release()
</code></pre>
<h3 id="5-avoid-nested-locks-when-possible"><a class="header" href="#5-avoid-nested-locks-when-possible">5. Avoid Nested Locks When Possible</a></h3>
<pre><code class="language-python"># BAD: Nested locks increase deadlock risk
with lock1:
    with lock2:
        do_something()

# GOOD: Single lock or lock-free
with combined_lock:
    do_something()

# GOOD: Lock ordering if nested necessary
locks = sorted([lock1, lock2], key=id)
with locks[0]:
    with locks[1]:
        do_something()
</code></pre>
<h3 id="6-document-thread-safety"><a class="header" href="#6-document-thread-safety">6. Document Thread Safety</a></h3>
<pre><code class="language-python">class BankAccount:
    """
    Thread-safe bank account.

    All methods are thread-safe and can be called concurrently.
    """
    def __init__(self):
        self._balance = 0
        self._lock = threading.Lock()

    def deposit(self, amount):
        """Thread-safe deposit."""
        with self._lock:
            self._balance += amount
</code></pre>
<h3 id="7-use-appropriate-concurrency-model"><a class="header" href="#7-use-appropriate-concurrency-model">7. Use Appropriate Concurrency Model</a></h3>
<ul>
<li><strong>CPU-bound:</strong> Use multiprocessing (Python), or threads in languages without GIL</li>
<li><strong>I/O-bound:</strong> Use async/await or threading</li>
<li><strong>Mixed:</strong> Combine approaches</li>
</ul>
<h3 id="8-set-thread-names-for-debugging"><a class="header" href="#8-set-thread-names-for-debugging">8. Set Thread Names for Debugging</a></h3>
<pre><code class="language-python">thread = threading.Thread(target=worker, name="Worker-1")
thread.start()

# In worker
print(f"Running in {threading.current_thread().name}")
</code></pre>
<h3 id="9-handle-exceptions-in-threads"><a class="header" href="#9-handle-exceptions-in-threads">9. Handle Exceptions in Threads</a></h3>
<pre><code class="language-python">def worker():
    try:
        do_work()
    except Exception as e:
        logging.error(f"Error in thread: {e}", exc_info=True)
        # Don't let exception kill thread silently
</code></pre>
<h3 id="10-use-daemon-threads-carefully"><a class="header" href="#10-use-daemon-threads-carefully">10. Use Daemon Threads Carefully</a></h3>
<pre><code class="language-python"># Daemon threads die when main thread exits
thread = threading.Thread(target=background_task, daemon=True)
thread.start()

# Non-daemon threads keep program running
thread = threading.Thread(target=important_task, daemon=False)
thread.start()
</code></pre>
<hr>
<h2 id="anti-patterns"><a class="header" href="#anti-patterns">Anti-Patterns</a></h2>
<h3 id="1-sleeping-instead-of-synchronization"><a class="header" href="#1-sleeping-instead-of-synchronization">1. Sleeping Instead of Synchronization</a></h3>
<pre><code class="language-python"># BAD: Race condition masked by sleep
def worker1():
    write_data()
    time.sleep(0.1)  # Hope worker2 is ready...
    read_shared_data()

# GOOD: Proper synchronization
event = threading.Event()

def worker1():
    write_data()
    event.set()

def worker2():
    event.wait()
    read_shared_data()
</code></pre>
<h3 id="2-busy-waiting"><a class="header" href="#2-busy-waiting">2. Busy-Waiting</a></h3>
<pre><code class="language-python"># BAD: Wastes CPU
while not data_ready:
    pass  # Spin!

# GOOD: Use condition variable
condition = threading.Condition()

def producer():
    with condition:
        prepare_data()
        data_ready = True
        condition.notify()

def consumer():
    with condition:
        while not data_ready:
            condition.wait()  # Sleeps, doesn't waste CPU
        process_data()
</code></pre>
<h3 id="3-lock-hogging"><a class="header" href="#3-lock-hogging">3. Lock Hogging</a></h3>
<pre><code class="language-python"># BAD: Hold lock too long
with lock:
    data = read_database()  # Long I/O
    result = expensive_compute(data)  # Long CPU
    write_cache(result)

# GOOD: Minimize critical section
data = read_database()
result = expensive_compute(data)
with lock:
    write_cache(result)  # Only lock needed part
</code></pre>
<h3 id="4-forgetting-to-join-threads"><a class="header" href="#4-forgetting-to-join-threads">4. Forgetting to Join Threads</a></h3>
<pre><code class="language-python"># BAD: Main exits before thread finishes
def main():
    thread = threading.Thread(target=important_work)
    thread.start()
    # Main exits, thread might be killed!

# GOOD: Wait for completion
def main():
    thread = threading.Thread(target=important_work)
    thread.start()
    thread.join()
</code></pre>
<h3 id="5-using-mutable-default-arguments"><a class="header" href="#5-using-mutable-default-arguments">5. Using Mutable Default Arguments</a></h3>
<pre><code class="language-python"># BAD: Default list shared between threads!
def worker(results=[]):
    results.append(compute())  # Race condition!
    return results

# GOOD: Immutable default
def worker(results=None):
    if results is None:
        results = []
    results.append(compute())
    return results
</code></pre>
<h3 id="6-double-checked-locking-without-proper-memory-barriers"><a class="header" href="#6-double-checked-locking-without-proper-memory-barriers">6. Double-Checked Locking (Without Proper Memory Barriers)</a></h3>
<pre><code class="language-python"># BAD: Broken double-checked locking
singleton = None

def get_singleton():
    global singleton
    if singleton is None:  # Check 1 (unlocked)
        with lock:
            if singleton is None:  # Check 2 (locked)
                singleton = Singleton()  # Can be partially visible!
    return singleton

# GOOD: Use proper synchronization or module-level initialization
_singleton = None
_lock = threading.Lock()

def get_singleton():
    global _singleton
    if _singleton is None:
        with _lock:
            if _singleton is None:
                _singleton = Singleton()
    return _singleton

# BETTER: Module-level (thread-safe in Python)
_singleton = Singleton()

def get_singleton():
    return _singleton
</code></pre>
<h3 id="7-not-considering-thread-count-vs-core-count"><a class="header" href="#7-not-considering-thread-count-vs-core-count">7. Not Considering Thread Count vs Core Count</a></h3>
<pre><code class="language-python"># BAD: Creating too many threads
threads = [threading.Thread(target=cpu_work) for _ in range(1000)]

# GOOD: Use thread pool with appropriate size
import multiprocessing
num_cores = multiprocessing.cpu_count()
with ThreadPoolExecutor(max_workers=num_cores) as executor:
    executor.map(cpu_work, range(1000))
</code></pre>
<hr>
<h2 id="debugging-concurrent-programs"><a class="header" href="#debugging-concurrent-programs">Debugging Concurrent Programs</a></h2>
<h3 id="1-logging-with-thread-information"><a class="header" href="#1-logging-with-thread-information">1. Logging with Thread Information</a></h3>
<pre><code class="language-python">import logging
import threading

logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s [%(threadName)-12s] %(levelname)-8s %(message)s'
)

def worker(n):
    logging.info(f"Starting work on {n}")
    # ... work ...
    logging.info(f"Finished work on {n}")

thread = threading.Thread(target=worker, args=(42,), name="Worker-1")
thread.start()
</code></pre>
<h3 id="2-deadlock-detection"><a class="header" href="#2-deadlock-detection">2. Deadlock Detection</a></h3>
<p><strong>Python: Use faulthandler</strong></p>
<pre><code class="language-python">import faulthandler
import signal

# Dump all thread stacks on SIGUSR1
faulthandler.register(signal.SIGUSR1)

# Or dump after timeout
faulthandler.dump_traceback_later(10, repeat=True)
</code></pre>
<p><strong>Tools:</strong></p>
<ul>
<li><strong>Python:</strong> <code>threading.enumerate()</code>, stack traces</li>
<li><strong>Java:</strong> jstack, VisualVM</li>
<li><strong>C++:</strong> gdb, lldb</li>
<li><strong>Helgrind (Valgrind):</strong> Detects race conditions and deadlocks</li>
</ul>
<h3 id="3-race-condition-detection"><a class="header" href="#3-race-condition-detection">3. Race Condition Detection</a></h3>
<p><strong>ThreadSanitizer (C/C++):</strong></p>
<pre><code class="language-bash"># Compile with -fsanitize=thread
g++ -fsanitize=thread -g program.cpp -o program
./program
</code></pre>
<p><strong>Python: Use threading debug mode</strong></p>
<pre><code class="language-python">import sys
import threading

# Enable thread debugging
threading.settrace(lambda *args: print(args))
</code></pre>
<h3 id="4-reproducible-debugging"><a class="header" href="#4-reproducible-debugging">4. Reproducible Debugging</a></h3>
<p>Add determinism for debugging:</p>
<pre><code class="language-python">import random
import threading

# Seed random for reproducibility
random.seed(42)

# Add random sleeps to expose race conditions
def worker():
    # Add jitter to expose timing issues
    time.sleep(random.random() * 0.01)
    critical_section()
</code></pre>
<h3 id="5-visualization"><a class="header" href="#5-visualization">5. Visualization</a></h3>
<p>Visualize thread execution:</p>
<pre><code class="language-python">import time
import threading

class ExecutionTracer:
    def __init__(self):
        self.events = []
        self.lock = threading.Lock()

    def log(self, event):
        with self.lock:
            self.events.append({
                'time': time.time(),
                'thread': threading.current_thread().name,
                'event': event
            })

    def print_trace(self):
        for e in sorted(self.events, key=lambda x: x['time']):
            print(f"{e['time']:.4f} [{e['thread']:15s}] {e['event']}")

tracer = ExecutionTracer()

def worker(n):
    tracer.log(f"Start {n}")
    time.sleep(0.1)
    tracer.log(f"End {n}")

threads = [threading.Thread(target=worker, args=(i,), name=f"Worker-{i}") for i in range(3)]
for t in threads:
    t.start()
for t in threads:
    t.join()

tracer.print_trace()
</code></pre>
<hr>
<h2 id="testing-concurrent-code"><a class="header" href="#testing-concurrent-code">Testing Concurrent Code</a></h2>
<h3 id="1-stress-testing"><a class="header" href="#1-stress-testing">1. Stress Testing</a></h3>
<p>Run many iterations to expose race conditions:</p>
<pre><code class="language-python">import threading

def test_concurrent_counter():
    counter = Counter()  # Your concurrent counter

    def increment_many():
        for _ in range(10000):
            counter.increment()

    threads = [threading.Thread(target=increment_many) for _ in range(10)]
    for t in threads:
        t.start()
    for t in threads:
        t.join()

    assert counter.get() == 100000, f"Expected 100000, got {counter.get()}"

# Run many times
for _ in range(100):
    test_concurrent_counter()
</code></pre>
<h3 id="2-property-based-testing"><a class="header" href="#2-property-based-testing">2. Property-Based Testing</a></h3>
<p>Use libraries like <code>hypothesis</code>:</p>
<pre><code class="language-python">from hypothesis import given, strategies as st
import threading

@given(st.lists(st.integers()))
def test_concurrent_list_operations(items):
    thread_safe_list = ThreadSafeList()

    def add_items():
        for item in items:
            thread_safe_list.append(item)

    threads = [threading.Thread(target=add_items) for _ in range(4)]
    for t in threads:
        t.start()
    for t in threads:
        t.join()

    assert len(thread_safe_list) == len(items) * 4
</code></pre>
<h3 id="3-deterministic-testing-with-barriers"><a class="header" href="#3-deterministic-testing-with-barriers">3. Deterministic Testing with Barriers</a></h3>
<pre><code class="language-python">def test_race_condition():
    barrier = threading.Barrier(2)
    result = []

    def thread1():
        barrier.wait()  # Synchronize start
        result.append(1)

    def thread2():
        barrier.wait()  # Synchronize start
        result.append(2)

    t1 = threading.Thread(target=thread1)
    t2 = threading.Thread(target=thread2)
    t1.start()
    t2.start()
    t1.join()
    t2.join()

    # Both 1 and 2 should be present
    assert set(result) == {1, 2}
</code></pre>
<h3 id="4-mock-synchronization-primitives"><a class="header" href="#4-mock-synchronization-primitives">4. Mock Synchronization Primitives</a></h3>
<p>Inject failures for testing:</p>
<pre><code class="language-python">class FailingLock:
    def __init__(self, fail_on=None):
        self.lock = threading.Lock()
        self.acquire_count = 0
        self.fail_on = fail_on or set()

    def acquire(self):
        self.acquire_count += 1
        if self.acquire_count in self.fail_on:
            raise Exception("Lock acquisition failed")
        return self.lock.acquire()

    def release(self):
        return self.lock.release()

def test_error_handling():
    lock = FailingLock(fail_on={2})

    with pytest.raises(Exception):
        for i in range(3):
            lock.acquire()
            # ... do work ...
            lock.release()
</code></pre>
<h3 id="5-timeout-testing"><a class="header" href="#5-timeout-testing">5. Timeout Testing</a></h3>
<p>Ensure no deadlocks:</p>
<pre><code class="language-python">import pytest
import signal

class TimeoutException(Exception):
    pass

def timeout_handler(signum, frame):
    raise TimeoutException()

def test_no_deadlock():
    signal.signal(signal.SIGALRM, timeout_handler)
    signal.alarm(5)  # 5 second timeout

    try:
        # Code that might deadlock
        run_concurrent_operation()
    except TimeoutException:
        pytest.fail("Operation timed out (possible deadlock)")
    finally:
        signal.alarm(0)  # Cancel alarm
</code></pre>
<hr>
<h2 id="summary"><a class="header" href="#summary">Summary</a></h2>
<p>Concurrency is a powerful tool but comes with complexity:</p>
<p><strong>Key Takeaways:</strong></p>
<ol>
<li><strong>Understand the difference</strong> between concurrency and parallelism</li>
<li><strong>Choose the right primitive:</strong> Mutex, semaphore, condition variable, atomic, etc.</li>
<li><strong>Follow patterns:</strong> Producer-consumer, thread pool, async/await</li>
<li><strong>Prevent deadlocks:</strong> Lock ordering, timeout, detection</li>
<li><strong>Optimize performance:</strong> Minimize contention, avoid false sharing, use lock-free when appropriate</li>
<li><strong>Apply to real problems:</strong> Web servers, connection pools, background processing</li>
<li><strong>Follow best practices:</strong> Immutability, minimal shared state, proper error handling</li>
<li><strong>Avoid anti-patterns:</strong> No busy-waiting, no lock hogging, proper thread management</li>
<li><strong>Debug effectively:</strong> Logging, sanitizers, visualization</li>
<li><strong>Test thoroughly:</strong> Stress testing, deterministic testing, timeout guards</li>
</ol>
<p><strong>Remember:</strong> The best concurrent program is one that minimizes shared mutable state and uses the simplest synchronization mechanism that works.</p>
<p><strong>Further Reading:</strong></p>
<ul>
<li>“The Art of Multiprocessor Programming” by Herlihy &amp; Shavit</li>
<li>“Java Concurrency in Practice” by Goetz et al.</li>
<li>“Seven Concurrency Models in Seven Weeks” by Butcher</li>
<li>“Programming Rust” (Chapter on Concurrency) by Blandy &amp; Orendorff</li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../programming/kotlin.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256 246.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"/></svg></span>
                            </a>

                            <a rel="next prefetch" href="../programming/memory_management.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M278.6 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L210.7 256 73.4 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"/></svg></span>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../programming/kotlin.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256 246.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"/></svg></span>
                    </a>

                    <a rel="next prefetch" href="../programming/memory_management.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M278.6 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L210.7 256 73.4 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"/></svg></span>
                    </a>
            </nav>

        </div>

        <template id=fa-eye><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M288 32c-80.8 0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7 0 24.6C17.3 304 48.6 356 95.4 399.4C142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144s64.5-144 144-144s144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64c-11.5 0-22.3-3-31.6-8.4c-.2 2.8-.4 5.5-.4 8.4c0 53 43 96 96 96s96-43 96-96s-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6z"/></svg></span></template>
        <template id=fa-eye-slash><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M38.8 5.1C28.4-3.1 13.3-1.2 5.1 9.2S-1.2 34.7 9.2 42.9l592 464c10.4 8.2 25.5 6.3 33.7-4.1s6.3-25.5-4.1-33.7L525.6 386.7c39.6-40.6 66.4-86.1 79.9-118.4c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C465.5 68.8 400.8 32 320 32c-68.2 0-125 26.3-169.3 60.8L38.8 5.1zM223.1 149.5C248.6 126.2 282.7 112 320 112c79.5 0 144 64.5 144 144c0 24.9-6.3 48.3-17.4 68.7L408 294.5c5.2-11.8 8-24.8 8-38.5c0-53-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6c0 10.2-2.4 19.8-6.6 28.3l-90.3-70.8zm223.1 298L373 389.9c-16.4 6.5-34.3 10.1-53 10.1c-79.5 0-144-64.5-144-144c0-6.9 .5-13.6 1.4-20.2L83.1 161.5C60.3 191.2 44 220.8 34.5 243.7c-3.3 7.9-3.3 16.7 0 24.6c14.9 35.7 46.2 87.7 93 131.1C174.5 443.2 239.2 480 320 480c47.8 0 89.9-12.9 126.2-32.5z"/></svg></span></template>
        <template id=fa-copy><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M502.6 70.63l-61.25-61.25C435.4 3.371 427.2 0 418.7 0H255.1c-35.35 0-64 28.66-64 64l.0195 256C192 355.4 220.7 384 256 384h192c35.2 0 64-28.8 64-64V93.25C512 84.77 508.6 76.63 502.6 70.63zM464 320c0 8.836-7.164 16-16 16H255.1c-8.838 0-16-7.164-16-16L239.1 64.13c0-8.836 7.164-16 16-16h128L384 96c0 17.67 14.33 32 32 32h47.1V320zM272 448c0 8.836-7.164 16-16 16H63.1c-8.838 0-16-7.164-16-16L47.98 192.1c0-8.836 7.164-16 16-16H160V128H63.99c-35.35 0-64 28.65-64 64l.0098 256C.002 483.3 28.66 512 64 512h192c35.2 0 64-28.8 64-64v-32h-47.1L272 448z"/></svg></span></template>
        <template id=fa-play><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M73 39c-14.8-9.1-33.4-9.4-48.5-.9S0 62.6 0 80V432c0 17.4 9.4 33.4 24.5 41.9s33.7 8.1 48.5-.9L361 297c14.3-8.7 23-24.2 23-41s-8.7-32.2-23-41L73 39z"/></svg></span></template>
        <template id=fa-clock-rotate-left><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M75 75L41 41C25.9 25.9 0 36.6 0 57.9V168c0 13.3 10.7 24 24 24H134.1c21.4 0 32.1-25.9 17-41l-30.8-30.8C155 85.5 203 64 256 64c106 0 192 86 192 192s-86 192-192 192c-40.8 0-78.6-12.7-109.7-34.4c-14.5-10.1-34.4-6.6-44.6 7.9s-6.6 34.4 7.9 44.6C151.2 495 201.7 512 256 512c141.4 0 256-114.6 256-256S397.4 0 256 0C185.3 0 121.3 28.7 75 75zm181 53c-13.3 0-24 10.7-24 24V256c0 6.4 2.5 12.5 7 17l72 72c9.4 9.4 24.6 9.4 33.9 0s9.4-24.6 0-33.9l-65-65V152c0-13.3-10.7-24-24-24z"/></svg></span></template>



        <script>
            window.playground_copyable = true;
        </script>


        <script src="../elasticlunr-ef4e11c1.min.js"></script>
        <script src="../mark-09e88c2c.min.js"></script>
        <script src="../searcher-c2a407aa.js"></script>

        <script src="../clipboard-1626706a.min.js"></script>
        <script src="../highlight-abc7f01d.js"></script>
        <script src="../book-a0b12cfe.js"></script>

        <!-- Custom JS scripts -->



    </div>
    </body>
</html>
