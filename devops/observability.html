<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Observability - My Notes</title>


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="../highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="../tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "../";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "../searchindex.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../toc.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="../toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">My Notes</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <i class="fa fa-spinner fa-spin"></i>
                            </div>
                        </div>
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="observability"><a class="header" href="#observability">Observability</a></h1>
<p>Comprehensive guide to building observable systems through strategic instrumentation, analysis, and continuous improvement.</p>
<h2 id="what-is-observability"><a class="header" href="#what-is-observability">What is Observability?</a></h2>
<p>Observability is the ability to understand internal system state from external outputs. Unlike monitoring (tracking known issues), observability helps debug unknown unknowns.</p>
<h3 id="monitoring-vs-observability"><a class="header" href="#monitoring-vs-observability">Monitoring vs Observability</a></h3>
<p><strong>Monitoring</strong> (Known unknowns):</p>
<ul>
<li>"Is the service up?"</li>
<li>"Is CPU above 80%?"</li>
<li>Predefined dashboards and alerts</li>
</ul>
<p><strong>Observability</strong> (Unknown unknowns):</p>
<ul>
<li>"Why is this specific request slow?"</li>
<li>"What changed between deployments?"</li>
<li>Ad-hoc queries and exploration</li>
</ul>
<pre><code>Monitoring: Health checks, uptime
Observability: Understanding system behavior
</code></pre>
<h2 id="the-three-pillars"><a class="header" href="#the-three-pillars">The Three Pillars</a></h2>
<h3 id="1-metrics"><a class="header" href="#1-metrics">1. Metrics</a></h3>
<p>Aggregated numerical measurements over time:</p>
<pre><code class="language-javascript">// Counter: Monotonically increasing
requests_total.inc({ path: '/api', status: 200 });

// Gauge: Point-in-time value
memory_usage.set(process.memoryUsage().heapUsed);

// Histogram: Distribution of values
request_duration.observe(duration);

// Summary: Similar to histogram, quantiles calculated client-side
latency_summary.observe(duration);
</code></pre>
<p><strong>When to use</strong>:</p>
<ul>
<li>System health (CPU, memory, disk)</li>
<li>Business metrics (signups, revenue)</li>
<li>Aggregated patterns (request rate, error rate)</li>
</ul>
<h3 id="2-logs"><a class="header" href="#2-logs">2. Logs</a></h3>
<p>Discrete event records with context. Logs are what happened and why.</p>
<h4 id="structured-logging"><a class="header" href="#structured-logging">Structured Logging</a></h4>
<p>Use JSON format for machine-readable logs:</p>
<pre><code class="language-json">{
  "timestamp": "2025-01-15T10:30:45Z",
  "level": "error",
  "message": "Database connection failed",
  "service": "api-gateway",
  "trace_id": "abc123",
  "span_id": "def456",
  "error": {
    "type": "ConnectionTimeout",
    "stack": "..."
  },
  "context": {
    "user_id": "user_789",
    "endpoint": "/checkout",
    "retry_count": 3
  }
}
</code></pre>
<p><strong>Log Levels:</strong></p>
<ul>
<li><strong>DEBUG</strong>: Detailed diagnostic info (development only)</li>
<li><strong>INFO</strong>: General informational messages</li>
<li><strong>WARN</strong>: Warning for potentially harmful situations</li>
<li><strong>ERROR</strong>: Error events allowing app to continue</li>
<li><strong>FATAL</strong>: Severe errors causing shutdown</li>
</ul>
<p><strong>Structured Logging Example:</strong></p>
<pre><code class="language-javascript">// ❌ Bad: String interpolation
logger.info('User ' + userId + ' purchased item ' + itemId + ' for $' + price);

// ✅ Good: Structured fields
logger.info('Purchase completed', {
  user_id: userId,
  item_id: itemId,
  price: price,
  currency: 'USD',
  payment_method: 'credit_card'
});
</code></pre>
<p><strong>When to use</strong>:</p>
<ul>
<li>Debugging specific issues</li>
<li>Audit trails</li>
<li>Unstructured investigation</li>
<li>Compliance and security events</li>
</ul>
<h3 id="3-traces"><a class="header" href="#3-traces">3. Traces</a></h3>
<p>Request journey through distributed system:</p>
<pre><code>[User Request] → API Gateway (50ms)
                 ├─ Auth Service (10ms)
                 ├─ Product Service (20ms)
                 │  └─ Database (15ms)
                 └─ Payment Service (200ms) ← SLOW!
                    └─ External API (190ms)
</code></pre>
<p><strong>When to use</strong>:</p>
<ul>
<li>Debugging latency</li>
<li>Understanding dependencies</li>
<li>Visualizing request flow</li>
</ul>
<h2 id="observability-strategy"><a class="header" href="#observability-strategy">Observability Strategy</a></h2>
<h3 id="maturity-model"><a class="header" href="#maturity-model">Maturity Model</a></h3>
<h4 id="level-1-reactive"><a class="header" href="#level-1-reactive">Level 1: Reactive</a></h4>
<ul>
<li>Basic logging</li>
<li>Simple uptime monitoring</li>
<li>Manual investigation</li>
<li><strong>Goal</strong>: Know when things break</li>
</ul>
<h4 id="level-2-proactive"><a class="header" href="#level-2-proactive">Level 2: Proactive</a></h4>
<ul>
<li>Structured logs</li>
<li>Metrics dashboards</li>
<li>Basic alerting</li>
<li><strong>Goal</strong>: Detect issues before users</li>
</ul>
<h4 id="level-3-strategic"><a class="header" href="#level-3-strategic">Level 3: Strategic</a></h4>
<ul>
<li>Distributed tracing</li>
<li>SLOs and error budgets</li>
<li>Advanced correlation</li>
<li><strong>Goal</strong>: Understand system behavior</li>
</ul>
<h4 id="level-4-predictive"><a class="header" href="#level-4-predictive">Level 4: Predictive</a></h4>
<ul>
<li>Anomaly detection</li>
<li>Predictive analytics</li>
<li>Auto-remediation</li>
<li><strong>Goal</strong>: Prevent issues before they occur</li>
</ul>
<h3 id="observability-driven-development"><a class="header" href="#observability-driven-development">Observability-Driven Development</a></h3>
<p>Build observability into development process:</p>
<pre><code class="language-javascript">// 1. Add instrumentation during development
async function processOrder(orderId) {
  const span = trace.startSpan('processOrder');
  span.setAttribute('order.id', orderId);

  try {
    logger.info({ orderId }, 'Processing order');

    const order = await fetchOrder(orderId);
    metrics.orderValue.observe(order.total);

    await validateInventory(order);
    await chargePayment(order);

    logger.info({ orderId, total: order.total }, 'Order processed');
    return order;
  } catch (error) {
    logger.error({ orderId, error }, 'Order processing failed');
    metrics.orderErrors.inc({ reason: error.code });
    span.recordException(error);
    throw error;
  } finally {
    span.end();
  }
}
</code></pre>
<p><strong>Best practices</strong>:</p>
<ul>
<li>Instrument code as you write it</li>
<li>Include observability in code reviews</li>
<li>Test instrumentation in development</li>
<li>Document expected metrics and logs</li>
</ul>
<h2 id="architecture-patterns"><a class="header" href="#architecture-patterns">Architecture Patterns</a></h2>
<h3 id="centralized-observability-platform"><a class="header" href="#centralized-observability-platform">Centralized Observability Platform</a></h3>
<pre><code>┌─────────────┐
│ Application │──┐
└─────────────┘  │
                 ├──→ ┌──────────────┐
┌─────────────┐  │    │  Collector   │
│   Service   │──┤    │  (OpenTelem) │
└─────────────┘  │    └──────────────┘
                 │           │
┌─────────────┐  │           ├──→ Metrics (Prometheus)
│  Database   │──┘           ├──→ Logs (Loki/ES)
└─────────────┘              └──→ Traces (Jaeger/Tempo)
</code></pre>
<p><strong>Benefits</strong>:</p>
<ul>
<li>Unified data collection</li>
<li>Single configuration point</li>
<li>Vendor neutrality</li>
<li>Cost optimization</li>
</ul>
<h3 id="sampling-strategy"><a class="header" href="#sampling-strategy">Sampling Strategy</a></h3>
<p>Not all data needs to be collected:</p>
<pre><code class="language-javascript">// Head-based sampling (decision at start)
const sampler = new TraceIdRatioBasedSampler(0.1); // 10%

// Tail-based sampling (decision at end)
if (span.duration &gt; 1000 || span.hasError) {
  span.setSampled(true); // Always keep slow/error traces
} else {
  span.setSampled(Math.random() &lt; 0.01); // 1% of normal traces
}

// Adaptive sampling
const rate = errorRate &gt; 0.01 ? 1.0 : 0.1; // 100% when errors high
</code></pre>
<p><strong>Strategies</strong>:</p>
<ul>
<li><strong>Always sample</strong>: Errors, slow requests, critical paths</li>
<li><strong>Never sample</strong>: Health checks, static assets</li>
<li><strong>Adaptive</strong>: Increase during incidents</li>
</ul>
<h3 id="context-propagation"><a class="header" href="#context-propagation">Context Propagation</a></h3>
<p>Link observability data across services:</p>
<pre><code class="language-javascript">// Service A: Create context
const trace = tracer.startSpan('handleRequest');
const context = {
  'trace-id': trace.spanContext().traceId,
  'span-id': trace.spanContext().spanId,
  'request-id': generateRequestId(),
  'user-id': req.user.id
};

// Pass to Service B
await axios.post('http://service-b/api', data, {
  headers: context
});

// Service B: Extract context
const traceId = req.headers['trace-id'];
const parentSpan = req.headers['span-id'];
const childSpan = tracer.startSpan('processData', {
  parent: parentSpan
});

// Both services now linked in distributed trace
</code></pre>
<h2 id="implementation-patterns"><a class="header" href="#implementation-patterns">Implementation Patterns</a></h2>
<h3 id="instrumentation-layers"><a class="header" href="#instrumentation-layers">Instrumentation Layers</a></h3>
<h4 id="1-infrastructure-layer"><a class="header" href="#1-infrastructure-layer">1. Infrastructure Layer</a></h4>
<pre><code class="language-yaml"># Kubernetes metrics via prometheus
apiVersion: v1
kind: Service
metadata:
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "9090"
    prometheus.io/path: "/metrics"
</code></pre>
<h4 id="2-application-layer"><a class="header" href="#2-application-layer">2. Application Layer</a></h4>
<pre><code class="language-javascript">// Auto-instrumentation
const { registerInstrumentations } = require('@opentelemetry/instrumentation');
registerInstrumentations({
  instrumentations: [
    new HttpInstrumentation(),
    new ExpressInstrumentation(),
    new PgInstrumentation(),
    new RedisInstrumentation()
  ]
});

// Custom business metrics
const checkoutMetrics = {
  started: new Counter({ name: 'checkout_started_total' }),
  completed: new Counter({ name: 'checkout_completed_total' }),
  abandoned: new Counter({ name: 'checkout_abandoned_total' }),
  value: new Histogram({
    name: 'checkout_value_dollars',
    buckets: [10, 50, 100, 500, 1000]
  })
};
</code></pre>
<h4 id="3-business-layer"><a class="header" href="#3-business-layer">3. Business Layer</a></h4>
<pre><code class="language-javascript">// Business events
async function completeCheckout(cart) {
  const startTime = Date.now();

  try {
    const order = await createOrder(cart);

    // Business observability
    events.emit('checkout.completed', {
      order_id: order.id,
      user_id: cart.userId,
      total: order.total,
      items: order.items.length,
      duration_ms: Date.now() - startTime,
      payment_method: order.paymentMethod,
      promocode: order.promoCode || null
    });

    return order;
  } catch (error) {
    events.emit('checkout.failed', {
      user_id: cart.userId,
      error_type: error.name,
      step: error.step
    });
    throw error;
  }
}
</code></pre>
<h3 id="correlation-patterns"><a class="header" href="#correlation-patterns">Correlation Patterns</a></h3>
<h4 id="correlating-metrics-and-logs"><a class="header" href="#correlating-metrics-and-logs">Correlating Metrics and Logs</a></h4>
<pre><code class="language-javascript">// Add trace context to logs
logger.info({
  trace_id: span.spanContext().traceId,
  span_id: span.spanContext().spanId,
  message: 'Order processed'
});

// Query logs by trace ID
// logs: trace_id:"abc123"
// See all logs for this request
</code></pre>
<h4 id="correlating-logs-and-traces"><a class="header" href="#correlating-logs-and-traces">Correlating Logs and Traces</a></h4>
<pre><code class="language-javascript">// Add log events to traces
span.addEvent('Payment authorized', {
  'payment.id': paymentId,
  'payment.method': 'credit_card'
});

// Link from trace span to logs
// Grafana: Click span → "View logs"
</code></pre>
<h4 id="correlating-metrics-and-traces"><a class="header" href="#correlating-metrics-and-traces">Correlating Metrics and Traces</a></h4>
<pre><code class="language-javascript">// Exemplars link metrics to traces
histogram.observe(
  { endpoint: '/checkout' },
  duration,
  { trace_id: traceId } // Exemplar
);

// In Grafana: Click metric spike → See example traces
</code></pre>
<h2 id="advanced-patterns"><a class="header" href="#advanced-patterns">Advanced Patterns</a></h2>
<h3 id="high-cardinality-data"><a class="header" href="#high-cardinality-data">High Cardinality Data</a></h3>
<p><strong>Problem</strong>: Too many unique label values</p>
<pre><code class="language-javascript">// ❌ Bad: User ID as label (millions of users)
requests.inc({ user_id: req.user.id });

// ✅ Good: User ID in logs only
logger.info({ user_id: req.user.id }, 'Request');
requests.inc({ endpoint: req.path });
</code></pre>
<p><strong>Solutions</strong>:</p>
<ul>
<li>Use aggregated labels in metrics</li>
<li>Store high-cardinality data in logs/traces</li>
<li>Use cardinality limits and alerts</li>
</ul>
<h3 id="dynamic-sampling"><a class="header" href="#dynamic-sampling">Dynamic Sampling</a></h3>
<p>Adjust sampling based on conditions:</p>
<pre><code class="language-javascript">class AdaptiveSampler {
  constructor() {
    this.errorRate = 0;
    this.baseRate = 0.01; // 1%
  }

  shouldSample(span) {
    // Always sample errors
    if (span.hasError) return true;

    // Sample more during high error rates
    const rate = this.errorRate &gt; 0.05
      ? 0.5  // 50% when errors high
      : this.baseRate;

    // Sample all slow requests
    if (span.duration &gt; 1000) return true;

    return Math.random() &lt; rate;
  }

  updateErrorRate(rate) {
    this.errorRate = rate;
  }
}
</code></pre>
<h3 id="real-user-monitoring-rum"><a class="header" href="#real-user-monitoring-rum">Real User Monitoring (RUM)</a></h3>
<p>Frontend observability:</p>
<pre><code class="language-javascript">// Browser instrumentation
import { WebTracerProvider } from '@opentelemetry/sdk-trace-web';
import { DocumentLoadInstrumentation } from '@opentelemetry/instrumentation-document-load';

const provider = new WebTracerProvider();
provider.register();

// Measure web vitals
import { onCLS, onFID, onLCP } from 'web-vitals';

onCLS(metric =&gt; {
  sendMetric('web_vitals_cls', metric.value, {
    page: window.location.pathname
  });
});

onFID(metric =&gt; {
  sendMetric('web_vitals_fid', metric.value);
});

onLCP(metric =&gt; {
  sendMetric('web_vitals_lcp', metric.value);
});

// User journey tracking
class UserJourney {
  constructor() {
    this.sessionId = generateSessionId();
    this.events = [];
  }

  track(event) {
    this.events.push({
      timestamp: Date.now(),
      type: event.type,
      data: event.data,
      page: window.location.pathname
    });

    // Send to analytics
    if (this.events.length &gt;= 10) {
      this.flush();
    }
  }

  flush() {
    sendEvents(this.sessionId, this.events);
    this.events = [];
  }
}
</code></pre>
<h3 id="synthetic-monitoring"><a class="header" href="#synthetic-monitoring">Synthetic Monitoring</a></h3>
<p>Proactive monitoring with simulated traffic:</p>
<pre><code class="language-javascript">// Synthetic health checks
const syntheticChecks = [
  {
    name: 'api_health',
    interval: '1m',
    endpoint: 'https://api.example.com/health',
    assertions: [
      { type: 'status', value: 200 },
      { type: 'latency', max: 500 },
      { type: 'body', contains: '"status":"ok"' }
    ]
  },
  {
    name: 'user_flow',
    interval: '5m',
    steps: [
      { action: 'visit', url: '/login' },
      { action: 'fill', field: 'email', value: 'test@example.com' },
      { action: 'fill', field: 'password', value: 'test123' },
      { action: 'click', selector: '#submit' },
      { action: 'assert', selector: '.dashboard', exists: true }
    ]
  }
];
</code></pre>
<h2 id="cost-optimization"><a class="header" href="#cost-optimization">Cost Optimization</a></h2>
<h3 id="data-retention-strategy"><a class="header" href="#data-retention-strategy">Data Retention Strategy</a></h3>
<pre><code class="language-yaml"># Tiered retention
retention:
  metrics:
    raw: 15d        # Full resolution
    5m: 90d         # 5min aggregation
    1h: 1y          # 1hour aggregation

  logs:
    hot: 7d         # Fast search (ES)
    warm: 30d       # Slower search (S3)
    cold: 90d       # Archive (Glacier)

  traces:
    full: 7d        # All traces
    sampled: 30d    # 10% sample
    errors: 90d     # Error traces only
</code></pre>
<h3 id="reducing-volume"><a class="header" href="#reducing-volume">Reducing Volume</a></h3>
<pre><code class="language-javascript">// 1. Smart log levels
if (process.env.NODE_ENV === 'production') {
  logger.level = 'info'; // Skip debug logs
}

// 2. Sampling
const shouldLog = req.path.startsWith('/api')
  || Math.random() &lt; 0.01; // 1% of other requests

// 3. Deduplication
const errorCache = new Map();
function logError(error) {
  const key = `${error.code}:${error.message}`;
  const lastSeen = errorCache.get(key);

  if (!lastSeen || Date.now() - lastSeen &gt; 60000) {
    logger.error(error);
    errorCache.set(key, Date.now());
  }
}

// 4. Aggregation
// Instead of individual request logs
metrics.httpRequests.inc(); // Much cheaper

// 5. Filtering
// Don't log health checks
if (req.path === '/health') return next();
</code></pre>
<h2 id="observability-for-microservices"><a class="header" href="#observability-for-microservices">Observability for Microservices</a></h2>
<h3 id="service-mesh-integration"><a class="header" href="#service-mesh-integration">Service Mesh Integration</a></h3>
<pre><code class="language-yaml"># Istio automatic observability
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: reviews
spec:
  hosts:
    - reviews
  http:
    - match:
        - headers:
            end-user:
              exact: jason
      route:
        - destination:
            host: reviews
            subset: v2
      # Automatic metrics for:
      # - Request rate
      # - Error rate
      # - Latency distribution
      # - Distributed traces
</code></pre>
<h3 id="distributed-tracing-best-practices"><a class="header" href="#distributed-tracing-best-practices">Distributed Tracing Best Practices</a></h3>
<pre><code class="language-javascript">// 1. Consistent naming
span.name = `${method} ${resource}`; // GET /users
span.name = `${service}.${operation}`; // userService.createUser

// 2. Rich attributes
span.setAttributes({
  // HTTP semantic conventions
  'http.method': 'POST',
  'http.url': req.url,
  'http.status_code': res.statusCode,

  // Database semantic conventions
  'db.system': 'postgresql',
  'db.statement': query,
  'db.name': 'users',

  // Custom business context
  'user.id': userId,
  'order.total': orderTotal
});

// 3. Span hierarchy
async function processOrder(orderId) {
  return tracer.startActiveSpan('processOrder', async (orderSpan) =&gt; {
    await tracer.startActiveSpan('validateOrder', async (validateSpan) =&gt; {
      // validation logic
      validateSpan.end();
    });

    await tracer.startActiveSpan('chargePayment', async (paymentSpan) =&gt; {
      // payment logic
      paymentSpan.end();
    });

    orderSpan.end();
  });
}
</code></pre>
<h3 id="service-dependencies"><a class="header" href="#service-dependencies">Service Dependencies</a></h3>
<p>Track and visualize service relationships:</p>
<pre><code class="language-javascript">// Dependency graph
const dependencies = {
  'api-gateway': ['auth-service', 'user-service', 'order-service'],
  'order-service': ['inventory-service', 'payment-service', 'notification-service'],
  'payment-service': ['stripe-api', 'fraud-detection']
};

// Detect circular dependencies
// Alert on new dependencies
// Visualize in Grafana/Jaeger
</code></pre>
<h2 id="alerting-strategy"><a class="header" href="#alerting-strategy">Alerting Strategy</a></h2>
<h3 id="alert-fatigue-prevention"><a class="header" href="#alert-fatigue-prevention">Alert Fatigue Prevention</a></h3>
<pre><code class="language-yaml"># Good alert characteristics
alert: HighErrorRate
expr: error_rate &gt; 0.05  # 5% errors
for: 5m                   # Sustained for 5 minutes
annotations:
  summary: "High error rate detected"
  runbook: "https://wiki.company.com/runbooks/high-error-rate"
  dashboard: "https://grafana.company.com/d/errors"
labels:
  severity: critical
  team: backend
  oncall: true

# Avoid alert fatigue
# ❌ Don't alert on:
#   - Symptoms without impact
#   - Transient spikes
#   - Non-actionable metrics
#   - Too many conditions

# ✅ Do alert on:
#   - User-facing issues
#   - SLO violations
#   - Security events
#   - Clear action needed
</code></pre>
<h3 id="progressive-rollout-observability"><a class="header" href="#progressive-rollout-observability">Progressive Rollout Observability</a></h3>
<p>Monitor during deployments:</p>
<pre><code class="language-javascript">// Canary analysis
const canaryMetrics = {
  baseline: await queryMetrics('version=v1', timeRange),
  canary: await queryMetrics('version=v2', timeRange)
};

const analysis = {
  errorRate: canary.errors / canary.requests,
  errorIncrease: (canary.errors / canary.requests) -
                 (baseline.errors / baseline.requests),
  p95Latency: canary.p95,
  latencyIncrease: canary.p95 - baseline.p95
};

if (analysis.errorIncrease &gt; 0.01 || analysis.latencyIncrease &gt; 100) {
  rollback();
} else {
  promote();
}
</code></pre>
<h2 id="logging-platforms"><a class="header" href="#logging-platforms">Logging Platforms</a></h2>
<h3 id="elk-stack-elasticsearch-logstash-kibana"><a class="header" href="#elk-stack-elasticsearch-logstash-kibana">ELK Stack (Elasticsearch, Logstash, Kibana)</a></h3>
<p>Complete log aggregation and analysis platform.</p>
<h4 id="architecture"><a class="header" href="#architecture">Architecture</a></h4>
<pre><code>Applications → Filebeat/Fluentd → Logstash → Elasticsearch → Kibana
                                      ↓
                                  Filtering
                                  Enrichment
</code></pre>
<h4 id="logstash-pipeline"><a class="header" href="#logstash-pipeline">Logstash Pipeline</a></h4>
<pre><code class="language-ruby">input {
  beats {
    port =&gt; 5044
  }
}

filter {
  # Parse JSON logs
  json {
    source =&gt; "message"
  }

  # Extract fields from message
  grok {
    match =&gt; { "message" =&gt; "%{COMBINEDAPACHELOG}" }
  }

  # Add geolocation
  geoip {
    source =&gt; "client_ip"
  }

  # Parse timestamps
  date {
    match =&gt; [ "timestamp", "ISO8601" ]
    target =&gt; "@timestamp"
  }

  # Add custom fields
  mutate {
    add_field =&gt; {
      "environment" =&gt; "production"
      "indexed_at" =&gt; "%{@timestamp}"
    }
    remove_field =&gt; ["temp_field"]
  }
}

output {
  elasticsearch {
    hosts =&gt; ["elasticsearch:9200"]
    index =&gt; "logs-%{+YYYY.MM.dd}"
  }
}
</code></pre>
<h4 id="kibana-query-language-kql"><a class="header" href="#kibana-query-language-kql">Kibana Query Language (KQL)</a></h4>
<pre><code># Simple field match
status: 500

# Boolean operators
status: 500 AND service: api

# Wildcards
message: *timeout*

# Range queries
response_time &gt;= 1000

# Exists query
_exists_: error.stack

# Time range
@timestamp &gt;= "2025-01-15T00:00:00"

# Aggregations
service: api | stats count() by status_code
</code></pre>
<h3 id="grafana-loki"><a class="header" href="#grafana-loki">Grafana Loki</a></h3>
<p>Lightweight, cost-effective log aggregation system.</p>
<h4 id="why-loki"><a class="header" href="#why-loki">Why Loki?</a></h4>
<ul>
<li><strong>Cost-effective</strong>: Only indexes labels, not full text</li>
<li><strong>Simple</strong>: Easy to operate, horizontally scalable</li>
<li><strong>Integrated</strong>: Works seamlessly with Grafana</li>
<li><strong>Prometheus-like</strong>: Uses familiar label model</li>
</ul>
<h4 id="architecture-1"><a class="header" href="#architecture-1">Architecture</a></h4>
<pre><code>Applications → Promtail → Loki → Grafana
                ↓
          Log files
</code></pre>
<h4 id="promtail-configuration"><a class="header" href="#promtail-configuration">Promtail Configuration</a></h4>
<pre><code class="language-yaml">server:
  http_listen_port: 9080
  grpc_listen_port: 0

positions:
  filename: /tmp/positions.yaml

clients:
  - url: http://loki:3100/loki/api/v1/push

scrape_configs:
  - job_name: system
    static_configs:
      - targets:
          - localhost
        labels:
          job: varlogs
          __path__: /var/log/*.log

  - job_name: containers
    docker_sd_configs:
      - host: unix:///var/run/docker.sock
        refresh_interval: 5s
    relabel_configs:
      - source_labels: ['__meta_docker_container_name']
        regex: '/(.*)'
        target_label: 'container'
      - source_labels: ['__meta_docker_container_log_stream']
        target_label: 'stream'
</code></pre>
<h4 id="logql-queries"><a class="header" href="#logql-queries">LogQL Queries</a></h4>
<pre><code class="language-logql"># Stream selector
{service="api", environment="production"}

# Filter by content
{service="api"} |= "error"
{service="api"} != "health"

# JSON parsing
{service="api"} | json | status_code &gt;= 500

# Pattern matching
{service="api"} |~ "timeout|deadline"

# Rate queries
rate({service="api"}[5m])

# Count over time
count_over_time({service="api", level="error"}[1h])

# Aggregation
sum(rate({service="api"}[5m])) by (status_code)
</code></pre>
<h4 id="loki-vs-elk-comparison"><a class="header" href="#loki-vs-elk-comparison">Loki vs ELK Comparison</a></h4>
<div class="table-wrapper"><table><thead><tr><th>Feature</th><th>Loki</th><th>ELK</th></tr></thead><tbody>
<tr><td><strong>Index Strategy</strong></td><td>Labels only</td><td>Full-text</td></tr>
<tr><td><strong>Cost</strong></td><td>Lower</td><td>Higher</td></tr>
<tr><td><strong>Query Speed</strong></td><td>Fast for label queries</td><td>Fast for full-text</td></tr>
<tr><td><strong>Storage</strong></td><td>More efficient</td><td>More expensive</td></tr>
<tr><td><strong>Complexity</strong></td><td>Simple</td><td>Complex</td></tr>
<tr><td><strong>Best For</strong></td><td>Metrics-style logs</td><td>Full-text search</td></tr>
</tbody></table>
</div>
<h3 id="log-sampling"><a class="header" href="#log-sampling">Log Sampling</a></h3>
<p>Reduce volume while maintaining visibility.</p>
<h4 id="head-sampling"><a class="header" href="#head-sampling">Head Sampling</a></h4>
<p>Decide at creation time:</p>
<pre><code class="language-javascript">const shouldLog = (level, req) =&gt; {
  // Always log errors
  if (level === 'error' || level === 'fatal') {
    return true;
  }

  // Always log important endpoints
  if (req.path.startsWith('/api/payment')) {
    return true;
  }

  // Sample 10% of info logs
  if (level === 'info') {
    return Math.random() &lt; 0.1;
  }

  // Sample 1% of debug logs
  return Math.random() &lt; 0.01;
};

if (shouldLog('info', req)) {
  logger.info({ req }, 'Request processed');
}
</code></pre>
<h4 id="tail-sampling"><a class="header" href="#tail-sampling">Tail Sampling</a></h4>
<p>Decide after processing:</p>
<pre><code class="language-javascript">class LogBuffer {
  constructor() {
    this.buffer = [];
    this.maxSize = 1000;
  }

  add(logEntry) {
    this.buffer.push(logEntry);

    if (this.buffer.length &gt; this.maxSize) {
      this.flush();
    }
  }

  flush() {
    const hasErrors = this.buffer.some(log =&gt; log.level === 'error');
    const isSlow = this.buffer.some(log =&gt; log.duration &gt; 1000);

    if (hasErrors || isSlow) {
      // Send all logs for this request
      this.sendLogs(this.buffer);
    } else {
      // Sample 1% of normal requests
      if (Math.random() &lt; 0.01) {
        this.sendLogs(this.buffer);
      }
    }

    this.buffer = [];
  }
}
</code></pre>
<h4 id="dynamic-sampling-1"><a class="header" href="#dynamic-sampling-1">Dynamic Sampling</a></h4>
<p>Adjust based on conditions:</p>
<pre><code class="language-javascript">class AdaptiveLogSampler {
  constructor() {
    this.errorRate = 0;
    this.baseRate = 0.01;
  }

  getSampleRate(level) {
    if (level === 'error' || level === 'fatal') {
      return 1.0; // 100%
    }

    // Increase sampling during high error rates
    if (this.errorRate &gt; 0.05) {
      return 0.5; // 50%
    }

    if (this.errorRate &gt; 0.01) {
      return 0.1; // 10%
    }

    return this.baseRate; // 1%
  }

  updateErrorRate(rate) {
    this.errorRate = rate;
  }
}
</code></pre>
<h3 id="log-aggregation-patterns"><a class="header" href="#log-aggregation-patterns">Log Aggregation Patterns</a></h3>
<h4 id="centralized-logging"><a class="header" href="#centralized-logging">Centralized Logging</a></h4>
<pre><code>Service A ──┐
Service B ──┼──→ Log Aggregator → Storage → Analysis
Service C ──┘
</code></pre>
<h4 id="multi-tier-logging"><a class="header" href="#multi-tier-logging">Multi-Tier Logging</a></h4>
<pre><code>Edge Logs → Regional Aggregator → Central Storage
                                      ↓
                                  Archive (S3)
</code></pre>
<h4 id="hybrid-approach"><a class="header" href="#hybrid-approach">Hybrid Approach</a></h4>
<pre><code>High-value logs → Real-time (Loki/ES)
All logs → Cold storage (S3)
</code></pre>
<h2 id="distributed-tracing-platforms"><a class="header" href="#distributed-tracing-platforms">Distributed Tracing Platforms</a></h2>
<h3 id="opentelemetry"><a class="header" href="#opentelemetry">OpenTelemetry</a></h3>
<p>Industry-standard observability framework.</p>
<pre><code class="language-javascript">const { NodeSDK } = require('@opentelemetry/sdk-node');
const { getNodeAutoInstrumentations } = require('@opentelemetry/auto-instrumentations-node');
const { JaegerExporter } = require('@opentelemetry/exporter-jaeger');
const { ZipkinExporter } = require('@opentelemetry/exporter-zipkin');

const sdk = new NodeSDK({
  serviceName: 'my-service',
  traceExporter: new JaegerExporter({
    endpoint: 'http://jaeger:14268/api/traces',
  }),
  instrumentations: [getNodeAutoInstrumentations()],
});

sdk.start();
</code></pre>
<h3 id="jaeger"><a class="header" href="#jaeger">Jaeger</a></h3>
<p>Distributed tracing platform inspired by Dapper and OpenZipkin.</p>
<p><strong>Features:</strong></p>
<ul>
<li>Distributed context propagation</li>
<li>Distributed transaction monitoring</li>
<li>Root cause analysis</li>
<li>Service dependency analysis</li>
<li>Performance optimization</li>
</ul>
<p><strong>Architecture:</strong></p>
<pre><code>Client → Agent → Collector → Storage (Cassandra/ES) → UI
</code></pre>
<h3 id="zipkin"><a class="header" href="#zipkin">Zipkin</a></h3>
<p>Distributed tracing system.</p>
<p><strong>Features:</strong></p>
<ul>
<li>Simpler than Jaeger</li>
<li>Good for smaller deployments</li>
<li>Native support in Spring Boot</li>
<li>Compatible with OpenTelemetry</li>
</ul>
<p><strong>Jaeger vs Zipkin:</strong></p>
<div class="table-wrapper"><table><thead><tr><th>Feature</th><th>Jaeger</th><th>Zipkin</th></tr></thead><tbody>
<tr><td><strong>Origin</strong></td><td>Uber</td><td>Twitter</td></tr>
<tr><td><strong>Storage</strong></td><td>Cassandra, ES, Badger</td><td>ES, MySQL, Cassandra</td></tr>
<tr><td><strong>Sampling</strong></td><td>Adaptive</td><td>Fixed</td></tr>
<tr><td><strong>Architecture</strong></td><td>More components</td><td>Simpler</td></tr>
<tr><td><strong>Best For</strong></td><td>Large scale</td><td>Simple setups</td></tr>
</tbody></table>
</div>
<h2 id="incident-response"><a class="header" href="#incident-response">Incident Response</a></h2>
<h3 id="incident-lifecycle"><a class="header" href="#incident-lifecycle">Incident Lifecycle</a></h3>
<pre><code>Detection → Response → Resolution → Postmortem
</code></pre>
<h4 id="1-detection"><a class="header" href="#1-detection">1. Detection</a></h4>
<pre><code class="language-javascript">// Automated detection
if (errorRate &gt; SLO_THRESHOLD) {
  incident.create({
    severity: 'critical',
    title: 'High error rate detected',
    affected_service: 'api-gateway',
    metrics: {
      current_error_rate: errorRate,
      threshold: SLO_THRESHOLD
    }
  });
}
</code></pre>
<h4 id="2-response"><a class="header" href="#2-response">2. Response</a></h4>
<pre><code class="language-yaml"># Incident response runbook
steps:
  1. Acknowledge alert
  2. Check dashboard: https://grafana.company.com/d/incident
  3. Review recent deployments
  4. Check dependencies status
  5. Enable debug logging if needed
  6. Communicate in #incidents channel
</code></pre>
<h4 id="3-resolution"><a class="header" href="#3-resolution">3. Resolution</a></h4>
<ul>
<li>Rollback bad deployment</li>
<li>Scale up resources</li>
<li>Fix configuration</li>
<li>Deploy hotfix</li>
</ul>
<h4 id="4-postmortem"><a class="header" href="#4-postmortem">4. Postmortem</a></h4>
<p><strong>Blameless Postmortem Template:</strong></p>
<pre><code class="language-markdown"># Incident Postmortem: [Title]

## Summary
Brief description of what happened

## Impact
- Duration: 2 hours 15 minutes
- Affected users: ~15% of traffic
- Revenue impact: $XX,XXX
- Service: api-gateway

## Timeline (all times UTC)
- 14:00 - Deploy v2.3.4 to production
- 14:05 - Error rate increases to 5%
- 14:08 - PagerDuty alert triggers
- 14:10 - On-call engineer starts investigation
- 14:20 - Root cause identified: DB connection pool exhausted
- 14:25 - Decision to rollback
- 14:30 - Rollback initiated
- 14:35 - Service recovered
- 15:00 - Confirmed stable

## Root Cause
Database connection pool size was too small for new traffic pattern.
New feature made 3x more DB calls per request than expected.

## Resolution
1. Rolled back to v2.3.3
2. Increased connection pool size
3. Re-deployed with fix

## What Went Well
- Alert triggered within 3 minutes
- Clear runbooks enabled fast response
- Communication was effective
- Rollback process worked smoothly

## What Went Wrong
- Connection pool not load tested
- No gradual rollout (canary)
- Missing query count metrics
- Load tests didn't simulate production pattern

## Action Items
- [ ] Add connection pool metrics (@alice, 2025-01-20)
- [ ] Implement canary deployments (@bob, 2025-01-25)
- [ ] Add query count per request metric (@charlie, 2025-01-22)
- [ ] Update load test scenarios (@dave, 2025-01-30)
- [ ] Document DB connection tuning (@eve, 2025-01-23)

## Lessons Learned
- Always canary deploy
- Monitor connection pools
- Load test with production-like data
</code></pre>
<h3 id="on-call-best-practices"><a class="header" href="#on-call-best-practices">On-Call Best Practices</a></h3>
<h4 id="on-call-rotation"><a class="header" href="#on-call-rotation">On-Call Rotation</a></h4>
<pre><code class="language-yaml">rotation:
  primary: 7 days
  secondary: 7 days
  handoff: Monday 10:00 AM

responsibilities:
  - Respond to pages within 15 minutes
  - Investigate and mitigate incidents
  - Write postmortems
  - Update runbooks

compensation:
  - Shift differential
  - Time off in lieu
  - Rotation credits
</code></pre>
<h4 id="runbook-template"><a class="header" href="#runbook-template">Runbook Template</a></h4>
<pre><code class="language-markdown"># Runbook: High API Error Rate

## Symptoms
- Alert: "High error rate on api-gateway"
- Dashboard: Error rate &gt; 5%
- User impact: API requests failing

## Severity
Critical (user-facing)

## Diagnosis
1. Check Grafana dashboard:
   https://grafana.company.com/d/api-errors

2. Query recent errors:
</code></pre>
<p>{service="api"} | json | status_code &gt;= 500</p>
<pre><code>
3. Check recent deployments:
```bash
kubectl rollout history deployment/api-gateway
</code></pre>
<ol start="4">
<li>Check dependencies:
<ul>
<li>Database: https://status.db.company.com</li>
<li>Cache: https://status.redis.company.com</li>
<li>External APIs: Check status pages</li>
</ul>
</li>
</ol>
<h2 id="mitigation"><a class="header" href="#mitigation">Mitigation</a></h2>
<h3 id="if-recent-deployment"><a class="header" href="#if-recent-deployment">If recent deployment:</a></h3>
<pre><code class="language-bash">kubectl rollout undo deployment/api-gateway
</code></pre>
<h3 id="if-database-issue"><a class="header" href="#if-database-issue">If database issue:</a></h3>
<pre><code class="language-bash"># Check connection pool
kubectl exec -it api-gateway-xxx -- curl localhost:9090/metrics | grep db_connections

# Scale up if needed
kubectl scale deployment/api-gateway --replicas=10
</code></pre>
<h3 id="if-external-api-down"><a class="header" href="#if-external-api-down">If external API down:</a></h3>
<pre><code class="language-bash"># Enable circuit breaker
kubectl set env deployment/api-gateway CIRCUIT_BREAKER_ENABLED=true
</code></pre>
<h2 id="escalation"><a class="header" href="#escalation">Escalation</a></h2>
<ul>
<li>Primary: @team-backend</li>
<li>Secondary: @team-platform</li>
<li>Manager: @engineering-manager</li>
</ul>
<h2 id="postmortem"><a class="header" href="#postmortem">Postmortem</a></h2>
<p>Required for all critical incidents</p>
<pre><code>
### Alerting Integration

#### PagerDuty Integration
```javascript
const { PagerDutyClient } = require('pagerduty-client');

const pd = new PagerDutyClient({
  integrationKey: process.env.PD_INTEGRATION_KEY
});

async function triggerIncident(alert) {
  await pd.sendEvent({
    event_action: 'trigger',
    payload: {
      summary: alert.title,
      severity: alert.severity,
      source: alert.source,
      custom_details: {
        error_rate: alert.metrics.error_rate,
        threshold: alert.threshold,
        dashboard: alert.dashboard_url
      }
    },
    links: [
      {
        href: alert.dashboard_url,
        text: 'View Dashboard'
      },
      {
        href: alert.runbook_url,
        text: 'View Runbook'
      }
    ]
  });
}
</code></pre>
<h4 id="opsgenie-integration"><a class="header" href="#opsgenie-integration">Opsgenie Integration</a></h4>
<pre><code class="language-javascript">const opsgenie = require('opsgenie-sdk');

const client = new opsgenie.AlertApi({
  apiKey: process.env.OPSGENIE_API_KEY
});

async function createAlert(alert) {
  await client.createAlert({
    message: alert.title,
    description: alert.description,
    priority: alertSeverityToPriority(alert.severity),
    tags: [alert.service, alert.environment],
    details: {
      error_rate: alert.metrics.error_rate,
      affected_users: alert.affected_users
    },
    responders: [
      { type: 'team', name: 'Backend Team' }
    ],
    actions: ['View Dashboard', 'View Logs'],
    entity: alert.service,
    source: 'Prometheus'
  });
}

function alertSeverityToPriority(severity) {
  const map = {
    critical: 'P1',
    high: 'P2',
    medium: 'P3',
    low: 'P4',
    info: 'P5'
  };
  return map[severity] || 'P3';
}
</code></pre>
<h2 id="slis-slos-and-slas"><a class="header" href="#slis-slos-and-slas">SLIs, SLOs, and SLAs</a></h2>
<h3 id="understanding-the-hierarchy"><a class="header" href="#understanding-the-hierarchy">Understanding the Hierarchy</a></h3>
<pre><code>SLA (Agreement)
  ↓ commits to
SLO (Objective)
  ↓ measured by
SLI (Indicator)
  ↓ tracked via
Metrics
</code></pre>
<h3 id="service-level-indicators-slis"><a class="header" href="#service-level-indicators-slis">Service Level Indicators (SLIs)</a></h3>
<p><strong>Definition</strong>: Quantitative measures of service behavior.</p>
<p><strong>What to measure</strong> (the "what's happening"):</p>
<pre><code class="language-javascript">// Availability SLI: % of successful requests
const availabilitySLI = successfulRequests / totalRequests;

// Latency SLI: % of requests faster than threshold
const latencySLI = requestsFasterThan200ms / totalRequests;

// Throughput SLI: Requests per second
const throughputSLI = totalRequests / timeWindowSeconds;

// Quality SLI: % of requests without data loss
const qualitySLI = requestsWithoutDataLoss / totalRequests;
</code></pre>
<p><strong>Common SLIs:</strong></p>
<div class="table-wrapper"><table><thead><tr><th>Category</th><th>SLI</th><th>Measurement</th></tr></thead><tbody>
<tr><td><strong>Availability</strong></td><td>Request success rate</td><td><code>(total - errors) / total</code></td></tr>
<tr><td><strong>Latency</strong></td><td>95th percentile response time</td><td><code>p95(response_time)</code></td></tr>
<tr><td><strong>Throughput</strong></td><td>Requests per second</td><td><code>rate(requests[5m])</code></td></tr>
<tr><td><strong>Durability</strong></td><td>Data retention rate</td><td><code>retained_data / stored_data</code></td></tr>
<tr><td><strong>Correctness</strong></td><td>Error-free transactions</td><td><code>(total - corrupt) / total</code></td></tr>
</tbody></table>
</div>
<p><strong>Example SLI Queries:</strong></p>
<pre><code class="language-promql"># Availability SLI: 99.9% of requests succeed
sum(rate(http_requests_total{status!~"5.."}[30d])) /
sum(rate(http_requests_total[30d]))

# Latency SLI: 95% of requests complete in &lt; 200ms
sum(rate(http_request_duration_seconds_bucket{le="0.2"}[30d])) /
sum(rate(http_request_duration_seconds_count[30d]))

# Quality SLI: 99.99% of writes are durable
sum(rate(db_writes_durable[30d])) /
sum(rate(db_writes_total[30d]))
</code></pre>
<h3 id="service-level-objectives-slos"><a class="header" href="#service-level-objectives-slos">Service Level Objectives (SLOs)</a></h3>
<p><strong>Definition</strong>: Target values for SLIs. Internal goals for service reliability.</p>
<p><strong>Setting Realistic SLOs:</strong></p>
<ol>
<li>
<p><strong>Start with current performance</strong>:</p>
<pre><code>Current: 99.5% availability
Target: 99.9% availability (achievable stretch)
</code></pre>
</li>
<li>
<p><strong>Consider user expectations</strong>:</p>
<ul>
<li>Consumer apps: 99.9% (3 nines)</li>
<li>Enterprise SaaS: 99.95% (3.5 nines)</li>
<li>Critical infrastructure: 99.99% (4 nines)</li>
</ul>
</li>
<li>
<p><strong>Balance reliability vs velocity</strong>:</p>
<ul>
<li>Higher SLO = slower development</li>
<li>Lower SLO = more risk, faster shipping</li>
</ul>
</li>
</ol>
<p><strong>SLO Examples:</strong></p>
<pre><code class="language-yaml"># API Gateway SLOs
slos:
  - name: availability
    description: "API requests succeed"
    target: 99.9%
    window: 30d
    sli: |
      sum(rate(http_requests_total{status!~"5.."}[30d])) /
      sum(rate(http_requests_total[30d]))

  - name: latency_p95
    description: "95% of requests &lt; 200ms"
    target: 95%
    window: 30d
    sli: |
      sum(rate(http_request_duration_seconds_bucket{le="0.2"}[30d])) /
      sum(rate(http_request_duration_seconds_count[30d]))

  - name: latency_p99
    description: "99% of requests &lt; 1s"
    target: 99%
    window: 30d
    sli: |
      sum(rate(http_request_duration_seconds_bucket{le="1.0"}[30d])) /
      sum(rate(http_request_duration_seconds_count[30d]))
</code></pre>
<p><strong>Multi-Window SLOs:</strong></p>
<pre><code class="language-yaml"># Check SLO compliance over multiple time windows
windows:
  - 1h    # Fast feedback
  - 24h   # Daily
  - 7d    # Weekly
  - 30d   # Monthly (official)
</code></pre>
<h3 id="service-level-agreements-slas"><a class="header" href="#service-level-agreements-slas">Service Level Agreements (SLAs)</a></h3>
<p><strong>Definition</strong>: Contractual commitments to customers with consequences.</p>
<p><strong>SLA vs SLO:</strong></p>
<pre><code>SLO (Internal): 99.9% availability
  ↓ (set stricter than SLA)
SLA (External): 99.5% availability
  ↓ (with penalties if breached)
Customer expectation met
</code></pre>
<p><strong>SLA Example:</strong></p>
<pre><code class="language-yaml">service_level_agreement:
  service: "API Platform"
  customer: "Enterprise Tier"
  effective_date: "2025-01-01"

  commitments:
    - metric: "Monthly Uptime"
      target: 99.5%
      measurement: |
        (total_minutes - downtime_minutes) / total_minutes
      measurement_period: calendar_month

    - metric: "API Response Time"
      target: "95% of requests &lt; 500ms"
      measurement_period: calendar_month

  consequences:
    - threshold: "&lt; 99.5%"
      credit: "10% monthly fee"
    - threshold: "&lt; 99.0%"
      credit: "25% monthly fee"
    - threshold: "&lt; 95.0%"
      credit: "100% monthly fee"

  exclusions:
    - "Customer-caused issues"
    - "Scheduled maintenance (with 7-day notice)"
    - "Force majeure events"
    - "Third-party service failures"

  measurement:
    source: "Prometheus metrics"
    dashboard: "https://status.company.com"
    review: "Monthly"
</code></pre>
<h3 id="error-budgets"><a class="header" href="#error-budgets">Error Budgets</a></h3>
<p><strong>Definition</strong>: Amount of unreliability allowed within SLO.</p>
<p><strong>Calculation:</strong></p>
<pre><code class="language-javascript">// For 99.9% availability SLO over 30 days
const sloTarget = 0.999;
const errorBudget = 1 - sloTarget; // 0.001 = 0.1%

// In time
const totalMinutesPerMonth = 30 * 24 * 60; // 43,200 minutes
const allowedDowntime = totalMinutesPerMonth * errorBudget; // 43.2 minutes

// In requests
const totalRequests = 10_000_000;
const allowedErrors = totalRequests * errorBudget; // 10,000 errors
</code></pre>
<p><strong>Error Budget Tracking:</strong></p>
<pre><code class="language-promql"># Error budget remaining (%)
(
  1 - (
    (1 - availability_sli) / (1 - availability_slo_target)
  )
) * 100

# Example: Current availability = 99.95%, Target = 99.9%
# Error budget used = (1 - 0.9995) / (1 - 0.999) = 50%
# Error budget remaining = 50%
</code></pre>
<p><strong>Burn Rate:</strong></p>
<pre><code class="language-promql"># How fast are we consuming error budget?
# 1.0 = consuming at exactly SLO rate
# 2.0 = consuming twice as fast (will exhaust in 15 days)
# 0.5 = consuming half as fast (will last 60 days)

burn_rate = (1 - current_availability) / (1 - slo_target)

# Alert on high burn rate
burn_rate &gt; 2.0  # Consuming budget too fast
</code></pre>
<p><strong>Error Budget Policy:</strong></p>
<pre><code class="language-yaml">error_budget_policy:
  when_budget_remaining:
    - range: "100% - 50%"
      action: "Normal development pace"
      deployments: "Multiple per day"
      features: "Ship new features"

    - range: "50% - 25%"
      action: "Caution mode"
      deployments: "Daily deploys only"
      features: "Prioritize reliability"

    - range: "25% - 0%"
      action: "Freeze mode"
      deployments: "Emergency fixes only"
      features: "All hands on reliability"

    - range: "&lt; 0%"
      action: "SLO breach"
      deployments: "Halted"
      features: "Postmortem required"
      notification: "Leadership escalation"
</code></pre>
<h3 id="implementing-slos"><a class="header" href="#implementing-slos">Implementing SLOs</a></h3>
<h4 id="1-choose-slis"><a class="header" href="#1-choose-slis">1. Choose SLIs</a></h4>
<pre><code class="language-javascript">// Identify what matters to users
const userExperienceSLIs = {
  // Can they access the service?
  availability: true,

  // Is it fast enough?
  latency: true,

  // Is data correct?
  quality: true,

  // Are actions completed?
  throughput: false // Nice-to-have, not critical
};
</code></pre>
<h4 id="2-collect-data"><a class="header" href="#2-collect-data">2. Collect Data</a></h4>
<pre><code class="language-javascript">// Instrument to measure SLIs
const sliMetrics = {
  total: new Counter('requests_total'),
  success: new Counter('requests_success'),
  latency: new Histogram('request_duration_seconds', {
    buckets: [0.1, 0.2, 0.5, 1.0, 2.0, 5.0]
  })
};

app.use((req, res, next) =&gt; {
  const start = Date.now();

  res.on('finish', () =&gt; {
    sliMetrics.total.inc();

    if (res.statusCode &lt; 500) {
      sliMetrics.success.inc();
    }

    const duration = (Date.now() - start) / 1000;
    sliMetrics.latency.observe(duration);
  });

  next();
});
</code></pre>
<h4 id="3-set-targets"><a class="header" href="#3-set-targets">3. Set Targets</a></h4>
<pre><code class="language-yaml"># Start conservative, iterate
initial_slo:
  availability: 99.0%   # Easy to achieve
  latency_p95: 500ms    # Comfortable target

after_3_months:
  availability: 99.5%   # Tighten based on data
  latency_p95: 300ms

after_6_months:
  availability: 99.9%   # Final target
  latency_p95: 200ms
</code></pre>
<h4 id="4-alert-on-slo-violations"><a class="header" href="#4-alert-on-slo-violations">4. Alert on SLO Violations</a></h4>
<pre><code class="language-yaml"># Prometheus alert
groups:
  - name: slo_alerts
    rules:
      - alert: SLOViolation_Availability
        expr: |
          (
            sum(rate(http_requests_total{status!~"5.."}[30d])) /
            sum(rate(http_requests_total[30d]))
          ) &lt; 0.999
        for: 1h
        annotations:
          summary: "Availability SLO violated"
          description: "Current: {{ $value }}, Target: 0.999"

      - alert: ErrorBudgetExhausted
        expr: |
          (
            1 - (
              (1 - availability_sli) / (1 - 0.999)
            )
          ) &lt; 0
        annotations:
          summary: "Error budget exhausted"
          description: "Stop feature development, focus on reliability"
</code></pre>
<h4 id="5-report-and-review"><a class="header" href="#5-report-and-review">5. Report and Review</a></h4>
<pre><code class="language-javascript">// Weekly SLO report
const sloReport = {
  week: '2025-W03',
  slos: [
    {
      name: 'availability',
      target: 99.9,
      actual: 99.95,
      status: 'met',
      error_budget_remaining: 50
    },
    {
      name: 'latency_p95',
      target: 200,
      actual: 185,
      status: 'met',
      error_budget_remaining: 75
    }
  ],
  incidents: [
    {
      date: '2025-01-15',
      duration: '15 minutes',
      impact: 'Used 10% of error budget',
      root_cause: 'Database connection pool exhausted'
    }
  ],
  actions: [
    'Increase DB connection pool size',
    'Add connection pool monitoring',
    'Update runbook'
  ]
};
</code></pre>
<h3 id="slo-dashboard"><a class="header" href="#slo-dashboard">SLO Dashboard</a></h3>
<pre><code class="language-javascript">// Grafana dashboard configuration
const sloDashboard = {
  title: 'SLO Dashboard',
  panels: [
    {
      title: 'Availability - Last 30 Days',
      query: `
        sum(rate(http_requests_total{status!~"5.."}[30d])) /
        sum(rate(http_requests_total[30d])) * 100
      `,
      target: 99.9,
      visualization: 'gauge'
    },
    {
      title: 'Error Budget Remaining',
      query: `
        (1 - ((1 - availability_sli) / (1 - 0.999))) * 100
      `,
      visualization: 'gauge',
      thresholds: {
        green: [50, 100],
        yellow: [25, 50],
        red: [0, 25]
      }
    },
    {
      title: 'Error Budget Burn Rate',
      query: `
        (1 - availability_sli) / (1 - 0.999)
      `,
      visualization: 'graph',
      alert_threshold: 2.0
    },
    {
      title: 'SLO Compliance - 30 Day Rolling',
      query: `
        avg_over_time(availability_sli[30d])
      `,
      visualization: 'graph',
      bands: [
        { min: 99.9, max: 100, color: 'green', label: 'Above SLO' },
        { min: 0, max: 99.9, color: 'red', label: 'Below SLO' }
      ]
    }
  ]
};
</code></pre>
<h3 id="best-practices"><a class="header" href="#best-practices">Best Practices</a></h3>
<p><strong>DO:</strong></p>
<ul>
<li>Set SLOs based on user needs, not system capabilities</li>
<li>Make SLOs stricter than SLAs (buffer for safety)</li>
<li>Start conservative, tighten over time</li>
<li>Track error budgets, use them to make decisions</li>
<li>Review SLOs quarterly</li>
<li>Document SLO rationale</li>
</ul>
<p><strong>DON'T:</strong></p>
<ul>
<li>Set 100% as SLO (no error budget for innovation)</li>
<li>Have too many SLOs (focus on what matters)</li>
<li>Ignore SLO violations (defeats the purpose)</li>
<li>Set SLOs without measuring current performance</li>
<li>Make SLOs a surprise (transparent with team)</li>
</ul>
<p><strong>Example: Too Many Nines</strong></p>
<pre><code>99% = 7.2 hours downtime/month = OK for internal tools
99.9% = 43 minutes downtime/month = Good for most services
99.95% = 22 minutes downtime/month = Great for critical services
99.99% = 4 minutes downtime/month = Extremely expensive
99.999% = 26 seconds downtime/month = Reserved for critical infrastructure
</code></pre>
<h2 id="debugging-production-issues"><a class="header" href="#debugging-production-issues">Debugging Production Issues</a></h2>
<h3 id="systematic-debugging-approach"><a class="header" href="#systematic-debugging-approach">Systematic Debugging Approach</a></h3>
<h4 id="1-gather-context"><a class="header" href="#1-gather-context">1. Gather Context</a></h4>
<pre><code class="language-bash"># What changed recently?
git log --since="2 hours ago" --oneline

# When did it start?
# Check metrics dashboard for inflection point

# What's the scope?
# All users? Specific region? Specific feature?
</code></pre>
<h4 id="2-form-hypothesis"><a class="header" href="#2-form-hypothesis">2. Form Hypothesis</a></h4>
<pre><code>Theory: Database connection pool exhausted
Evidence needed:
  - Connection pool metrics
  - Database query latency
  - Error messages mentioning connections
</code></pre>
<h4 id="3-test-hypothesis"><a class="header" href="#3-test-hypothesis">3. Test Hypothesis</a></h4>
<pre><code class="language-bash"># Check connection pool
curl http://api:9090/metrics | grep db_pool

# Check database
kubectl logs -l app=api --tail=100 | grep -i connection

# Check traces for slow DB queries
# Jaeger UI: Filter by service=api, minDuration=1000ms
</code></pre>
<h4 id="4-mitigate"><a class="header" href="#4-mitigate">4. Mitigate</a></h4>
<pre><code class="language-bash"># Quick fix: Scale up
kubectl scale deployment/api --replicas=10

# Better fix: Increase pool size
kubectl set env deployment/api DB_POOL_SIZE=50
</code></pre>
<h4 id="5-verify"><a class="header" href="#5-verify">5. Verify</a></h4>
<pre><code class="language-bash"># Check error rate returned to normal
curl -s http://prometheus:9090/api/v1/query?query='error_rate' | jq .

# Check latency
curl -s http://prometheus:9090/api/v1/query?query='p95_latency' | jq .
</code></pre>
<h3 id="production-debugging-tools"><a class="header" href="#production-debugging-tools">Production Debugging Tools</a></h3>
<h4 id="live-debugging"><a class="header" href="#live-debugging">Live Debugging</a></h4>
<pre><code class="language-bash"># Attach debugger to running container (Node.js)
kubectl exec -it api-gateway-xxx -- kill -USR1 1
kubectl port-forward api-gateway-xxx 9229:9229
# Chrome DevTools: chrome://inspect

# Python
kubectl exec -it api-gateway-xxx -- python -m pdb app.py

# Go (requires delve)
kubectl exec -it api-gateway-xxx -- dlv attach $(pidof app)
</code></pre>
<h4 id="dynamic-logging"><a class="header" href="#dynamic-logging">Dynamic Logging</a></h4>
<pre><code class="language-javascript">// Enable debug logs for specific user
app.use((req, res, next) =&gt; {
  if (req.headers['x-debug-user'] === 'user_123') {
    req.log = logger.child({ level: 'debug' });
  }
  next();
});

// Enable via feature flag
if (featureFlags.isEnabled('debug-logging', userId)) {
  logger.level = 'debug';
}
</code></pre>
<h4 id="traffic-replay"><a class="header" href="#traffic-replay">Traffic Replay</a></h4>
<pre><code class="language-bash"># Capture traffic with tcpdump
tcpdump -i eth0 -w capture.pcap port 8080

# Replay with tcpreplay
tcpreplay --topspeed -i eth0 capture.pcap

# Or use gor for more control
gor --input-raw :8080 --output-http="http://staging:8080"
</code></pre>
<h4 id="query-analysis"><a class="header" href="#query-analysis">Query Analysis</a></h4>
<pre><code class="language-javascript">// Add query explanation
const explain = await db.query('EXPLAIN ANALYZE ' + sqlQuery);
logger.info({ explain }, 'Query plan');

// Log slow queries
const start = Date.now();
const result = await db.query(sqlQuery);
const duration = Date.now() - start;

if (duration &gt; 1000) {
  logger.warn({
    query: sqlQuery,
    duration,
    rows: result.rowCount
  }, 'Slow query detected');
}
</code></pre>
<h3 id="common-production-issues"><a class="header" href="#common-production-issues">Common Production Issues</a></h3>
<h4 id="memory-leaks"><a class="header" href="#memory-leaks">Memory Leaks</a></h4>
<pre><code class="language-javascript">// Detect memory leaks
const heapdump = require('heapdump');

setInterval(() =&gt; {
  const usage = process.memoryUsage();
  logger.info({ memory: usage }, 'Memory usage');

  if (usage.heapUsed &gt; THRESHOLD) {
    heapdump.writeSnapshot(`/tmp/heap-${Date.now()}.heapsnapshot`);
  }
}, 60000);

// Analyze with Chrome DevTools
</code></pre>
<h4 id="connection-leaks"><a class="header" href="#connection-leaks">Connection Leaks</a></h4>
<pre><code class="language-javascript">// Track connection lifecycle
class ConnectionPool {
  constructor() {
    this.active = new Set();
  }

  async acquire() {
    const conn = await this.pool.acquire();
    this.active.add(conn);
    conn._acquiredAt = Date.now();
    return conn;
  }

  release(conn) {
    this.active.delete(conn);
    this.pool.release(conn);
  }

  checkLeaks() {
    const now = Date.now();
    for (const conn of this.active) {
      if (now - conn._acquiredAt &gt; 30000) {
        logger.warn({
          age: now - conn._acquiredAt,
          stack: conn._stack
        }, 'Potential connection leak');
      }
    }
  }
}
</code></pre>
<h4 id="race-conditions"><a class="header" href="#race-conditions">Race Conditions</a></h4>
<pre><code class="language-javascript">// Add request tracing
const traceRequest = (req, res, next) =&gt; {
  req.id = generateId();
  req.startTime = Date.now();

  logger.info({
    req_id: req.id,
    method: req.method,
    path: req.path
  }, 'Request start');

  res.on('finish', () =&gt; {
    logger.info({
      req_id: req.id,
      duration: Date.now() - req.startTime,
      status: res.statusCode
    }, 'Request end');
  });

  next();
};
</code></pre>
<h2 id="tools-and-platforms"><a class="header" href="#tools-and-platforms">Tools and Platforms</a></h2>
<h3 id="open-source-stack"><a class="header" href="#open-source-stack">Open Source Stack</a></h3>
<pre><code class="language-yaml"># Metrics
prometheus:
  image: prom/prometheus
  volumes:
    - ./prometheus.yml:/etc/prometheus/prometheus.yml
  ports:
    - 9090:9090

# Visualization
grafana:
  image: grafana/grafana
  ports:
    - 3000:3000
  environment:
    - GF_AUTH_ANONYMOUS_ENABLED=true

# Logs
loki:
  image: grafana/loki
  ports:
    - 3100:3100

# Traces
jaeger:
  image: jaegertracing/all-in-one
  ports:
    - 16686:16686  # UI
    - 14268:14268  # Collector

# Collector
otel-collector:
  image: otel/opentelemetry-collector
  volumes:
    - ./otel-config.yaml:/etc/otel-collector-config.yaml
</code></pre>
<h3 id="commercial-platforms"><a class="header" href="#commercial-platforms">Commercial Platforms</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Platform</th><th>Strengths</th><th>Best For</th></tr></thead><tbody>
<tr><td><strong>Datadog</strong></td><td>All-in-one, great UX</td><td>Teams wanting simplicity</td></tr>
<tr><td><strong>New Relic</strong></td><td>APM, easy setup</td><td>Application monitoring</td></tr>
<tr><td><strong>Splunk</strong></td><td>Log analysis, enterprise</td><td>Large organizations</td></tr>
<tr><td><strong>Honeycomb</strong></td><td>High-cardinality, exploration</td><td>Complex debugging</td></tr>
<tr><td><strong>Lightstep</strong></td><td>Distributed tracing</td><td>Microservices</td></tr>
<tr><td><strong>Grafana Cloud</strong></td><td>Managed open source</td><td>OSS stack without ops</td></tr>
</tbody></table>
</div>
<h3 id="evaluation-criteria"><a class="header" href="#evaluation-criteria">Evaluation Criteria</a></h3>
<pre><code>✓ Data retention policies
✓ Query performance
✓ Cost at scale
✓ Integration ecosystem
✓ Team expertise
✓ Vendor lock-in
✓ SLA guarantees
✓ Support quality
</code></pre>
<h2 id="observability-culture"><a class="header" href="#observability-culture">Observability Culture</a></h2>
<h3 id="building-observability-practice"><a class="header" href="#building-observability-practice">Building Observability Practice</a></h3>
<p><strong>Phase 1: Foundation</strong> (Months 1-3)</p>
<ul>
<li>Standardize logging format</li>
<li>Deploy metrics collection</li>
<li>Create first dashboards</li>
<li>Document on-call process</li>
</ul>
<p><strong>Phase 2: Expansion</strong> (Months 4-6)</p>
<ul>
<li>Add distributed tracing</li>
<li>Define SLOs</li>
<li>Build runbooks</li>
<li>Train team</li>
</ul>
<p><strong>Phase 3: Maturity</strong> (Months 7-12)</p>
<ul>
<li>Observability in code reviews</li>
<li>Automated analysis</li>
<li>Predictive alerting</li>
<li>Continuous improvement</li>
</ul>
<h3 id="team-practices"><a class="header" href="#team-practices">Team Practices</a></h3>
<pre><code>Daily:
  - Check dashboards
  - Review overnight alerts
  - Triage new issues

Weekly:
  - Alert review (remove noise)
  - Incident retrospectives
  - Dashboard improvements

Monthly:
  - SLO review
  - Cost optimization
  - Tool evaluation
  - Training sessions

Quarterly:
  - Observability roadmap
  - Platform upgrades
  - Process improvements
</code></pre>
<h2 id="common-pitfalls"><a class="header" href="#common-pitfalls">Common Pitfalls</a></h2>
<h3 id="1-too-much-data"><a class="header" href="#1-too-much-data">1. Too Much Data</a></h3>
<p><strong>Problem</strong>: Collecting everything, analyzing nothing
<strong>Solution</strong>: Start with golden signals, expand based on needs</p>
<h3 id="2-vanity-metrics"><a class="header" href="#2-vanity-metrics">2. Vanity Metrics</a></h3>
<p><strong>Problem</strong>: Tracking metrics that don't drive decisions
<strong>Solution</strong>: Ask "what action would we take?" for each metric</p>
<h3 id="3-alert-fatigue"><a class="header" href="#3-alert-fatigue">3. Alert Fatigue</a></h3>
<p><strong>Problem</strong>: Too many alerts, all ignored
<strong>Solution</strong>: Ruthlessly prune non-actionable alerts</p>
<h3 id="4-tool-sprawl"><a class="header" href="#4-tool-sprawl">4. Tool Sprawl</a></h3>
<p><strong>Problem</strong>: Different tool for each team
<strong>Solution</strong>: Standardize on platform, federate access</p>
<h3 id="5-missing-context"><a class="header" href="#5-missing-context">5. Missing Context</a></h3>
<p><strong>Problem</strong>: Metrics without business meaning
<strong>Solution</strong>: Link technical metrics to business outcomes</p>
<h3 id="6-inconsistent-instrumentation"><a class="header" href="#6-inconsistent-instrumentation">6. Inconsistent Instrumentation</a></h3>
<p><strong>Problem</strong>: Each service does it differently
<strong>Solution</strong>: Shared libraries, code generation, conventions</p>
<h2 id="measuring-success"><a class="header" href="#measuring-success">Measuring Success</a></h2>
<h3 id="observability-kpis"><a class="header" href="#observability-kpis">Observability KPIs</a></h3>
<pre><code class="language-javascript">const observabilityKPIs = {
  // Detection
  meanTimeToDetect: 'MTTD',      // How fast we notice issues

  // Investigation
  meanTimeToUnderstand: 'MTTU',  // How fast we understand root cause

  // Resolution
  meanTimeToResolve: 'MTTR',     // How fast we fix issues

  // Prevention
  changeFailureRate: 'CFR',      // % of changes causing issues
  deploymentFrequency: 'DF'      // How often we can deploy
};

// Track improvement over time
// Before observability: MTTR = 4 hours
// After observability: MTTR = 20 minutes
</code></pre>
<h3 id="roi-calculation"><a class="header" href="#roi-calculation">ROI Calculation</a></h3>
<pre><code>Downtime cost reduction:
  Before: 10 hours/month × $10k/hour = $100k/month
  After:  2 hours/month × $10k/hour = $20k/month
  Savings: $80k/month

Development efficiency:
  Faster debugging: 5 hours/week × 10 engineers = 50 hours
  Value: $10k/month

Total value: $90k/month
Tool cost: $5k/month
ROI: 18x
</code></pre>
<h2 id="resources"><a class="header" href="#resources">Resources</a></h2>
<h3 id="books"><a class="header" href="#books">Books</a></h3>
<ul>
<li>Site Reliability Engineering (Google)</li>
<li>Observability Engineering (Honeycomb)</li>
<li>Distributed Systems Observability (Cindy Sridharan)</li>
</ul>
<h3 id="tools"><a class="header" href="#tools">Tools</a></h3>
<ul>
<li><a href="https://opentelemetry.io/">OpenTelemetry</a> - Vendor-neutral observability</li>
<li><a href="https://prometheus.io/">Prometheus</a> - Metrics collection</li>
<li><a href="https://grafana.com/">Grafana</a> - Visualization</li>
<li><a href="https://www.jaegertracing.io/">Jaeger</a> - Distributed tracing</li>
</ul>
<h3 id="learning"><a class="header" href="#learning">Learning</a></h3>
<ul>
<li><a href="https://grafana.com/tutorials/">Grafana Labs Tutorials</a></li>
<li><a href="https://github.com/open-telemetry/opentelemetry-demo">OpenTelemetry Demo</a></li>
<li><a href="https://charity.wtf/">Charity Majors Blog</a></li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../devops/terraform.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../system_design/index.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../devops/terraform.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../system_design/index.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="../elasticlunr.min.js"></script>
        <script src="../mark.min.js"></script>
        <script src="../searcher.js"></script>

        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->



    </div>
    </body>
</html>
