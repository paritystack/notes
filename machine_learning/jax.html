<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Jax - My Notes</title>


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon-de23e50b.svg">
        <link rel="shortcut icon" href="../favicon-8114d1fc.png">
        <link rel="stylesheet" href="../css/variables-8adf115d.css">
        <link rel="stylesheet" href="../css/general-2459343d.css">
        <link rel="stylesheet" href="../css/chrome-ae938929.css">
        <link rel="stylesheet" href="../css/print-9e4910d8.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../fonts/fonts-9644e21d.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="mdbook-highlight-css" href="../highlight-493f70e1.css">
        <link rel="stylesheet" id="mdbook-tomorrow-night-css" href="../tomorrow-night-4c0ae647.css">
        <link rel="stylesheet" id="mdbook-ayu-highlight-css" href="../ayu-highlight-3fdfc3ac.css">

        <!-- Custom theme stylesheets -->

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "../";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "../searchindex-aafba5d1.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../toc-f4c5bb4e.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="mdbook-body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="mdbook-sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("mdbook-sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="mdbook-sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="../toc.html"></iframe>
            </noscript>
            <div id="mdbook-sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="mdbook-page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="mdbook-menu-bar-hover-placeholder"></div>
                <div id="mdbook-menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="mdbook-sidebar-toggle" class="icon-button" for="mdbook-sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="mdbook-sidebar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"/></svg></span>
                        </label>
                        <button id="mdbook-theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="mdbook-theme-list">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M371.3 367.1c27.3-3.9 51.9-19.4 67.2-42.9L600.2 74.1c12.6-19.5 9.4-45.3-7.6-61.2S549.7-4.4 531.1 9.6L294.4 187.2c-24 18-38.2 46.1-38.4 76.1L371.3 367.1zm-19.6 25.4l-116-104.4C175.9 290.3 128 339.6 128 400c0 3.9 .2 7.8 .6 11.6c1.8 17.5-10.2 36.4-27.8 36.4H96c-17.7 0-32 14.3-32 32s14.3 32 32 32H240c61.9 0 112-50.1 112-112c0-2.5-.1-5-.2-7.5z"/></svg></span>
                        </button>
                        <ul id="mdbook-theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-ayu">Ayu</button></li>
                        </ul>
                        <button id="mdbook-search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="mdbook-searchbar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M416 208c0 45.9-14.9 88.3-40 122.7L502.6 457.4c12.5 12.5 12.5 32.8 0 45.3s-32.8 12.5-45.3 0L330.7 376c-34.4 25.2-76.8 40-122.7 40C93.1 416 0 322.9 0 208S93.1 0 208 0S416 93.1 416 208zM208 352c79.5 0 144-64.5 144-144s-64.5-144-144-144S64 128.5 64 208s64.5 144 144 144z"/></svg></span>
                        </button>
                    </div>

                    <h1 class="menu-title">My Notes</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <span class=fa-svg id="print-button"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M128 0C92.7 0 64 28.7 64 64v96h64V64H354.7L384 93.3V160h64V93.3c0-17-6.7-33.3-18.7-45.3L400 18.7C388 6.7 371.7 0 354.7 0H128zM384 352v32 64H128V384 368 352H384zm64 32h32c17.7 0 32-14.3 32-32V256c0-35.3-28.7-64-64-64H64c-35.3 0-64 28.7-64 64v96c0 17.7 14.3 32 32 32H64v64c0 35.3 28.7 64 64 64H384c35.3 0 64-28.7 64-64V384zm-16-88c-13.3 0-24-10.7-24-24s10.7-24 24-24s24 10.7 24 24s-10.7 24-24 24z"/></svg></span>
                        </a>

                    </div>
                </div>

                <div id="mdbook-search-wrapper" class="hidden">
                    <form id="mdbook-searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="mdbook-searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="mdbook-searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <span class=fa-svg id="fa-spin"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M304 48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zm0 416c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM48 304c26.5 0 48-21.5 48-48s-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48zm464-48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM142.9 437c18.7-18.7 18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zm0-294.2c18.7-18.7 18.7-49.1 0-67.9S93.7 56.2 75 75s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zM369.1 437c18.7 18.7 49.1 18.7 67.9 0s18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9z"/></svg></span>
                            </div>
                        </div>
                    </form>
                    <div id="mdbook-searchresults-outer" class="searchresults-outer hidden">
                        <div id="mdbook-searchresults-header" class="searchresults-header"></div>
                        <ul id="mdbook-searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('mdbook-sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('mdbook-sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#mdbook-sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="mdbook-content" class="content">
                    <main>
                        <h1 id="jax"><a class="header" href="#jax">JAX</a></h1>
<p>JAX is a Python library for high-performance numerical computing and machine learning research. It combines the familiar NumPy API with automatic differentiation, JIT compilation via XLA, and easy parallelization across GPUs and TPUs. JAX embraces functional programming principles and composable transformations.</p>
<p><strong>Key Features:</strong></p>
<ul>
<li>NumPy-compatible API with GPU/TPU acceleration</li>
<li>Automatic differentiation (forward and reverse mode)</li>
<li>JIT compilation for performance optimization</li>
<li>Vectorization (vmap) and parallelization (pmap)</li>
<li>Functional approach with pure functions and immutability</li>
</ul>
<h2 id="installation"><a class="header" href="#installation">Installation</a></h2>
<pre><code class="language-bash"># CPU version
pip install jax jaxlib

# GPU version (CUDA 12)
pip install -U "jax[cuda12]"

# TPU version
pip install -U "jax[tpu]" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html

# Common ML libraries
pip install flax optax
</code></pre>
<h2 id="table-of-contents"><a class="header" href="#table-of-contents">Table of Contents</a></h2>
<ol>
<li><a href="#core-concepts">Core Concepts</a></li>
<li><a href="#automatic-differentiation">Automatic Differentiation</a></li>
<li><a href="#jit-compilation">JIT Compilation</a></li>
<li><a href="#vectorization-with-vmap">Vectorization with vmap</a></li>
<li><a href="#parallelization-with-pmap">Parallelization with pmap</a></li>
<li><a href="#random-numbers">Random Numbers</a></li>
<li><a href="#pytrees">PyTrees</a></li>
<li><a href="#neural-networks-with-flax">Neural Networks with Flax</a></li>
<li><a href="#optimization-with-optax">Optimization with Optax</a></li>
<li><a href="#advanced-techniques">Advanced Techniques</a></li>
<li><a href="#best-practices">Best Practices</a></li>
<li><a href="#common-issues">Common Issues</a></li>
<li><a href="#further-resources">Further Resources</a></li>
</ol>
<h2 id="core-concepts"><a class="header" href="#core-concepts">Core Concepts</a></h2>
<h3 id="jax-arrays"><a class="header" href="#jax-arrays">JAX Arrays</a></h3>
<p>JAX arrays are similar to NumPy arrays but immutable and designed for acceleration.</p>
<pre><code class="language-python">import jax.numpy as jnp
import numpy as np

# Create JAX arrays (similar to NumPy)
x = jnp.array([1, 2, 3, 4, 5])
y = jnp.linspace(0, 10, 100)
z = jnp.zeros((3, 4))

# NumPy compatibility
np_array = np.array([1, 2, 3])
jax_array = jnp.array(np_array)  # Convert NumPy to JAX
back_to_numpy = np.array(jax_array)  # Convert JAX to NumPy

# Most NumPy operations work identically
result = jnp.dot(x, x)  # 55
mean = jnp.mean(y)
reshaped = z.reshape(4, 3)
</code></pre>
<h3 id="immutability"><a class="header" href="#immutability">Immutability</a></h3>
<p>JAX arrays are immutable - operations return new arrays rather than modifying in place.</p>
<pre><code class="language-python"># This doesn't work in JAX (will raise an error)
# x[0] = 10  # TypeError: JAX arrays are immutable

# Instead, use .at[] syntax for updates
x = jnp.array([1, 2, 3, 4, 5])
x_updated = x.at[0].set(10)  # Returns new array: [10, 2, 3, 4, 5]
x_incremented = x.at[1].add(5)  # Returns: [1, 7, 3, 4, 5]

# Multiple updates
x_multi = x.at[0].set(10).at[2].multiply(3)  # [10, 2, 9, 4, 5]

# Slice updates
y = jnp.zeros(10)
y_updated = y.at[2:5].set(jnp.array([7, 8, 9]))  # Updates indices 2, 3, 4
</code></pre>
<h3 id="pure-functions"><a class="header" href="#pure-functions">Pure Functions</a></h3>
<p>JAX transformations require pure functions (no side effects, deterministic output).</p>
<pre><code class="language-python"># Pure function (good for JAX)
def pure_function(x):
    return x ** 2 + 2 * x + 1

# Impure function (avoid with JAX transformations)
counter = 0
def impure_function(x):
    global counter
    counter += 1  # Side effect!
    return x ** 2

# Another pure function example
def compute_loss(params, x, y):
    """Pure function - output depends only on inputs"""
    predictions = params['w'] * x + params['b']
    return jnp.mean((predictions - y) ** 2)
</code></pre>
<h2 id="automatic-differentiation"><a class="header" href="#automatic-differentiation">Automatic Differentiation</a></h2>
<p>JAX provides powerful automatic differentiation through <code>grad</code>, <code>value_and_grad</code>, and more.</p>
<h3 id="basic-gradients"><a class="header" href="#basic-gradients">Basic Gradients</a></h3>
<pre><code class="language-python">import jax
from jax import grad, value_and_grad

# Simple function
def f(x):
    return x ** 3 + 2 * x ** 2 - 5 * x + 3

# Compute gradient (derivative)
df_dx = grad(f)
print(df_dx(2.0))  # 3*(2^2) + 4*2 - 5 = 15.0

# Get both value and gradient
value, gradient = value_and_grad(f)(2.0)
print(f"f(2.0) = {value}, f'(2.0) = {gradient}")  # f(2.0) = 5.0, f'(2.0) = 15.0
</code></pre>
<h3 id="gradients-with-multiple-arguments"><a class="header" href="#gradients-with-multiple-arguments">Gradients with Multiple Arguments</a></h3>
<pre><code class="language-python"># Function with multiple arguments
def loss(params, x, y):
    w, b = params
    pred = w * x + b
    return jnp.mean((pred - y) ** 2)

# Gradient with respect to first argument (params)
grad_loss = grad(loss)

params = (2.0, 0.5)
x = jnp.array([1.0, 2.0, 3.0])
y = jnp.array([2.0, 4.0, 6.0])

grads = grad_loss(params, x, y)  # Gradient w.r.t. params
print(grads)  # Tuple of gradients for w and b

# Specify which argument to differentiate
grad_wrt_x = grad(loss, argnums=1)  # Gradient w.r.t. x (second argument)
grad_wrt_y = grad(loss, argnums=2)  # Gradient w.r.t. y (third argument)

# Multiple arguments at once
grad_multi = grad(loss, argnums=(0, 1))  # Gradients w.r.t. params and x
</code></pre>
<h3 id="jacobians-and-hessians"><a class="header" href="#jacobians-and-hessians">Jacobians and Hessians</a></h3>
<pre><code class="language-python">from jax import jacfwd, jacrev, hessian

# Jacobian (for vector-valued functions)
def vector_function(x):
    return jnp.array([x[0]**2, x[1]**3, x[0]*x[1]])

x = jnp.array([2.0, 3.0])

# Forward-mode Jacobian (efficient for few inputs, many outputs)
jacobian_fwd = jacfwd(vector_function)(x)
print(jacobian_fwd)
# [[4.  0. ]
#  [0.  27.]
#  [3.  2. ]]

# Reverse-mode Jacobian (efficient for many inputs, few outputs)
jacobian_rev = jacrev(vector_function)(x)

# Hessian (second derivatives)
def scalar_function(x):
    return x[0]**3 + x[1]**2 + x[0]*x[1]

hess = hessian(scalar_function)(x)
print(hess)
# [[12.  1.]
#  [ 1.  2.]]
</code></pre>
<h3 id="gradients-in-machine-learning"><a class="header" href="#gradients-in-machine-learning">Gradients in Machine Learning</a></h3>
<pre><code class="language-python"># Typical ML loss function
def mse_loss(params, batch):
    x, y = batch
    predictions = params['w'] @ x + params['b']
    return jnp.mean((predictions - y) ** 2)

# Initialize parameters
params = {
    'w': jnp.array([[0.5, 0.3], [0.2, 0.8]]),
    'b': jnp.array([0.1, 0.2])
}

batch = (jnp.ones((2, 10)), jnp.ones((2, 10)))

# Compute loss and gradients
loss_val, grads = value_and_grad(mse_loss)(params, batch)
print(f"Loss: {loss_val}")
print(f"Gradients: {grads}")

# Update parameters (simple gradient descent)
learning_rate = 0.01
params_updated = jax.tree_map(
    lambda p, g: p - learning_rate * g,
    params, grads
)
</code></pre>
<h2 id="jit-compilation"><a class="header" href="#jit-compilation">JIT Compilation</a></h2>
<p>JIT (Just-In-Time) compilation via XLA dramatically improves performance by compiling functions to optimized machine code.</p>
<h3 id="basic-jit"><a class="header" href="#basic-jit">Basic JIT</a></h3>
<pre><code class="language-python">from jax import jit
import time

# Regular function
def slow_function(x):
    return jnp.sum(x ** 2) + jnp.mean(x) * 2

# JIT-compiled function
fast_function = jit(slow_function)

# Or use decorator
@jit
def another_fast_function(x):
    return jnp.sum(x ** 2) + jnp.mean(x) * 2

# Benchmark
x = jnp.ones((1000, 1000))

# First call compiles (slower)
start = time.time()
result1 = fast_function(x)
first_call_time = time.time() - start

# Subsequent calls use compiled version (much faster)
start = time.time()
result2 = fast_function(x)
subsequent_call_time = time.time() - start

print(f"First call: {first_call_time:.4f}s (includes compilation)")
print(f"Second call: {subsequent_call_time:.4f}s (cached)")
</code></pre>
<h3 id="jit-with-static-arguments"><a class="header" href="#jit-with-static-arguments">JIT with Static Arguments</a></h3>
<pre><code class="language-python">from functools import partial

# Functions with static arguments
@partial(jit, static_argnums=(1,))
def power_function(x, n):
    """n is static - will recompile if n changes"""
    return x ** n

result = power_function(jnp.array([1, 2, 3]), 2)  # Compiles for n=2
result = power_function(jnp.array([4, 5, 6]), 2)  # Reuses compilation
result = power_function(jnp.array([7, 8, 9]), 3)  # Recompiles for n=3

# Static argument names
@partial(jit, static_argnames=['activation'])
def apply_activation(x, activation='relu'):
    if activation == 'relu':
        return jnp.maximum(0, x)
    elif activation == 'tanh':
        return jnp.tanh(x)
    else:
        return x
</code></pre>
<h3 id="jit-constraints"><a class="header" href="#jit-constraints">JIT Constraints</a></h3>
<pre><code class="language-python"># Good: Array shapes known at compile time
@jit
def good_function(x):
    return x.reshape(10, -1)  # Shape is concrete

# Bad: Control flow based on array values
@jit
def bad_function(x):
    if x[0] &gt; 0:  # Error! Can't JIT boolean from array
        return x * 2
    else:
        return x * 3

# Good: Use jnp.where for conditional logic
@jit
def good_conditional(x):
    return jnp.where(x &gt; 0, x * 2, x * 3)

# Good: Use lax.cond for branching
from jax import lax

@jit
def good_branching(x, flag):
    return lax.cond(
        flag,
        lambda x: x * 2,  # True branch
        lambda x: x * 3,  # False branch
        x
    )
</code></pre>
<h2 id="vectorization-with-vmap"><a class="header" href="#vectorization-with-vmap">Vectorization with vmap</a></h2>
<p><code>vmap</code> automatically vectorizes functions, eliminating manual loops and improving performance.</p>
<h3 id="basic-vmap"><a class="header" href="#basic-vmap">Basic vmap</a></h3>
<pre><code class="language-python">from jax import vmap

# Function that works on single example
def predict(params, x):
    return params['w'] @ x + params['b']

params = {'w': jnp.array([0.5, 0.3]), 'b': 1.0}

# Single example
x_single = jnp.array([1.0, 2.0])
result_single = predict(params, x_single)  # Shape: ()

# Batch of examples (manual loop - slow)
x_batch = jnp.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])
results_manual = jnp.array([predict(params, x) for x in x_batch])

# Vectorized (automatic - fast!)
predict_batch = vmap(predict, in_axes=(None, 0))
# in_axes=(None, 0) means: don't batch params, batch x along axis 0
results_vmap = predict_batch(params, x_batch)  # Shape: (3,)
</code></pre>
<h3 id="advanced-vmap"><a class="header" href="#advanced-vmap">Advanced vmap</a></h3>
<pre><code class="language-python"># Batch over specific axes
def matrix_multiply(A, B):
    return A @ B

A_batch = jnp.ones((5, 3, 4))  # 5 matrices of shape (3, 4)
B = jnp.ones((4, 2))

# Apply to each matrix in the batch
result = vmap(matrix_multiply, in_axes=(0, None))(A_batch, B)
print(result.shape)  # (5, 3, 2)

# Batch over multiple axes
def compute_distances(x, y):
    return jnp.linalg.norm(x - y)

X = jnp.ones((10, 3))  # 10 points in 3D
Y = jnp.ones((20, 3))  # 20 points in 3D

# Compute all pairwise distances
distances = vmap(
    lambda x: vmap(lambda y: compute_distances(x, y))(Y)
)(X)
print(distances.shape)  # (10, 20)

# Or use vmap with out_axes
@vmap
def normalize_rows(matrix):
    return matrix / jnp.linalg.norm(matrix, axis=1, keepdims=True)
</code></pre>
<h3 id="vmap-for-gradients"><a class="header" href="#vmap-for-gradients">vmap for Gradients</a></h3>
<pre><code class="language-python"># Compute per-example gradients
def loss(params, x, y):
    pred = params['w'] @ x + params['b']
    return (pred - y) ** 2

params = {'w': jnp.array([0.5, 0.3]), 'b': 1.0}
x_batch = jnp.ones((100, 2))
y_batch = jnp.ones(100)

# Per-example gradients
per_example_grads = vmap(
    grad(loss),
    in_axes=(None, 0, 0)
)(params, x_batch, y_batch)

print(per_example_grads['w'].shape)  # (100, 2) - gradient for each example

# Average gradient
avg_grad = jax.tree_map(lambda g: jnp.mean(g, axis=0), per_example_grads)
</code></pre>
<h2 id="parallelization-with-pmap"><a class="header" href="#parallelization-with-pmap">Parallelization with pmap</a></h2>
<p><code>pmap</code> parallelizes computation across multiple devices (GPUs/TPUs).</p>
<h3 id="basic-pmap"><a class="header" href="#basic-pmap">Basic pmap</a></h3>
<pre><code class="language-python">from jax import pmap, local_device_count

# Check available devices
n_devices = local_device_count()
print(f"Available devices: {n_devices}")

# Function to parallelize
def f(x):
    return x ** 2 + x

# Create data for each device
x = jnp.arange(n_devices * 10).reshape(n_devices, 10)
print(x.shape)  # (n_devices, 10)

# Parallelize across devices
f_parallel = pmap(f)
result = f_parallel(x)
print(result.shape)  # (n_devices, 10)
# Each device processes one slice of the batch
</code></pre>
<h3 id="pmap-for-training"><a class="header" href="#pmap-for-training">pmap for Training</a></h3>
<pre><code class="language-python"># Parallel training step
@pmap
def train_step(params, batch):
    x, y = batch
    loss_val, grads = value_and_grad(loss)(params, x, y)
    # Update params
    new_params = jax.tree_map(
        lambda p, g: p - 0.01 * g,
        params, grads
    )
    return new_params, loss_val

# Replicate parameters across devices
params = {'w': jnp.ones(10), 'b': 0.0}
params_replicated = jax.tree_map(
    lambda x: jnp.array([x] * n_devices),
    params
)

# Shard batch across devices
batch_size = 32
x_batch = jnp.ones((batch_size, 10))
y_batch = jnp.ones(batch_size)

# Reshape to (n_devices, batch_per_device, ...)
x_sharded = x_batch.reshape(n_devices, -1, 10)
y_sharded = y_batch.reshape(n_devices, -1)

batch_sharded = (x_sharded, y_sharded)

# Train in parallel
new_params, losses = train_step(params_replicated, batch_sharded)
print(f"Loss on each device: {losses}")
</code></pre>
<h3 id="collective-operations"><a class="header" href="#collective-operations">Collective Operations</a></h3>
<pre><code class="language-python"># Communication between devices
@pmap
def allreduce_mean(x):
    return lax.pmean(x, axis_name='devices')

# Use axis_name to specify communication
@pmap(axis_name='batch')
def normalize_across_devices(x):
    # Compute mean across all devices
    global_mean = lax.pmean(x, 'batch')
    return x - global_mean
</code></pre>
<h2 id="random-numbers"><a class="header" href="#random-numbers">Random Numbers</a></h2>
<p>JAX uses explicit PRNG keys for reproducibility and parallelization.</p>
<h3 id="prng-keys"><a class="header" href="#prng-keys">PRNG Keys</a></h3>
<pre><code class="language-python">from jax import random

# Create a random key
key = random.PRNGKey(0)

# Generate random numbers
random_uniform = random.uniform(key, shape=(5,))
print(random_uniform)

# WRONG: Reusing the same key gives same numbers
random_1 = random.uniform(key, shape=(3,))
random_2 = random.uniform(key, shape=(3,))  # Same as random_1!

# CORRECT: Split keys for independent random numbers
key, subkey1, subkey2 = random.split(key, 3)
random_1 = random.uniform(subkey1, shape=(3,))
random_2 = random.uniform(subkey2, shape=(3,))  # Different from random_1

# Common pattern: split before each use
key, subkey = random.split(key)
x = random.normal(subkey, shape=(10,))

key, subkey = random.split(key)
y = random.normal(subkey, shape=(10,))
</code></pre>
<h3 id="random-distributions"><a class="header" href="#random-distributions">Random Distributions</a></h3>
<pre><code class="language-python">key = random.PRNGKey(42)

# Uniform distribution
key, subkey = random.split(key)
uniform = random.uniform(subkey, shape=(5,), minval=0, maxval=10)

# Normal distribution
key, subkey = random.split(key)
normal = random.normal(subkey, shape=(5,))

# Categorical
key, subkey = random.split(key)
logits = jnp.array([1.0, 2.0, 3.0])
samples = random.categorical(subkey, logits, shape=(10,))

# Permutation
key, subkey = random.split(key)
x = jnp.arange(10)
shuffled = random.permutation(subkey, x)

# Random choice
key, subkey = random.split(key)
indices = random.choice(subkey, 100, shape=(10,), replace=False)
</code></pre>
<h3 id="random-in-training-loops"><a class="header" href="#random-in-training-loops">Random in Training Loops</a></h3>
<pre><code class="language-python">def train_step(key, params, batch):
    # Split key for dropout
    key, dropout_key = random.split(key)

    def loss_fn(params):
        x, y = batch
        # Use dropout_key for randomness
        pred = forward_with_dropout(params, x, dropout_key)
        return jnp.mean((pred - y) ** 2)

    loss, grads = value_and_grad(loss_fn)(params)
    new_params = update_params(params, grads)

    return key, new_params, loss

# Training loop
key = random.PRNGKey(0)
for epoch in range(10):
    for batch in data_loader:
        key, params, loss = train_step(key, params, batch)
</code></pre>
<h2 id="pytrees"><a class="header" href="#pytrees">PyTrees</a></h2>
<p>PyTrees are nested structures (dicts, lists, tuples) that JAX can traverse and transform.</p>
<h3 id="working-with-pytrees"><a class="header" href="#working-with-pytrees">Working with PyTrees</a></h3>
<pre><code class="language-python">import jax.tree_util as tree

# PyTree examples
params_dict = {
    'layer1': {'w': jnp.ones((5, 3)), 'b': jnp.zeros(5)},
    'layer2': {'w': jnp.ones((10, 5)), 'b': jnp.zeros(10)}
}

params_list = [jnp.ones(5), jnp.zeros(3), jnp.ones((2, 2))]

# tree_map: Apply function to all leaves
scaled = tree.tree_map(lambda x: x * 2, params_dict)

# Combine two pytrees
grads = tree.tree_map(lambda x: jnp.ones_like(x), params_dict)
updated = tree.tree_map(
    lambda p, g: p - 0.01 * g,
    params_dict, grads
)

# tree_leaves: Get all leaf values
leaves = tree.tree_leaves(params_dict)
print(f"Number of parameters: {sum(x.size for x in leaves)}")

# tree_structure: Get structure without values
structure = tree.tree_structure(params_dict)

# Flatten and unflatten
flat, treedef = tree.tree_flatten(params_dict)
reconstructed = tree.tree_unflatten(treedef, flat)
</code></pre>
<h3 id="custom-pytree-classes"><a class="header" href="#custom-pytree-classes">Custom PyTree Classes</a></h3>
<pre><code class="language-python">from jax.tree_util import register_pytree_node_class

@register_pytree_node_class
class MLPParams:
    def __init__(self, weights, biases):
        self.weights = weights
        self.biases = biases

    def tree_flatten(self):
        # Return (children, aux_data)
        children = (self.weights, self.biases)
        aux_data = None
        return children, aux_data

    def tree_unflatten(aux_data, children):
        return MLPParams(*children)

# Now can use with tree_map
params = MLPParams([jnp.ones((5, 3))], [jnp.zeros(5)])
scaled = tree.tree_map(lambda x: x * 2, params)
</code></pre>
<h2 id="neural-networks-with-flax"><a class="header" href="#neural-networks-with-flax">Neural Networks with Flax</a></h2>
<p>Flax is a high-level neural network library built on JAX.</p>
<h3 id="basic-flax-module"><a class="header" href="#basic-flax-module">Basic Flax Module</a></h3>
<pre><code class="language-python">from flax import linen as nn

class SimpleMLP(nn.Module):
    hidden_dim: int
    output_dim: int

    @nn.compact
    def __call__(self, x):
        # Layers are created on first call
        x = nn.Dense(self.hidden_dim)(x)
        x = nn.relu(x)
        x = nn.Dense(self.output_dim)(x)
        return x

# Initialize model
model = SimpleMLP(hidden_dim=128, output_dim=10)

# Initialize parameters
key = random.PRNGKey(0)
dummy_input = jnp.ones((1, 784))  # Batch of 1, 784 features
params = model.init(key, dummy_input)

# Forward pass
output = model.apply(params, dummy_input)
print(output.shape)  # (1, 10)
</code></pre>
<h3 id="training-with-flax"><a class="header" href="#training-with-flax">Training with Flax</a></h3>
<pre><code class="language-python">class CNN(nn.Module):
    @nn.compact
    def __call__(self, x, training: bool):
        x = nn.Conv(features=32, kernel_size=(3, 3))(x)
        x = nn.relu(x)
        x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))

        x = nn.Conv(features=64, kernel_size=(3, 3))(x)
        x = nn.relu(x)
        x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))

        x = x.reshape((x.shape[0], -1))  # Flatten
        x = nn.Dense(features=256)(x)
        x = nn.relu(x)
        x = nn.Dropout(rate=0.5, deterministic=not training)(x)
        x = nn.Dense(features=10)(x)
        return x

# Create training step
@jit
def train_step(params, batch, rng):
    x, y = batch

    def loss_fn(params):
        logits = CNN().apply(params, x, training=True, rngs={'dropout': rng})
        loss = jnp.mean(optax.softmax_cross_entropy_with_integer_labels(logits, y))
        return loss, logits

    (loss, logits), grads = value_and_grad(loss_fn, has_aux=True)(params)
    return grads, loss

# Initialize
model = CNN()
key = random.PRNGKey(0)
key, init_key = random.split(key)
dummy_x = jnp.ones((1, 28, 28, 1))
params = model.init(init_key, dummy_x, training=False)
</code></pre>
<h3 id="state-management-batchnorm"><a class="header" href="#state-management-batchnorm">State Management (BatchNorm)</a></h3>
<pre><code class="language-python">class ModelWithBatchNorm(nn.Module):
    @nn.compact
    def __call__(self, x, training: bool):
        x = nn.Dense(features=128)(x)
        x = nn.BatchNorm(use_running_average=not training)(x)
        x = nn.relu(x)
        x = nn.Dense(features=10)(x)
        return x

# Initialize with batch_stats
model = ModelWithBatchNorm()
variables = model.init(key, dummy_input, training=False)
params = variables['params']
batch_stats = variables['batch_stats']

# Training step with mutable state
def train_step_with_bn(params, batch_stats, batch):
    x, y = batch

    def loss_fn(params):
        logits, new_batch_stats = model.apply(
            {'params': params, 'batch_stats': batch_stats},
            x, training=True,
            mutable=['batch_stats']
        )
        loss = jnp.mean((logits - y) ** 2)
        return loss, new_batch_stats

    (loss, new_batch_stats), grads = value_and_grad(loss_fn, has_aux=True)(params)
    return grads, loss, new_batch_stats['batch_stats']
</code></pre>
<h2 id="optimization-with-optax"><a class="header" href="#optimization-with-optax">Optimization with Optax</a></h2>
<p>Optax provides composable gradient transformations and optimizers.</p>
<h3 id="basic-optimizers"><a class="header" href="#basic-optimizers">Basic Optimizers</a></h3>
<pre><code class="language-python">import optax

# Create optimizer
optimizer = optax.adam(learning_rate=0.001)

# Initialize optimizer state
params = {'w': jnp.ones((5, 3)), 'b': jnp.zeros(5)}
opt_state = optimizer.init(params)

# Training step
def train_step(params, opt_state, batch):
    loss, grads = value_and_grad(compute_loss)(params, batch)

    # Update using optimizer
    updates, opt_state = optimizer.update(grads, opt_state)
    params = optax.apply_updates(params, updates)

    return params, opt_state, loss

# Training loop
for epoch in range(100):
    for batch in data_loader:
        params, opt_state, loss = train_step(params, opt_state, batch)
</code></pre>
<h3 id="common-optimizers"><a class="header" href="#common-optimizers">Common Optimizers</a></h3>
<pre><code class="language-python"># SGD with momentum
optimizer = optax.sgd(learning_rate=0.01, momentum=0.9)

# Adam
optimizer = optax.adam(learning_rate=0.001, b1=0.9, b2=0.999)

# AdamW (Adam with weight decay)
optimizer = optax.adamw(learning_rate=0.001, weight_decay=0.01)

# RMSprop
optimizer = optax.rmsprop(learning_rate=0.001)

# Learning rate schedules
schedule = optax.exponential_decay(
    init_value=0.1,
    transition_steps=1000,
    decay_rate=0.99
)
optimizer = optax.adam(learning_rate=schedule)

# Cosine decay
schedule = optax.cosine_decay_schedule(
    init_value=0.1,
    decay_steps=10000
)
optimizer = optax.adam(learning_rate=schedule)
</code></pre>
<h3 id="gradient-transformations"><a class="header" href="#gradient-transformations">Gradient Transformations</a></h3>
<pre><code class="language-python"># Combine transformations
optimizer = optax.chain(
    optax.clip_by_global_norm(1.0),  # Gradient clipping
    optax.scale_by_adam(),  # Adam updates
    optax.scale(-0.001)  # Learning rate
)

# Gradient accumulation
optimizer = optax.MultiSteps(
    optax.adam(0.001),
    every_k_schedule=4  # Accumulate over 4 steps
)

# Different learning rates for different parameters
def label_fn(path, _):
    if 'bias' in path:
        return 'bias'
    return 'weight'

optimizer = optax.multi_transform(
    {
        'weight': optax.adam(0.001),
        'bias': optax.adam(0.01)  # Higher LR for biases
    },
    label_fn
)
</code></pre>
<h2 id="advanced-techniques"><a class="header" href="#advanced-techniques">Advanced Techniques</a></h2>
<h3 id="custom-gradients"><a class="header" href="#custom-gradients">Custom Gradients</a></h3>
<pre><code class="language-python">from jax import custom_vjp

# Define custom gradient for a function
@custom_vjp
def clip_gradient(x):
    return x

def clip_gradient_fwd(x):
    # Forward pass
    return x, None

def clip_gradient_bwd(res, g):
    # Custom backward pass - clip gradients
    return (jnp.clip(g, -1.0, 1.0),)

clip_gradient.defvjp(clip_gradient_fwd, clip_gradient_bwd)

# Use in computation
def loss_with_clipped_grad(x):
    x = clip_gradient(x)  # Gradients will be clipped
    return x ** 2

grad_fn = grad(loss_with_clipped_grad)
print(grad_fn(5.0))  # Gradient is clipped to 1.0 instead of 10.0
</code></pre>
<h3 id="scan-for-loops"><a class="header" href="#scan-for-loops">Scan for Loops</a></h3>
<pre><code class="language-python">from jax.lax import scan

# Efficient loop implementation
def rnn_cell(carry, x):
    h = carry
    h_new = jnp.tanh(jnp.dot(W_h, h) + jnp.dot(W_x, x))
    return h_new, h_new

# Initialize
W_h = jnp.ones((10, 10))
W_x = jnp.ones((10, 5))
h_0 = jnp.zeros(10)
xs = jnp.ones((20, 5))  # Sequence of 20 inputs

# Run RNN with scan (much faster than Python loop)
final_h, all_h = scan(rnn_cell, h_0, xs)
print(all_h.shape)  # (20, 10) - hidden states for each timestep

# Reverse scan
final_h, all_h = scan(rnn_cell, h_0, xs, reverse=True)
</code></pre>
<h3 id="checkpointing-gradient-checkpointing"><a class="header" href="#checkpointing-gradient-checkpointing">Checkpointing (Gradient Checkpointing)</a></h3>
<pre><code class="language-python">from jax.checkpoint import checkpoint

# Regular function (stores all intermediates)
def expensive_layer(x):
    for _ in range(100):
        x = jnp.tanh(x @ W + b)
    return x

# Checkpointed version (recomputes on backward pass)
expensive_layer_checkpointed = checkpoint(expensive_layer)

# Use in larger model to save memory
def large_model(x):
    x = expensive_layer_checkpointed(x)
    x = another_layer(x)
    return x
</code></pre>
<h3 id="custom-training-loop"><a class="header" href="#custom-training-loop">Custom Training Loop</a></h3>
<pre><code class="language-python">def create_train_state(rng, learning_rate, input_shape):
    """Create initial training state"""
    model = SimpleMLP(hidden_dim=128, output_dim=10)
    params = model.init(rng, jnp.ones(input_shape))
    optimizer = optax.adam(learning_rate)
    opt_state = optimizer.init(params)
    return model, params, optimizer, opt_state

@jit
def train_step(model, params, opt_state, optimizer, batch):
    """Single training step"""
    x, y = batch

    def loss_fn(params):
        logits = model.apply(params, x)
        loss = jnp.mean(optax.softmax_cross_entropy_with_integer_labels(logits, y))
        return loss, logits

    (loss, logits), grads = value_and_grad(loss_fn, has_aux=True)(params)
    updates, opt_state = optimizer.update(grads, opt_state)
    params = optax.apply_updates(params, updates)

    # Compute accuracy
    accuracy = jnp.mean(jnp.argmax(logits, axis=-1) == y)

    return params, opt_state, loss, accuracy

@jit
def eval_step(model, params, batch):
    """Evaluation step"""
    x, y = batch
    logits = model.apply(params, x)
    loss = jnp.mean(optax.softmax_cross_entropy_with_integer_labels(logits, y))
    accuracy = jnp.mean(jnp.argmax(logits, axis=-1) == y)
    return loss, accuracy

# Full training loop
def train(num_epochs, train_data, val_data):
    rng = random.PRNGKey(0)
    model, params, optimizer, opt_state = create_train_state(
        rng, learning_rate=0.001, input_shape=(1, 784)
    )

    for epoch in range(num_epochs):
        # Training
        train_loss, train_acc = 0.0, 0.0
        for batch in train_data:
            params, opt_state, loss, acc = train_step(
                model, params, opt_state, optimizer, batch
            )
            train_loss += loss
            train_acc += acc

        # Validation
        val_loss, val_acc = 0.0, 0.0
        for batch in val_data:
            loss, acc = eval_step(model, params, batch)
            val_loss += loss
            val_acc += acc

        print(f"Epoch {epoch}: Train Loss={train_loss:.4f}, Val Acc={val_acc:.4f}")

    return params
</code></pre>
<h2 id="best-practices"><a class="header" href="#best-practices">Best Practices</a></h2>
<h3 id="performance-optimization"><a class="header" href="#performance-optimization">Performance Optimization</a></h3>
<ul>
<li><strong>Use JIT compilation</strong> for all performance-critical functions</li>
<li><strong>Vectorize with vmap</strong> instead of Python loops</li>
<li><strong>Batch operations</strong> to maximize hardware utilization</li>
<li><strong>Avoid unnecessary array copies</strong> - JAX arrays are immutable but efficient</li>
<li><strong>Profile your code</strong> using <code>jax.profiler</code></li>
</ul>
<pre><code class="language-python"># Good: Vectorized and JIT-compiled
@jit
def efficient_computation(x):
    return vmap(lambda a: jnp.sum(a ** 2))(x)

# Bad: Python loop
def inefficient_computation(x):
    return jnp.array([jnp.sum(a ** 2) for a in x])
</code></pre>
<h3 id="memory-management"><a class="header" href="#memory-management">Memory Management</a></h3>
<ul>
<li><strong>Use gradient checkpointing</strong> for very deep networks</li>
<li><strong>Clear unused arrays</strong> to free memory</li>
<li><strong>Use <code>jax.device_put</code></strong> to control array placement</li>
<li><strong>Monitor memory</strong> with <code>jax.local_devices()</code></li>
</ul>
<pre><code class="language-python">from jax import device_put

# Explicitly place array on device
x_gpu = device_put(x, device=jax.devices()[0])

# Clear cache if needed (after debugging)
from jax import clear_caches
clear_caches()
</code></pre>
<h3 id="debugging"><a class="header" href="#debugging">Debugging</a></h3>
<ul>
<li><strong>Disable JIT</strong> during debugging: set <code>JAX_DISABLE_JIT=1</code> environment variable</li>
<li><strong>Use <code>jax.debug.print()</code></strong> inside JIT-compiled functions</li>
<li><strong>Check for NaNs</strong> with <code>jax.debug.check_nan()</code></li>
</ul>
<pre><code class="language-python">import jax.debug as debug

@jit
def debug_function(x):
    debug.print("x = {}", x)  # Prints during execution
    y = x ** 2
    debug.print("y = {}", y)
    return y

# Check for NaNs
x = jnp.array([1.0, float('nan'), 3.0])
# debug.check_nan(x)  # Raises error if NaNs present
</code></pre>
<h3 id="code-organization"><a class="header" href="#code-organization">Code Organization</a></h3>
<ul>
<li><strong>Separate model definition from training logic</strong></li>
<li><strong>Use configuration files</strong> (e.g., with Hydra) for hyperparameters</li>
<li><strong>Modularize transformations</strong> (grad, jit, vmap) for reusability</li>
<li><strong>Type hints</strong> improve code clarity</li>
</ul>
<pre><code class="language-python">from typing import Dict, Tuple
import jax.numpy as jnp

def forward(params: Dict, x: jnp.ndarray) -&gt; jnp.ndarray:
    """Type-annotated forward pass"""
    return params['w'] @ x + params['b']

def loss_fn(params: Dict, batch: Tuple) -&gt; float:
    """Type-annotated loss function"""
    x, y = batch
    pred = forward(params, x)
    return jnp.mean((pred - y) ** 2)
</code></pre>
<h3 id="random-number-management"><a class="header" href="#random-number-management">Random Number Management</a></h3>
<ul>
<li><strong>Always split keys</strong> before use</li>
<li><strong>Pass keys explicitly</strong> through function calls</li>
<li><strong>Don’t reuse keys</strong> - leads to correlated randomness</li>
</ul>
<pre><code class="language-python"># Good: Proper key management
def training_epoch(key, params, data):
    losses = []
    for batch in data:
        key, subkey = random.split(key)
        params, loss = train_step(params, batch, subkey)
        losses.append(loss)
    return key, params, losses

# Bad: Reusing key
def bad_training_epoch(key, params, data):
    for batch in data:
        params, loss = train_step(params, batch, key)  # Same key every time!
</code></pre>
<h2 id="common-issues"><a class="header" href="#common-issues">Common Issues</a></h2>
<h3 id="concretizationtypeerror"><a class="header" href="#concretizationtypeerror">ConcretizationTypeError</a></h3>
<p><strong>Problem:</strong> Trying to use array values in control flow inside JIT.</p>
<pre><code class="language-python"># Error
@jit
def bad(x):
    if x.sum() &gt; 0:  # Can't convert array to bool in JIT
        return x
    return -x

# Solution: Use jnp.where or lax.cond
@jit
def good(x):
    return jnp.where(x.sum() &gt; 0, x, -x)
</code></pre>
<h3 id="tracerarrayconversionerror"><a class="header" href="#tracerarrayconversionerror">TracerArrayConversionError</a></h3>
<p><strong>Problem:</strong> Trying to convert JAX array to NumPy array inside transformation.</p>
<pre><code class="language-python"># Error
@jit
def bad(x):
    return np.array(x)  # Can't convert to NumPy in JIT

# Solution: Use JAX operations
@jit
def good(x):
    return jnp.array(x)
</code></pre>
<h3 id="unexpectedtracererror"><a class="header" href="#unexpectedtracererror">UnexpectedTracerError</a></h3>
<p><strong>Problem:</strong> Leaking tracers outside of transformations.</p>
<pre><code class="language-python"># Error
cached_value = None

@jit
def bad(x):
    global cached_value
    cached_value = x  # Leaks tracer!
    return x * 2

# Solution: Keep everything inside the function
@jit
def good(x):
    temp = x  # Local variable
    return temp * 2
</code></pre>
<h3 id="out-of-memory"><a class="header" href="#out-of-memory">Out of Memory</a></h3>
<p><strong>Problem:</strong> Running out of GPU memory.</p>
<p><strong>Solutions:</strong></p>
<ul>
<li>Use gradient checkpointing for large models</li>
<li>Reduce batch size</li>
<li>Use mixed precision training</li>
<li>Clear caches: <code>jax.clear_caches()</code></li>
</ul>
<pre><code class="language-python"># Use smaller dtype
x = jnp.array(data, dtype=jnp.float16)  # Instead of float32

# Gradient checkpointing
from jax.checkpoint import checkpoint
layer = checkpoint(expensive_layer)
</code></pre>
<h3 id="slow-first-iteration"><a class="header" href="#slow-first-iteration">Slow First Iteration</a></h3>
<p><strong>Problem:</strong> First call to JIT function is slow.</p>
<p><strong>Explanation:</strong> This is normal - JAX compiles on first call. Subsequent calls are fast.</p>
<pre><code class="language-python"># Warm up by calling once
@jit
def f(x):
    return x ** 2

_ = f(jnp.ones(10))  # Compilation happens here
# Now subsequent calls are fast
result = f(jnp.ones(10))  # Fast!
</code></pre>
<h2 id="further-resources"><a class="header" href="#further-resources">Further Resources</a></h2>
<h3 id="official-documentation"><a class="header" href="#official-documentation">Official Documentation</a></h3>
<ul>
<li><a href="https://jax.readthedocs.io/">JAX Documentation</a></li>
<li><a href="https://github.com/google/jax">JAX GitHub</a></li>
<li><a href="https://flax.readthedocs.io/">Flax Documentation</a></li>
<li><a href="https://optax.readthedocs.io/">Optax Documentation</a></li>
</ul>
<h3 id="tutorials"><a class="header" href="#tutorials">Tutorials</a></h3>
<ul>
<li><a href="https://jax.readthedocs.io/en/latest/quickstart.html">JAX Quickstart</a></li>
<li><a href="https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html">The Autodiff Cookbook</a></li>
<li><a href="https://jax.readthedocs.io/en/latest/notebooks/thinking_in_jax.html">Thinking in JAX</a></li>
</ul>
<h3 id="advanced-topics"><a class="header" href="#advanced-topics">Advanced Topics</a></h3>
<ul>
<li><a href="https://jax.readthedocs.io/en/latest/jax-101/index.html">JAX 101</a></li>
<li><a href="https://jax.readthedocs.io/en/latest/notebooks/Advanced_Autodiff.html">Advanced Autodiff</a></li>
<li><a href="https://jax.readthedocs.io/en/latest/jax-101/06-parallelism.html">Parallelism</a></li>
</ul>
<h3 id="community"><a class="header" href="#community">Community</a></h3>
<ul>
<li><a href="https://github.com/google/jax/discussions">JAX Discussions</a></li>
<li><a href="https://github.com/google/jax/tree/main/examples">JAX Examples</a></li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../machine_learning/moe.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256 246.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"/></svg></span>
                            </a>

                            <a rel="next prefetch" href="../machine_learning/convolution.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M278.6 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L210.7 256 73.4 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"/></svg></span>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../machine_learning/moe.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256 246.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"/></svg></span>
                    </a>

                    <a rel="next prefetch" href="../machine_learning/convolution.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M278.6 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L210.7 256 73.4 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"/></svg></span>
                    </a>
            </nav>

        </div>

        <template id=fa-eye><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M288 32c-80.8 0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7 0 24.6C17.3 304 48.6 356 95.4 399.4C142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144s64.5-144 144-144s144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64c-11.5 0-22.3-3-31.6-8.4c-.2 2.8-.4 5.5-.4 8.4c0 53 43 96 96 96s96-43 96-96s-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6z"/></svg></span></template>
        <template id=fa-eye-slash><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M38.8 5.1C28.4-3.1 13.3-1.2 5.1 9.2S-1.2 34.7 9.2 42.9l592 464c10.4 8.2 25.5 6.3 33.7-4.1s6.3-25.5-4.1-33.7L525.6 386.7c39.6-40.6 66.4-86.1 79.9-118.4c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C465.5 68.8 400.8 32 320 32c-68.2 0-125 26.3-169.3 60.8L38.8 5.1zM223.1 149.5C248.6 126.2 282.7 112 320 112c79.5 0 144 64.5 144 144c0 24.9-6.3 48.3-17.4 68.7L408 294.5c5.2-11.8 8-24.8 8-38.5c0-53-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6c0 10.2-2.4 19.8-6.6 28.3l-90.3-70.8zm223.1 298L373 389.9c-16.4 6.5-34.3 10.1-53 10.1c-79.5 0-144-64.5-144-144c0-6.9 .5-13.6 1.4-20.2L83.1 161.5C60.3 191.2 44 220.8 34.5 243.7c-3.3 7.9-3.3 16.7 0 24.6c14.9 35.7 46.2 87.7 93 131.1C174.5 443.2 239.2 480 320 480c47.8 0 89.9-12.9 126.2-32.5z"/></svg></span></template>
        <template id=fa-copy><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M502.6 70.63l-61.25-61.25C435.4 3.371 427.2 0 418.7 0H255.1c-35.35 0-64 28.66-64 64l.0195 256C192 355.4 220.7 384 256 384h192c35.2 0 64-28.8 64-64V93.25C512 84.77 508.6 76.63 502.6 70.63zM464 320c0 8.836-7.164 16-16 16H255.1c-8.838 0-16-7.164-16-16L239.1 64.13c0-8.836 7.164-16 16-16h128L384 96c0 17.67 14.33 32 32 32h47.1V320zM272 448c0 8.836-7.164 16-16 16H63.1c-8.838 0-16-7.164-16-16L47.98 192.1c0-8.836 7.164-16 16-16H160V128H63.99c-35.35 0-64 28.65-64 64l.0098 256C.002 483.3 28.66 512 64 512h192c35.2 0 64-28.8 64-64v-32h-47.1L272 448z"/></svg></span></template>
        <template id=fa-play><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M73 39c-14.8-9.1-33.4-9.4-48.5-.9S0 62.6 0 80V432c0 17.4 9.4 33.4 24.5 41.9s33.7 8.1 48.5-.9L361 297c14.3-8.7 23-24.2 23-41s-8.7-32.2-23-41L73 39z"/></svg></span></template>
        <template id=fa-clock-rotate-left><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M75 75L41 41C25.9 25.9 0 36.6 0 57.9V168c0 13.3 10.7 24 24 24H134.1c21.4 0 32.1-25.9 17-41l-30.8-30.8C155 85.5 203 64 256 64c106 0 192 86 192 192s-86 192-192 192c-40.8 0-78.6-12.7-109.7-34.4c-14.5-10.1-34.4-6.6-44.6 7.9s-6.6 34.4 7.9 44.6C151.2 495 201.7 512 256 512c141.4 0 256-114.6 256-256S397.4 0 256 0C185.3 0 121.3 28.7 75 75zm181 53c-13.3 0-24 10.7-24 24V256c0 6.4 2.5 12.5 7 17l72 72c9.4 9.4 24.6 9.4 33.9 0s9.4-24.6 0-33.9l-65-65V152c0-13.3-10.7-24-24-24z"/></svg></span></template>



        <script>
            window.playground_copyable = true;
        </script>


        <script src="../elasticlunr-ef4e11c1.min.js"></script>
        <script src="../mark-09e88c2c.min.js"></script>
        <script src="../searcher-c2a407aa.js"></script>

        <script src="../clipboard-1626706a.min.js"></script>
        <script src="../highlight-abc7f01d.js"></script>
        <script src="../book-a0b12cfe.js"></script>

        <!-- Custom JS scripts -->



    </div>
    </body>
</html>
