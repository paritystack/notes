<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Convolution - My Notes</title>


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon-de23e50b.svg">
        <link rel="shortcut icon" href="../favicon-8114d1fc.png">
        <link rel="stylesheet" href="../css/variables-8adf115d.css">
        <link rel="stylesheet" href="../css/general-2459343d.css">
        <link rel="stylesheet" href="../css/chrome-ae938929.css">
        <link rel="stylesheet" href="../css/print-9e4910d8.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../fonts/fonts-9644e21d.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="mdbook-highlight-css" href="../highlight-493f70e1.css">
        <link rel="stylesheet" id="mdbook-tomorrow-night-css" href="../tomorrow-night-4c0ae647.css">
        <link rel="stylesheet" id="mdbook-ayu-highlight-css" href="../ayu-highlight-3fdfc3ac.css">

        <!-- Custom theme stylesheets -->

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "../";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "../searchindex-aafba5d1.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../toc-f4c5bb4e.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="mdbook-body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="mdbook-sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("mdbook-sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="mdbook-sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="../toc.html"></iframe>
            </noscript>
            <div id="mdbook-sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="mdbook-page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="mdbook-menu-bar-hover-placeholder"></div>
                <div id="mdbook-menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="mdbook-sidebar-toggle" class="icon-button" for="mdbook-sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="mdbook-sidebar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"/></svg></span>
                        </label>
                        <button id="mdbook-theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="mdbook-theme-list">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M371.3 367.1c27.3-3.9 51.9-19.4 67.2-42.9L600.2 74.1c12.6-19.5 9.4-45.3-7.6-61.2S549.7-4.4 531.1 9.6L294.4 187.2c-24 18-38.2 46.1-38.4 76.1L371.3 367.1zm-19.6 25.4l-116-104.4C175.9 290.3 128 339.6 128 400c0 3.9 .2 7.8 .6 11.6c1.8 17.5-10.2 36.4-27.8 36.4H96c-17.7 0-32 14.3-32 32s14.3 32 32 32H240c61.9 0 112-50.1 112-112c0-2.5-.1-5-.2-7.5z"/></svg></span>
                        </button>
                        <ul id="mdbook-theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-ayu">Ayu</button></li>
                        </ul>
                        <button id="mdbook-search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="mdbook-searchbar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M416 208c0 45.9-14.9 88.3-40 122.7L502.6 457.4c12.5 12.5 12.5 32.8 0 45.3s-32.8 12.5-45.3 0L330.7 376c-34.4 25.2-76.8 40-122.7 40C93.1 416 0 322.9 0 208S93.1 0 208 0S416 93.1 416 208zM208 352c79.5 0 144-64.5 144-144s-64.5-144-144-144S64 128.5 64 208s64.5 144 144 144z"/></svg></span>
                        </button>
                    </div>

                    <h1 class="menu-title">My Notes</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <span class=fa-svg id="print-button"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M128 0C92.7 0 64 28.7 64 64v96h64V64H354.7L384 93.3V160h64V93.3c0-17-6.7-33.3-18.7-45.3L400 18.7C388 6.7 371.7 0 354.7 0H128zM384 352v32 64H128V384 368 352H384zm64 32h32c17.7 0 32-14.3 32-32V256c0-35.3-28.7-64-64-64H64c-35.3 0-64 28.7-64 64v96c0 17.7 14.3 32 32 32H64v64c0 35.3 28.7 64 64 64H384c35.3 0 64-28.7 64-64V384zm-16-88c-13.3 0-24-10.7-24-24s10.7-24 24-24s24 10.7 24 24s-10.7 24-24 24z"/></svg></span>
                        </a>

                    </div>
                </div>

                <div id="mdbook-search-wrapper" class="hidden">
                    <form id="mdbook-searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="mdbook-searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="mdbook-searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <span class=fa-svg id="fa-spin"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M304 48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zm0 416c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM48 304c26.5 0 48-21.5 48-48s-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48zm464-48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM142.9 437c18.7-18.7 18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zm0-294.2c18.7-18.7 18.7-49.1 0-67.9S93.7 56.2 75 75s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zM369.1 437c18.7 18.7 49.1 18.7 67.9 0s18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9z"/></svg></span>
                            </div>
                        </div>
                    </form>
                    <div id="mdbook-searchresults-outer" class="searchresults-outer hidden">
                        <div id="mdbook-searchresults-header" class="searchresults-header"></div>
                        <ul id="mdbook-searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('mdbook-sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('mdbook-sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#mdbook-sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="mdbook-content" class="content">
                    <main>
                        <h1 id="convolution-operations-in-machine-learning"><a class="header" href="#convolution-operations-in-machine-learning">Convolution Operations in Machine Learning</a></h1>
<h2 id="table-of-contents"><a class="header" href="#table-of-contents">Table of Contents</a></h2>
<ol>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#mathematical-foundation">Mathematical Foundation</a></li>
<li><a href="#discrete-convolution">Discrete Convolution</a></li>
<li><a href="#convolutions-in-neural-networks">Convolutions in Neural Networks</a></li>
<li><a href="#key-parameters">Key Parameters</a></li>
<li><a href="#types-of-convolutions">Types of Convolutions</a></li>
<li><a href="#computational-considerations">Computational Considerations</a></li>
<li><a href="#applications">Applications</a></li>
<li><a href="#advanced-topics">Advanced Topics</a></li>
</ol>
<h2 id="introduction"><a class="header" href="#introduction">Introduction</a></h2>
<p>Convolution is a mathematical operation that combines two functions to produce a third function. In machine learning, particularly in Convolutional Neural Networks (CNNs), convolutions are fundamental operations that enable models to learn spatial hierarchies of features from input data.</p>
<h3 id="why-convolutions-matter"><a class="header" href="#why-convolutions-matter">Why Convolutions Matter</a></h3>
<ul>
<li><strong>Parameter Sharing</strong>: The same filter is applied across the entire input, drastically reducing parameters</li>
<li><strong>Translation Invariance</strong>: Features can be detected regardless of their position in the input</li>
<li><strong>Local Connectivity</strong>: Each output neuron connects to only a local region of the input</li>
<li><strong>Hierarchical Feature Learning</strong>: Lower layers learn simple features (edges), higher layers learn complex patterns</li>
</ul>
<h2 id="mathematical-foundation"><a class="header" href="#mathematical-foundation">Mathematical Foundation</a></h2>
<h3 id="continuous-convolution"><a class="header" href="#continuous-convolution">Continuous Convolution</a></h3>
<p>In continuous mathematics, convolution of two functions f and g is defined as:</p>
<pre><code>(f * g)(t) = ∫ f(τ)g(t - τ)dτ
</code></pre>
<p>This integral represents the amount of overlap between f and a reversed, shifted version of g.</p>
<h3 id="properties-of-convolution"><a class="header" href="#properties-of-convolution">Properties of Convolution</a></h3>
<ol>
<li><strong>Commutativity</strong>: <code>f * g = g * f</code></li>
<li><strong>Associativity</strong>: <code>f * (g * h) = (f * g) * h</code></li>
<li><strong>Distributivity</strong>: <code>f * (g + h) = f * g + f * h</code></li>
<li><strong>Identity element</strong>: <code>f * δ = f</code> (where δ is the Dirac delta)</li>
<li><strong>Derivative property</strong>: <code>d/dt(f * g) = (df/dt) * g = f * (dg/dt)</code></li>
</ol>
<h2 id="discrete-convolution"><a class="header" href="#discrete-convolution">Discrete Convolution</a></h2>
<p>In machine learning, we work with discrete signals (images, sequences), so we use discrete convolution:</p>
<h3 id="1d-discrete-convolution"><a class="header" href="#1d-discrete-convolution">1D Discrete Convolution</a></h3>
<p>For sequences/signals:</p>
<pre><code>(f * g)[n] = Σ f[m]g[n - m]
             m
</code></pre>
<p><strong>Example</strong>:</p>
<pre><code>Input:  [1, 2, 3, 4, 5]
Kernel: [1, 0, -1]

Output[0] = 1×1 + 2×0 + 3×(-1) = -2
Output[1] = 2×1 + 3×0 + 4×(-1) = -2
Output[2] = 3×1 + 4×0 + 5×(-1) = -2
</code></pre>
<h3 id="2d-discrete-convolution"><a class="header" href="#2d-discrete-convolution">2D Discrete Convolution</a></h3>
<p>For images:</p>
<pre><code>(I * K)[i,j] = Σ  Σ  I[m,n] × K[i-m, j-n]
               m  n
</code></pre>
<p>Where:</p>
<ul>
<li><code>I</code> is the input image</li>
<li><code>K</code> is the kernel (filter)</li>
<li><code>[i,j]</code> are output coordinates</li>
</ul>
<p><strong>Practical Example (3×3 kernel on 5×5 image)</strong>:</p>
<pre><code>Input Image (5×5):
[1  2  3  4  5]
[5  6  7  8  9]
[9  10 11 12 13]
[13 14 15 16 17]
[17 18 19 20 21]

Kernel (3×3 edge detector):
[-1  -1  -1]
[ 0   0   0]
[ 1   1   1]

Convolution at position (1,1):
(-1×1 + -1×2 + -1×3) + (0×5 + 0×6 + 0×7) + (1×9 + 1×10 + 1×11)
= -6 + 0 + 30 = 24
</code></pre>
<h2 id="convolutions-in-neural-networks"><a class="header" href="#convolutions-in-neural-networks">Convolutions in Neural Networks</a></h2>
<h3 id="cross-correlation-vs-convolution"><a class="header" href="#cross-correlation-vs-convolution">Cross-Correlation vs Convolution</a></h3>
<p>In deep learning, what we call “convolution” is technically <strong>cross-correlation</strong>:</p>
<pre><code># True convolution (kernel is flipped)
(f * g)[i,j] = Σ Σ f[i-m, j-n] × g[m,n]

# Cross-correlation (used in deep learning)
(f ⋆ g)[i,j] = Σ Σ f[i+m, j+n] × g[m,n]
</code></pre>
<p>The difference doesn’t matter in CNNs because kernels are learned, not predefined.</p>
<h3 id="convolutional-layer-architecture"><a class="header" href="#convolutional-layer-architecture">Convolutional Layer Architecture</a></h3>
<pre><code class="language-python"># Conceptual structure
Input: (H, W, C_in)  # Height, Width, Input Channels
Kernel: (K_h, K_w, C_in, C_out)  # Kernel size, In/Out channels
Bias: (C_out,)
Output: (H_out, W_out, C_out)
</code></pre>
<h4 id="forward-pass"><a class="header" href="#forward-pass">Forward Pass</a></h4>
<p>For each output channel c_out:</p>
<ol>
<li>Take the corresponding filter (K_h, K_w, C_in)</li>
<li>Slide it across the input volume</li>
<li>At each position, compute dot product between filter and input patch</li>
<li>Add bias term</li>
<li>Apply activation function</li>
</ol>
<p><strong>Complete Operation</strong>:</p>
<pre><code>Output[i,j,c_out] = Σ Σ Σ Input[i+m, j+n, c_in] × Kernel[m,n,c_in,c_out] + Bias[c_out]
                    m n c_in
</code></pre>
<h3 id="receptive-field"><a class="header" href="#receptive-field">Receptive Field</a></h3>
<p>The <strong>receptive field</strong> is the region in the input that affects a particular output neuron.</p>
<p><strong>Calculation</strong>:</p>
<pre><code>RF_out = RF_in + (K - 1) × Π(stride_i)
</code></pre>
<p><strong>Example</strong>:</p>
<ul>
<li>Layer 1: 3×3 kernel → RF = 3</li>
<li>Layer 2: 3×3 kernel, stride 1 → RF = 5</li>
<li>Layer 3: 3×3 kernel, stride 1 → RF = 7</li>
</ul>
<h2 id="key-parameters"><a class="header" href="#key-parameters">Key Parameters</a></h2>
<h3 id="1-kernel-size-filter-size"><a class="header" href="#1-kernel-size-filter-size">1. Kernel Size (Filter Size)</a></h3>
<p>The dimensions of the convolutional filter.</p>
<p><strong>Common sizes</strong>:</p>
<ul>
<li><strong>1×1</strong>: Channel-wise transformations, dimensionality reduction</li>
<li><strong>3×3</strong>: Most common, good trade-off between receptive field and computation</li>
<li><strong>5×5, 7×7</strong>: Larger receptive fields, used in early layers</li>
<li><strong>11×11</strong>: Less common now, used in AlexNet</li>
</ul>
<p><strong>Trade-offs</strong>:</p>
<ul>
<li>Larger kernels → Larger receptive field but more parameters</li>
<li>Smaller kernels → Fewer parameters but need more layers for same receptive field</li>
</ul>
<h3 id="2-stride"><a class="header" href="#2-stride">2. Stride</a></h3>
<p>The step size when sliding the kernel across the input.</p>
<p><strong>Output size formula</strong>:</p>
<pre><code>H_out = floor((H_in - K_h) / stride_h) + 1
W_out = floor((W_in - K_w) / stride_w) + 1
</code></pre>
<p><strong>Example</strong>:</p>
<pre><code>Input: 7×7
Kernel: 3×3
Stride: 1 → Output: 5×5
Stride: 2 → Output: 3×3
Stride: 3 → Output: 2×2
</code></pre>
<p><strong>Usage</strong>:</p>
<ul>
<li>Stride = 1: Dense processing, maintains resolution</li>
<li>Stride &gt; 1: Downsampling, reduces spatial dimensions</li>
</ul>
<h3 id="3-padding"><a class="header" href="#3-padding">3. Padding</a></h3>
<p>Adding borders to the input to control output size.</p>
<p><strong>Types</strong>:</p>
<h4 id="valid-padding-no-padding"><a class="header" href="#valid-padding-no-padding">Valid Padding (No Padding)</a></h4>
<pre><code>Output size: (H - K + 1) × (W - K + 1)
</code></pre>
<h4 id="same-padding"><a class="header" href="#same-padding">Same Padding</a></h4>
<p>Pads to keep output size equal to input size (with stride=1):</p>
<pre><code>Padding = floor(K / 2)
Output size: H × W
</code></pre>
<h4 id="full-padding"><a class="header" href="#full-padding">Full Padding</a></h4>
<p>Maximum padding where every input pixel affects output:</p>
<pre><code>Padding = K - 1
Output size: (H + K - 1) × (W + K - 1)
</code></pre>
<p><strong>General formula with padding</strong>:</p>
<pre><code>H_out = floor((H_in + 2×padding - K_h) / stride_h) + 1
W_out = floor((W_in + 2×padding - K_w) / stride_w) + 1
</code></pre>
<p><strong>Padding strategies</strong>:</p>
<ul>
<li><strong>Zero padding</strong>: Fill with zeros (most common)</li>
<li><strong>Reflection padding</strong>: Mirror edge values</li>
<li><strong>Replication padding</strong>: Repeat edge values</li>
<li><strong>Circular padding</strong>: Wrap around</li>
</ul>
<h3 id="4-dilation"><a class="header" href="#4-dilation">4. Dilation</a></h3>
<p>Spacing between kernel elements (atrous/dilated convolution).</p>
<p><strong>Effective kernel size</strong>:</p>
<pre><code>K_effective = K + (K - 1) × (dilation - 1)
</code></pre>
<p><strong>Example (3×3 kernel)</strong>:</p>
<pre><code>Dilation = 1 (standard):
[x x x]
[x x x]
[x x x]

Dilation = 2:
[x . x . x]
[. . . . .]
[x . x . x]
[. . . . .]
[x . x . x]

Dilation = 3:
[x . . x . . x]
[. . . . . . .]
[. . . . . . .]
[x . . x . . x]
[. . . . . . .]
[. . . . . . .]
[x . . x . . x]
</code></pre>
<p><strong>Benefits</strong>:</p>
<ul>
<li>Exponentially expand receptive field without increasing parameters</li>
<li>Capture multi-scale contextual information</li>
<li>Used in WaveNet, DeepLab (semantic segmentation)</li>
</ul>
<h3 id="5-groups"><a class="header" href="#5-groups">5. Groups</a></h3>
<p>Splits channels into groups and performs separate convolutions.</p>
<p><strong>Grouped Convolution</strong>:</p>
<pre><code>Input channels: C_in = 64
Output channels: C_out = 128
Groups: G = 2

Each group:
- Input: 32 channels
- Output: 64 channels
- Parameters reduced by factor of G
</code></pre>
<p><strong>Special cases</strong>:</p>
<ul>
<li><strong>Groups = 1</strong>: Standard convolution</li>
<li><strong>Groups = C_in = C_out</strong>: Depthwise convolution (used in MobileNets)</li>
</ul>
<h2 id="types-of-convolutions"><a class="header" href="#types-of-convolutions">Types of Convolutions</a></h2>
<h3 id="1-standard-convolution"><a class="header" href="#1-standard-convolution">1. Standard Convolution</a></h3>
<p>The basic operation described above.</p>
<p><strong>Parameters</strong>: <code>K_h × K_w × C_in × C_out</code></p>
<p><strong>FLOPs</strong>: <code>H_out × W_out × K_h × K_w × C_in × C_out</code></p>
<h3 id="2-depthwise-convolution"><a class="header" href="#2-depthwise-convolution">2. Depthwise Convolution</a></h3>
<p>Applies a single filter per input channel.</p>
<pre><code class="language-python"># Each channel gets its own filter
Input: (H, W, C)
Kernel: (K_h, K_w, C)  # Note: C filters, not C_in × C_out
Output: (H_out, W_out, C)
</code></pre>
<p><strong>Parameters</strong>: <code>K_h × K_w × C</code></p>
<p><strong>Use case</strong>: MobileNets, EfficientNets (reduces parameters dramatically)</p>
<h3 id="3-pointwise-convolution-11-convolution"><a class="header" href="#3-pointwise-convolution-11-convolution">3. Pointwise Convolution (1×1 Convolution)</a></h3>
<p>Convolution with 1×1 kernel.</p>
<p><strong>Purposes</strong>:</p>
<ul>
<li>Change number of channels (dimensionality reduction/expansion)</li>
<li>Add non-linearity without spatial convolution</li>
<li>Mix information across channels</li>
<li>Implement bottleneck architectures</li>
</ul>
<p><strong>Example</strong>:</p>
<pre><code>Input: (56, 56, 256)
1×1 Conv with 64 filters
Output: (56, 56, 64)
</code></pre>
<h3 id="4-depthwise-separable-convolution"><a class="header" href="#4-depthwise-separable-convolution">4. Depthwise Separable Convolution</a></h3>
<p>Combines depthwise + pointwise convolution.</p>
<p><strong>Steps</strong>:</p>
<ol>
<li>Depthwise: Apply one filter per channel</li>
<li>Pointwise: Use 1×1 conv to combine channels</li>
</ol>
<p><strong>Parameter Reduction</strong>:</p>
<pre><code>Standard: K² × C_in × C_out
Separable: K² × C_in + C_in × C_out

Example (3×3, 256→256):
Standard: 9 × 256 × 256 = 589,824
Separable: 9 × 256 + 256 × 256 = 67,840
Reduction: ~8.7× fewer parameters
</code></pre>
<h3 id="5-transposed-convolution-deconvolution"><a class="header" href="#5-transposed-convolution-deconvolution">5. Transposed Convolution (Deconvolution)</a></h3>
<p>Upsamples the input (learned upsampling).</p>
<p><strong>Purpose</strong>:</p>
<ul>
<li>Increase spatial dimensions</li>
<li>Used in generators (GANs), segmentation (U-Net), autoencoders</li>
</ul>
<p><strong>How it works</strong>:</p>
<ul>
<li>Inserts zeros between input pixels</li>
<li>Applies standard convolution</li>
<li>Effectively “reverses” the downsampling effect</li>
</ul>
<p><strong>Relation to standard convolution</strong>:</p>
<pre><code>If forward conv: (H, W) → (H', W')
Then transposed conv: (H', W') → (H, W)
</code></pre>
<p><strong>Formula</strong>:</p>
<pre><code>H_out = (H_in - 1) × stride - 2×padding + K_h + output_padding
</code></pre>
<p><strong>Checkerboard Artifacts</strong>: Can create artifacts due to uneven overlap. Solutions:</p>
<ul>
<li>Use kernel size divisible by stride</li>
<li>Use resize + convolution instead</li>
</ul>
<h3 id="6-dilatedatrous-convolution"><a class="header" href="#6-dilatedatrous-convolution">6. Dilated/Atrous Convolution</a></h3>
<p>Described in the Dilation section above. Key points:</p>
<ul>
<li>Expands receptive field without increasing parameters</li>
<li>Maintains resolution (unlike stride)</li>
<li>Used for dense prediction tasks</li>
</ul>
<h3 id="7-spatial-separable-convolution"><a class="header" href="#7-spatial-separable-convolution">7. Spatial Separable Convolution</a></h3>
<p>Factorizes 2D convolution into two 1D convolutions.</p>
<pre><code>Instead of K×K convolution:
→ K×1 convolution followed by 1×K convolution

Example (3×3):
[a b c]       [a]         [d e f]
[d e f]  ≈    [b]    ×
[g h i]       [c]

Parameters: 2×K instead of K²
</code></pre>
<p><strong>Used in</strong>: Inception networks</p>
<h3 id="8-grouped-convolution"><a class="header" href="#8-grouped-convolution">8. Grouped Convolution</a></h3>
<p>Described in Groups section. Splits channels and processes independently.</p>
<p><strong>Extreme case (Depthwise)</strong>: Groups = Channels</p>
<p><strong>Used in</strong>: ResNeXt, MobileNets</p>
<h3 id="9-shuffled-grouped-convolution"><a class="header" href="#9-shuffled-grouped-convolution">9. Shuffled Grouped Convolution</a></h3>
<p>Adds channel shuffle after grouped convolution to allow cross-group information flow.</p>
<p><strong>Steps</strong>:</p>
<ol>
<li>Grouped convolution</li>
<li>Channel shuffle (permute channels across groups)</li>
<li>Next grouped convolution can access all channels</li>
</ol>
<p><strong>Used in</strong>: ShuffleNet</p>
<h3 id="10-deformable-convolution"><a class="header" href="#10-deformable-convolution">10. Deformable Convolution</a></h3>
<p>Adds learnable offsets to the sampling grid.</p>
<p><strong>Standard convolution</strong>: Fixed rectangular grid
<strong>Deformable</strong>: Grid positions are learned and can adapt to object geometry</p>
<p><strong>Formula</strong>:</p>
<pre><code>y[p₀] = Σ w[pₙ] × x[p₀ + pₙ + Δpₙ]
</code></pre>
<p>Where Δpₙ are learned offsets.</p>
<p><strong>Use cases</strong>: Object detection, where objects have varying shapes and scales</p>
<h2 id="computational-considerations"><a class="header" href="#computational-considerations">Computational Considerations</a></h2>
<h3 id="parameter-count"><a class="header" href="#parameter-count">Parameter Count</a></h3>
<p>For a convolutional layer:</p>
<pre><code>Parameters = K_h × K_w × C_in × C_out + C_out
            |________________________|   |___|
                   Weights              Bias
</code></pre>
<p><strong>Example</strong>:</p>
<pre><code>Input: 224×224×3
Conv: 64 filters, 7×7 kernel
Parameters = 7 × 7 × 3 × 64 + 64 = 9,472
</code></pre>
<h3 id="flops-floating-point-operations"><a class="header" href="#flops-floating-point-operations">FLOPs (Floating Point Operations)</a></h3>
<pre><code>FLOPs = H_out × W_out × K_h × K_w × C_in × C_out × 2
        |_____________|   |__________________|   |_|
        Output positions    Ops per position    MAC
</code></pre>
<p>MAC = Multiply-Accumulate (counted as 2 ops)</p>
<p><strong>Example</strong>:</p>
<pre><code>Input: 224×224×3
Conv: 64 filters, 7×7, stride 2, padding 3
Output: 112×112×64
FLOPs = 112 × 112 × 7 × 7 × 3 × 64 × 2 ≈ 118M
</code></pre>
<h3 id="memory-requirements"><a class="header" href="#memory-requirements">Memory Requirements</a></h3>
<p><strong>Activation memory</strong>:</p>
<pre><code>Forward: Store all activations for backward pass
Memory = batch_size × H × W × C × sizeof(dtype)
</code></pre>
<p><strong>Gradient memory</strong>: Same as activation memory</p>
<p><strong>Techniques to reduce memory</strong>:</p>
<ul>
<li>Gradient checkpointing: Recompute activations during backward</li>
<li>Mixed precision training: Use FP16 instead of FP32</li>
<li>Activation compression</li>
</ul>
<h3 id="optimization-techniques"><a class="header" href="#optimization-techniques">Optimization Techniques</a></h3>
<h4 id="1-im2col-image-to-column"><a class="header" href="#1-im2col-image-to-column">1. im2col (Image to Column)</a></h4>
<p>Transforms convolution into matrix multiplication:</p>
<ul>
<li>Unfold input patches into columns</li>
<li>Reshape filters into rows</li>
<li>Perform GEMM (General Matrix Multiply)</li>
<li>Reshape output</li>
</ul>
<p><strong>Advantage</strong>: Leverage highly optimized BLAS libraries
<strong>Disadvantage</strong>: Increased memory usage</p>
<h4 id="2-fft-based-convolution"><a class="header" href="#2-fft-based-convolution">2. FFT-based Convolution</a></h4>
<p>Use Fast Fourier Transform to perform convolution in frequency domain.</p>
<p><strong>Complexity</strong>:</p>
<ul>
<li>Spatial domain: O(K² × H × W)</li>
<li>Frequency domain: O(H × W × log(H × W))</li>
</ul>
<p><strong>Efficient when</strong>: K is large (typically K &gt; 7)</p>
<h4 id="3-winograd-convolution"><a class="header" href="#3-winograd-convolution">3. Winograd Convolution</a></h4>
<p>Mathematical algorithm to reduce multiplications.</p>
<p><strong>For 3×3 convolution</strong>: Reduces FLOPs by ~2.25×
<strong>Trade-off</strong>: More additions, numerical stability concerns</p>
<h2 id="applications"><a class="header" href="#applications">Applications</a></h2>
<h3 id="1-image-classification"><a class="header" href="#1-image-classification">1. Image Classification</a></h3>
<p>Extract hierarchical features from images.</p>
<p><strong>Architecture pattern</strong>:</p>
<pre><code>Input (224×224×3)
    ↓
Conv Block 1 (7×7, 64 filters) → (112×112×64)
    ↓ [MaxPool]
Conv Block 2 (3×3, 128 filters) → (56×56×128)
    ↓ [MaxPool]
Conv Block 3 (3×3, 256 filters) → (28×28×256)
    ↓ [MaxPool]
Conv Block 4 (3×3, 512 filters) → (14×14×512)
    ↓ [Global Average Pool]
Fully Connected → Classes
</code></pre>
<p><strong>Key networks</strong>: AlexNet, VGG, ResNet, EfficientNet</p>
<h3 id="2-object-detection"><a class="header" href="#2-object-detection">2. Object Detection</a></h3>
<p>Detect and localize objects in images.</p>
<p><strong>Approaches</strong>:</p>
<ul>
<li><strong>Two-stage</strong>: R-CNN, Fast R-CNN, Faster R-CNN</li>
<li><strong>One-stage</strong>: YOLO, SSD, RetinaNet</li>
</ul>
<p><strong>Convolution roles</strong>:</p>
<ul>
<li>Feature extraction (backbone)</li>
<li>Region proposal (RPN)</li>
<li>Classification and bounding box regression</li>
</ul>
<h3 id="3-semantic-segmentation"><a class="header" href="#3-semantic-segmentation">3. Semantic Segmentation</a></h3>
<p>Classify each pixel in an image.</p>
<p><strong>Architectures</strong>:</p>
<ul>
<li><strong>FCN</strong> (Fully Convolutional Networks): Replace FC layers with conv</li>
<li><strong>U-Net</strong>: Encoder-decoder with skip connections</li>
<li><strong>DeepLab</strong>: Atrous convolution for dense prediction</li>
</ul>
<p><strong>Key techniques</strong>:</p>
<ul>
<li>Transposed convolutions for upsampling</li>
<li>Dilated convolutions for larger receptive fields</li>
<li>Skip connections to preserve spatial information</li>
</ul>
<h3 id="4-image-generation"><a class="header" href="#4-image-generation">4. Image Generation</a></h3>
<p>Generate realistic images.</p>
<p><strong>GANs</strong> (Generative Adversarial Networks):</p>
<ul>
<li>Generator: Transposed convolutions to upsample</li>
<li>Discriminator: Standard convolutions to downsample</li>
</ul>
<p><strong>VAEs</strong> (Variational Autoencoders):</p>
<ul>
<li>Encoder: Convolutions</li>
<li>Decoder: Transposed convolutions</li>
</ul>
<h3 id="5-video-processing"><a class="header" href="#5-video-processing">5. Video Processing</a></h3>
<p>Process temporal sequences of frames.</p>
<p><strong>Approaches</strong>:</p>
<ul>
<li><strong>2D Conv + RNN</strong>: Extract frame features, model temporal</li>
<li><strong>3D Convolution</strong>: Convolve over spatial + temporal dimensions</li>
<li><strong>(2+1)D Convolution</strong>: Separate spatial and temporal convolutions</li>
</ul>
<h3 id="6-time-series-analysis"><a class="header" href="#6-time-series-analysis">6. Time Series Analysis</a></h3>
<p>Apply convolutions to sequential data.</p>
<p><strong>1D Convolutions</strong> for:</p>
<ul>
<li>Audio processing (speech recognition)</li>
<li>Text classification (character/word CNNs)</li>
<li>Sensor data (activity recognition)</li>
<li>Financial data (forecasting)</li>
</ul>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Translation invariance</li>
<li>Efficient for long sequences</li>
<li>Can be parallelized (unlike RNNs)</li>
</ul>
<h3 id="7-graph-neural-networks"><a class="header" href="#7-graph-neural-networks">7. Graph Neural Networks</a></h3>
<p>Graph convolutions operate on graph-structured data.</p>
<p><strong>Spectral methods</strong>: Convolution in spectral domain
<strong>Spatial methods</strong>: Aggregate neighbor features</p>
<h2 id="advanced-topics"><a class="header" href="#advanced-topics">Advanced Topics</a></h2>
<h3 id="1-neural-architecture-search-nas"><a class="header" href="#1-neural-architecture-search-nas">1. Neural Architecture Search (NAS)</a></h3>
<p>Automatically search for optimal convolution configurations:</p>
<ul>
<li>Kernel sizes</li>
<li>Number of filters</li>
<li>Layer connections</li>
<li>Skip connections</li>
</ul>
<p><strong>Methods</strong>: Reinforcement learning, evolutionary algorithms, gradient-based</p>
<p><strong>Results</strong>: EfficientNet, NASNet, AmoebaNet</p>
<h3 id="2-attention-mechanisms-in-cnns"><a class="header" href="#2-attention-mechanisms-in-cnns">2. Attention Mechanisms in CNNs</a></h3>
<p>Combine convolutions with attention:</p>
<p><strong>Squeeze-and-Excitation (SE) blocks</strong>:</p>
<ol>
<li>Global average pooling (squeeze)</li>
<li>FC layers (excitation)</li>
<li>Sigmoid activation</li>
<li>Multiply with original features</li>
</ol>
<p><strong>CBAM</strong> (Convolutional Block Attention Module):</p>
<ul>
<li>Channel attention</li>
<li>Spatial attention</li>
</ul>
<p><strong>Non-local Neural Networks</strong>:</p>
<ul>
<li>Self-attention over spatial positions</li>
</ul>
<h3 id="3-multi-scale-processing"><a class="header" href="#3-multi-scale-processing">3. Multi-Scale Processing</a></h3>
<p>Process input at multiple scales:</p>
<p><strong>Inception modules</strong>:</p>
<ul>
<li>Parallel convolutions with different kernel sizes</li>
<li>Concatenate results</li>
</ul>
<p><strong>Feature Pyramid Networks (FPN)</strong>:</p>
<ul>
<li>Build pyramids of features at multiple scales</li>
<li>Top-down pathway with lateral connections</li>
</ul>
<p><strong>Atrous Spatial Pyramid Pooling (ASPP)</strong>:</p>
<ul>
<li>Parallel dilated convolutions with different rates</li>
</ul>
<h3 id="4-efficient-architectures"><a class="header" href="#4-efficient-architectures">4. Efficient Architectures</a></h3>
<p>Design CNNs for resource-constrained environments:</p>
<p><strong>MobileNets</strong>:</p>
<ul>
<li>Depthwise separable convolutions</li>
<li>Width multiplier and resolution multiplier</li>
</ul>
<p><strong>ShuffleNet</strong>:</p>
<ul>
<li>Pointwise grouped convolutions</li>
<li>Channel shuffle</li>
</ul>
<p><strong>EfficientNet</strong>:</p>
<ul>
<li>Compound scaling (depth, width, resolution)</li>
<li>Neural Architecture Search</li>
</ul>
<p><strong>SqueezeNet</strong>:</p>
<ul>
<li>Fire modules (squeeze + expand)</li>
<li>Aggressive parameter reduction</li>
</ul>
<h3 id="5-learnable-convolutions"><a class="header" href="#5-learnable-convolutions">5. Learnable Convolutions</a></h3>
<p><strong>Dynamic Convolution</strong>:</p>
<ul>
<li>Aggregate multiple kernels with attention-based weights</li>
<li>Weights depend on input</li>
</ul>
<p><strong>CondConv</strong> (Conditionally Parameterized Convolution):</p>
<ul>
<li>Multiple expert kernels</li>
<li>Router network selects/combines experts per example</li>
</ul>
<p><strong>Involution</strong>:</p>
<ul>
<li>Generate kernel conditioned on each spatial location</li>
<li>Complements convolution (location-specific vs channel-specific)</li>
</ul>
<h3 id="6-continuous-convolutions"><a class="header" href="#6-continuous-convolutions">6. Continuous Convolutions</a></h3>
<p><strong>Neural ODEs with Convolutions</strong>:</p>
<ul>
<li>Model continuous depth with ODEs</li>
<li>Convolutions in continuous time</li>
</ul>
<p><strong>Implicit Neural Representations</strong>:</p>
<ul>
<li>Coordinate-based convolutions</li>
<li>Infinite resolution</li>
</ul>
<h3 id="7-equivariant-and-invariant-cnns"><a class="header" href="#7-equivariant-and-invariant-cnns">7. Equivariant and Invariant CNNs</a></h3>
<p>Design convolutions with geometric properties:</p>
<p><strong>Rotation Equivariance</strong>:</p>
<ul>
<li>Group convolutions (G-CNNs)</li>
<li>Steerable CNNs</li>
<li>Harmonic networks</li>
</ul>
<p><strong>Scale Equivariance</strong>:</p>
<ul>
<li>Scale-space theory</li>
<li>Learnable scale parameters</li>
</ul>
<p><strong>Applications</strong>: Molecular property prediction, physical simulations</p>
<h2 id="best-practices"><a class="header" href="#best-practices">Best Practices</a></h2>
<h3 id="1-choosing-kernel-size"><a class="header" href="#1-choosing-kernel-size">1. Choosing Kernel Size</a></h3>
<ul>
<li><strong>Early layers</strong>: 7×7 or 5×5 for larger receptive fields</li>
<li><strong>Middle/Deep layers</strong>: 3×3 (most common)</li>
<li><strong>1×1</strong>: For channel transformation and bottlenecks</li>
<li><strong>Modern trend</strong>: Stack multiple 3×3 instead of one large kernel</li>
</ul>
<h3 id="2-choosing-stride-and-padding"><a class="header" href="#2-choosing-stride-and-padding">2. Choosing Stride and Padding</a></h3>
<ul>
<li><strong>Stride 1, same padding</strong>: Maintain resolution within blocks</li>
<li><strong>Stride 2</strong>: Downsample (alternative to pooling)</li>
<li><strong>Stride &gt; 2</strong>: Generally avoided (aggressive downsampling)</li>
</ul>
<h3 id="3-activation-functions"><a class="header" href="#3-activation-functions">3. Activation Functions</a></h3>
<p>Place after convolution:</p>
<pre><code>Conv → BatchNorm → ReLU
</code></pre>
<p><strong>Common choices</strong>:</p>
<ul>
<li><strong>ReLU</strong>: Most common, simple, effective</li>
<li><strong>Leaky ReLU / PReLU</strong>: Prevent dying ReLU problem</li>
<li><strong>GELU</strong>: Used in transformers, smooth approximation</li>
<li><strong>Swish / SiLU</strong>: Self-gated, used in EfficientNet</li>
</ul>
<h3 id="4-normalization"><a class="header" href="#4-normalization">4. Normalization</a></h3>
<ul>
<li><strong>Batch Normalization</strong>: After conv, before activation</li>
<li><strong>Group Normalization</strong>: Better for small batch sizes</li>
<li><strong>Layer Normalization</strong>: Used in Vision Transformers</li>
</ul>
<h3 id="5-initialization"><a class="header" href="#5-initialization">5. Initialization</a></h3>
<ul>
<li><strong>Kaiming (He) initialization</strong>: For ReLU networks
<pre><code>std = sqrt(2 / n_in)
</code></pre>
</li>
<li><strong>Xavier (Glorot) initialization</strong>: For tanh/sigmoid
<pre><code>std = sqrt(2 / (n_in + n_out))
</code></pre>
</li>
</ul>
<h3 id="6-regularization"><a class="header" href="#6-regularization">6. Regularization</a></h3>
<ul>
<li><strong>Dropout</strong>: After FC layers, sometimes after conv</li>
<li><strong>DropBlock</strong>: Structured dropout for convolutions</li>
<li><strong>Data augmentation</strong>: Random crops, flips, color jitter</li>
<li><strong>Weight decay</strong>: L2 regularization on parameters</li>
</ul>
<h2 id="common-pitfalls"><a class="header" href="#common-pitfalls">Common Pitfalls</a></h2>
<ol>
<li><strong>Forgetting to account for padding in output size calculations</strong></li>
<li><strong>Not matching dimensions in skip connections</strong></li>
<li><strong>Using too large kernels (excessive parameters)</strong></li>
<li><strong>Insufficient receptive field for task</strong></li>
<li><strong>Ignoring computational costs (FLOPs and memory)</strong></li>
<li><strong>Not using batch normalization (training instability)</strong></li>
<li><strong>Poor initialization (vanishing/exploding gradients)</strong></li>
<li><strong>Confusing convolution and cross-correlation</strong></li>
</ol>
<h2 id="conclusion"><a class="header" href="#conclusion">Conclusion</a></h2>
<p>Convolutions are the cornerstone of modern computer vision and have applications far beyond images. Understanding the mathematical foundations, parameter choices, and various types of convolutions is essential for:</p>
<ul>
<li>Designing efficient neural networks</li>
<li>Debugging model performance</li>
<li>Adapting architectures to new domains</li>
<li>Pushing the boundaries of deep learning research</li>
</ul>
<p>The field continues to evolve with new architectures, attention mechanisms, and hybrid models that combine the best of CNNs and Transformers (e.g., ConvNeXt, which shows that pure CNNs can match Vision Transformers with proper design).</p>
<h2 id="references-and-further-reading"><a class="header" href="#references-and-further-reading">References and Further Reading</a></h2>
<ul>
<li>
<p><strong>Classic Papers</strong>:</p>
<ul>
<li>LeCun et al. (1998) - “Gradient-based learning applied to document recognition”</li>
<li>Krizhevsky et al. (2012) - “ImageNet Classification with Deep CNNs” (AlexNet)</li>
<li>Simonyan &amp; Zisserman (2014) - “Very Deep CNNs for Large-Scale Image Recognition” (VGG)</li>
<li>He et al. (2016) - “Deep Residual Learning for Image Recognition” (ResNet)</li>
</ul>
</li>
<li>
<p><strong>Efficient Architectures</strong>:</p>
<ul>
<li>Howard et al. (2017) - “MobileNets: Efficient CNNs for Mobile Vision Applications”</li>
<li>Tan &amp; Le (2019) - “EfficientNet: Rethinking Model Scaling for CNNs”</li>
</ul>
</li>
<li>
<p><strong>Advanced Convolutions</strong>:</p>
<ul>
<li>Yu &amp; Koltun (2016) - “Multi-Scale Context Aggregation by Dilated Convolutions”</li>
<li>Dai et al. (2017) - “Deformable Convolutional Networks”</li>
</ul>
</li>
<li>
<p><strong>Books</strong>:</p>
<ul>
<li>Goodfellow et al. - “Deep Learning” (Chapter 9: Convolutional Networks)</li>
<li>Zhang et al. - “Dive into Deep Learning”</li>
</ul>
</li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../machine_learning/jax.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256 246.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"/></svg></span>
                            </a>

                            <a rel="next prefetch" href="../ai/index.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M278.6 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L210.7 256 73.4 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"/></svg></span>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../machine_learning/jax.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256 246.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"/></svg></span>
                    </a>

                    <a rel="next prefetch" href="../ai/index.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M278.6 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L210.7 256 73.4 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"/></svg></span>
                    </a>
            </nav>

        </div>

        <template id=fa-eye><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M288 32c-80.8 0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7 0 24.6C17.3 304 48.6 356 95.4 399.4C142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144s64.5-144 144-144s144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64c-11.5 0-22.3-3-31.6-8.4c-.2 2.8-.4 5.5-.4 8.4c0 53 43 96 96 96s96-43 96-96s-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6z"/></svg></span></template>
        <template id=fa-eye-slash><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M38.8 5.1C28.4-3.1 13.3-1.2 5.1 9.2S-1.2 34.7 9.2 42.9l592 464c10.4 8.2 25.5 6.3 33.7-4.1s6.3-25.5-4.1-33.7L525.6 386.7c39.6-40.6 66.4-86.1 79.9-118.4c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C465.5 68.8 400.8 32 320 32c-68.2 0-125 26.3-169.3 60.8L38.8 5.1zM223.1 149.5C248.6 126.2 282.7 112 320 112c79.5 0 144 64.5 144 144c0 24.9-6.3 48.3-17.4 68.7L408 294.5c5.2-11.8 8-24.8 8-38.5c0-53-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6c0 10.2-2.4 19.8-6.6 28.3l-90.3-70.8zm223.1 298L373 389.9c-16.4 6.5-34.3 10.1-53 10.1c-79.5 0-144-64.5-144-144c0-6.9 .5-13.6 1.4-20.2L83.1 161.5C60.3 191.2 44 220.8 34.5 243.7c-3.3 7.9-3.3 16.7 0 24.6c14.9 35.7 46.2 87.7 93 131.1C174.5 443.2 239.2 480 320 480c47.8 0 89.9-12.9 126.2-32.5z"/></svg></span></template>
        <template id=fa-copy><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M502.6 70.63l-61.25-61.25C435.4 3.371 427.2 0 418.7 0H255.1c-35.35 0-64 28.66-64 64l.0195 256C192 355.4 220.7 384 256 384h192c35.2 0 64-28.8 64-64V93.25C512 84.77 508.6 76.63 502.6 70.63zM464 320c0 8.836-7.164 16-16 16H255.1c-8.838 0-16-7.164-16-16L239.1 64.13c0-8.836 7.164-16 16-16h128L384 96c0 17.67 14.33 32 32 32h47.1V320zM272 448c0 8.836-7.164 16-16 16H63.1c-8.838 0-16-7.164-16-16L47.98 192.1c0-8.836 7.164-16 16-16H160V128H63.99c-35.35 0-64 28.65-64 64l.0098 256C.002 483.3 28.66 512 64 512h192c35.2 0 64-28.8 64-64v-32h-47.1L272 448z"/></svg></span></template>
        <template id=fa-play><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M73 39c-14.8-9.1-33.4-9.4-48.5-.9S0 62.6 0 80V432c0 17.4 9.4 33.4 24.5 41.9s33.7 8.1 48.5-.9L361 297c14.3-8.7 23-24.2 23-41s-8.7-32.2-23-41L73 39z"/></svg></span></template>
        <template id=fa-clock-rotate-left><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M75 75L41 41C25.9 25.9 0 36.6 0 57.9V168c0 13.3 10.7 24 24 24H134.1c21.4 0 32.1-25.9 17-41l-30.8-30.8C155 85.5 203 64 256 64c106 0 192 86 192 192s-86 192-192 192c-40.8 0-78.6-12.7-109.7-34.4c-14.5-10.1-34.4-6.6-44.6 7.9s-6.6 34.4 7.9 44.6C151.2 495 201.7 512 256 512c141.4 0 256-114.6 256-256S397.4 0 256 0C185.3 0 121.3 28.7 75 75zm181 53c-13.3 0-24 10.7-24 24V256c0 6.4 2.5 12.5 7 17l72 72c9.4 9.4 24.6 9.4 33.9 0s9.4-24.6 0-33.9l-65-65V152c0-13.3-10.7-24-24-24z"/></svg></span></template>



        <script>
            window.playground_copyable = true;
        </script>


        <script src="../elasticlunr-ef4e11c1.min.js"></script>
        <script src="../mark-09e88c2c.min.js"></script>
        <script src="../searcher-c2a407aa.js"></script>

        <script src="../clipboard-1626706a.min.js"></script>
        <script src="../highlight-abc7f01d.js"></script>
        <script src="../book-a0b12cfe.js"></script>

        <!-- Custom JS scripts -->



    </div>
    </body>
</html>
