<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Blockchain - My Notes</title>


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="../highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="../tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "../";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "../searchindex.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../toc.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="../toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">My Notes</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <i class="fa fa-spinner fa-spin"></i>
                            </div>
                        </div>
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="blockchain-website-crawling-guide"><a class="header" href="#blockchain-website-crawling-guide">Blockchain Website Crawling Guide</a></h1>
<p>This guide provides best practices and configuration examples for crawling blockchain-related websites using this crawler library.</p>
<h2 id="overview"><a class="header" href="#overview">Overview</a></h2>
<p>Blockchain websites (explorers, DeFi platforms, documentation sites) often have unique characteristics that require special consideration when crawling:</p>
<ul>
<li>Dynamic content loaded via JavaScript</li>
<li>Real-time data updates</li>
<li>Rate limiting and API quotas</li>
<li>Complex URL structures</li>
<li>Large datasets</li>
<li>WebSocket connections for live data</li>
<li>Anti-bot protections (Cloudflare, etc.)</li>
<li>High-frequency content updates</li>
<li>Decentralized architectures</li>
</ul>
<h2 id="table-of-contents"><a class="header" href="#table-of-contents">Table of Contents</a></h2>
<ol>
<li><a href="#common-blockchain-website-types">Common Blockchain Website Types</a></li>
<li><a href="#best-practices">Best Practices</a></li>
<li><a href="#advanced-crawling-techniques">Advanced Crawling Techniques</a></li>
<li><a href="#platform-specific-guides">Platform-Specific Guides</a></li>
<li><a href="#example-configurations">Example Configurations</a></li>
<li><a href="#data-extraction-patterns">Data Extraction Patterns</a></li>
<li><a href="#performance-optimization">Performance Optimization</a></li>
<li><a href="#troubleshooting">Troubleshooting</a></li>
<li><a href="#security-and-privacy">Security and Privacy</a></li>
<li><a href="#ethical-considerations">Ethical Considerations</a></li>
</ol>
<h2 id="common-blockchain-website-types"><a class="header" href="#common-blockchain-website-types">Common Blockchain Website Types</a></h2>
<h3 id="block-explorers"><a class="header" href="#block-explorers">Block Explorers</a></h3>
<p>Block explorers (Etherscan, Blockchain.com, BscScan, etc.) display blockchain data through web interfaces, providing human-readable access to transaction data, smart contract information, and network statistics.</p>
<p><strong>Challenges:</strong></p>
<ul>
<li>Heavy JavaScript rendering (React, Vue.js applications)</li>
<li>Infinite scroll patterns for transaction lists</li>
<li>Aggressive rate limiting (often 1-5 requests/second)</li>
<li>Dynamic content updates via WebSockets</li>
<li>Anti-bot protections (Cloudflare, reCAPTCHA)</li>
<li>Pagination tokens that expire quickly</li>
<li>Content loaded asynchronously after initial page load</li>
</ul>
<p><strong>Recommended Settings:</strong></p>
<pre><code class="language-toml">[fetcher]
mode = "dynamic"  # Most explorers require JavaScript
required_element = "div.transaction-list"  # Validate content loaded
wait_time = 3.0  # Wait for async content to load
user_agent = "Mozilla/5.0 (compatible; ResearchBot/1.0)"

[network]
min_delay = 2.0
max_delay = 5.0  # Respect rate limits
retry_attempts = 3
timeout = 30.0  # Some pages take time to render

[patterns]
# Avoid crawling individual transaction/address pages
exclude = [
    ".*\\/tx\\/0x[a-fA-F0-9]{64}$",
    ".*\\/address\\/0x[a-fA-F0-9]{40}$",
    ".*\\/block\\/\\d+$"
]
</code></pre>
<p><strong>Specific Explorer Notes:</strong></p>
<p><strong>Etherscan (Ethereum):</strong></p>
<ul>
<li>Heavily JavaScript-dependent</li>
<li>Requires dynamic mode for most pages</li>
<li>Consider using their API instead for bulk data</li>
<li>Free tier: 5 calls/second</li>
</ul>
<p><strong>BscScan (Binance Smart Chain):</strong></p>
<ul>
<li>Similar structure to Etherscan (same codebase)</li>
<li>Apply same patterns and delays</li>
</ul>
<p><strong>Blockchain.com:</strong></p>
<ul>
<li>Mixed static/dynamic content</li>
<li>Use auto mode for flexibility</li>
<li>Better tolerance for static fetching on older pages</li>
</ul>
<h3 id="blockchain-documentation-sites"><a class="header" href="#blockchain-documentation-sites">Blockchain Documentation Sites</a></h3>
<p>Documentation for blockchain protocols, APIs, and development tools. These sites typically use static site generators (Docusaurus, VuePress, GitBook) making them easier to crawl than dynamic explorers.</p>
<p><strong>Common Platforms:</strong></p>
<ul>
<li>Ethereum.org (Ethereum documentation)</li>
<li>Solana Docs (Solana development guides)</li>
<li>Polkadot Wiki (Substrate and Polkadot)</li>
<li>Cosmos Hub Docs (Cosmos SDK)</li>
<li>Hyperledger documentation</li>
<li>Web3.js, Ethers.js, and other library docs</li>
</ul>
<p><strong>Challenges:</strong></p>
<ul>
<li>Version-specific documentation paths</li>
<li>Multiple language variants</li>
<li>Deep navigation hierarchies</li>
<li>Search pages with minimal content value</li>
<li>API reference pages with generated content</li>
</ul>
<p><strong>Example Configuration:</strong></p>
<pre><code class="language-toml">[crawler]
name = "blockchain_docs"
start_urls = ["https://ethereum.org/en/developers/docs/"]
max_depth = 5
follow_redirects = true

[fetcher]
mode = "static"  # Docs often work with static fetching
user_agent = "DocumentationBot/1.0"

[patterns]
include = ["^https://ethereum\\.org/en/developers/.*"]
exclude = [
    ".*\\#.*",  # Exclude anchor links
    ".*\\/translations/.*",  # Exclude translation files
    ".*\\.pdf$",  # Exclude PDF downloads
    ".*\\/search\\?.*"  # Exclude search result pages
]

[network]
min_delay = 0.5
max_delay = 1.5  # Docs sites typically more tolerant
</code></pre>
<p><strong>Best Practices:</strong></p>
<ul>
<li>Check for versioned documentation (v1, v2, latest)</li>
<li>Look for sidebar navigation to identify main sections</li>
<li>Exclude community/forum sections if only docs needed</li>
<li>Save markdown sources if available</li>
</ul>
<h3 id="defi-platforms"><a class="header" href="#defi-platforms">DeFi Platforms</a></h3>
<p>Decentralized finance platforms with protocol information and analytics. These include DEXs (decentralized exchanges), lending protocols, yield aggregators, and DeFi analytics dashboards.</p>
<p><strong>Examples:</strong></p>
<ul>
<li>Uniswap (DEX)</li>
<li>Aave (Lending protocol)</li>
<li>DeFi Llama (Analytics)</li>
<li>DeFi Pulse (DeFi tracking)</li>
<li>Yearn Finance (Yield aggregator)</li>
</ul>
<p><strong>Considerations:</strong></p>
<ul>
<li>Often heavily JavaScript-dependent (React SPAs)</li>
<li>May require wallet connection (not crawlable via web scraping)</li>
<li>Focus on informational pages only (about, docs, blog)</li>
<li>Real-time price data via WebSocket (can't be scraped effectively)</li>
<li>Many features are dApp-only (skip interactive features)</li>
</ul>
<p><strong>Recommended Approach:</strong></p>
<pre><code class="language-toml">[crawler]
name = "defi_info"
start_urls = ["https://docs.uniswap.org/"]
max_depth = 3

[fetcher]
mode = "auto"
required_element = "main"

[patterns]
include = [
    ".*\\/docs/.*",
    ".*\\/blog/.*",
    ".*\\/about.*"
]
exclude = [
    ".*\\/app/.*",  # Exclude dApp pages
    ".*\\/swap.*",  # Exclude interactive features
    ".*\\/pool.*"
]

[network]
min_delay = 2.0
max_delay = 4.0
</code></pre>
<p><strong>Note:</strong> For real protocol data, use blockchain APIs or subgraphs instead of web scraping.</p>
<h3 id="nft-marketplaces"><a class="header" href="#nft-marketplaces">NFT Marketplaces</a></h3>
<p>NFT platforms like OpenSea, Rarible, and Magic Eden display digital collectibles and marketplace data.</p>
<p><strong>Challenges:</strong></p>
<ul>
<li>Heavy reliance on JavaScript and Web3</li>
<li>Wallet-gated content</li>
<li>Large image assets</li>
<li>Infinite scroll collections</li>
<li>Rate limiting</li>
</ul>
<p><strong>Configuration:</strong></p>
<pre><code class="language-toml">[crawler]
name = "nft_marketplace"
start_urls = ["https://opensea.io/learn"]
max_depth = 2

[fetcher]
mode = "dynamic"
wait_time = 4.0

[patterns]
include = [
    ".*\\/learn/.*",
    ".*\\/blog/.*",
    ".*\\/resources/.*"
]
exclude = [
    ".*\\/assets/.*",  # Skip individual NFT pages
    ".*\\/collection/.*"  # Skip collection pages
]

[network]
min_delay = 3.0
max_delay = 6.0
</code></pre>
<h3 id="layer-2-and-sidechain-sites"><a class="header" href="#layer-2-and-sidechain-sites">Layer 2 and Sidechain Sites</a></h3>
<p>Layer 2 solutions (Optimism, Arbitrum, Polygon) and their documentation.</p>
<p><strong>Characteristics:</strong></p>
<ul>
<li>Similar to L1 documentation</li>
<li>Bridge interfaces (avoid crawling interactive tools)</li>
<li>Network status dashboards</li>
</ul>
<p><strong>Example:</strong></p>
<pre><code class="language-toml">[crawler]
name = "layer2_docs"
start_urls = ["https://docs.optimism.io/"]
max_depth = 4

[fetcher]
mode = "static"

[patterns]
include = ["^https://docs\\.optimism\\.io/.*"]
exclude = [".*\\/bridge.*", ".*\\/gateway.*"]
</code></pre>
<h3 id="blockchain-news-and-media"><a class="header" href="#blockchain-news-and-media">Blockchain News and Media</a></h3>
<p>CoinDesk, CoinTelegraph, The Block, and other crypto news sites.</p>
<p><strong>Considerations:</strong></p>
<ul>
<li>Standard news site structure</li>
<li>Heavy advertising (may slow page loads)</li>
<li>Paywalls on some content</li>
<li>Newsletter signup modals</li>
</ul>
<p><strong>Configuration:</strong></p>
<pre><code class="language-toml">[crawler]
name = "crypto_news"
start_urls = ["https://www.coindesk.com/"]
max_depth = 3

[fetcher]
mode = "auto"
required_element = "article"

[patterns]
include = [".*\\/\\d{4}\\/\\d{2}\\/\\d{2}/.*"]  # Date-based URLs
exclude = [
    ".*\\/tag/.*",
    ".*\\/author/.*",
    ".*\\/newsletter.*",
    ".*\\/sponsored.*"
]

[network]
min_delay = 1.5
max_delay = 3.0
</code></pre>
<h2 id="best-practices"><a class="header" href="#best-practices">Best Practices</a></h2>
<h3 id="1-respect-rate-limits"><a class="header" href="#1-respect-rate-limits">1. Respect Rate Limits</a></h3>
<p>Blockchain services often have strict rate limiting due to infrastructure costs and abuse prevention:</p>
<pre><code class="language-toml">[network]
min_delay = 3.0
max_delay = 10.0
concurrent_requests = 1  # Avoid parallel requests to same domain
</code></pre>
<p><strong>Rate Limit Guidelines by Platform:</strong></p>
<ul>
<li><strong>Etherscan/BscScan</strong>: 5 req/sec (free), 15 req/sec (premium)</li>
<li><strong>Blockchain.com</strong>: ~10 req/min recommended</li>
<li><strong>Documentation sites</strong>: 1-2 req/sec typically safe</li>
<li><strong>News sites</strong>: 2-5 req/sec usually acceptable</li>
</ul>
<p><strong>Detecting Rate Limits:</strong></p>
<ul>
<li>HTTP 429 (Too Many Requests)</li>
<li>HTTP 403 (Forbidden) - may indicate blocking</li>
<li>Cloudflare challenge pages</li>
<li>Empty/error responses</li>
</ul>
<h3 id="2-use-appropriate-fetching-mode"><a class="header" href="#2-use-appropriate-fetching-mode">2. Use Appropriate Fetching Mode</a></h3>
<ul>
<li>
<p><strong>Static mode</strong>: Documentation, blogs, static content</p>
<ul>
<li>Faster and more efficient</li>
<li>Lower resource usage</li>
<li>Works for server-rendered pages</li>
</ul>
</li>
<li>
<p><strong>Dynamic mode</strong>: Explorers, dashboards, real-time data</p>
<ul>
<li>Required for JavaScript-heavy SPAs</li>
<li>Waits for content to render</li>
<li>Higher resource usage (headless browser)</li>
</ul>
</li>
<li>
<p><strong>Auto mode</strong>: Mixed content types</p>
<ul>
<li>Automatically detects need for JavaScript</li>
<li>Good for diverse content</li>
<li>Balances speed and compatibility</li>
</ul>
</li>
</ul>
<p><strong>Decision Matrix:</strong></p>
<pre><code>Static:  Docs sites, blogs, simple news sites
Dynamic: Block explorers, DeFi dashboards, NFT marketplaces
Auto:    Mixed content, unknown site structure
</code></pre>
<h3 id="3-pattern-matching"><a class="header" href="#3-pattern-matching">3. Pattern Matching</a></h3>
<p>Blockchain URLs often contain hashes and addresses. Use careful pattern matching to avoid crawling millions of transaction/address pages:</p>
<pre><code class="language-toml">[patterns]
# Include specific sections
include = [
    "^https://etherscan\\.io/blocks.*",
    "^https://etherscan\\.io/txs.*",
    "^https://etherscan\\.io/charts.*"
]

# Exclude specific transaction/address pages to avoid infinite crawling
exclude = [
    ".*\\/tx\\/0x[a-fA-F0-9]{64}$",  # Individual transactions
    ".*\\/address\\/0x[a-fA-F0-9]{40}$",  # Ethereum addresses
    ".*\\/block\\/\\d+$",  # Individual blocks
    ".*\\/token/0x[a-fA-F0-9]{40}$",  # Token contract pages
    ".*\\/nft/.*",  # Individual NFT pages
    ".*\\?.*page=\\d+$"  # Pagination (if you don't want all pages)
]
</code></pre>
<p><strong>Common Blockchain Address Formats:</strong></p>
<pre><code class="language-toml"># Ethereum/EVM: 0x + 40 hex chars
".*\\/0x[a-fA-F0-9]{40}.*"

# Bitcoin: varies, often starts with 1, 3, or bc1
".*\\/[13][a-km-zA-HJ-NP-Z1-9]{25,34}.*"
".*\\/bc1[a-z0-9]{39,87}.*"

# Solana: base58, typically 32-44 chars
".*\\/[1-9A-HJ-NP-Za-km-z]{32,44}.*"

# Transaction hashes (64 hex chars)
".*\\/tx\\/0x[a-fA-F0-9]{64}.*"
</code></pre>
<h3 id="4-content-validation"><a class="header" href="#4-content-validation">4. Content Validation</a></h3>
<p>Ensure critical elements are loaded before saving:</p>
<pre><code class="language-toml">[fetcher]
required_element = "div.container"  # Adjust based on target site
wait_time = 3.0  # Wait for async content
</code></pre>
<p><strong>Common validation elements:</strong></p>
<ul>
<li>Block explorers: <code>"div.transaction-list"</code>, <code>"table.transactions"</code></li>
<li>Documentation: <code>"article"</code>, <code>"main.content"</code></li>
<li>News sites: <code>"article.post"</code>, <code>"div.article-body"</code></li>
</ul>
<h3 id="5-storage-considerations"><a class="header" href="#5-storage-considerations">5. Storage Considerations</a></h3>
<p>Blockchain data can be large. Monitor disk space and organize output:</p>
<pre><code class="language-toml">[storage]
output_dir = "/path/to/large/storage/blockchain_data"
compress = true  # Enable compression if supported
max_file_size = 10485760  # 10MB limit per file
</code></pre>
<p><strong>Storage Tips:</strong></p>
<ul>
<li>Separate crawls by blockchain (ethereum/, solana/, etc.)</li>
<li>Use date-based directories for news content</li>
<li>Consider database storage for structured data</li>
<li>Regular cleanup of old/duplicate content</li>
</ul>
<h3 id="6-user-agent-identification"><a class="header" href="#6-user-agent-identification">6. User Agent Identification</a></h3>
<p>Use descriptive user agents to help site operators understand your bot:</p>
<pre><code class="language-toml">[fetcher]
user_agent = "ResearchBot/1.0 (+https://yoursite.com/bot-info)"
</code></pre>
<p><strong>Best practices:</strong></p>
<ul>
<li>Include bot name and version</li>
<li>Provide contact URL or email</li>
<li>Be honest about your purpose</li>
<li>Don't impersonate regular browsers for evasion</li>
</ul>
<h3 id="7-handle-javascript-heavy-sites"><a class="header" href="#7-handle-javascript-heavy-sites">7. Handle JavaScript-Heavy Sites</a></h3>
<p>Many blockchain sites are SPAs (Single Page Applications):</p>
<pre><code class="language-toml">[fetcher]
mode = "dynamic"
wait_time = 5.0
wait_for_network_idle = true  # Wait for API calls to complete
</code></pre>
<p><strong>Additional considerations:</strong></p>
<ul>
<li>Increase timeouts for slow-loading content</li>
<li>Watch for infinite scroll (may never complete)</li>
<li>Some content may require user interaction (skip it)</li>
</ul>
<h3 id="8-monitor-and-log"><a class="header" href="#8-monitor-and-log">8. Monitor and Log</a></h3>
<p>Enable comprehensive logging to track issues:</p>
<pre><code class="language-toml">[logging]
level = "INFO"
log_file = "blockchain_crawler.log"
log_errors = true
</code></pre>
<p><strong>What to monitor:</strong></p>
<ul>
<li>Rate limit errors (HTTP 429)</li>
<li>Timeout errors</li>
<li>Content validation failures</li>
<li>Redirect chains</li>
<li>Storage usage</li>
</ul>
<h2 id="advanced-crawling-techniques"><a class="header" href="#advanced-crawling-techniques">Advanced Crawling Techniques</a></h2>
<h3 id="handling-infinite-scroll"><a class="header" href="#handling-infinite-scroll">Handling Infinite Scroll</a></h3>
<p>Many blockchain sites use infinite scroll for transaction lists and other data:</p>
<p><strong>Problem:</strong> Page never "completes" loading as new content continuously loads.</p>
<p><strong>Solutions:</strong></p>
<ol>
<li><strong>Set maximum scroll depth:</strong></li>
</ol>
<pre><code class="language-toml">[fetcher]
mode = "dynamic"
max_scroll_depth = 5  # Scroll 5 times maximum
scroll_delay = 2.0  # Wait 2s between scrolls
</code></pre>
<ol start="2">
<li><strong>Use pagination URLs instead:</strong></li>
</ol>
<pre><code class="language-toml">[patterns]
# Target paginated URLs instead of infinite scroll
include = [".*\\?page=\\d+$"]
</code></pre>
<ol start="3">
<li><strong>Extract API endpoints:</strong></li>
</ol>
<ul>
<li>Inspect network traffic to find API endpoints</li>
<li>Use API directly instead of web scraping</li>
</ul>
<h3 id="bypassing-anti-bot-protections"><a class="header" href="#bypassing-anti-bot-protections">Bypassing Anti-Bot Protections</a></h3>
<p><strong>Cloudflare and Similar Services:</strong></p>
<p>Some blockchain sites use Cloudflare or other anti-bot services.</p>
<p><strong>Approaches:</strong></p>
<pre><code class="language-toml">[fetcher]
mode = "dynamic"
user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
wait_time = 5.0  # Wait for Cloudflare check
</code></pre>
<p><strong>Important:</strong></p>
<ul>
<li>Respect site terms of service</li>
<li>Don't attempt to evade protections for malicious purposes</li>
<li>Consider using official APIs instead</li>
<li>Some protections are legitimate abuse prevention</li>
</ul>
<h3 id="dealing-with-websocket-data"><a class="header" href="#dealing-with-websocket-data">Dealing with WebSocket Data</a></h3>
<p>Real-time blockchain data often uses WebSockets for live updates.</p>
<p><strong>Reality:</strong> Web crawlers can't effectively capture WebSocket streams.</p>
<p><strong>Alternatives:</strong></p>
<ul>
<li>Focus on historical/static data</li>
<li>Use blockchain node APIs (web3, ethers.js)</li>
<li>Query indexing services (The Graph, Alchemy)</li>
<li>Use exchange APIs for price data</li>
</ul>
<h3 id="multi-chain-crawling"><a class="header" href="#multi-chain-crawling">Multi-Chain Crawling</a></h3>
<p>Crawling multiple blockchain ecosystems simultaneously:</p>
<pre><code class="language-toml"># ethereum_config.toml
[crawler]
name = "ethereum_crawler"
start_urls = ["https://ethereum.org/en/"]

# solana_config.toml
[crawler]
name = "solana_crawler"
start_urls = ["https://solana.com/docs"]

# Run multiple crawlers:
# ./crawler --config ethereum_config.toml &amp;
# ./crawler --config solana_config.toml &amp;
</code></pre>
<p><strong>Coordination strategies:</strong></p>
<ul>
<li>Separate configuration files per chain</li>
<li>Separate output directories</li>
<li>Shared rate limit pool if crawling same domain</li>
<li>Centralized logging for monitoring</li>
</ul>
<h3 id="api-integration"><a class="header" href="#api-integration">API Integration</a></h3>
<p>Many blockchain sites have APIs that are more reliable than web scraping:</p>
<p><strong>When to use APIs instead:</strong></p>
<ul>
<li>Transaction data (use block explorer APIs)</li>
<li>Token prices (use CoinGecko, CoinMarketCap APIs)</li>
<li>Protocol metrics (use protocol-specific APIs)</li>
<li>DeFi data (use DeFi Llama, The Graph)</li>
</ul>
<p><strong>Hybrid approach:</strong></p>
<pre><code>Documentation → Web crawling
Real-time data → APIs
Historical data → APIs or web scraping
Static content → Web crawling
</code></pre>
<h2 id="platform-specific-guides"><a class="header" href="#platform-specific-guides">Platform-Specific Guides</a></h2>
<h3 id="ethereum-ecosystem"><a class="header" href="#ethereum-ecosystem">Ethereum Ecosystem</a></h3>
<p><strong>Key Sites:</strong></p>
<ul>
<li>ethereum.org (documentation)</li>
<li>etherscan.io (block explorer)</li>
<li>docs.soliditylang.org (Solidity docs)</li>
</ul>
<p><strong>Configuration:</strong></p>
<pre><code class="language-toml">[crawler]
name = "ethereum_ecosystem"
start_urls = [
    "https://ethereum.org/en/developers/",
    "https://docs.soliditylang.org/en/latest/"
]
max_depth = 4

[fetcher]
mode = "static"

[patterns]
include = [
    "^https://ethereum\\.org/en/developers/.*",
    "^https://docs\\.soliditylang\\.org/.*"
]
exclude = [".*\\/translations/.*"]

[network]
min_delay = 1.0
max_delay = 2.0
</code></pre>
<h3 id="solana-ecosystem"><a class="header" href="#solana-ecosystem">Solana Ecosystem</a></h3>
<p><strong>Key Sites:</strong></p>
<ul>
<li>solana.com/docs</li>
<li>solscan.io (block explorer)</li>
<li>docs.metaplex.com (NFT standard)</li>
</ul>
<p><strong>Considerations:</strong></p>
<ul>
<li>Heavily JavaScript-dependent sites</li>
<li>Use dynamic mode for explorers</li>
<li>Documentation uses modern static generators</li>
</ul>
<pre><code class="language-toml">[crawler]
name = "solana_docs"
start_urls = ["https://solana.com/docs"]
max_depth = 4

[fetcher]
mode = "static"

[patterns]
include = ["^https://solana\\.com/docs/.*"]

[network]
min_delay = 1.0
max_delay = 2.0
</code></pre>
<h3 id="bitcoin-ecosystem"><a class="header" href="#bitcoin-ecosystem">Bitcoin Ecosystem</a></h3>
<p><strong>Key Sites:</strong></p>
<ul>
<li>bitcoin.org/en/developer-documentation</li>
<li>blockchain.com</li>
<li>developer.bitcoin.org</li>
</ul>
<p><strong>Characteristics:</strong></p>
<ul>
<li>More static content than newer blockchains</li>
<li>Well-established documentation</li>
<li>Block explorers with traditional pagination</li>
</ul>
<pre><code class="language-toml">[crawler]
name = "bitcoin_docs"
start_urls = ["https://developer.bitcoin.org/"]
max_depth = 3

[fetcher]
mode = "static"

[patterns]
include = ["^https://developer\\.bitcoin\\.org/.*"]
</code></pre>
<h3 id="polkadotsubstrate-ecosystem"><a class="header" href="#polkadotsubstrate-ecosystem">Polkadot/Substrate Ecosystem</a></h3>
<p><strong>Key Sites:</strong></p>
<ul>
<li>wiki.polkadot.network</li>
<li>docs.substrate.io</li>
<li>polkadot.js.org</li>
</ul>
<p><strong>Configuration:</strong></p>
<pre><code class="language-toml">[crawler]
name = "polkadot_ecosystem"
start_urls = [
    "https://wiki.polkadot.network/",
    "https://docs.substrate.io/"
]
max_depth = 5

[fetcher]
mode = "static"

[patterns]
include = [
    "^https://wiki\\.polkadot\\.network/.*",
    "^https://docs\\.substrate\\.io/.*"
]
exclude = [".*\\/en/.*"]  # If limiting to English only
</code></pre>
<h2 id="data-extraction-patterns"><a class="header" href="#data-extraction-patterns">Data Extraction Patterns</a></h2>
<h3 id="extracting-structured-data"><a class="header" href="#extracting-structured-data">Extracting Structured Data</a></h3>
<p>Beyond just saving HTML, you may want to extract specific data.</p>
<p><strong>Common extractions:</strong></p>
<ol>
<li><strong>Documentation code examples:</strong></li>
</ol>
<pre><code class="language-javascript">// Look for code blocks
document.querySelectorAll('pre code')
</code></pre>
<ol start="2">
<li><strong>Transaction lists (if crawling lists):</strong></li>
</ol>
<pre><code class="language-javascript">// Extract transaction data from tables
document.querySelectorAll('table.transactions tr')
</code></pre>
<ol start="3">
<li><strong>Token/Protocol metadata:</strong></li>
</ol>
<pre><code class="language-javascript">// Extract protocol stats from pages
document.querySelector('.protocol-stats')
</code></pre>
<p><strong>Post-processing:</strong></p>
<ul>
<li>Parse HTML with BeautifulSoup, lxml, or similar</li>
<li>Extract text, links, code blocks</li>
<li>Store in structured format (JSON, CSV, database)</li>
</ul>
<h3 id="url-pattern-analysis"><a class="header" href="#url-pattern-analysis">URL Pattern Analysis</a></h3>
<p>Understanding blockchain URL structures:</p>
<p><strong>Block Explorer Patterns:</strong></p>
<pre><code>Transactions: /tx/[hash]
Addresses:    /address/[address]
Blocks:       /block/[number]
Tokens:       /token/[address]
Charts:       /chart/[metric]
</code></pre>
<p><strong>Documentation Patterns:</strong></p>
<pre><code>Guides:       /docs/guides/[topic]
API Ref:      /docs/api/[endpoint]
Tutorials:    /docs/tutorials/[tutorial]
</code></pre>
<p><strong>Strategy:</strong> Focus on list/index pages, avoid individual item pages.</p>
<h3 id="content-deduplication"><a class="header" href="#content-deduplication">Content Deduplication</a></h3>
<p>Blockchain sites often have duplicate content:</p>
<p><strong>Common duplicates:</strong></p>
<ul>
<li>Multiple language versions</li>
<li>Versioned documentation (v1, v2, latest)</li>
<li>Mirror sites</li>
<li>Archived content</li>
</ul>
<p><strong>Solutions:</strong></p>
<pre><code class="language-toml">[patterns]
exclude = [
    ".*\\/v[0-9]+/.*",  # Exclude old versions
    ".*\\/[a-z]{2}/.*",  # Exclude non-English (adjust as needed)
    ".*\\/archive/.*"
]
</code></pre>
<h2 id="performance-optimization"><a class="header" href="#performance-optimization">Performance Optimization</a></h2>
<h3 id="crawl-speed-vs-politeness"><a class="header" href="#crawl-speed-vs-politeness">Crawl Speed vs. Politeness</a></h3>
<p>Balance between speed and being a good web citizen:</p>
<p><strong>Aggressive (use cautiously):</strong></p>
<pre><code class="language-toml">[network]
min_delay = 0.5
max_delay = 1.0
concurrent_requests = 3
</code></pre>
<p><strong>Polite (recommended):</strong></p>
<pre><code class="language-toml">[network]
min_delay = 2.0
max_delay = 5.0
concurrent_requests = 1
</code></pre>
<p><strong>Very polite (for sensitive sites):</strong></p>
<pre><code class="language-toml">[network]
min_delay = 5.0
max_delay = 10.0
concurrent_requests = 1
</code></pre>
<h3 id="resource-management"><a class="header" href="#resource-management">Resource Management</a></h3>
<p><strong>Memory optimization:</strong></p>
<ul>
<li>Limit maximum depth to prevent explosion</li>
<li>Use pagination limits</li>
<li>Clear browser cache periodically (dynamic mode)</li>
</ul>
<p><strong>Disk optimization:</strong></p>
<ul>
<li>Enable compression</li>
<li>Limit file sizes</li>
<li>Periodic cleanup of old data</li>
</ul>
<p><strong>Network optimization:</strong></p>
<ul>
<li>Reuse connections where possible</li>
<li>Enable HTTP/2 if supported</li>
<li>Use conditional requests (If-Modified-Since)</li>
</ul>
<h3 id="parallel-crawling"><a class="header" href="#parallel-crawling">Parallel Crawling</a></h3>
<p>Crawl multiple domains in parallel:</p>
<pre><code class="language-bash"># Terminal 1
./crawler --config ethereum.toml

# Terminal 2
./crawler --config solana.toml

# Terminal 3
./crawler --config bitcoin.toml
</code></pre>
<p><strong>Important:</strong> Don't parallelize requests to the same domain (respect rate limits).</p>
<h2 id="troubleshooting"><a class="header" href="#troubleshooting">Troubleshooting</a></h2>
<h3 id="common-issues"><a class="header" href="#common-issues">Common Issues</a></h3>
<h4 id="1-empty-pages-saved"><a class="header" href="#1-empty-pages-saved">1. Empty Pages Saved</a></h4>
<p><strong>Symptom:</strong> HTML files saved but content missing.</p>
<p><strong>Causes:</strong></p>
<ul>
<li>JavaScript not executed (using static mode on dynamic site)</li>
<li>Content not loaded before save</li>
<li>Required element not found</li>
</ul>
<p><strong>Solutions:</strong></p>
<pre><code class="language-toml">[fetcher]
mode = "dynamic"  # Switch to dynamic
wait_time = 5.0  # Increase wait time
required_element = "main"  # Adjust validation element
</code></pre>
<h4 id="2-rate-limited--blocked"><a class="header" href="#2-rate-limited--blocked">2. Rate Limited / Blocked</a></h4>
<p><strong>Symptom:</strong> HTTP 429, 403, or Cloudflare challenges.</p>
<p><strong>Solutions:</strong></p>
<pre><code class="language-toml">[network]
min_delay = 5.0  # Increase delays
max_delay = 10.0

[fetcher]
user_agent = "YourBot/1.0 (+contact@email.com)"  # Identify yourself
</code></pre>
<ul>
<li>Check robots.txt</li>
<li>Review site terms of service</li>
<li>Consider using official API</li>
</ul>
<h4 id="3-timeouts"><a class="header" href="#3-timeouts">3. Timeouts</a></h4>
<p><strong>Symptom:</strong> Requests timing out, incomplete pages.</p>
<p><strong>Causes:</strong></p>
<ul>
<li>Slow site or network</li>
<li>Site is overloaded</li>
<li>Infinite loading content</li>
</ul>
<p><strong>Solutions:</strong></p>
<pre><code class="language-toml">[network]
timeout = 60.0  # Increase timeout

[fetcher]
wait_time = 10.0  # Wait longer for content
</code></pre>
<h4 id="4-too-many-urls"><a class="header" href="#4-too-many-urls">4. Too Many URLs</a></h4>
<p><strong>Symptom:</strong> Crawler finds millions of URLs.</p>
<p><strong>Cause:</strong> Not excluding transaction/address pages.</p>
<p><strong>Solution:</strong></p>
<pre><code class="language-toml">[patterns]
exclude = [
    ".*\\/tx\\/.*",
    ".*\\/address\\/.*",
    ".*\\/block\\/\\d+$"
]

[crawler]
max_depth = 3  # Limit depth
max_pages = 10000  # Set maximum pages
</code></pre>
<h4 id="5-duplicate-content"><a class="header" href="#5-duplicate-content">5. Duplicate Content</a></h4>
<p><strong>Symptom:</strong> Same content saved multiple times.</p>
<p><strong>Causes:</strong></p>
<ul>
<li>Multiple URLs for same content</li>
<li>Query parameters</li>
<li>Trailing slashes</li>
</ul>
<p><strong>Solutions:</strong></p>
<pre><code class="language-toml">[crawler]
normalize_urls = true  # Remove trailing slashes, etc.

[patterns]
exclude = [".*\\?.*"]  # Exclude query strings if not needed
</code></pre>
<h3 id="debugging-tips"><a class="header" href="#debugging-tips">Debugging Tips</a></h3>
<ol>
<li><strong>Start small:</strong></li>
</ol>
<pre><code class="language-toml">[crawler]
max_depth = 1  # Test with limited depth first
max_pages = 10
</code></pre>
<ol start="2">
<li><strong>Enable verbose logging:</strong></li>
</ol>
<pre><code class="language-toml">[logging]
level = "DEBUG"
log_file = "debug.log"
</code></pre>
<ol start="3">
<li><strong>Test pattern matching:</strong></li>
</ol>
<pre><code class="language-bash"># Check what URLs match your patterns
./crawler --test-patterns --config yourconfig.toml
</code></pre>
<ol start="4">
<li><strong>Inspect saved content:</strong></li>
</ol>
<pre><code class="language-bash"># Verify pages saved correctly
ls -lh output/
head -n 50 output/page1.html
</code></pre>
<ol start="5">
<li><strong>Monitor in real-time:</strong></li>
</ol>
<pre><code class="language-bash"># Watch log file
tail -f blockchain_crawler.log
</code></pre>
<h2 id="security-and-privacy"><a class="header" href="#security-and-privacy">Security and Privacy</a></h2>
<h3 id="data-handling"><a class="header" href="#data-handling">Data Handling</a></h3>
<p><strong>Sensitive information in blockchain data:</strong></p>
<ul>
<li>Wallet addresses (public but can be tracked)</li>
<li>Transaction amounts and patterns</li>
<li>User behavior on platforms</li>
<li>IP addresses (in logs)</li>
</ul>
<p><strong>Best practices:</strong></p>
<ul>
<li>Don't republish scraped wallet addresses without context</li>
<li>Respect user privacy even for public blockchain data</li>
<li>Secure your scraped data storage</li>
<li>Follow GDPR and data protection regulations</li>
</ul>
<h3 id="scraping-ethics"><a class="header" href="#scraping-ethics">Scraping Ethics</a></h3>
<p><strong>Do:</strong></p>
<ul>
<li>Respect robots.txt</li>
<li>Use reasonable rate limits</li>
<li>Identify your bot with user agent</li>
<li>Provide contact information</li>
<li>Honor site terms of service</li>
<li>Use official APIs when available</li>
</ul>
<p><strong>Don't:</strong></p>
<ul>
<li>Overwhelm sites with requests</li>
<li>Evade anti-bot protections maliciously</li>
<li>Scrape wallet-gated or authenticated content</li>
<li>Republish copyrighted content without permission</li>
<li>Use scraped data for harassment or tracking</li>
</ul>
<h3 id="legal-considerations"><a class="header" href="#legal-considerations">Legal Considerations</a></h3>
<p><strong>Important notes:</strong></p>
<ul>
<li>Web scraping legality varies by jurisdiction</li>
<li>Review site terms of service</li>
<li>Public data != license to republish</li>
<li>Consider copyright on content</li>
<li>Some sites explicitly forbid scraping</li>
</ul>
<p><strong>Safer alternatives:</strong></p>
<ul>
<li>Use official APIs</li>
<li>Request data access from site owners</li>
<li>Use publicly available datasets</li>
<li>Access blockchain data directly via nodes</li>
</ul>
<h3 id="secure-configuration"><a class="header" href="#secure-configuration">Secure Configuration</a></h3>
<p><strong>Protect your crawler configuration:</strong></p>
<pre><code class="language-toml">[auth]
# Never commit API keys to version control
api_key = "${ENV_API_KEY}"  # Use environment variables

[network]
# Use secure connections
https_only = true
verify_ssl = true
</code></pre>
<p><strong>Secure data storage:</strong></p>
<ul>
<li>Encrypt sensitive scraped data</li>
<li>Limit file permissions (chmod 600)</li>
<li>Use secure storage locations</li>
<li>Regular backups with encryption</li>
</ul>
<h2 id="example-configurations"><a class="header" href="#example-configurations">Example Configurations</a></h2>
<h3 id="ethereum-documentation-crawler"><a class="header" href="#ethereum-documentation-crawler">Ethereum Documentation Crawler</a></h3>
<p>Comprehensive configuration for crawling Ethereum developer documentation:</p>
<pre><code class="language-toml">[crawler]
name = "ethereum_docs"
start_urls = ["https://ethereum.org/en/"]
max_depth = 4
max_pages = 5000

[fetcher]
mode = "static"
user_agent = "EthereumDocsCrawler/1.0 (+https://yoursite.com)"

[patterns]
include = [
    "^https://ethereum\\.org/en/developers/.*",
    "^https://ethereum\\.org/en/whitepaper.*"
]
exclude = [
    ".*\\/translations/.*",
    ".*\\.pdf$",
    ".*\\/contributing/.*"
]

[network]
min_delay = 1.0
max_delay = 3.0
timeout = 30.0
retry_attempts = 3

[storage]
output_dir = "./ethereum_docs"
db_path = "./ethereum_docs.db"

[logging]
level = "INFO"
log_file = "ethereum_crawler.log"
</code></pre>
<h3 id="multi-chain-block-explorer-read-only-pages"><a class="header" href="#multi-chain-block-explorer-read-only-pages">Multi-Chain Block Explorer (Read-Only Pages)</a></h3>
<p>For crawling informational pages on block explorers (not transaction/address pages):</p>
<pre><code class="language-toml">[crawler]
name = "explorer_info"
start_urls = [
    "https://etherscan.io/charts",
    "https://etherscan.io/apis"
]
max_depth = 2
max_pages = 500

[fetcher]
mode = "dynamic"
required_element = "main"
wait_time = 4.0

[patterns]
include = [
    "^https://etherscan\\.io/charts/.*",
    "^https://etherscan\\.io/apis.*",
    "^https://etherscan\\.io/tokencheck.*"
]
exclude = [
    ".*\\/tx\\/0x[a-fA-F0-9]{64}$",
    ".*\\/address\\/0x[a-fA-F0-9]{40}$",
    ".*\\/block\\/\\d+$",
    ".*\\/txs\\?.*"  # Exclude transaction list pages
]

[network]
min_delay = 3.0
max_delay = 6.0
timeout = 45.0

[storage]
output_dir = "./explorer_data"
</code></pre>
<h3 id="defi-documentation-aggregator"><a class="header" href="#defi-documentation-aggregator">DeFi Documentation Aggregator</a></h3>
<p>Crawling documentation from multiple DeFi protocols:</p>
<pre><code class="language-toml">[crawler]
name = "defi_docs"
start_urls = [
    "https://docs.uniswap.org/",
    "https://docs.aave.com/",
    "https://docs.compound.finance/"
]
max_depth = 3

[fetcher]
mode = "static"

[patterns]
include = [
    "^https://docs\\.uniswap\\.org/.*",
    "^https://docs\\.aave\\.com/.*",
    "^https://docs\\.compound\\.finance/.*"
]
exclude = [
    ".*\\/api/.*",  # Exclude API reference if too large
    ".*\\/v1/.*"    # Exclude old versions
]

[network]
min_delay = 1.5
max_delay = 3.0

[storage]
output_dir = "./defi_docs"
</code></pre>
<h3 id="blockchain-news-archive"><a class="header" href="#blockchain-news-archive">Blockchain News Archive</a></h3>
<p>For archiving blockchain news articles:</p>
<pre><code class="language-toml">[crawler]
name = "crypto_news"
start_urls = ["https://www.coindesk.com/"]
max_depth = 3
max_pages = 2000

[fetcher]
mode = "auto"
required_element = "article"

[patterns]
include = [
    ".*\\/\\d{4}\\/\\d{2}\\/\\d{2}/.*"  # Date-based article URLs
]
exclude = [
    ".*\\/tag/.*",
    ".*\\/author/.*",
    ".*\\/newsletter.*",
    ".*\\/sponsored.*",
    ".*\\/press-releases/.*"
]

[network]
min_delay = 2.0
max_delay = 5.0
timeout = 30.0

[storage]
output_dir = "./crypto_news"
db_path = "./crypto_news.db"

[logging]
level = "INFO"
log_file = "news_crawler.log"
</code></pre>
<h3 id="nft-marketplace-educational-content"><a class="header" href="#nft-marketplace-educational-content">NFT Marketplace Educational Content</a></h3>
<p>Crawling learning resources from NFT marketplaces (not marketplace listings):</p>
<pre><code class="language-toml">[crawler]
name = "nft_education"
start_urls = [
    "https://opensea.io/learn",
    "https://support.opensea.io/"
]
max_depth = 2

[fetcher]
mode = "dynamic"
wait_time = 3.0

[patterns]
include = [
    "^https://opensea\\.io/learn/.*",
    "^https://support\\.opensea\\.io/.*"
]
exclude = [
    ".*\\/collection/.*",
    ".*\\/assets/.*",
    ".*\\/account/.*"
]

[network]
min_delay = 2.5
max_delay = 5.0

[storage]
output_dir = "./nft_education"
</code></pre>
<h3 id="layer-2-documentation-complete"><a class="header" href="#layer-2-documentation-complete">Layer 2 Documentation Complete</a></h3>
<p>Comprehensive Layer 2 documentation crawl:</p>
<pre><code class="language-toml">[crawler]
name = "layer2_comprehensive"
start_urls = [
    "https://docs.optimism.io/",
    "https://docs.arbitrum.io/",
    "https://wiki.polygon.technology/"
]
max_depth = 5

[fetcher]
mode = "static"

[patterns]
include = [
    "^https://docs\\.optimism\\.io/.*",
    "^https://docs\\.arbitrum\\.io/.*",
    "^https://wiki\\.polygon\\.technology/.*"
]
exclude = [
    ".*\\/bridge/.*",  # Exclude bridge interfaces
    ".*\\/translations/.*"
]

[network]
min_delay = 1.0
max_delay = 2.0

[storage]
output_dir = "./layer2_docs"
db_path = "./layer2_docs.db"
</code></pre>
<h3 id="research-focused-configuration"><a class="header" href="#research-focused-configuration">Research-Focused Configuration</a></h3>
<p>For academic/research purposes with maximum detail:</p>
<pre><code class="language-toml">[crawler]
name = "blockchain_research"
start_urls = [
    "https://ethereum.org/en/",
    "https://bitcoin.org/en/developer-documentation"
]
max_depth = 6  # Deeper crawl
max_pages = 10000

[fetcher]
mode = "auto"
save_screenshots = true  # Save screenshots for analysis

[patterns]
include = [
    "^https://ethereum\\.org/en/developers/.*",
    "^https://ethereum\\.org/en/whitepaper.*",
    "^https://bitcoin\\.org/en/developer-.*"
]

[network]
min_delay = 2.0
max_delay = 4.0
timeout = 60.0

[storage]
output_dir = "./blockchain_research"
save_metadata = true  # Save crawl metadata

[logging]
level = "DEBUG"  # Verbose logging for research
log_file = "research_crawler.log"
</code></pre>
<h2 id="ethical-considerations"><a class="header" href="#ethical-considerations">Ethical Considerations</a></h2>
<p>When crawling blockchain-related websites:</p>
<h3 id="1-respect-robotstxt"><a class="header" href="#1-respect-robotstxt">1. Respect robots.txt</a></h3>
<p>Always check and respect robots.txt directives:</p>
<pre><code class="language-bash"># Check robots.txt before crawling
curl https://etherscan.io/robots.txt
</code></pre>
<p><strong>Important:</strong></p>
<ul>
<li>Some sites disallow all crawlers</li>
<li>Others specify allowed paths</li>
<li>Ignoring robots.txt may violate terms of service</li>
<li>Can lead to IP bans or legal issues</li>
</ul>
<h3 id="2-review-terms-of-service"><a class="header" href="#2-review-terms-of-service">2. Review Terms of Service</a></h3>
<p>Many blockchain sites have specific ToS regarding automated access:</p>
<p><strong>Common restrictions:</strong></p>
<ul>
<li>Maximum request rates</li>
<li>Prohibited content usage</li>
<li>Attribution requirements</li>
<li>Commercial use limitations</li>
</ul>
<p><strong>Before crawling:</strong></p>
<ul>
<li>Read the site's terms of service</li>
<li>Check for "API Terms" or "Developer Terms"</li>
<li>Look for explicit scraping policies</li>
<li>Consider contacting site operators for permission</li>
</ul>
<h3 id="3-rate-limiting-and-server-load"><a class="header" href="#3-rate-limiting-and-server-load">3. Rate Limiting and Server Load</a></h3>
<p>Don't overload services, especially free public infrastructure:</p>
<p><strong>Impact of aggressive crawling:</strong></p>
<ul>
<li>Increased server costs for operators</li>
<li>Degraded service for legitimate users</li>
<li>Potential service outages</li>
<li>IP blocking or legal action</li>
</ul>
<p><strong>Best practices:</strong></p>
<ul>
<li>Use conservative delays (2-5 seconds minimum)</li>
<li>Crawl during off-peak hours</li>
<li>Limit concurrent requests</li>
<li>Monitor for error responses (429, 503)</li>
</ul>
<h3 id="4-data-usage-and-copyright"><a class="header" href="#4-data-usage-and-copyright">4. Data Usage and Copyright</a></h3>
<p>Respect copyright and licensing of crawled content:</p>
<p><strong>Considerations:</strong></p>
<ul>
<li>Documentation may be copyrighted</li>
<li>Code examples may have specific licenses</li>
<li>Images and graphics have separate rights</li>
<li>Commercial use may require permission</li>
</ul>
<p><strong>Proper usage:</strong></p>
<ul>
<li>Attribute sources appropriately</li>
<li>Respect license terms (MIT, GPL, etc.)</li>
<li>Don't republish as your own work</li>
<li>Link back to original sources</li>
</ul>
<h3 id="5-privacy-and-blockchain-data"><a class="header" href="#5-privacy-and-blockchain-data">5. Privacy and Blockchain Data</a></h3>
<p>Be cautious with addresses and transaction data:</p>
<p><strong>Privacy concerns:</strong></p>
<ul>
<li>Wallet addresses are pseudonymous, not anonymous</li>
<li>Transaction patterns can reveal identities</li>
<li>Aggregate data can deanonymize users</li>
<li>GDPR and privacy laws may apply</li>
</ul>
<p><strong>Responsible handling:</strong></p>
<ul>
<li>Don't republish address/transaction mappings</li>
<li>Aggregate data to protect privacy</li>
<li>Follow data protection regulations</li>
<li>Consider ethical implications of deanonymization</li>
</ul>
<h3 id="6-community-impact"><a class="header" href="#6-community-impact">6. Community Impact</a></h3>
<p>Your crawling affects the broader blockchain community:</p>
<p><strong>Positive contributions:</strong></p>
<ul>
<li>Archive important documentation</li>
<li>Enable research and analysis</li>
<li>Improve search and discovery</li>
<li>Preserve historical data</li>
</ul>
<p><strong>Negative impacts:</strong></p>
<ul>
<li>Strain on community resources</li>
<li>Potential abuse of scraped data</li>
<li>Violation of community trust</li>
<li>Reduced availability for others</li>
</ul>
<h3 id="7-alternative-approaches"><a class="header" href="#7-alternative-approaches">7. Alternative Approaches</a></h3>
<p>Consider alternatives to web scraping:</p>
<p><strong>Better options:</strong></p>
<ul>
<li><strong>Official APIs</strong>: Most sites provide APIs (Etherscan API, CoinGecko API)</li>
<li><strong>GraphQL endpoints</strong>: The Graph protocol for blockchain data</li>
<li><strong>Node access</strong>: Direct blockchain node queries (Infura, Alchemy)</li>
<li><strong>Data services</strong>: Paid data providers with proper licensing</li>
<li><strong>Public datasets</strong>: Existing archived datasets (Kaggle, etc.)</li>
<li><strong>Partnerships</strong>: Contact site operators for data access</li>
</ul>
<h3 id="8-transparency"><a class="header" href="#8-transparency">8. Transparency</a></h3>
<p>Be transparent about your bot:</p>
<p><strong>User agent:</strong></p>
<pre><code class="language-toml">[fetcher]
user_agent = "ResearchBot/1.0 (+https://yoursite.com/bot-info; contact@email.com)"
</code></pre>
<p><strong>Bot info page should include:</strong></p>
<ul>
<li>Purpose of crawling</li>
<li>Contact information</li>
<li>Crawl frequency and scope</li>
<li>Data usage policy</li>
<li>Opt-out instructions</li>
</ul>
<h2 id="limitations"><a class="header" href="#limitations">Limitations</a></h2>
<p>This crawler is designed for <strong>public web content</strong> only. It cannot and should not be used to:</p>
<h3 id="technical-limitations"><a class="header" href="#technical-limitations">Technical Limitations</a></h3>
<ul>
<li><strong>Interact with blockchain networks directly</strong> - Use web3 libraries instead</li>
<li><strong>Access wallet-gated content</strong> - Requires authentication not suitable for crawlers</li>
<li><strong>Execute smart contracts</strong> - Use blockchain SDKs (web3.js, ethers.js)</li>
<li><strong>Crawl APIs</strong> - Use proper API clients with authentication</li>
<li><strong>Capture WebSocket streams</strong> - Real-time data requires different tools</li>
<li><strong>Bypass CAPTCHAs</strong> - Don't attempt to evade security measures</li>
<li><strong>Handle infinite scroll effectively</strong> - May miss dynamically loaded content</li>
</ul>
<h3 id="ethical-limitations"><a class="header" href="#ethical-limitations">Ethical Limitations</a></h3>
<ul>
<li><strong>Don't scrape private/gated content</strong> - Respect access controls</li>
<li><strong>Don't evade rate limits</strong> - Respect site protections</li>
<li><strong>Don't impersonate users</strong> - Use honest user agents</li>
<li><strong>Don't crawl during attacks</strong> - Avoid adding load during incidents</li>
<li><strong>Don't sell scraped data</strong> - Check licensing terms</li>
</ul>
<h3 id="legal-limitations"><a class="header" href="#legal-limitations">Legal Limitations</a></h3>
<ul>
<li><strong>Comply with local laws</strong> - Web scraping legality varies by jurisdiction</li>
<li><strong>Respect intellectual property</strong> - Don't violate copyright</li>
<li><strong>Follow data protection laws</strong> - GDPR, CCPA, etc.</li>
<li><strong>Honor contracts</strong> - ToS are often legally binding</li>
</ul>
<h2 id="additional-resources"><a class="header" href="#additional-resources">Additional Resources</a></h2>
<h3 id="blockchain-documentation"><a class="header" href="#blockchain-documentation">Blockchain Documentation</a></h3>
<ul>
<li><a href="https://ethereum.org/en/developers/">Ethereum Developer Documentation</a></li>
<li><a href="https://developer.bitcoin.org/">Bitcoin Developer Guide</a></li>
<li><a href="https://solana.com/docs">Solana Documentation</a></li>
<li><a href="https://wiki.polkadot.network/">Polkadot Wiki</a></li>
<li><a href="https://docs.cosmos.network/">Cosmos Documentation</a></li>
</ul>
<h3 id="apis-and-data-services"><a class="header" href="#apis-and-data-services">APIs and Data Services</a></h3>
<ul>
<li><a href="https://docs.etherscan.io/">Etherscan API</a></li>
<li><a href="https://thegraph.com/">The Graph Protocol</a></li>
<li><a href="https://www.alchemy.com/">Alchemy API</a></li>
<li><a href="https://www.infura.io/">Infura API</a></li>
<li><a href="https://www.coingecko.com/en/api">CoinGecko API</a></li>
<li><a href="https://defillama.com/docs/api">DeFi Llama API</a></li>
</ul>
<h3 id="web-scraping-best-practices"><a class="header" href="#web-scraping-best-practices">Web Scraping Best Practices</a></h3>
<ul>
<li><a href="https://www.scraperapi.com/blog/web-scraping-best-practices/">Web Scraping Best Practices</a></li>
<li><a href="https://towardsdatascience.com/ethics-in-web-scraping-b96b18136f01">Ethical Web Scraping Guide</a></li>
<li><a href="https://www.robotstxt.org/">robots.txt Specification</a></li>
</ul>
<h3 id="legal-and-ethical-resources"><a class="header" href="#legal-and-ethical-resources">Legal and Ethical Resources</a></h3>
<ul>
<li><a href="https://www.eff.org/">EFF on Web Scraping</a></li>
<li><a href="https://gdpr.eu/">GDPR Compliance</a></li>
<li><a href="https://creativecommons.org/licenses/">Creative Commons Licenses</a></li>
</ul>
<h2 id="support"><a class="header" href="#support">Support</a></h2>
<p>For issues specific to blockchain website crawling, please provide:</p>
<p><strong>Required information:</strong></p>
<ol>
<li><strong>Target website URL</strong> - Full URL you're trying to crawl</li>
<li><strong>Configuration file</strong> - Your complete TOML configuration</li>
<li><strong>Error messages</strong> - Exact error text and log output</li>
<li><strong>Content type</strong> - Static or JavaScript-rendered</li>
<li><strong>Crawl scope</strong> - How many pages, what depth</li>
</ol>
<p><strong>Helpful additional info:</strong></p>
<ul>
<li>Browser DevTools network tab screenshot</li>
<li>robots.txt content from target site</li>
<li>Example URLs that fail/succeed</li>
<li>Your crawler version</li>
<li>Operating system and environment</li>
</ul>
<p><strong>Getting help:</strong></p>
<ul>
<li>Check documentation first</li>
<li>Search existing issues</li>
<li>Provide minimal reproducible example</li>
<li>Include relevant logs (not entire log dump)</li>
<li>Describe what you've already tried</li>
</ul>
<h2 id="quick-reference"><a class="header" href="#quick-reference">Quick Reference</a></h2>
<h3 id="blockchain-url-patterns-to-exclude"><a class="header" href="#blockchain-url-patterns-to-exclude">Blockchain URL Patterns to Exclude</a></h3>
<pre><code class="language-toml">[patterns]
exclude = [
    # Ethereum-style addresses and transactions
    ".*\\/tx\\/0x[a-fA-F0-9]{64}$",
    ".*\\/address\\/0x[a-fA-F0-9]{40}$",
    ".*\\/token\\/0x[a-fA-F0-9]{40}$",
    ".*\\/block\\/\\d+$",

    # Bitcoin addresses
    ".*\\/address/[13][a-km-zA-HJ-NP-Z1-9]{25,34}$",
    ".*\\/address/bc1[a-z0-9]{39,87}$",

    # Solana addresses
    ".*\\/address/[1-9A-HJ-NP-Za-km-z]{32,44}$",

    # Common excludes
    ".*\\/search\\?.*",
    ".*\\/translations/.*",
    ".*\\#.*",  # Anchors
    ".*\\.pdf$"
]
</code></pre>
<h3 id="recommended-delays-by-site-type"><a class="header" href="#recommended-delays-by-site-type">Recommended Delays by Site Type</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Site Type</th><th>Min Delay</th><th>Max Delay</th><th>Notes</th></tr></thead><tbody>
<tr><td>Documentation</td><td>0.5s</td><td>1.5s</td><td>Usually tolerant</td></tr>
<tr><td>News sites</td><td>1.5s</td><td>3.0s</td><td>Standard politeness</td></tr>
<tr><td>Block explorers</td><td>3.0s</td><td>6.0s</td><td>Heavily rate limited</td></tr>
<tr><td>DeFi platforms</td><td>2.0s</td><td>4.0s</td><td>Moderate protection</td></tr>
<tr><td>NFT marketplaces</td><td>3.0s</td><td>6.0s</td><td>Heavy JavaScript</td></tr>
</tbody></table>
</div>
<h3 id="fetcher-mode-selection"><a class="header" href="#fetcher-mode-selection">Fetcher Mode Selection</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Site Type</th><th>Mode</th><th>Reason</th></tr></thead><tbody>
<tr><td>Docs (Docusaurus, GitBook)</td><td>static</td><td>Server-rendered</td></tr>
<tr><td>Block explorers</td><td>dynamic</td><td>React/Vue SPAs</td></tr>
<tr><td>News sites</td><td>auto</td><td>Mixed content</td></tr>
<tr><td>DeFi dashboards</td><td>dynamic</td><td>Heavy JavaScript</td></tr>
<tr><td>Blogs</td><td>static</td><td>Traditional HTML</td></tr>
</tbody></table>
</div>
<h2 id="conclusion"><a class="header" href="#conclusion">Conclusion</a></h2>
<p>Blockchain website crawling requires careful consideration of technical, ethical, and legal factors. Always prioritize:</p>
<ol>
<li><strong>Respect</strong> - For site operators, users, and community</li>
<li><strong>Transparency</strong> - Identify your bot and purpose</li>
<li><strong>Moderation</strong> - Use conservative rate limits</li>
<li><strong>Legality</strong> - Comply with laws and terms of service</li>
<li><strong>Alternatives</strong> - Consider APIs and official data sources</li>
</ol>
<p>When in doubt, err on the side of caution and reach out to site operators for permission.</p>
<p>Happy (ethical) crawling!</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../misc/pandas.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../misc/operating_systems.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../misc/pandas.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../misc/operating_systems.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="../elasticlunr.min.js"></script>
        <script src="../mark.min.js"></script>
        <script src="../searcher.js"></script>

        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->



    </div>
    </body>
</html>
