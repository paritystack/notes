<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Reinforcement Learning - My Notes</title>


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon-de23e50b.svg">
        <link rel="shortcut icon" href="../favicon-8114d1fc.png">
        <link rel="stylesheet" href="../css/variables-8adf115d.css">
        <link rel="stylesheet" href="../css/general-2459343d.css">
        <link rel="stylesheet" href="../css/chrome-ae938929.css">
        <link rel="stylesheet" href="../css/print-9e4910d8.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../fonts/fonts-9644e21d.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="mdbook-highlight-css" href="../highlight-493f70e1.css">
        <link rel="stylesheet" id="mdbook-tomorrow-night-css" href="../tomorrow-night-4c0ae647.css">
        <link rel="stylesheet" id="mdbook-ayu-highlight-css" href="../ayu-highlight-3fdfc3ac.css">

        <!-- Custom theme stylesheets -->

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "../";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "../searchindex-811110ea.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../toc-bd4efd9a.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="mdbook-body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="mdbook-sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("mdbook-sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="mdbook-sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="../toc.html"></iframe>
            </noscript>
            <div id="mdbook-sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="mdbook-page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="mdbook-menu-bar-hover-placeholder"></div>
                <div id="mdbook-menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="mdbook-sidebar-toggle" class="icon-button" for="mdbook-sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="mdbook-sidebar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"/></svg></span>
                        </label>
                        <button id="mdbook-theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="mdbook-theme-list">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M371.3 367.1c27.3-3.9 51.9-19.4 67.2-42.9L600.2 74.1c12.6-19.5 9.4-45.3-7.6-61.2S549.7-4.4 531.1 9.6L294.4 187.2c-24 18-38.2 46.1-38.4 76.1L371.3 367.1zm-19.6 25.4l-116-104.4C175.9 290.3 128 339.6 128 400c0 3.9 .2 7.8 .6 11.6c1.8 17.5-10.2 36.4-27.8 36.4H96c-17.7 0-32 14.3-32 32s14.3 32 32 32H240c61.9 0 112-50.1 112-112c0-2.5-.1-5-.2-7.5z"/></svg></span>
                        </button>
                        <ul id="mdbook-theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-ayu">Ayu</button></li>
                        </ul>
                        <button id="mdbook-search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="mdbook-searchbar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M416 208c0 45.9-14.9 88.3-40 122.7L502.6 457.4c12.5 12.5 12.5 32.8 0 45.3s-32.8 12.5-45.3 0L330.7 376c-34.4 25.2-76.8 40-122.7 40C93.1 416 0 322.9 0 208S93.1 0 208 0S416 93.1 416 208zM208 352c79.5 0 144-64.5 144-144s-64.5-144-144-144S64 128.5 64 208s64.5 144 144 144z"/></svg></span>
                        </button>
                    </div>

                    <h1 class="menu-title">My Notes</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <span class=fa-svg id="print-button"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M128 0C92.7 0 64 28.7 64 64v96h64V64H354.7L384 93.3V160h64V93.3c0-17-6.7-33.3-18.7-45.3L400 18.7C388 6.7 371.7 0 354.7 0H128zM384 352v32 64H128V384 368 352H384zm64 32h32c17.7 0 32-14.3 32-32V256c0-35.3-28.7-64-64-64H64c-35.3 0-64 28.7-64 64v96c0 17.7 14.3 32 32 32H64v64c0 35.3 28.7 64 64 64H384c35.3 0 64-28.7 64-64V384zm-16-88c-13.3 0-24-10.7-24-24s10.7-24 24-24s24 10.7 24 24s-10.7 24-24 24z"/></svg></span>
                        </a>

                    </div>
                </div>

                <div id="mdbook-search-wrapper" class="hidden">
                    <form id="mdbook-searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="mdbook-searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="mdbook-searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <span class=fa-svg id="fa-spin"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M304 48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zm0 416c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM48 304c26.5 0 48-21.5 48-48s-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48zm464-48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM142.9 437c18.7-18.7 18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zm0-294.2c18.7-18.7 18.7-49.1 0-67.9S93.7 56.2 75 75s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zM369.1 437c18.7 18.7 49.1 18.7 67.9 0s18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9z"/></svg></span>
                            </div>
                        </div>
                    </form>
                    <div id="mdbook-searchresults-outer" class="searchresults-outer hidden">
                        <div id="mdbook-searchresults-header" class="searchresults-header"></div>
                        <ul id="mdbook-searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('mdbook-sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('mdbook-sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#mdbook-sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="mdbook-content" class="content">
                    <main>
                        <h1 id="reinforcement-learning"><a class="header" href="#reinforcement-learning">Reinforcement Learning</a></h1>
<p>Reinforcement Learning (RL) is about learning to make decisions by interacting with an environment to maximize cumulative reward.</p>
<h2 id="table-of-contents"><a class="header" href="#table-of-contents">Table of Contents</a></h2>
<ol>
<li><a href="#core-concepts">Core Concepts</a></li>
<li><a href="#markov-decision-processes">Markov Decision Processes</a></li>
<li><a href="#dynamic-programming">Dynamic Programming</a></li>
<li><a href="#monte-carlo-methods">Monte Carlo Methods</a></li>
<li><a href="#temporal-difference-learning">Temporal Difference Learning</a></li>
<li><a href="#q-learning">Q-Learning</a></li>
<li><a href="#sarsa">SARSA</a></li>
<li><a href="#policy-gradient-methods">Policy Gradient Methods</a></li>
<li><a href="#actor-critic-methods">Actor-Critic Methods</a></li>
<li><a href="#multi-armed-bandits">Multi-Armed Bandits</a></li>
</ol>
<h2 id="core-concepts"><a class="header" href="#core-concepts">Core Concepts</a></h2>
<h3 id="the-rl-framework"><a class="header" href="#the-rl-framework">The RL Framework</a></h3>
<p><strong>Key Components:</strong></p>
<ul>
<li><strong>Agent</strong>: The learner/decision maker</li>
<li><strong>Environment</strong>: What the agent interacts with</li>
<li><strong>State (s)</strong>: Current situation</li>
<li><strong>Action (a)</strong>: What the agent can do</li>
<li><strong>Reward (r)</strong>: Feedback signal</li>
<li><strong>Policy (π)</strong>: Strategy for selecting actions</li>
<li><strong>Value Function (V)</strong>: Expected future reward from a state</li>
<li><strong>Q-Function (Q)</strong>: Expected future reward for state-action pairs</li>
</ul>
<p><strong>Mathematical Framework:</strong></p>
<pre><code>At each time step t:
- Agent observes state s_t
- Agent takes action a_t
- Environment transitions to s_{t+1}
- Agent receives reward r_{t+1}
</code></pre>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

# Simple RL environment example
class GridWorld:
    def __init__(self, size=5):
        self.size = size
        self.state = (0, 0)
        self.goal = (size-1, size-1)
        
    def reset(self):
        self.state = (0, 0)
        return self.state
    
    def step(self, action):
        # Actions: 0=up, 1=right, 2=down, 3=left
        x, y = self.state
        
        if action == 0:  # up
            x = max(0, x - 1)
        elif action == 1:  # right
            y = min(self.size - 1, y + 1)
        elif action == 2:  # down
            x = min(self.size - 1, x + 1)
        elif action == 3:  # left
            y = max(0, y - 1)
        
        self.state = (x, y)
        
        # Reward
        if self.state == self.goal:
            reward = 1.0
            done = True
        else:
            reward = -0.01  # Small penalty for each step
            done = False
        
        return self.state, reward, done
    
    def render(self):
        grid = np.zeros((self.size, self.size))
        grid[self.state] = 1
        grid[self.goal] = 0.5
        plt.imshow(grid, cmap='hot')
        plt.title(f'State: {self.state}')
        plt.show()

# Example usage
env = GridWorld(size=5)
state = env.reset()
print(f"Initial state: {state}")

# Take random actions
for _ in range(5):
    action = np.random.randint(0, 4)
    state, reward, done = env.step(action)
    print(f"State: {state}, Reward: {reward}, Done: {done}")
    if done:
        break
</code></pre>
<h3 id="return-and-discounting"><a class="header" href="#return-and-discounting">Return and Discounting</a></h3>
<p><strong>Return (G_t)</strong>: Total cumulative reward from time t</p>
<pre><code>G_t = R_{t+1} + γR_{t+2} + γ²R_{t+3} + ... = Σ_{k=0}^∞ γ^k R_{t+k+1}
</code></pre>
<p>Where γ (gamma) is the discount factor (0 ≤ γ ≤ 1):</p>
<ul>
<li>γ = 0: Only immediate rewards matter</li>
<li>γ = 1: All future rewards equally important</li>
<li>γ closer to 1: More far-sighted agent</li>
</ul>
<pre><code class="language-python">def calculate_return(rewards, gamma=0.99):
    """Calculate discounted return from a list of rewards"""
    G = 0
    returns = []
    for r in reversed(rewards):
        G = r + gamma * G
        returns.insert(0, G)
    return returns

# Example
rewards = [1, 0, 0, 1, 0]
returns = calculate_return(rewards, gamma=0.9)
print(f"Rewards: {rewards}")
print(f"Returns: {returns}")
</code></pre>
<h2 id="markov-decision-processes"><a class="header" href="#markov-decision-processes">Markov Decision Processes</a></h2>
<p>An MDP is defined by a tuple (S, A, P, R, γ):</p>
<ul>
<li><strong>S</strong>: Set of states</li>
<li><strong>A</strong>: Set of actions</li>
<li><strong>P</strong>: Transition probability P(s’|s,a)</li>
<li><strong>R</strong>: Reward function R(s,a,s’)</li>
<li><strong>γ</strong>: Discount factor</li>
</ul>
<p><strong>Markov Property</strong>: Future depends only on current state, not history</p>
<pre><code>P(s_{t+1}|s_t, a_t, s_{t-1}, a_{t-1}, ...) = P(s_{t+1}|s_t, a_t)
</code></pre>
<pre><code class="language-python">class MDP:
    def __init__(self, states, actions, transitions, rewards, gamma=0.99):
        self.states = states
        self.actions = actions
        self.transitions = transitions  # P(s'|s,a)
        self.rewards = rewards          # R(s,a,s')
        self.gamma = gamma
    
    def get_transition_prob(self, state, action, next_state):
        return self.transitions.get((state, action, next_state), 0.0)
    
    def get_reward(self, state, action, next_state):
        return self.rewards.get((state, action, next_state), 0.0)

# Example: Simple MDP
states = ['s0', 's1', 's2']
actions = ['a0', 'a1']

transitions = {
    ('s0', 'a0', 's1'): 0.8,
    ('s0', 'a0', 's0'): 0.2,
    ('s0', 'a1', 's2'): 0.9,
    ('s0', 'a1', 's0'): 0.1,
    ('s1', 'a0', 's2'): 1.0,
    ('s2', 'a0', 's2'): 1.0,
}

rewards = {
    ('s0', 'a0', 's1'): -1,
    ('s0', 'a1', 's2'): 10,
    ('s1', 'a0', 's2'): 5,
}

mdp = MDP(states, actions, transitions, rewards)
</code></pre>
<h3 id="value-functions"><a class="header" href="#value-functions">Value Functions</a></h3>
<p><strong>State-Value Function V^π(s):</strong></p>
<pre><code>V^π(s) = E_π[G_t | S_t = s]
       = E_π[Σ_{k=0}^∞ γ^k R_{t+k+1} | S_t = s]
</code></pre>
<p><strong>Action-Value Function Q^π(s,a):</strong></p>
<pre><code>Q^π(s,a) = E_π[G_t | S_t = s, A_t = a]
</code></pre>
<p><strong>Bellman Equations:</strong></p>
<pre><code>V^π(s) = Σ_a π(a|s) Σ_{s',r} p(s',r|s,a)[r + γV^π(s')]
Q^π(s,a) = Σ_{s',r} p(s',r|s,a)[r + γΣ_{a'} π(a'|s')Q^π(s',a')]
</code></pre>
<p><strong>Optimal Value Functions:</strong></p>
<pre><code>V*(s) = max_π V^π(s) = max_a Q*(s,a)
Q*(s,a) = E[R_{t+1} + γV*(S_{t+1}) | S_t=s, A_t=a]
</code></pre>
<h2 id="dynamic-programming"><a class="header" href="#dynamic-programming">Dynamic Programming</a></h2>
<p>DP methods assume full knowledge of the MDP.</p>
<h3 id="policy-evaluation"><a class="header" href="#policy-evaluation">Policy Evaluation</a></h3>
<p>Compute value function for a given policy.</p>
<pre><code class="language-python">def policy_evaluation(policy, mdp, theta=1e-6):
    """
    Evaluate a policy using iterative policy evaluation
    
    Args:
        policy: dict mapping states to action probabilities
        mdp: MDP object
        theta: convergence threshold
    """
    V = {s: 0 for s in mdp.states}
    
    while True:
        delta = 0
        for s in mdp.states:
            v = V[s]
            new_v = 0
            
            # Sum over actions
            for a in mdp.actions:
                action_prob = policy.get((s, a), 0)
                
                # Sum over next states
                for s_prime in mdp.states:
                    trans_prob = mdp.get_transition_prob(s, a, s_prime)
                    reward = mdp.get_reward(s, a, s_prime)
                    new_v += action_prob * trans_prob * (reward + mdp.gamma * V[s_prime])
            
            V[s] = new_v
            delta = max(delta, abs(v - V[s]))
        
        if delta &lt; theta:
            break
    
    return V

# Example: Uniform random policy
random_policy = {
    ('s0', 'a0'): 0.5,
    ('s0', 'a1'): 0.5,
    ('s1', 'a0'): 1.0,
    ('s2', 'a0'): 1.0,
}

V = policy_evaluation(random_policy, mdp)
print("State values:", V)
</code></pre>
<h3 id="policy-iteration"><a class="header" href="#policy-iteration">Policy Iteration</a></h3>
<pre><code class="language-python">def policy_iteration(mdp, theta=1e-6):
    """
    Find optimal policy using policy iteration
    """
    # Initialize random policy
    policy = {}
    for s in mdp.states:
        action = np.random.choice(mdp.actions)
        for a in mdp.actions:
            policy[(s, a)] = 1.0 if a == action else 0.0
    
    while True:
        # Policy Evaluation
        V = policy_evaluation(policy, mdp, theta)
        
        # Policy Improvement
        policy_stable = True
        
        for s in mdp.states:
            old_action = None
            for a in mdp.actions:
                if policy.get((s, a), 0) == 1.0:
                    old_action = a
                    break
            
            # Find best action
            action_values = {}
            for a in mdp.actions:
                q = 0
                for s_prime in mdp.states:
                    trans_prob = mdp.get_transition_prob(s, a, s_prime)
                    reward = mdp.get_reward(s, a, s_prime)
                    q += trans_prob * (reward + mdp.gamma * V[s_prime])
                action_values[a] = q
            
            best_action = max(action_values, key=action_values.get)
            
            # Update policy
            for a in mdp.actions:
                policy[(s, a)] = 1.0 if a == best_action else 0.0
            
            if best_action != old_action:
                policy_stable = False
        
        if policy_stable:
            break
    
    return policy, V

optimal_policy, optimal_V = policy_iteration(mdp)
print("Optimal policy:", optimal_policy)
print("Optimal values:", optimal_V)
</code></pre>
<h3 id="value-iteration"><a class="header" href="#value-iteration">Value Iteration</a></h3>
<pre><code class="language-python">def value_iteration(mdp, theta=1e-6):
    """
    Find optimal policy using value iteration
    """
    V = {s: 0 for s in mdp.states}
    
    while True:
        delta = 0
        for s in mdp.states:
            v = V[s]
            
            # Find max over actions
            action_values = []
            for a in mdp.actions:
                q = 0
                for s_prime in mdp.states:
                    trans_prob = mdp.get_transition_prob(s, a, s_prime)
                    reward = mdp.get_reward(s, a, s_prime)
                    q += trans_prob * (reward + mdp.gamma * V[s_prime])
                action_values.append(q)
            
            V[s] = max(action_values) if action_values else 0
            delta = max(delta, abs(v - V[s]))
        
        if delta &lt; theta:
            break
    
    # Extract policy
    policy = {}
    for s in mdp.states:
        action_values = {}
        for a in mdp.actions:
            q = 0
            for s_prime in mdp.states:
                trans_prob = mdp.get_transition_prob(s, a, s_prime)
                reward = mdp.get_reward(s, a, s_prime)
                q += trans_prob * (reward + mdp.gamma * V[s_prime])
            action_values[a] = q
        
        best_action = max(action_values, key=action_values.get)
        for a in mdp.actions:
            policy[(s, a)] = 1.0 if a == best_action else 0.0
    
    return policy, V

optimal_policy, optimal_V = value_iteration(mdp)
</code></pre>
<h2 id="monte-carlo-methods"><a class="header" href="#monte-carlo-methods">Monte Carlo Methods</a></h2>
<p>MC methods learn from complete episodes without needing environment model.</p>
<h3 id="first-visit-mc-prediction"><a class="header" href="#first-visit-mc-prediction">First-Visit MC Prediction</a></h3>
<pre><code class="language-python">def first_visit_mc_prediction(env, policy, num_episodes=1000, gamma=0.99):
    """
    Estimate state-value function using first-visit MC
    """
    returns = {s: [] for s in env.states}
    V = {s: 0 for s in env.states}
    
    for episode in range(num_episodes):
        # Generate episode
        episode_data = []
        state = env.reset()
        done = False
        
        while not done:
            action = policy[state]
            next_state, reward, done = env.step(action)
            episode_data.append((state, action, reward))
            state = next_state
        
        # Calculate returns
        G = 0
        visited_states = set()
        
        for t in reversed(range(len(episode_data))):
            state, action, reward = episode_data[t]
            G = reward + gamma * G
            
            # First-visit: only update if state not seen earlier
            if state not in visited_states:
                returns[state].append(G)
                V[state] = np.mean(returns[state])
                visited_states.add(state)
    
    return V

# Example usage with GridWorld
env = GridWorld(size=4)
# Define a simple policy
policy = {state: np.random.randint(0, 4) for state in 
          [(i, j) for i in range(4) for j in range(4)]}

V = first_visit_mc_prediction(env, policy, num_episodes=10000)
</code></pre>
<h3 id="monte-carlo-control-epsilon-greedy"><a class="header" href="#monte-carlo-control-epsilon-greedy">Monte Carlo Control (Epsilon-Greedy)</a></h3>
<pre><code class="language-python">def mc_control_epsilon_greedy(env, num_episodes=10000, gamma=0.99, epsilon=0.1):
    """
    Monte Carlo control with epsilon-greedy policy
    """
    Q = {}
    returns = {}
    
    # Initialize Q-values
    for state in env.get_all_states():
        for action in range(env.num_actions):
            Q[(state, action)] = 0
            returns[(state, action)] = []
    
    for episode in range(num_episodes):
        # Generate episode with epsilon-greedy policy
        episode_data = []
        state = env.reset()
        done = False
        
        while not done:
            # Epsilon-greedy action selection
            if np.random.random() &lt; epsilon:
                action = np.random.randint(0, env.num_actions)
            else:
                q_values = [Q.get((state, a), 0) for a in range(env.num_actions)]
                action = np.argmax(q_values)
            
            next_state, reward, done = env.step(action)
            episode_data.append((state, action, reward))
            state = next_state
        
        # Update Q-values
        G = 0
        visited = set()
        
        for t in reversed(range(len(episode_data))):
            state, action, reward = episode_data[t]
            G = reward + gamma * G
            
            if (state, action) not in visited:
                returns[(state, action)].append(G)
                Q[(state, action)] = np.mean(returns[(state, action)])
                visited.add((state, action))
    
    # Extract policy
    policy = {}
    for state in env.get_all_states():
        q_values = [Q.get((state, a), 0) for a in range(env.num_actions)]
        policy[state] = np.argmax(q_values)
    
    return policy, Q
</code></pre>
<h2 id="temporal-difference-learning"><a class="header" href="#temporal-difference-learning">Temporal Difference Learning</a></h2>
<p>TD methods learn from incomplete episodes by bootstrapping.</p>
<h3 id="td0-prediction"><a class="header" href="#td0-prediction">TD(0) Prediction</a></h3>
<p><strong>TD Update Rule:</strong></p>
<pre><code>V(S_t) ← V(S_t) + α[R_{t+1} + γV(S_{t+1}) - V(S_t)]
</code></pre>
<p>Where:</p>
<ul>
<li>α is the learning rate</li>
<li>R_{t+1} + γV(S_{t+1}) is the TD target</li>
<li>δ_t = R_{t+1} + γV(S_{t+1}) - V(S_t) is the TD error</li>
</ul>
<pre><code class="language-python">def td_0_prediction(env, policy, num_episodes=1000, alpha=0.1, gamma=0.99):
    """
    TD(0) prediction for estimating state values
    """
    V = {s: 0 for s in env.get_all_states()}
    
    for episode in range(num_episodes):
        state = env.reset()
        done = False
        
        while not done:
            action = policy[state]
            next_state, reward, done = env.step(action)
            
            # TD update
            if not done:
                td_target = reward + gamma * V[next_state]
            else:
                td_target = reward
            
            td_error = td_target - V[state]
            V[state] += alpha * td_error
            
            state = next_state
    
    return V
</code></pre>
<h3 id="tdλ---eligibility-traces"><a class="header" href="#tdλ---eligibility-traces">TD(λ) - Eligibility Traces</a></h3>
<pre><code class="language-python">def td_lambda_prediction(env, policy, num_episodes=1000, 
                        alpha=0.1, gamma=0.99, lambda_=0.9):
    """
    TD(λ) prediction with eligibility traces
    """
    V = {s: 0 for s in env.get_all_states()}
    
    for episode in range(num_episodes):
        E = {s: 0 for s in env.get_all_states()}  # Eligibility traces
        state = env.reset()
        done = False
        
        while not done:
            action = policy[state]
            next_state, reward, done = env.step(action)
            
            # Calculate TD error
            if not done:
                td_error = reward + gamma * V[next_state] - V[state]
            else:
                td_error = reward - V[state]
            
            # Update eligibility trace for current state
            E[state] += 1
            
            # Update all states
            for s in env.get_all_states():
                V[s] += alpha * td_error * E[s]
                E[s] *= gamma * lambda_
            
            state = next_state
    
    return V
</code></pre>
<h2 id="q-learning"><a class="header" href="#q-learning">Q-Learning</a></h2>
<p>Q-Learning is an off-policy TD control algorithm.</p>
<p><strong>Q-Learning Update:</strong></p>
<pre><code>Q(S_t, A_t) ← Q(S_t, A_t) + α[R_{t+1} + γ max_a Q(S_{t+1}, a) - Q(S_t, A_t)]
</code></pre>
<pre><code class="language-python">class QLearningAgent:
    def __init__(self, state_space, action_space, alpha=0.1, gamma=0.99, epsilon=0.1):
        self.state_space = state_space
        self.action_space = action_space
        self.alpha = alpha
        self.gamma = gamma
        self.epsilon = epsilon
        
        # Initialize Q-table
        self.q_table = {}
        for state in state_space:
            for action in action_space:
                self.q_table[(state, action)] = 0.0
    
    def get_action(self, state, training=True):
        """Epsilon-greedy action selection"""
        if training and np.random.random() &lt; self.epsilon:
            return np.random.choice(self.action_space)
        else:
            q_values = [self.q_table.get((state, a), 0) for a in self.action_space]
            return self.action_space[np.argmax(q_values)]
    
    def update(self, state, action, reward, next_state, done):
        """Q-learning update"""
        # Current Q-value
        current_q = self.q_table[(state, action)]
        
        # Maximum Q-value for next state
        if not done:
            max_next_q = max([self.q_table.get((next_state, a), 0) 
                            for a in self.action_space])
        else:
            max_next_q = 0
        
        # Q-learning update
        td_target = reward + self.gamma * max_next_q
        td_error = td_target - current_q
        self.q_table[(state, action)] += self.alpha * td_error
        
        return td_error
    
    def train(self, env, num_episodes=1000):
        """Train the agent"""
        episode_rewards = []
        
        for episode in range(num_episodes):
            state = env.reset()
            total_reward = 0
            done = False
            
            while not done:
                action = self.get_action(state, training=True)
                next_state, reward, done = env.step(action)
                
                self.update(state, action, reward, next_state, done)
                
                state = next_state
                total_reward += reward
            
            episode_rewards.append(total_reward)
            
            # Decay epsilon
            self.epsilon = max(0.01, self.epsilon * 0.995)
            
            if (episode + 1) % 100 == 0:
                avg_reward = np.mean(episode_rewards[-100:])
                print(f"Episode {episode + 1}, Avg Reward: {avg_reward:.2f}, Epsilon: {self.epsilon:.3f}")
        
        return episode_rewards

# Example usage
env = GridWorld(size=5)
state_space = [(i, j) for i in range(5) for j in range(5)]
action_space = [0, 1, 2, 3]  # up, right, down, left

agent = QLearningAgent(state_space, action_space)
rewards = agent.train(env, num_episodes=5000)

# Plot learning curve
plt.figure(figsize=(10, 6))
plt.plot(np.convolve(rewards, np.ones(100)/100, mode='valid'))
plt.xlabel('Episode')
plt.ylabel('Average Reward (100 episodes)')
plt.title('Q-Learning Training Progress')
plt.grid(True)
plt.show()
</code></pre>
<h3 id="double-q-learning"><a class="header" href="#double-q-learning">Double Q-Learning</a></h3>
<p>Reduces maximization bias in Q-learning.</p>
<pre><code class="language-python">class DoubleQLearningAgent:
    def __init__(self, state_space, action_space, alpha=0.1, gamma=0.99, epsilon=0.1):
        self.state_space = state_space
        self.action_space = action_space
        self.alpha = alpha
        self.gamma = gamma
        self.epsilon = epsilon
        
        # Two Q-tables
        self.q_table_1 = {(s, a): 0.0 for s in state_space for a in action_space}
        self.q_table_2 = {(s, a): 0.0 for s in state_space for a in action_space}
    
    def get_action(self, state, training=True):
        """Epsilon-greedy using average of both Q-tables"""
        if training and np.random.random() &lt; self.epsilon:
            return np.random.choice(self.action_space)
        else:
            q_values = [(self.q_table_1[(state, a)] + self.q_table_2[(state, a)]) / 2 
                       for a in self.action_space]
            return self.action_space[np.argmax(q_values)]
    
    def update(self, state, action, reward, next_state, done):
        """Double Q-learning update"""
        # Randomly choose which Q-table to update
        if np.random.random() &lt; 0.5:
            q_table_update = self.q_table_1
            q_table_target = self.q_table_2
        else:
            q_table_update = self.q_table_2
            q_table_target = self.q_table_1
        
        current_q = q_table_update[(state, action)]
        
        if not done:
            # Use one Q-table to select action, other to evaluate
            best_action = max(self.action_space, 
                            key=lambda a: q_table_update[(next_state, a)])
            max_next_q = q_table_target[(next_state, best_action)]
        else:
            max_next_q = 0
        
        td_target = reward + self.gamma * max_next_q
        td_error = td_target - current_q
        q_table_update[(state, action)] += self.alpha * td_error
        
        return td_error
</code></pre>
<h2 id="sarsa"><a class="header" href="#sarsa">SARSA</a></h2>
<p>SARSA is an on-policy TD control algorithm.</p>
<p><strong>SARSA Update:</strong></p>
<pre><code>Q(S_t, A_t) ← Q(S_t, A_t) + α[R_{t+1} + γQ(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]
</code></pre>
<pre><code class="language-python">class SARSAAgent:
    def __init__(self, state_space, action_space, alpha=0.1, gamma=0.99, epsilon=0.1):
        self.state_space = state_space
        self.action_space = action_space
        self.alpha = alpha
        self.gamma = gamma
        self.epsilon = epsilon
        
        # Initialize Q-table
        self.q_table = {(s, a): 0.0 for s in state_space for a in action_space}
    
    def get_action(self, state, training=True):
        """Epsilon-greedy action selection"""
        if training and np.random.random() &lt; self.epsilon:
            return np.random.choice(self.action_space)
        else:
            q_values = [self.q_table[(state, a)] for a in self.action_space]
            return self.action_space[np.argmax(q_values)]
    
    def update(self, state, action, reward, next_state, next_action, done):
        """SARSA update"""
        current_q = self.q_table[(state, action)]
        
        if not done:
            next_q = self.q_table[(next_state, next_action)]
        else:
            next_q = 0
        
        td_target = reward + self.gamma * next_q
        td_error = td_target - current_q
        self.q_table[(state, action)] += self.alpha * td_error
        
        return td_error
    
    def train(self, env, num_episodes=1000):
        """Train the agent"""
        episode_rewards = []
        
        for episode in range(num_episodes):
            state = env.reset()
            action = self.get_action(state, training=True)
            total_reward = 0
            done = False
            
            while not done:
                next_state, reward, done = env.step(action)
                next_action = self.get_action(next_state, training=True)
                
                self.update(state, action, reward, next_state, next_action, done)
                
                state = next_state
                action = next_action
                total_reward += reward
            
            episode_rewards.append(total_reward)
            self.epsilon = max(0.01, self.epsilon * 0.995)
        
        return episode_rewards
</code></pre>
<h2 id="policy-gradient-methods"><a class="header" href="#policy-gradient-methods">Policy Gradient Methods</a></h2>
<p>Policy gradient methods directly optimize the policy.</p>
<h3 id="reinforce-algorithm"><a class="header" href="#reinforce-algorithm">REINFORCE Algorithm</a></h3>
<p><strong>Policy Gradient Theorem:</strong></p>
<pre><code>∇_θ J(θ) = E_π[∇_θ log π(a|s,θ) Q^π(s,a)]
</code></pre>
<pre><code class="language-python">import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

class PolicyNetwork(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=64):
        super(PolicyNetwork, self).__init__()
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, action_dim)
    
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = F.softmax(self.fc3(x), dim=-1)
        return x

class REINFORCEAgent:
    def __init__(self, state_dim, action_dim, lr=0.001, gamma=0.99):
        self.gamma = gamma
        self.policy_net = PolicyNetwork(state_dim, action_dim)
        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)
        
        self.saved_log_probs = []
        self.rewards = []
    
    def select_action(self, state):
        """Select action using current policy"""
        state = torch.FloatTensor(state).unsqueeze(0)
        probs = self.policy_net(state)
        action_dist = torch.distributions.Categorical(probs)
        action = action_dist.sample()
        
        # Save log probability for training
        self.saved_log_probs.append(action_dist.log_prob(action))
        
        return action.item()
    
    def update(self):
        """Update policy using REINFORCE"""
        R = 0
        returns = []
        
        # Calculate returns
        for r in reversed(self.rewards):
            R = r + self.gamma * R
            returns.insert(0, R)
        
        # Normalize returns
        returns = torch.tensor(returns)
        returns = (returns - returns.mean()) / (returns.std() + 1e-8)
        
        # Calculate loss
        policy_loss = []
        for log_prob, R in zip(self.saved_log_probs, returns):
            policy_loss.append(-log_prob * R)
        
        # Update policy
        self.optimizer.zero_grad()
        policy_loss = torch.stack(policy_loss).sum()
        policy_loss.backward()
        self.optimizer.step()
        
        # Clear saved values
        self.saved_log_probs = []
        self.rewards = []
        
        return policy_loss.item()
    
    def train(self, env, num_episodes=1000):
        """Train the agent"""
        episode_rewards = []
        
        for episode in range(num_episodes):
            state = env.reset()
            total_reward = 0
            done = False
            
            while not done:
                action = self.select_action(state)
                next_state, reward, done = env.step(action)
                
                self.rewards.append(reward)
                state = next_state
                total_reward += reward
            
            # Update policy after episode
            loss = self.update()
            episode_rewards.append(total_reward)
            
            if (episode + 1) % 100 == 0:
                avg_reward = np.mean(episode_rewards[-100:])
                print(f"Episode {episode + 1}, Avg Reward: {avg_reward:.2f}")
        
        return episode_rewards
</code></pre>
<h2 id="actor-critic-methods"><a class="header" href="#actor-critic-methods">Actor-Critic Methods</a></h2>
<p>Combine value-based and policy-based methods.</p>
<h3 id="advantage-actor-critic-a2c"><a class="header" href="#advantage-actor-critic-a2c">Advantage Actor-Critic (A2C)</a></h3>
<pre><code class="language-python">class ActorCritic(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=64):
        super(ActorCritic, self).__init__()
        
        self.shared = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU()
        )
        
        # Actor (policy)
        self.actor = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim),
            nn.Softmax(dim=-1)
        )
        
        # Critic (value function)
        self.critic = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
    
    def forward(self, x):
        shared_features = self.shared(x)
        action_probs = self.actor(shared_features)
        state_value = self.critic(shared_features)
        return action_probs, state_value

class A2CAgent:
    def __init__(self, state_dim, action_dim, lr=0.001, gamma=0.99):
        self.gamma = gamma
        self.model = ActorCritic(state_dim, action_dim)
        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)
    
    def select_action(self, state):
        """Select action and get state value"""
        state = torch.FloatTensor(state).unsqueeze(0)
        action_probs, state_value = self.model(state)
        action_dist = torch.distributions.Categorical(action_probs)
        action = action_dist.sample()
        
        return action.item(), action_dist.log_prob(action), state_value
    
    def train_step(self, log_prob, value, reward, next_value, done):
        """Single training step"""
        # Calculate advantage
        if done:
            td_target = reward
        else:
            td_target = reward + self.gamma * next_value
        
        advantage = td_target - value
        
        # Actor loss (policy gradient)
        actor_loss = -log_prob * advantage.detach()
        
        # Critic loss (value function)
        critic_loss = F.mse_loss(value, torch.tensor([td_target]))
        
        # Total loss
        loss = actor_loss + critic_loss
        
        # Update
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
        return loss.item()
</code></pre>
<h2 id="multi-armed-bandits"><a class="header" href="#multi-armed-bandits">Multi-Armed Bandits</a></h2>
<p>Simplified RL problem with one state.</p>
<h3 id="epsilon-greedy-bandit"><a class="header" href="#epsilon-greedy-bandit">Epsilon-Greedy Bandit</a></h3>
<pre><code class="language-python">class EpsilonGreedyBandit:
    def __init__(self, n_arms, epsilon=0.1):
        self.n_arms = n_arms
        self.epsilon = epsilon
        self.q_values = np.zeros(n_arms)  # Estimated action values
        self.action_counts = np.zeros(n_arms)  # Number of times each action selected
    
    def select_action(self):
        """Epsilon-greedy action selection"""
        if np.random.random() &lt; self.epsilon:
            return np.random.randint(self.n_arms)
        else:
            return np.argmax(self.q_values)
    
    def update(self, action, reward):
        """Update Q-value estimate"""
        self.action_counts[action] += 1
        alpha = 1 / self.action_counts[action]
        self.q_values[action] += alpha * (reward - self.q_values[action])

# Test bandit
true_rewards = [0.1, 0.5, 0.3, 0.7, 0.2]
bandit = EpsilonGreedyBandit(n_arms=5, epsilon=0.1)

total_reward = 0
for t in range(1000):
    action = bandit.select_action()
    reward = true_rewards[action] + np.random.normal(0, 0.1)
    bandit.update(action, reward)
    total_reward += reward

print(f"True rewards: {true_rewards}")
print(f"Estimated rewards: {bandit.q_values}")
print(f"Total reward: {total_reward:.2f}")
</code></pre>
<h3 id="upper-confidence-bound-ucb"><a class="header" href="#upper-confidence-bound-ucb">Upper Confidence Bound (UCB)</a></h3>
<pre><code class="language-python">class UCBBandit:
    def __init__(self, n_arms, c=2):
        self.n_arms = n_arms
        self.c = c
        self.q_values = np.zeros(n_arms)
        self.action_counts = np.zeros(n_arms)
        self.t = 0
    
    def select_action(self):
        """UCB action selection"""
        self.t += 1
        
        # Select each arm at least once
        if 0 in self.action_counts:
            return np.argmin(self.action_counts)
        
        # UCB formula
        ucb_values = self.q_values + self.c * np.sqrt(np.log(self.t) / self.action_counts)
        return np.argmax(ucb_values)
    
    def update(self, action, reward):
        """Update Q-value estimate"""
        self.action_counts[action] += 1
        alpha = 1 / self.action_counts[action]
        self.q_values[action] += alpha * (reward - self.q_values[action])
</code></pre>
<h2 id="practical-tips"><a class="header" href="#practical-tips">Practical Tips</a></h2>
<ol>
<li><strong>Start Simple</strong>: Begin with simple environments and algorithms</li>
<li><strong>Hyperparameter Tuning</strong>: Learning rate, discount factor, and exploration rate are crucial</li>
<li><strong>Experience Replay</strong>: Store and replay past experiences (covered in deep RL)</li>
<li><strong>Reward Shaping</strong>: Design rewards carefully to guide learning</li>
<li><strong>Exploration vs Exploitation</strong>: Balance is key for good performance</li>
<li><strong>Curriculum Learning</strong>: Start with easy tasks and gradually increase difficulty</li>
</ol>
<h2 id="resources"><a class="header" href="#resources">Resources</a></h2>
<ul>
<li>“Reinforcement Learning: An Introduction” by Sutton and Barto</li>
<li>OpenAI Gym: https://gym.openai.com/</li>
<li>Stable Baselines3: https://stable-baselines3.readthedocs.io/</li>
<li>David Silver’s RL Course: https://www.davidsilver.uk/teaching/</li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../machine_learning/unsupervised_learning.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256 246.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"/></svg></span>
                            </a>

                            <a rel="next prefetch" href="../machine_learning/deep_reinforcement_learning.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M278.6 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L210.7 256 73.4 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"/></svg></span>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../machine_learning/unsupervised_learning.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256 246.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"/></svg></span>
                    </a>

                    <a rel="next prefetch" href="../machine_learning/deep_reinforcement_learning.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M278.6 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L210.7 256 73.4 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"/></svg></span>
                    </a>
            </nav>

        </div>

        <template id=fa-eye><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M288 32c-80.8 0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7 0 24.6C17.3 304 48.6 356 95.4 399.4C142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144s64.5-144 144-144s144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64c-11.5 0-22.3-3-31.6-8.4c-.2 2.8-.4 5.5-.4 8.4c0 53 43 96 96 96s96-43 96-96s-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6z"/></svg></span></template>
        <template id=fa-eye-slash><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M38.8 5.1C28.4-3.1 13.3-1.2 5.1 9.2S-1.2 34.7 9.2 42.9l592 464c10.4 8.2 25.5 6.3 33.7-4.1s6.3-25.5-4.1-33.7L525.6 386.7c39.6-40.6 66.4-86.1 79.9-118.4c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C465.5 68.8 400.8 32 320 32c-68.2 0-125 26.3-169.3 60.8L38.8 5.1zM223.1 149.5C248.6 126.2 282.7 112 320 112c79.5 0 144 64.5 144 144c0 24.9-6.3 48.3-17.4 68.7L408 294.5c5.2-11.8 8-24.8 8-38.5c0-53-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6c0 10.2-2.4 19.8-6.6 28.3l-90.3-70.8zm223.1 298L373 389.9c-16.4 6.5-34.3 10.1-53 10.1c-79.5 0-144-64.5-144-144c0-6.9 .5-13.6 1.4-20.2L83.1 161.5C60.3 191.2 44 220.8 34.5 243.7c-3.3 7.9-3.3 16.7 0 24.6c14.9 35.7 46.2 87.7 93 131.1C174.5 443.2 239.2 480 320 480c47.8 0 89.9-12.9 126.2-32.5z"/></svg></span></template>
        <template id=fa-copy><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M502.6 70.63l-61.25-61.25C435.4 3.371 427.2 0 418.7 0H255.1c-35.35 0-64 28.66-64 64l.0195 256C192 355.4 220.7 384 256 384h192c35.2 0 64-28.8 64-64V93.25C512 84.77 508.6 76.63 502.6 70.63zM464 320c0 8.836-7.164 16-16 16H255.1c-8.838 0-16-7.164-16-16L239.1 64.13c0-8.836 7.164-16 16-16h128L384 96c0 17.67 14.33 32 32 32h47.1V320zM272 448c0 8.836-7.164 16-16 16H63.1c-8.838 0-16-7.164-16-16L47.98 192.1c0-8.836 7.164-16 16-16H160V128H63.99c-35.35 0-64 28.65-64 64l.0098 256C.002 483.3 28.66 512 64 512h192c35.2 0 64-28.8 64-64v-32h-47.1L272 448z"/></svg></span></template>
        <template id=fa-play><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M73 39c-14.8-9.1-33.4-9.4-48.5-.9S0 62.6 0 80V432c0 17.4 9.4 33.4 24.5 41.9s33.7 8.1 48.5-.9L361 297c14.3-8.7 23-24.2 23-41s-8.7-32.2-23-41L73 39z"/></svg></span></template>
        <template id=fa-clock-rotate-left><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M75 75L41 41C25.9 25.9 0 36.6 0 57.9V168c0 13.3 10.7 24 24 24H134.1c21.4 0 32.1-25.9 17-41l-30.8-30.8C155 85.5 203 64 256 64c106 0 192 86 192 192s-86 192-192 192c-40.8 0-78.6-12.7-109.7-34.4c-14.5-10.1-34.4-6.6-44.6 7.9s-6.6 34.4 7.9 44.6C151.2 495 201.7 512 256 512c141.4 0 256-114.6 256-256S397.4 0 256 0C185.3 0 121.3 28.7 75 75zm181 53c-13.3 0-24 10.7-24 24V256c0 6.4 2.5 12.5 7 17l72 72c9.4 9.4 24.6 9.4 33.9 0s9.4-24.6 0-33.9l-65-65V152c0-13.3-10.7-24-24-24z"/></svg></span></template>



        <script>
            window.playground_copyable = true;
        </script>


        <script src="../elasticlunr-ef4e11c1.min.js"></script>
        <script src="../mark-09e88c2c.min.js"></script>
        <script src="../searcher-c2a407aa.js"></script>

        <script src="../clipboard-1626706a.min.js"></script>
        <script src="../highlight-abc7f01d.js"></script>
        <script src="../book-a0b12cfe.js"></script>

        <!-- Custom JS scripts -->



    </div>
    </body>
</html>
