<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Phi - My Notes</title>


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon-de23e50b.svg">
        <link rel="shortcut icon" href="../favicon-8114d1fc.png">
        <link rel="stylesheet" href="../css/variables-8adf115d.css">
        <link rel="stylesheet" href="../css/general-2459343d.css">
        <link rel="stylesheet" href="../css/chrome-ae938929.css">
        <link rel="stylesheet" href="../css/print-9e4910d8.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../fonts/fonts-9644e21d.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="mdbook-highlight-css" href="../highlight-493f70e1.css">
        <link rel="stylesheet" id="mdbook-tomorrow-night-css" href="../tomorrow-night-4c0ae647.css">
        <link rel="stylesheet" id="mdbook-ayu-highlight-css" href="../ayu-highlight-3fdfc3ac.css">

        <!-- Custom theme stylesheets -->

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "../";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "../searchindex-56f85b31.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../toc-a4fa5267.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="mdbook-body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="mdbook-sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("mdbook-sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="mdbook-sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="../toc.html"></iframe>
            </noscript>
            <div id="mdbook-sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="mdbook-page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="mdbook-menu-bar-hover-placeholder"></div>
                <div id="mdbook-menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="mdbook-sidebar-toggle" class="icon-button" for="mdbook-sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="mdbook-sidebar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"/></svg></span>
                        </label>
                        <button id="mdbook-theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="mdbook-theme-list">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M371.3 367.1c27.3-3.9 51.9-19.4 67.2-42.9L600.2 74.1c12.6-19.5 9.4-45.3-7.6-61.2S549.7-4.4 531.1 9.6L294.4 187.2c-24 18-38.2 46.1-38.4 76.1L371.3 367.1zm-19.6 25.4l-116-104.4C175.9 290.3 128 339.6 128 400c0 3.9 .2 7.8 .6 11.6c1.8 17.5-10.2 36.4-27.8 36.4H96c-17.7 0-32 14.3-32 32s14.3 32 32 32H240c61.9 0 112-50.1 112-112c0-2.5-.1-5-.2-7.5z"/></svg></span>
                        </button>
                        <ul id="mdbook-theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-ayu">Ayu</button></li>
                        </ul>
                        <button id="mdbook-search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="mdbook-searchbar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M416 208c0 45.9-14.9 88.3-40 122.7L502.6 457.4c12.5 12.5 12.5 32.8 0 45.3s-32.8 12.5-45.3 0L330.7 376c-34.4 25.2-76.8 40-122.7 40C93.1 416 0 322.9 0 208S93.1 0 208 0S416 93.1 416 208zM208 352c79.5 0 144-64.5 144-144s-64.5-144-144-144S64 128.5 64 208s64.5 144 144 144z"/></svg></span>
                        </button>
                    </div>

                    <h1 class="menu-title">My Notes</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <span class=fa-svg id="print-button"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M128 0C92.7 0 64 28.7 64 64v96h64V64H354.7L384 93.3V160h64V93.3c0-17-6.7-33.3-18.7-45.3L400 18.7C388 6.7 371.7 0 354.7 0H128zM384 352v32 64H128V384 368 352H384zm64 32h32c17.7 0 32-14.3 32-32V256c0-35.3-28.7-64-64-64H64c-35.3 0-64 28.7-64 64v96c0 17.7 14.3 32 32 32H64v64c0 35.3 28.7 64 64 64H384c35.3 0 64-28.7 64-64V384zm-16-88c-13.3 0-24-10.7-24-24s10.7-24 24-24s24 10.7 24 24s-10.7 24-24 24z"/></svg></span>
                        </a>

                    </div>
                </div>

                <div id="mdbook-search-wrapper" class="hidden">
                    <form id="mdbook-searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="mdbook-searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="mdbook-searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <span class=fa-svg id="fa-spin"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M304 48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zm0 416c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM48 304c26.5 0 48-21.5 48-48s-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48zm464-48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM142.9 437c18.7-18.7 18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zm0-294.2c18.7-18.7 18.7-49.1 0-67.9S93.7 56.2 75 75s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zM369.1 437c18.7 18.7 49.1 18.7 67.9 0s18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9z"/></svg></span>
                            </div>
                        </div>
                    </form>
                    <div id="mdbook-searchresults-outer" class="searchresults-outer hidden">
                        <div id="mdbook-searchresults-header" class="searchresults-header"></div>
                        <ul id="mdbook-searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('mdbook-sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('mdbook-sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#mdbook-sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="mdbook-content" class="content">
                    <main>
                        <h1 id="microsoft-phi-models"><a class="header" href="#microsoft-phi-models">Microsoft Phi Models</a></h1>
<h2 id="overview"><a class="header" href="#overview">Overview</a></h2>
<p>Microsoft’s Phi model family represents a series of Small Language Models (SLMs) that deliver strong performance relative to their size, particularly excelling in reasoning-focused tasks. The Phi models are distinguished by their focus on data quality, strategic use of synthetic data, and efficient architecture that enables deployment on edge devices and local environments.</p>
<p><strong>Key Characteristics:</strong></p>
<ul>
<li>Small model sizes (3.8B to 14B parameters)</li>
<li>Strong reasoning capabilities despite compact size</li>
<li>Open source under MIT license</li>
<li>Optimized for on-device deployment</li>
<li>No cloud connectivity required for inference</li>
</ul>
<h2 id="model-family"><a class="header" href="#model-family">Model Family</a></h2>
<h3 id="phi-4-latest---december-2024"><a class="header" href="#phi-4-latest---december-2024">Phi-4 (Latest - December 2024)</a></h3>
<p><strong>Phi-4 (14B parameters)</strong></p>
<ul>
<li>Architecture: Decoder-only transformer</li>
<li>Parameters: 14 billion</li>
<li>Default context length: 4096 tokens</li>
<li>Extended context: 16K tokens (during midtraining)</li>
<li>Focus: Complex reasoning and mathematical tasks</li>
<li>Training: Centrally focused on data quality with strategic synthetic data incorporation</li>
<li>Performance: Strong performance on reasoning benchmarks relative to size</li>
</ul>
<p><strong>Phi-4-mini (3.8B parameters)</strong></p>
<ul>
<li>Dense, decoder-only transformer</li>
<li>Grouped-query attention mechanism</li>
<li>Vocabulary size: 200,000 tokens</li>
<li>Shared input-output embeddings</li>
<li>Optimized for: Speed and efficiency</li>
<li>Ideal for: Resource-constrained environments</li>
</ul>
<p><strong>Phi-4-multimodal (5.6B parameters)</strong></p>
<ul>
<li>Unified architecture integrating: Speech, Vision, Text</li>
<li>Top performer on Huggingface OpenASR leaderboard (WER: 6.14% as of Feb 2025)</li>
<li>Previous best: 6.5%</li>
<li>Use cases: Multi-modal applications requiring speech and vision understanding</li>
</ul>
<h3 id="phi-3-family"><a class="header" href="#phi-3-family">Phi-3 Family</a></h3>
<p><strong>Phi-3-mini (3.8B parameters)</strong></p>
<ul>
<li>Baseline small model</li>
<li>Optimized for mobile and edge deployment</li>
<li>Capable of running on phones</li>
</ul>
<p><strong>Phi-3-small</strong></p>
<ul>
<li>Hybrid attention mechanism:
<ul>
<li>Alternating dense attention layers</li>
<li>Blocksparse attention layers</li>
</ul>
</li>
<li>Optimizes KV cache savings</li>
<li>Maintains long context retrieval performance</li>
</ul>
<p><strong>Phi-3-medium (14B parameters)</strong></p>
<ul>
<li>Same tokenizer and architecture as Phi-3-mini</li>
<li>Architecture specs:
<ul>
<li>40 attention heads</li>
<li>40 layers</li>
<li>Embedding dimension: 5120</li>
</ul>
</li>
<li>Enhanced capacity for complex tasks</li>
</ul>
<p><strong>Phi-3-MoE (Mixture of Experts)</strong></p>
<ul>
<li>Activated parameters: 6.6B</li>
<li>Total parameters: 42B</li>
<li>Routing: Top-2 among 16 expert networks</li>
<li>Expert architecture: Separate GLU networks</li>
<li>Efficiency: Sparse activation enables large capacity with moderate compute</li>
</ul>
<h2 id="architecture-details"><a class="header" href="#architecture-details">Architecture Details</a></h2>
<h3 id="core-architecture"><a class="header" href="#core-architecture">Core Architecture</a></h3>
<pre><code>Model Type: Decoder-only Transformer
Training Recipe:
├── High-quality curated data
├── Strategic synthetic data generation
├── Multi-stage training curriculum
└── Advanced post-training techniques

Key Features:
├── Grouped Query Attention (GQA)
├── Efficient KV cache management
├── Optimized tokenizer (200K vocabulary for Phi-4-mini)
└── Shared input-output embeddings
</code></pre>
<h3 id="attention-mechanisms"><a class="header" href="#attention-mechanisms">Attention Mechanisms</a></h3>
<p><strong>Standard Dense Attention</strong> (Phi-4, Phi-3-mini, Phi-3-medium)</p>
<ul>
<li>Full attention across all positions</li>
<li>Standard transformer architecture</li>
<li>Grouped-query attention for efficiency</li>
</ul>
<p><strong>Hybrid Attention</strong> (Phi-3-small)</p>
<ul>
<li>Alternates between dense and blocksparse layers</li>
<li>Reduces memory footprint</li>
<li>Maintains performance on long sequences</li>
</ul>
<p><strong>MoE Architecture</strong> (Phi-3-MoE)</p>
<ul>
<li>16 expert networks with top-2 routing</li>
<li>Each token processed by 2 of 16 experts</li>
<li>Sparse activation reduces compute requirements</li>
</ul>
<h2 id="fine-tuning"><a class="header" href="#fine-tuning">Fine-Tuning</a></h2>
<h3 id="when-to-fine-tune"><a class="header" href="#when-to-fine-tune">When to Fine-Tune</a></h3>
<p>Fine-tune Phi models when:</p>
<ol>
<li>Domain-specific language or terminology is required</li>
<li>Task-specific behavior needs optimization</li>
<li>Custom instruction following is needed</li>
<li>Adapting to proprietary data or workflows</li>
<li>Improving performance on specific benchmark tasks</li>
</ol>
<h3 id="fine-tuning-approaches"><a class="header" href="#fine-tuning-approaches">Fine-Tuning Approaches</a></h3>
<h4 id="1-full-fine-tuning"><a class="header" href="#1-full-fine-tuning">1. Full Fine-Tuning</a></h4>
<ul>
<li>Updates all model parameters</li>
<li>Highest accuracy potential</li>
<li>Requires significant compute resources</li>
<li>Memory intensive</li>
</ul>
<h4 id="2-lora-low-rank-adaptation"><a class="header" href="#2-lora-low-rank-adaptation">2. LoRA (Low-Rank Adaptation)</a></h4>
<ul>
<li>Adds trainable low-rank matrices to attention layers</li>
<li>Freezes base model weights</li>
<li>Memory efficient</li>
<li>Recommended approach for most use cases</li>
</ul>
<h4 id="3-qlora-quantized-lora"><a class="header" href="#3-qlora-quantized-lora">3. QLoRA (Quantized LoRA)</a></h4>
<ul>
<li>Combines 4-bit quantization with LoRA</li>
<li>Quantizes base model to 4-bit</li>
<li>Trains only LoRA adapters in higher precision</li>
<li>Minimal memory footprint</li>
<li>Ideal for consumer GPUs</li>
</ul>
<h3 id="lora-configuration-best-practices"><a class="header" href="#lora-configuration-best-practices">LoRA Configuration Best Practices</a></h3>
<h4 id="rank-and-alpha-settings"><a class="header" href="#rank-and-alpha-settings">Rank and Alpha Settings</a></h4>
<pre><code class="language-python">from peft import LoraConfig, get_peft_model

lora_config = LoraConfig(
    r=16,                    # LoRA rank (8-16 is sufficient baseline)
    lora_alpha=16,           # Alpha = rank for small datasets
    target_modules=[
        "q_proj",            # Query projection
        "k_proj",            # Key projection
        "v_proj",            # Value projection
        "o_proj",            # Output projection
        "gate_proj",         # MLP gate
        "down_proj",         # MLP down
        "up_proj"            # MLP up
    ],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)
</code></pre>
<p><strong>Key Guidelines:</strong></p>
<ul>
<li><strong>Rank</strong>: 8-16 is sufficient for most tasks (higher ranks not necessarily better)</li>
<li><strong>Alpha</strong>: Set <code>alpha = rank</code> for small datasets</li>
<li><strong>Avoid</strong>: Using <code>2*rank</code> or <code>4*rank</code> on small datasets (often unstable)</li>
<li><strong>Target Modules</strong>: Include all attention and MLP projection layers</li>
</ul>
<h4 id="phi-2-specific-configuration"><a class="header" href="#phi-2-specific-configuration">Phi-2 Specific Configuration</a></h4>
<pre><code class="language-python"># Phi-2 uses Wqkv instead of separate q/k/v projections
lora_config_phi2 = LoraConfig(
    r=16,
    lora_alpha=16,
    target_modules=["Wqkv", "out_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)
</code></pre>
<h3 id="training-hyperparameters"><a class="header" href="#training-hyperparameters">Training Hyperparameters</a></h3>
<h4 id="learning-rate"><a class="header" href="#learning-rate">Learning Rate</a></h4>
<pre><code class="language-python">from transformers import TrainingArguments

training_args = TrainingArguments(
    learning_rate=2e-5,              # Start conservative
    lr_scheduler_type="constant",     # Constant schedule works well
    warmup_steps=100,
    max_steps=1000,
    # Alternative learning rates to try:
    # 5e-5: More aggressive
    # 8e-4: Maximum recommended for LoRA
)
</code></pre>
<p><strong>Guidelines:</strong></p>
<ul>
<li><strong>DO NOT</strong> use high learning rates (1e-3, 2e-4) with LoRA</li>
<li><strong>Recommended range</strong>: 2e-5 to 8e-4</li>
<li><strong>Start with</strong>: 2e-5 or 5e-5 for safety</li>
<li><strong>Schedule</strong>: Constant learning rate (per QLoRA author Tim Dettmers)</li>
<li><strong>Warmup</strong>: 100-500 steps helps stabilization</li>
</ul>
<h4 id="precision-and-memory-management"><a class="header" href="#precision-and-memory-management">Precision and Memory Management</a></h4>
<pre><code class="language-python">from transformers import TrainingArguments, BitsAndBytesConfig
import torch

# Use bfloat16 for training (NOT fp16)
training_args = TrainingArguments(
    bf16=True,                        # Use bfloat16
    fp16=False,                       # Avoid fp16 (causes NaN errors)
    gradient_checkpointing=True,      # Reduce memory usage
    gradient_accumulation_steps=4,    # Effective batch size = batch * accum
    per_device_train_batch_size=1,    # Adjust based on memory
    optim="paged_adamw_8bit",        # Memory-efficient optimizer
)

# QLoRA quantization config
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)
</code></pre>
<p><strong>Key Points:</strong></p>
<ul>
<li><strong>Use bfloat16</strong>: Better dynamic range, fewer NaN issues than fp16</li>
<li><strong>Avoid fp16</strong>: Known to cause NaN errors with Phi-2</li>
<li><strong>Gradient Checkpointing</strong>: Trades compute for memory</li>
<li><strong>Gradient Accumulation</strong>: Simulates larger batch sizes</li>
<li><strong>Optimizer Choices</strong>:
<ul>
<li><code>paged_adamw_8bit</code>: Best balance (recommended)</li>
<li><code>adamw_torch</code>: Standard but memory intensive</li>
<li><code>sgd</code>: Memory efficient but slower convergence</li>
</ul>
</li>
</ul>
<h4 id="batch-size-strategy"><a class="header" href="#batch-size-strategy">Batch Size Strategy</a></h4>
<pre><code class="language-python"># Strategy 1: Small batch with gradient accumulation
per_device_train_batch_size = 1
gradient_accumulation_steps = 8
# Effective batch size = 1 * 8 = 8

# Strategy 2: Larger batch if memory allows
per_device_train_batch_size = 4
gradient_accumulation_steps = 2
# Effective batch size = 4 * 2 = 8
</code></pre>
<p><strong>Considerations:</strong></p>
<ul>
<li>Check GPU memory with long context lengths (4K, 8K tokens)</li>
<li>OOM errors common with large context + large batch</li>
<li>Use gradient checkpointing if memory constrained</li>
<li>Monitor actual GPU utilization</li>
</ul>
<h3 id="complete-fine-tuning-example"><a class="header" href="#complete-fine-tuning-example">Complete Fine-Tuning Example</a></h3>
<pre><code class="language-python">from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    BitsAndBytesConfig
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from datasets import load_dataset
import torch

# 1. Quantization configuration
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

# 2. Load model and tokenizer
model_id = "microsoft/phi-4"
tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True
)

# 3. Prepare model for k-bit training
model = prepare_model_for_kbit_training(model)

# 4. Configure LoRA
lora_config = LoraConfig(
    r=16,
    lora_alpha=16,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                   "gate_proj", "down_proj", "up_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

# 5. Prepare dataset
dataset = load_dataset("your-dataset")

def preprocess_function(examples):
    return tokenizer(examples["text"], truncation=True, max_length=2048)

tokenized_dataset = dataset.map(preprocess_function, batched=True)

# 6. Training arguments
training_args = TrainingArguments(
    output_dir="./phi-4-finetuned",
    learning_rate=2e-5,
    lr_scheduler_type="constant",
    warmup_steps=100,
    max_steps=1000,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=8,
    gradient_checkpointing=True,
    bf16=True,
    logging_steps=10,
    save_steps=100,
    save_total_limit=3,
    optim="paged_adamw_8bit",
    report_to="tensorboard"
)

# 7. Train
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"],
)

trainer.train()

# 8. Save
model.save_pretrained("./phi-4-lora-adapters")
tokenizer.save_pretrained("./phi-4-lora-adapters")
</code></pre>
<h3 id="data-preparation-best-practices"><a class="header" href="#data-preparation-best-practices">Data Preparation Best Practices</a></h3>
<h4 id="dataset-format"><a class="header" href="#dataset-format">Dataset Format</a></h4>
<pre><code class="language-python"># Instruction-following format
data = [
    {
        "instruction": "Explain quantum computing",
        "input": "",
        "output": "Quantum computing is..."
    },
    {
        "instruction": "Translate to French",
        "input": "Hello, how are you?",
        "output": "Bonjour, comment allez-vous?"
    }
]

# Convert to prompt template
def format_instruction(sample):
    return f"""### Instruction:
{sample['instruction']}

### Input:
{sample['input']}

### Response:
{sample['output']}"""

# Apply to dataset
formatted_data = [format_instruction(item) for item in data]
</code></pre>
<h4 id="dataset-size-guidelines"><a class="header" href="#dataset-size-guidelines">Dataset Size Guidelines</a></h4>
<ul>
<li><strong>Minimum</strong>: 100-500 high-quality examples</li>
<li><strong>Optimal</strong>: 1,000-10,000 examples for specialized tasks</li>
<li><strong>Large-scale</strong>: 10,000+ for broad domain adaptation</li>
</ul>
<p><strong>Quality over Quantity:</strong></p>
<ul>
<li>Clean, well-formatted data is critical</li>
<li>Remove duplicates and low-quality samples</li>
<li>Balance class distributions</li>
<li>Include diverse examples</li>
</ul>
<h2 id="common-patterns"><a class="header" href="#common-patterns">Common Patterns</a></h2>
<h3 id="1-prompt-engineering"><a class="header" href="#1-prompt-engineering">1. Prompt Engineering</a></h3>
<h4 id="basic-completion"><a class="header" href="#basic-completion">Basic Completion</a></h4>
<pre><code class="language-python">from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("microsoft/phi-4")
tokenizer = AutoTokenizer.from_pretrained("microsoft/phi-4")

prompt = "Explain the concept of recursion in programming:"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_length=200)
print(tokenizer.decode(outputs[0]))
</code></pre>
<h4 id="instruction-following"><a class="header" href="#instruction-following">Instruction-Following</a></h4>
<pre><code class="language-python">instruction_prompt = """### Instruction:
Write a Python function to calculate the Fibonacci sequence.

### Response:"""

inputs = tokenizer(instruction_prompt, return_tensors="pt")
outputs = model.generate(
    **inputs,
    max_length=500,
    temperature=0.7,
    top_p=0.9,
    do_sample=True
)
</code></pre>
<h4 id="few-shot-learning"><a class="header" href="#few-shot-learning">Few-Shot Learning</a></h4>
<pre><code class="language-python">few_shot_prompt = """Classify the sentiment of movie reviews.

Review: "This movie was amazing! Best film of the year."
Sentiment: Positive

Review: "Terrible acting and boring plot."
Sentiment: Negative

Review: "It was okay, nothing special."
Sentiment: Neutral

Review: "Absolutely loved every minute of it!"
Sentiment:"""

# Model continues with prediction
</code></pre>
<h3 id="2-chain-of-thought-reasoning"><a class="header" href="#2-chain-of-thought-reasoning">2. Chain-of-Thought Reasoning</a></h3>
<pre><code class="language-python">cot_prompt = """Solve this math problem step by step:

Problem: If a train travels 120 miles in 2 hours, and then 180 miles in 3 hours, what is its average speed for the entire journey?

Let's solve this step by step:
"""

# Phi-4 excels at mathematical reasoning with CoT prompts
</code></pre>
<h3 id="3-context-window-management"><a class="header" href="#3-context-window-management">3. Context Window Management</a></h3>
<pre><code class="language-python"># For long documents, use sliding window approach
def process_long_document(text, chunk_size=3000, overlap=500):
    chunks = []
    for i in range(0, len(text), chunk_size - overlap):
        chunk = text[i:i + chunk_size]
        chunks.append(chunk)

    results = []
    for chunk in chunks:
        prompt = f"Summarize the following text:\n\n{chunk}"
        # Process each chunk
        results.append(generate(prompt))

    return results
</code></pre>
<h3 id="4-multi-modal-applications-phi-4-multimodal"><a class="header" href="#4-multi-modal-applications-phi-4-multimodal">4. Multi-Modal Applications (Phi-4-multimodal)</a></h3>
<pre><code class="language-python"># Phi-4-multimodal supports vision, speech, and text
from transformers import AutoProcessor, AutoModelForVision2Seq

model = AutoModelForVision2Seq.from_pretrained("microsoft/phi-4-multimodal")
processor = AutoProcessor.from_pretrained("microsoft/phi-4-multimodal")

# Image + Text
image = load_image("path/to/image.jpg")
prompt = "Describe what you see in this image"
inputs = processor(text=prompt, images=image, return_tensors="pt")
outputs = model.generate(**inputs)

# Speech + Text
audio = load_audio("path/to/audio.wav")
prompt = "Transcribe this audio"
inputs = processor(text=prompt, audio=audio, return_tensors="pt")
outputs = model.generate(**inputs)
</code></pre>
<h3 id="5-retrieval-augmented-generation-rag"><a class="header" href="#5-retrieval-augmented-generation-rag">5. Retrieval-Augmented Generation (RAG)</a></h3>
<pre><code class="language-python">from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.llms import HuggingFacePipeline

# Setup vector store
embeddings = HuggingFaceEmbeddings()
vectorstore = FAISS.from_documents(documents, embeddings)

# Setup Phi model
phi_pipeline = HuggingFacePipeline.from_model_id(
    model_id="microsoft/phi-4",
    task="text-generation",
    device=0
)

# RAG pipeline
def rag_query(question):
    # Retrieve relevant context
    docs = vectorstore.similarity_search(question, k=3)
    context = "\n".join([doc.page_content for doc in docs])

    # Generate answer with context
    prompt = f"""Use the following context to answer the question.

Context:
{context}

Question: {question}

Answer:"""

    return phi_pipeline(prompt)
</code></pre>
<h3 id="6-streaming-generation"><a class="header" href="#6-streaming-generation">6. Streaming Generation</a></h3>
<pre><code class="language-python">from transformers import TextIteratorStreamer
from threading import Thread

streamer = TextIteratorStreamer(tokenizer, skip_special_tokens=True)

# Generate in separate thread
generation_kwargs = dict(
    inputs=input_ids,
    streamer=streamer,
    max_length=500,
    temperature=0.7
)

thread = Thread(target=model.generate, kwargs=generation_kwargs)
thread.start()

# Stream output
for text in streamer:
    print(text, end="", flush=True)

thread.join()
</code></pre>
<h2 id="operations"><a class="header" href="#operations">Operations</a></h2>
<h3 id="deployment-options"><a class="header" href="#deployment-options">Deployment Options</a></h3>
<h4 id="1-local-deployment-pytorch"><a class="header" href="#1-local-deployment-pytorch">1. Local Deployment (PyTorch)</a></h4>
<pre><code class="language-python">from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Standard loading
model = AutoModelForCausalLM.from_pretrained(
    "microsoft/phi-4",
    torch_dtype=torch.float16,
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("microsoft/phi-4")

# Inference
def generate_response(prompt, max_length=200):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(
        **inputs,
        max_length=max_length,
        temperature=0.7,
        top_p=0.9,
        do_sample=True
    )
    return tokenizer.decode(outputs[0], skip_special_tokens=True)
</code></pre>
<h4 id="2-quantized-deployment"><a class="header" href="#2-quantized-deployment">2. Quantized Deployment</a></h4>
<pre><code class="language-python">from transformers import BitsAndBytesConfig

# 4-bit quantization
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

model = AutoModelForCausalLM.from_pretrained(
    "microsoft/phi-4",
    quantization_config=bnb_config,
    device_map="auto"
)

# Memory usage:
# - Phi-2 unquantized: ~6.5GB VRAM
# - Phi-2 4-bit NF4: ~2.1GB loading, ~5GB during inference
# - Phi-4 unquantized: ~14.96GB
# - Phi-4 4-bit: ~5.42GB (~64% reduction)
</code></pre>
<h4 id="3-gguf-format-llamacpp"><a class="header" href="#3-gguf-format-llamacpp">3. GGUF Format (llama.cpp)</a></h4>
<pre><code class="language-bash"># Download GGUF quantized models
# Available quantizations: Q2_K, Q4_K, Q6_K, Q8_0

# Using llama.cpp
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
make

# Run inference
./main -m phi-2-Q4_K.gguf -p "Your prompt here" -n 200
</code></pre>
<h4 id="4-onnx-runtime"><a class="header" href="#4-onnx-runtime">4. ONNX Runtime</a></h4>
<pre><code class="language-python">from optimum.onnxruntime import ORTModelForCausalLM

# Export to ONNX
model = ORTModelForCausalLM.from_pretrained(
    "microsoft/phi-4",
    export=True
)

# Optimized inference
model.save_pretrained("phi-4-onnx")
</code></pre>
<h4 id="5-mobileedge-deployment"><a class="header" href="#5-mobileedge-deployment">5. Mobile/Edge Deployment</a></h4>
<pre><code class="language-python"># Platform-specific optimizations:

# Intel OpenVINO (x86 processors)
# - Supports INT4, INT8, FP16, FP32
# - Optimized for Intel CPUs and GPUs

# Qualcomm QNN (Snapdragon)
# - Optimized for mobile ARM processors
# - Hardware acceleration support

# Apple MLX (Apple Silicon)
# - Native M1/M2/M3 optimization
# - Metal acceleration

# NVIDIA CUDA (NVIDIA GPUs)
# - Full GPU acceleration
# - TensorRT optimization
</code></pre>
<h3 id="inference-optimization"><a class="header" href="#inference-optimization">Inference Optimization</a></h3>
<h4 id="1-batch-processing"><a class="header" href="#1-batch-processing">1. Batch Processing</a></h4>
<pre><code class="language-python"># Process multiple prompts efficiently
prompts = [
    "Translate to Spanish: Hello world",
    "Summarize: The quick brown fox...",
    "Calculate: 15 * 24"
]

# Batch tokenization
inputs = tokenizer(prompts, return_tensors="pt", padding=True)

# Batch generation
outputs = model.generate(
    **inputs,
    max_length=100,
    pad_token_id=tokenizer.pad_token_id
)

# Decode all
results = [tokenizer.decode(out, skip_special_tokens=True) for out in outputs]
</code></pre>
<h4 id="2-kv-cache-optimization"><a class="header" href="#2-kv-cache-optimization">2. KV Cache Optimization</a></h4>
<pre><code class="language-python"># Enable past_key_values caching for faster generation
def generate_with_cache(prompt, max_new_tokens=100):
    inputs = tokenizer(prompt, return_tensors="pt")

    # First token
    outputs = model(**inputs, use_cache=True)
    past_key_values = outputs.past_key_values
    next_token = outputs.logits[:, -1:].argmax(dim=-1)

    generated = [next_token.item()]

    # Subsequent tokens use cache
    for _ in range(max_new_tokens - 1):
        outputs = model(
            input_ids=next_token,
            past_key_values=past_key_values,
            use_cache=True
        )
        past_key_values = outputs.past_key_values
        next_token = outputs.logits[:, -1:].argmax(dim=-1)
        generated.append(next_token.item())

        if next_token == tokenizer.eos_token_id:
            break

    return tokenizer.decode(generated)
</code></pre>
<h4 id="3-flash-attention"><a class="header" href="#3-flash-attention">3. Flash Attention</a></h4>
<pre><code class="language-python"># Use Flash Attention 2 for faster inference
model = AutoModelForCausalLM.from_pretrained(
    "microsoft/phi-4",
    torch_dtype=torch.float16,
    attn_implementation="flash_attention_2",  # Requires flash-attn package
    device_map="auto"
)

# Significant speedup for long sequences
</code></pre>
<h4 id="4-speculative-decoding"><a class="header" href="#4-speculative-decoding">4. Speculative Decoding</a></h4>
<pre><code class="language-python"># Use smaller model for draft, larger for verification
draft_model = AutoModelForCausalLM.from_pretrained("microsoft/phi-3-mini")
target_model = AutoModelForCausalLM.from_pretrained("microsoft/phi-4")

# Can achieve 2-3x speedup with quality preservation
</code></pre>
<h3 id="quantization-deep-dive"><a class="header" href="#quantization-deep-dive">Quantization Deep Dive</a></h3>
<h4 id="quantization-methods"><a class="header" href="#quantization-methods">Quantization Methods</a></h4>
<p><strong>1. Post-Training Quantization (PTQ)</strong></p>
<ul>
<li>No retraining required</li>
<li>Quick conversion process</li>
<li>Slight accuracy degradation</li>
<li>Methods: Dynamic, Static, Weight-only</li>
</ul>
<p><strong>2. Quantization-Aware Training (QAT)</strong></p>
<ul>
<li>Retrains with quantization in mind</li>
<li>Better accuracy preservation</li>
<li>Longer process</li>
<li>More compute intensive</li>
</ul>
<h4 id="quantization-formats-comparison"><a class="header" href="#quantization-formats-comparison">Quantization Formats Comparison</a></h4>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Format</th><th>Bits</th><th>Size (Phi-4)</th><th>Speed</th><th>Quality</th><th>Use Case</th></tr>
</thead>
<tbody>
<tr><td>FP32</td><td>32</td><td>~56GB</td><td>Baseline</td><td>Best</td><td>Training</td></tr>
<tr><td>FP16</td><td>16</td><td>~28GB</td><td>1.5-2x</td><td>Excellent</td><td>GPU inference</td></tr>
<tr><td>BF16</td><td>16</td><td>~28GB</td><td>1.5-2x</td><td>Excellent</td><td>Training &amp; inference</td></tr>
<tr><td>INT8</td><td>8</td><td>~14GB</td><td>2-3x</td><td>Very Good</td><td>Production</td></tr>
<tr><td>NF4</td><td>4</td><td>~5.4GB</td><td>1.3x*</td><td>Good</td><td>Memory-constrained</td></tr>
<tr><td>Q4_K</td><td>4</td><td>~5.5GB</td><td>2-4x</td><td>Good</td><td>Edge devices</td></tr>
<tr><td>Q2_K</td><td>2</td><td>~3GB</td><td>3-5x</td><td>Fair</td><td>Extreme edge</td></tr>
</tbody>
</table>
</div>
<p>*4-bit inference slower than FP16 but enables larger models in limited memory</p>
<h4 id="implementation-examples"><a class="header" href="#implementation-examples">Implementation Examples</a></h4>
<p><strong>BitsAndBytes Quantization</strong></p>
<pre><code class="language-python">from transformers import BitsAndBytesConfig
import torch

# 8-bit quantization
config_8bit = BitsAndBytesConfig(load_in_8bit=True)

# 4-bit quantization (NF4)
config_4bit = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,      # Double quantization
    bnb_4bit_quant_type="nf4",            # NormalFloat4
    bnb_4bit_compute_dtype=torch.bfloat16 # Compute dtype
)

model = AutoModelForCausalLM.from_pretrained(
    "microsoft/phi-4",
    quantization_config=config_4bit,
    device_map="auto"
)
</code></pre>
<p><strong>GPTQ Quantization (Auto-Round)</strong></p>
<pre><code class="language-python">from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig

# Configure quantization
quantize_config = BaseQuantizeConfig(
    bits=4,
    group_size=128,
    desc_act=False
)

# Load and quantize
model = AutoGPTQForCausalLM.from_pretrained(
    "microsoft/phi-4",
    quantize_config=quantize_config
)

# Quantize with calibration data
model.quantize(calibration_data)

# Save
model.save_quantized("phi-4-gptq")
</code></pre>
<p><strong>AWQ Quantization</strong></p>
<pre><code class="language-python">from awq import AutoAWQForCausalLM

# Load model
model = AutoAWQForCausalLM.from_pretrained("microsoft/phi-4")

# Quantize
model.quantize(tokenizer, quant_config={"zero_point": True, "q_group_size": 128})

# Save
model.save_quantized("phi-4-awq")
</code></pre>
<h3 id="performance-benchmarks"><a class="header" href="#performance-benchmarks">Performance Benchmarks</a></h3>
<h4 id="decoding-speed-phi-2"><a class="header" href="#decoding-speed-phi-2">Decoding Speed (Phi-2)</a></h4>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Configuration</th><th>Tokens/Second</th><th>Memory (VRAM)</th></tr>
</thead>
<tbody>
<tr><td>FP16</td><td>21</td><td>6.5GB</td></tr>
<tr><td>4-bit NF4</td><td>15.7</td><td>2.1GB (load), 5GB (inference)</td></tr>
</tbody>
</table>
</div>
<h4 id="memory-footprint-phi-4"><a class="header" href="#memory-footprint-phi-4">Memory Footprint (Phi-4)</a></h4>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Configuration</th><th>Memory</th><th>Reduction</th></tr>
</thead>
<tbody>
<tr><td>Unquantized</td><td>14.96GB</td><td>-</td></tr>
<tr><td>4-bit</td><td>5.42GB</td><td>64%</td></tr>
<tr><td>2-bit</td><td>~3GB</td><td>80%</td></tr>
</tbody>
</table>
</div>
<h3 id="serving-architecture"><a class="header" href="#serving-architecture">Serving Architecture</a></h3>
<h4 id="single-model-serving"><a class="header" href="#single-model-serving">Single Model Serving</a></h4>
<pre><code class="language-python">from fastapi import FastAPI
from pydantic import BaseModel
import uvicorn

app = FastAPI()
model = AutoModelForCausalLM.from_pretrained("microsoft/phi-4")
tokenizer = AutoTokenizer.from_pretrained("microsoft/phi-4")

class GenerationRequest(BaseModel):
    prompt: str
    max_length: int = 200
    temperature: float = 0.7

@app.post("/generate")
async def generate(request: GenerationRequest):
    inputs = tokenizer(request.prompt, return_tensors="pt")
    outputs = model.generate(
        **inputs,
        max_length=request.max_length,
        temperature=request.temperature
    )
    text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return {"generated_text": text}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
</code></pre>
<h4 id="vllm-high-performance-serving"><a class="header" href="#vllm-high-performance-serving">vLLM High-Performance Serving</a></h4>
<pre><code class="language-python">from vllm import LLM, SamplingParams

# Initialize with PagedAttention
llm = LLM(
    model="microsoft/phi-4",
    tensor_parallel_size=1,
    dtype="half",
    max_model_len=4096
)

# Sampling parameters
sampling_params = SamplingParams(
    temperature=0.7,
    top_p=0.9,
    max_tokens=200
)

# Batch inference
prompts = ["Prompt 1", "Prompt 2", "Prompt 3"]
outputs = llm.generate(prompts, sampling_params)

for output in outputs:
    print(output.outputs[0].text)
</code></pre>
<h4 id="text-generation-inference-tgi"><a class="header" href="#text-generation-inference-tgi">Text Generation Inference (TGI)</a></h4>
<pre><code class="language-bash"># Run with Docker
docker run --gpus all \
  -p 8080:80 \
  -v $(pwd)/data:/data \
  ghcr.io/huggingface/text-generation-inference:latest \
  --model-id microsoft/phi-4 \
  --max-total-tokens 4096 \
  --max-input-length 3584

# Query the endpoint
curl http://localhost:8080/generate \
  -X POST \
  -d '{"inputs":"What is machine learning?","parameters":{"max_new_tokens":200}}' \
  -H 'Content-Type: application/json'
</code></pre>
<h3 id="monitoring-and-evaluation"><a class="header" href="#monitoring-and-evaluation">Monitoring and Evaluation</a></h3>
<h4 id="performance-metrics"><a class="header" href="#performance-metrics">Performance Metrics</a></h4>
<pre><code class="language-python">import time
import torch

def benchmark_model(model, tokenizer, prompt, runs=10):
    # Warmup
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    _ = model.generate(**inputs, max_length=100)

    # Benchmark
    times = []
    for _ in range(runs):
        torch.cuda.synchronize() if torch.cuda.is_available() else None
        start = time.time()

        outputs = model.generate(**inputs, max_length=100)

        torch.cuda.synchronize() if torch.cuda.is_available() else None
        end = time.time()

        times.append(end - start)

    avg_time = sum(times) / len(times)
    tokens_generated = len(outputs[0]) - len(inputs.input_ids[0])
    tokens_per_sec = tokens_generated / avg_time

    return {
        "avg_time": avg_time,
        "tokens_generated": tokens_generated,
        "tokens_per_second": tokens_per_sec
    }

# Run benchmark
results = benchmark_model(model, tokenizer, "Explain quantum computing:")
print(f"Average generation time: {results['avg_time']:.2f}s")
print(f"Tokens per second: {results['tokens_per_second']:.2f}")
</code></pre>
<h4 id="quality-evaluation"><a class="header" href="#quality-evaluation">Quality Evaluation</a></h4>
<pre><code class="language-python">from evaluate import load

# Perplexity
perplexity = load("perplexity")
results = perplexity.compute(predictions=predictions, model_id="microsoft/phi-4")

# BLEU score (for translation tasks)
bleu = load("bleu")
results = bleu.compute(predictions=predictions, references=references)

# ROUGE score (for summarization)
rouge = load("rouge")
results = rouge.compute(predictions=predictions, references=references)
</code></pre>
<h2 id="advanced-techniques"><a class="header" href="#advanced-techniques">Advanced Techniques</a></h2>
<h3 id="1-model-merging"><a class="header" href="#1-model-merging">1. Model Merging</a></h3>
<pre><code class="language-python">from transformers import AutoModelForCausalLM
import torch

# Load base and fine-tuned models
base_model = AutoModelForCausalLM.from_pretrained("microsoft/phi-4")
ft_model = AutoModelForCausalLM.from_pretrained("./phi-4-finetuned")

# Merge with weighted average
alpha = 0.7  # Weight for fine-tuned model

for name, param in base_model.named_parameters():
    if name in ft_model.state_dict():
        param.data = alpha * ft_model.state_dict()[name] + (1 - alpha) * param.data

base_model.save_pretrained("./phi-4-merged")
</code></pre>
<h3 id="2-multi-adapter-loading"><a class="header" href="#2-multi-adapter-loading">2. Multi-Adapter Loading</a></h3>
<pre><code class="language-python">from peft import PeftModel

# Load base model
base_model = AutoModelForCausalLM.from_pretrained("microsoft/phi-4")

# Load multiple adapters
model_with_adapter1 = PeftModel.from_pretrained(base_model, "./adapter1")
model_with_adapter2 = PeftModel.from_pretrained(base_model, "./adapter2")

# Switch between adapters dynamically
def generate_with_adapter(prompt, adapter_name):
    if adapter_name == "adapter1":
        model = model_with_adapter1
    else:
        model = model_with_adapter2

    # Generate
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(**inputs)
    return tokenizer.decode(outputs[0])
</code></pre>
<h3 id="3-constrained-generation"><a class="header" href="#3-constrained-generation">3. Constrained Generation</a></h3>
<pre><code class="language-python">from transformers import LogitsProcessor

class ForceWordsLogitsProcessor(LogitsProcessor):
    def __init__(self, force_word_ids):
        self.force_word_ids = force_word_ids

    def __call__(self, input_ids, scores):
        if len(input_ids[0]) in self.force_word_ids:
            word_ids = self.force_word_ids[len(input_ids[0])]
            mask = torch.full_like(scores, float('-inf'))
            mask[:, word_ids] = 0
            scores = scores + mask
        return scores

# Use in generation
logits_processor = LogitsProcessorList([
    ForceWordsLogitsProcessor({5: [tokenizer.encode("yes")[0]]})
])

outputs = model.generate(
    **inputs,
    logits_processor=logits_processor
)
</code></pre>
<h2 id="troubleshooting"><a class="header" href="#troubleshooting">Troubleshooting</a></h2>
<h3 id="common-issues-and-solutions"><a class="header" href="#common-issues-and-solutions">Common Issues and Solutions</a></h3>
<h4 id="1-out-of-memory-oom"><a class="header" href="#1-out-of-memory-oom">1. Out of Memory (OOM)</a></h4>
<p><strong>Solutions:</strong></p>
<ul>
<li>Enable gradient checkpointing</li>
<li>Reduce batch size</li>
<li>Increase gradient accumulation steps</li>
<li>Use quantization (4-bit or 8-bit)</li>
<li>Reduce sequence length</li>
<li>Use DeepSpeed ZeRO optimization</li>
</ul>
<pre><code class="language-python"># Example fix
training_args = TrainingArguments(
    per_device_train_batch_size=1,        # Reduce from 4 to 1
    gradient_accumulation_steps=16,        # Increase from 4 to 16
    gradient_checkpointing=True,           # Enable checkpointing
    deepspeed="ds_config.json"            # Use DeepSpeed
)
</code></pre>
<h4 id="2-nan-loss-during-training"><a class="header" href="#2-nan-loss-during-training">2. NaN Loss During Training</a></h4>
<p><strong>Causes:</strong></p>
<ul>
<li>Using fp16 instead of bfloat16</li>
<li>Learning rate too high</li>
<li>Gradient explosion</li>
</ul>
<p><strong>Solutions:</strong></p>
<pre><code class="language-python"># Use bfloat16
training_args = TrainingArguments(
    bf16=True,
    fp16=False,
    learning_rate=2e-5,              # Lower learning rate
    max_grad_norm=1.0,               # Gradient clipping
)
</code></pre>
<h4 id="3-slow-inference"><a class="header" href="#3-slow-inference">3. Slow Inference</a></h4>
<p><strong>Solutions:</strong></p>
<ul>
<li>Use quantization</li>
<li>Enable Flash Attention 2</li>
<li>Batch requests</li>
<li>Use KV cache</li>
<li>Consider vLLM or TGI for serving</li>
</ul>
<h4 id="4-poor-fine-tuning-results"><a class="header" href="#4-poor-fine-tuning-results">4. Poor Fine-Tuning Results</a></h4>
<p><strong>Diagnostics:</strong></p>
<ul>
<li>Check data quality and format</li>
<li>Verify learning rate and schedule</li>
<li>Monitor training loss curve</li>
<li>Evaluate on validation set</li>
<li>Check for overfitting</li>
</ul>
<p><strong>Solutions:</strong></p>
<ul>
<li>Increase dataset size</li>
<li>Adjust learning rate</li>
<li>Add regularization (dropout)</li>
<li>Use early stopping</li>
<li>Try different LoRA ranks</li>
</ul>
<h2 id="best-practices-summary"><a class="header" href="#best-practices-summary">Best Practices Summary</a></h2>
<h3 id="training"><a class="header" href="#training">Training</a></h3>
<ol>
<li>✅ Use bfloat16 for training (not fp16)</li>
<li>✅ Start with learning rate 2e-5 to 5e-5</li>
<li>✅ Use constant learning rate schedule</li>
<li>✅ Set LoRA rank 8-16 (higher not always better)</li>
<li>✅ Enable gradient checkpointing for memory</li>
<li>✅ Use QLoRA for consumer GPUs</li>
<li>✅ Monitor validation metrics to prevent overfitting</li>
</ol>
<h3 id="inference"><a class="header" href="#inference">Inference</a></h3>
<ol>
<li>✅ Quantize to 4-bit for memory-constrained devices</li>
<li>✅ Use Flash Attention 2 for long sequences</li>
<li>✅ Enable KV cache for faster generation</li>
<li>✅ Batch requests when possible</li>
<li>✅ Use vLLM or TGI for production serving</li>
<li>✅ Profile and monitor performance metrics</li>
</ol>
<h3 id="deployment"><a class="header" href="#deployment">Deployment</a></h3>
<ol>
<li>✅ Choose quantization based on hardware/accuracy tradeoff</li>
<li>✅ Use platform-specific optimizations (OpenVINO, QNN, MLX)</li>
<li>✅ Implement proper error handling and retries</li>
<li>✅ Monitor memory usage and latency</li>
<li>✅ Cache frequent requests when appropriate</li>
<li>✅ Set appropriate timeout values</li>
</ol>
<h2 id="resources"><a class="header" href="#resources">Resources</a></h2>
<h3 id="official-links"><a class="header" href="#official-links">Official Links</a></h3>
<ul>
<li><strong>Hugging Face Hub</strong>: https://huggingface.co/microsoft/phi-4</li>
<li><strong>Azure AI</strong>: https://azure.microsoft.com/en-us/products/phi</li>
<li><strong>Phi-4 Technical Report</strong>: https://www.microsoft.com/en-us/research/publication/phi-4-technical-report/</li>
<li><strong>Phi-3 Technical Report</strong>: https://arxiv.org/abs/2404.14219</li>
</ul>
<h3 id="tools-and-libraries"><a class="header" href="#tools-and-libraries">Tools and Libraries</a></h3>
<ul>
<li><strong>Transformers</strong>: https://github.com/huggingface/transformers</li>
<li><strong>PEFT</strong>: https://github.com/huggingface/peft</li>
<li><strong>BitsAndBytes</strong>: https://github.com/TimDettmers/bitsandbytes</li>
<li><strong>vLLM</strong>: https://github.com/vllm-project/vllm</li>
<li><strong>llama.cpp</strong>: https://github.com/ggerganov/llama.cpp</li>
<li><strong>Text Generation Inference</strong>: https://github.com/huggingface/text-generation-inference</li>
</ul>
<h3 id="community"><a class="header" href="#community">Community</a></h3>
<ul>
<li><strong>Phi-3 Cookbook</strong>: https://github.com/microsoft/Phi-3CookBook</li>
<li><strong>Discussions</strong>: https://huggingface.co/microsoft/phi-4/discussions</li>
<li><strong>Issues</strong>: https://github.com/microsoft/phi-4/issues</li>
</ul>
<h2 id="license"><a class="header" href="#license">License</a></h2>
<p>Microsoft Phi models are released under the <strong>MIT License</strong>, allowing commercial use, modification, and distribution with minimal restrictions.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../ai/whisper.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256 246.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"/></svg></span>
                            </a>

                            <a rel="next prefetch" href="../ai/vllm.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M278.6 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L210.7 256 73.4 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"/></svg></span>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../ai/whisper.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256 246.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"/></svg></span>
                    </a>

                    <a rel="next prefetch" href="../ai/vllm.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M278.6 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L210.7 256 73.4 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"/></svg></span>
                    </a>
            </nav>

        </div>

        <template id=fa-eye><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M288 32c-80.8 0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7 0 24.6C17.3 304 48.6 356 95.4 399.4C142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144s64.5-144 144-144s144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64c-11.5 0-22.3-3-31.6-8.4c-.2 2.8-.4 5.5-.4 8.4c0 53 43 96 96 96s96-43 96-96s-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6z"/></svg></span></template>
        <template id=fa-eye-slash><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M38.8 5.1C28.4-3.1 13.3-1.2 5.1 9.2S-1.2 34.7 9.2 42.9l592 464c10.4 8.2 25.5 6.3 33.7-4.1s6.3-25.5-4.1-33.7L525.6 386.7c39.6-40.6 66.4-86.1 79.9-118.4c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C465.5 68.8 400.8 32 320 32c-68.2 0-125 26.3-169.3 60.8L38.8 5.1zM223.1 149.5C248.6 126.2 282.7 112 320 112c79.5 0 144 64.5 144 144c0 24.9-6.3 48.3-17.4 68.7L408 294.5c5.2-11.8 8-24.8 8-38.5c0-53-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6c0 10.2-2.4 19.8-6.6 28.3l-90.3-70.8zm223.1 298L373 389.9c-16.4 6.5-34.3 10.1-53 10.1c-79.5 0-144-64.5-144-144c0-6.9 .5-13.6 1.4-20.2L83.1 161.5C60.3 191.2 44 220.8 34.5 243.7c-3.3 7.9-3.3 16.7 0 24.6c14.9 35.7 46.2 87.7 93 131.1C174.5 443.2 239.2 480 320 480c47.8 0 89.9-12.9 126.2-32.5z"/></svg></span></template>
        <template id=fa-copy><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M502.6 70.63l-61.25-61.25C435.4 3.371 427.2 0 418.7 0H255.1c-35.35 0-64 28.66-64 64l.0195 256C192 355.4 220.7 384 256 384h192c35.2 0 64-28.8 64-64V93.25C512 84.77 508.6 76.63 502.6 70.63zM464 320c0 8.836-7.164 16-16 16H255.1c-8.838 0-16-7.164-16-16L239.1 64.13c0-8.836 7.164-16 16-16h128L384 96c0 17.67 14.33 32 32 32h47.1V320zM272 448c0 8.836-7.164 16-16 16H63.1c-8.838 0-16-7.164-16-16L47.98 192.1c0-8.836 7.164-16 16-16H160V128H63.99c-35.35 0-64 28.65-64 64l.0098 256C.002 483.3 28.66 512 64 512h192c35.2 0 64-28.8 64-64v-32h-47.1L272 448z"/></svg></span></template>
        <template id=fa-play><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M73 39c-14.8-9.1-33.4-9.4-48.5-.9S0 62.6 0 80V432c0 17.4 9.4 33.4 24.5 41.9s33.7 8.1 48.5-.9L361 297c14.3-8.7 23-24.2 23-41s-8.7-32.2-23-41L73 39z"/></svg></span></template>
        <template id=fa-clock-rotate-left><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M75 75L41 41C25.9 25.9 0 36.6 0 57.9V168c0 13.3 10.7 24 24 24H134.1c21.4 0 32.1-25.9 17-41l-30.8-30.8C155 85.5 203 64 256 64c106 0 192 86 192 192s-86 192-192 192c-40.8 0-78.6-12.7-109.7-34.4c-14.5-10.1-34.4-6.6-44.6 7.9s-6.6 34.4 7.9 44.6C151.2 495 201.7 512 256 512c141.4 0 256-114.6 256-256S397.4 0 256 0C185.3 0 121.3 28.7 75 75zm181 53c-13.3 0-24 10.7-24 24V256c0 6.4 2.5 12.5 7 17l72 72c9.4 9.4 24.6 9.4 33.9 0s9.4-24.6 0-33.9l-65-65V152c0-13.3-10.7-24-24-24z"/></svg></span></template>



        <script>
            window.playground_copyable = true;
        </script>


        <script src="../elasticlunr-ef4e11c1.min.js"></script>
        <script src="../mark-09e88c2c.min.js"></script>
        <script src="../searcher-c2a407aa.js"></script>

        <script src="../clipboard-1626706a.min.js"></script>
        <script src="../highlight-abc7f01d.js"></script>
        <script src="../book-a0b12cfe.js"></script>

        <!-- Custom JS scripts -->



    </div>
    </body>
</html>
