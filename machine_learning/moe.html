<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Moe - My Notes</title>


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon-de23e50b.svg">
        <link rel="shortcut icon" href="../favicon-8114d1fc.png">
        <link rel="stylesheet" href="../css/variables-8adf115d.css">
        <link rel="stylesheet" href="../css/general-2459343d.css">
        <link rel="stylesheet" href="../css/chrome-ae938929.css">
        <link rel="stylesheet" href="../css/print-9e4910d8.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../fonts/fonts-9644e21d.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="mdbook-highlight-css" href="../highlight-493f70e1.css">
        <link rel="stylesheet" id="mdbook-tomorrow-night-css" href="../tomorrow-night-4c0ae647.css">
        <link rel="stylesheet" id="mdbook-ayu-highlight-css" href="../ayu-highlight-3fdfc3ac.css">

        <!-- Custom theme stylesheets -->

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "../";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "../searchindex-64df36f8.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../toc-b765e602.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="mdbook-body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="mdbook-sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("mdbook-sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="mdbook-sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="../toc.html"></iframe>
            </noscript>
            <div id="mdbook-sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="mdbook-page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="mdbook-menu-bar-hover-placeholder"></div>
                <div id="mdbook-menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="mdbook-sidebar-toggle" class="icon-button" for="mdbook-sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="mdbook-sidebar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"/></svg></span>
                        </label>
                        <button id="mdbook-theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="mdbook-theme-list">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M371.3 367.1c27.3-3.9 51.9-19.4 67.2-42.9L600.2 74.1c12.6-19.5 9.4-45.3-7.6-61.2S549.7-4.4 531.1 9.6L294.4 187.2c-24 18-38.2 46.1-38.4 76.1L371.3 367.1zm-19.6 25.4l-116-104.4C175.9 290.3 128 339.6 128 400c0 3.9 .2 7.8 .6 11.6c1.8 17.5-10.2 36.4-27.8 36.4H96c-17.7 0-32 14.3-32 32s14.3 32 32 32H240c61.9 0 112-50.1 112-112c0-2.5-.1-5-.2-7.5z"/></svg></span>
                        </button>
                        <ul id="mdbook-theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-ayu">Ayu</button></li>
                        </ul>
                        <button id="mdbook-search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="mdbook-searchbar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M416 208c0 45.9-14.9 88.3-40 122.7L502.6 457.4c12.5 12.5 12.5 32.8 0 45.3s-32.8 12.5-45.3 0L330.7 376c-34.4 25.2-76.8 40-122.7 40C93.1 416 0 322.9 0 208S93.1 0 208 0S416 93.1 416 208zM208 352c79.5 0 144-64.5 144-144s-64.5-144-144-144S64 128.5 64 208s64.5 144 144 144z"/></svg></span>
                        </button>
                    </div>

                    <h1 class="menu-title">My Notes</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <span class=fa-svg id="print-button"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M128 0C92.7 0 64 28.7 64 64v96h64V64H354.7L384 93.3V160h64V93.3c0-17-6.7-33.3-18.7-45.3L400 18.7C388 6.7 371.7 0 354.7 0H128zM384 352v32 64H128V384 368 352H384zm64 32h32c17.7 0 32-14.3 32-32V256c0-35.3-28.7-64-64-64H64c-35.3 0-64 28.7-64 64v96c0 17.7 14.3 32 32 32H64v64c0 35.3 28.7 64 64 64H384c35.3 0 64-28.7 64-64V384zm-16-88c-13.3 0-24-10.7-24-24s10.7-24 24-24s24 10.7 24 24s-10.7 24-24 24z"/></svg></span>
                        </a>

                    </div>
                </div>

                <div id="mdbook-search-wrapper" class="hidden">
                    <form id="mdbook-searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="mdbook-searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="mdbook-searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <span class=fa-svg id="fa-spin"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M304 48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zm0 416c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM48 304c26.5 0 48-21.5 48-48s-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48zm464-48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM142.9 437c18.7-18.7 18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zm0-294.2c18.7-18.7 18.7-49.1 0-67.9S93.7 56.2 75 75s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zM369.1 437c18.7 18.7 49.1 18.7 67.9 0s18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9z"/></svg></span>
                            </div>
                        </div>
                    </form>
                    <div id="mdbook-searchresults-outer" class="searchresults-outer hidden">
                        <div id="mdbook-searchresults-header" class="searchresults-header"></div>
                        <ul id="mdbook-searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('mdbook-sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('mdbook-sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#mdbook-sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="mdbook-content" class="content">
                    <main>
                        <h1 id="mixture-of-experts-moe"><a class="header" href="#mixture-of-experts-moe">Mixture of Experts (MoE)</a></h1>
<p>A scalable neural network architecture that uses conditional computation to dramatically increase model capacity while maintaining computational efficiency through sparse activation.</p>
<h2 id="table-of-contents"><a class="header" href="#table-of-contents">Table of Contents</a></h2>
<ol>
<li><a href="#overview">Overview</a></li>
<li><a href="#core-intuition">Core Intuition</a></li>
<li><a href="#when-to-use-moe">When to Use MoE</a></li>
<li><a href="#mathematical-foundation">Mathematical Foundation</a></li>
<li><a href="#architecture-components">Architecture Components</a></li>
<li><a href="#pytorch-implementation">PyTorch Implementation</a></li>
<li><a href="#training-considerations">Training Considerations</a></li>
<li><a href="#load-balancing-strategies">Load Balancing Strategies</a></li>
<li><a href="#moe-variants">MoE Variants</a></li>
<li><a href="#advanced-topics">Advanced Topics</a></li>
<li><a href="#best-practices">Best Practices</a></li>
<li><a href="#common-pitfalls">Common Pitfalls</a></li>
<li><a href="#resources">Resources</a></li>
</ol>
<h2 id="overview"><a class="header" href="#overview">Overview</a></h2>
<p>Mixture of Experts (MoE) is a neural network architecture that divides the model into multiple “expert” sub-networks, where only a subset of experts is activated for each input. This conditional computation approach allows models to scale to trillions of parameters while keeping inference costs manageable.</p>
<p><strong>Key Concept</strong>: Instead of routing every input through the entire network, MoE uses a gating/routing mechanism to select which experts should process each input. This means you can have a massive model capacity, but only use a fraction of it for any given input.</p>
<h3 id="historical-context"><a class="header" href="#historical-context">Historical Context</a></h3>
<ul>
<li><strong>1991</strong>: Jacobs et al. introduced the original MoE concept</li>
<li><strong>2017</strong>: Shazeer et al. scaled MoE to billions of parameters (“Outrageously Large Neural Networks”)</li>
<li><strong>2021</strong>: Google’s Switch Transformer achieved 1.6 trillion parameters</li>
<li><strong>2022-2024</strong>: MoE became standard in modern LLMs (Mixtral, GPT-4, DeepSeek-V2)</li>
</ul>
<h3 id="key-advantages"><a class="header" href="#key-advantages">Key Advantages</a></h3>
<ol>
<li><strong>Scalability</strong>: Add model capacity without proportional compute increase</li>
<li><strong>Efficiency</strong>: Only activate relevant experts per input (sparse activation)</li>
<li><strong>Specialization</strong>: Experts can specialize in different patterns/domains</li>
<li><strong>Training Speed</strong>: Faster training than equivalent dense models</li>
<li><strong>Sample Efficiency</strong>: Better performance with less training data</li>
</ol>
<h3 id="key-challenges"><a class="header" href="#key-challenges">Key Challenges</a></h3>
<ol>
<li><strong>Load Balancing</strong>: Ensuring all experts are utilized evenly</li>
<li><strong>Communication</strong>: Expert parallelism requires efficient inter-device communication</li>
<li><strong>Memory</strong>: All experts must be kept in memory even if not all are active</li>
<li><strong>Fine-tuning</strong>: Can be tricky to fine-tune on small datasets</li>
<li><strong>Routing Collapse</strong>: Risk of all inputs routing to same experts</li>
</ol>
<h2 id="core-intuition"><a class="header" href="#core-intuition">Core Intuition</a></h2>
<h3 id="the-central-idea"><a class="header" href="#the-central-idea">The Central Idea</a></h3>
<p>Imagine you’re running a hospital:</p>
<ul>
<li><strong>Dense Model</strong>: Every patient sees every doctor (neurologist, cardiologist, dermatologist, etc.)</li>
<li><strong>MoE Model</strong>: A triage nurse (router) sends each patient to 1-2 relevant specialists</li>
</ul>
<p>The MoE approach is:</p>
<ul>
<li>More efficient (patients only see relevant doctors)</li>
<li>More scalable (you can have 100 specialists without 100x wait time)</li>
<li>More specialized (each doctor becomes expert in their domain)</li>
</ul>
<h3 id="visual-representation"><a class="header" href="#visual-representation">Visual Representation</a></h3>
<pre><code>Input Token: "The heart muscle contracts..."

                    ┌─────────────┐
                    │   Router    │  &lt;-- Learns which experts to use
                    │  (Gating)   │
                    └──────┬──────┘
                           │
        ┌──────────────────┼──────────────────┐
        │                  │                  │
        │ Score: 0.8       │ Score: 0.7       │ Score: 0.1
        ▼                  ▼                  ▼
    ┌───────┐          ┌───────┐          ┌───────┐
    │Expert │          │Expert │          │Expert │
    │   1   │  ✓       │   2   │  ✓       │   3   │  ✗
    │Medical│ Active   │Biology│ Active   │ Code  │ Inactive
    └───┬───┘          └───┬───┘          └───────┘
        │                  │
        └────────┬─────────┘
                 ▼
        Weighted combination
         (0.53 * E1 + 0.47 * E2)
                 │
                 ▼
            Final output
</code></pre>
<h3 id="how-it-works"><a class="header" href="#how-it-works">How It Works</a></h3>
<ol>
<li><strong>Input Arrives</strong>: A token/sample enters the MoE layer</li>
<li><strong>Router Decides</strong>: A learned gating network computes scores for each expert</li>
<li><strong>Top-K Selection</strong>: Select top-k experts (typically k=1 or k=2)</li>
<li><strong>Expert Processing</strong>: Only selected experts process the input</li>
<li><strong>Combine Outputs</strong>: Weighted sum based on router scores</li>
<li><strong>Update Router</strong>: Router learns which experts work best for which inputs</li>
</ol>
<h3 id="expert-specialization"><a class="header" href="#expert-specialization">Expert Specialization</a></h3>
<p>Experts naturally specialize during training:</p>
<ul>
<li><strong>Expert 1</strong>: Might specialize in medical/biology text</li>
<li><strong>Expert 2</strong>: Might specialize in code/programming</li>
<li><strong>Expert 3</strong>: Might specialize in mathematics</li>
<li><strong>Expert 4</strong>: Might specialize in creative writing</li>
</ul>
<p>This specialization emerges automatically through the routing mechanism and gradient descent!</p>
<h2 id="when-to-use-moe"><a class="header" href="#when-to-use-moe">When to Use MoE</a></h2>
<h3 id="use-moe-when"><a class="header" href="#use-moe-when">Use MoE When:</a></h3>
<p>✅ <strong>Large-scale training</strong> with massive datasets
✅ <strong>Need for high capacity</strong> without proportional compute cost
✅ <strong>Diverse input domains</strong> where specialization helps
✅ <strong>Inference efficiency matters</strong> (sparse activation reduces cost)
✅ <strong>You have distributed training infrastructure</strong> (for expert parallelism)</p>
<h3 id="avoid-moe-when"><a class="header" href="#avoid-moe-when">Avoid MoE When:</a></h3>
<p>❌ <strong>Small datasets</strong> (load balancing becomes difficult)
❌ <strong>Limited memory</strong> (all experts must fit in memory)
❌ <strong>Single device training</strong> (loses main efficiency advantage)
❌ <strong>Fine-tuning critical</strong> (MoE can be tricky to fine-tune)
❌ <strong>Need deterministic behavior</strong> (routing can be unstable)</p>
<h3 id="practical-decision-matrix"><a class="header" href="#practical-decision-matrix">Practical Decision Matrix</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Scenario</th><th>Dense Model</th><th>MoE Model</th><th>Winner</th></tr>
</thead>
<tbody>
<tr><td>Dataset &lt; 1B tokens</td><td>✓</td><td>✗</td><td>Dense</td></tr>
<tr><td>Dataset &gt; 100B tokens</td><td>✗</td><td>✓</td><td>MoE</td></tr>
<tr><td>Single GPU</td><td>✓</td><td>✗</td><td>Dense</td></tr>
<tr><td>Multi-device cluster</td><td>~</td><td>✓</td><td>MoE</td></tr>
<tr><td>Inference latency critical</td><td>✓</td><td>~</td><td>Dense</td></tr>
<tr><td>Training cost critical</td><td>✗</td><td>✓</td><td>MoE</td></tr>
<tr><td>Fine-tuning required</td><td>✓</td><td>✗</td><td>Dense</td></tr>
<tr><td>Domain diversity high</td><td>~</td><td>✓</td><td>MoE</td></tr>
</tbody>
</table>
</div>
<h2 id="mathematical-foundation"><a class="header" href="#mathematical-foundation">Mathematical Foundation</a></h2>
<h3 id="basic-moe-formulation"><a class="header" href="#basic-moe-formulation">Basic MoE Formulation</a></h3>
<p>For an input $x$, the MoE output is:</p>
<p>$$y = \sum_{i=1}^{N} G(x)_i \cdot E_i(x)$$</p>
<p>Where:</p>
<ul>
<li>$N$ is the number of experts</li>
<li>$G(x)$ is the gating function (router) outputting probabilities</li>
<li>$E_i(x)$ is the output of expert $i$</li>
<li>$G(x)_i$ is the gating weight for expert $i$</li>
</ul>
<h3 id="gating-function-router"><a class="header" href="#gating-function-router">Gating Function (Router)</a></h3>
<p>The router computes a probability distribution over experts:</p>
<p>$$G(x) = \text{Softmax}(x \cdot W_g)$$</p>
<p>Where:</p>
<ul>
<li>$x \in \mathbb{R}^d$ is the input</li>
<li>$W_g \in \mathbb{R}^{d \times N}$ is the learned gating weights</li>
<li>Output: $G(x) \in \mathbb{R}^N$ with $\sum_{i=1}^{N} G(x)_i = 1$</li>
</ul>
<h3 id="top-k-sparsity"><a class="header" href="#top-k-sparsity">Top-K Sparsity</a></h3>
<p>To enforce sparsity, we only keep the top-k experts:</p>
<p>$$G_{\text{sparse}}(x)_i = \begin{cases}
\frac{G(x)<em>i}{\sum</em>{j \in \text{TopK}} G(x)_j} &amp; \text{if } i \in \text{TopK}(G(x), k) \
0 &amp; \text{otherwise}
\end{cases}$$</p>
<p>This ensures only $k$ experts are activated per input.</p>
<h3 id="noisy-top-k-gating"><a class="header" href="#noisy-top-k-gating">Noisy Top-K Gating</a></h3>
<p>To encourage exploration and prevent routing collapse, add noise:</p>
<p>$$H(x)_i = (x \cdot W_g)<em>i + \text{StandardNormal}() \cdot \text{Softplus}((x \cdot W</em>{\text{noise}})_i)$$</p>
<p>$$\text{TopK}(H(x), k)$$</p>
<p>The noise helps during training but is typically removed at inference.</p>
<h3 id="load-balancing-loss"><a class="header" href="#load-balancing-loss">Load Balancing Loss</a></h3>
<p>To prevent all inputs routing to the same experts, we add an auxiliary loss:</p>
<p>$$\mathcal{L}_{\text{balance}} = \alpha \cdot CV(\text{expert_usage})^2$$</p>
<p>Where $CV$ is the coefficient of variation:</p>
<p>$$CV(x) = \frac{\sigma(x)}{\mu(x)} = \frac{\text{std}(x)}{\text{mean}(x)}$$</p>
<p>More sophisticated version (used in Switch Transformer):</p>
<p>$$\mathcal{L}<em>{\text{aux}} = \alpha \cdot N \cdot \sum</em>{i=1}^{N} f_i \cdot P_i$$</p>
<p>Where:</p>
<ul>
<li>$f_i$ = fraction of tokens routed to expert $i$</li>
<li>$P_i$ = average router probability for expert $i$</li>
<li>$\alpha$ is a hyperparameter (typically 0.01)</li>
<li>$N$ is the number of experts</li>
</ul>
<h3 id="router-z-loss"><a class="header" href="#router-z-loss">Router Z-Loss</a></h3>
<p>To prevent router logits from growing too large (numerical instability):</p>
<p>$$\mathcal{L}<em>{\text{z}} = \frac{1}{B} \sum</em>{x \in \text{batch}} \left(\log \sum_{i=1}^{N} e^{x \cdot W_{g,i}}\right)^2$$</p>
<p>This encourages the router to produce moderate logit values.</p>
<h3 id="complete-training-objective"><a class="header" href="#complete-training-objective">Complete Training Objective</a></h3>
<p>$$\mathcal{L}<em>{\text{total}} = \mathcal{L}</em>{\text{task}} + \alpha \cdot \mathcal{L}<em>{\text{aux}} + \beta \cdot \mathcal{L}</em>{\text{z}}$$</p>
<p>Where:</p>
<ul>
<li>$\mathcal{L}_{\text{task}}$ is the primary task loss (e.g., cross-entropy)</li>
<li>$\alpha$ typically 0.01</li>
<li>$\beta$ typically 0.001</li>
</ul>
<h3 id="capacity-factor"><a class="header" href="#capacity-factor">Capacity Factor</a></h3>
<p>To prevent overflow when too many tokens route to one expert:</p>
<p>$$\text{expert_capacity} = \left(\frac{\text{tokens_per_batch}}{N}\right) \cdot \text{capacity_factor} \cdot k$$</p>
<p>Where:</p>
<ul>
<li>$k$ is top-k value</li>
<li>capacity_factor &gt; 1.0 (typically 1.25)</li>
<li>Tokens exceeding capacity are either dropped or sent to next-best expert</li>
</ul>
<h2 id="architecture-components"><a class="header" href="#architecture-components">Architecture Components</a></h2>
<h3 id="1-expert-networks"><a class="header" href="#1-expert-networks">1. Expert Networks</a></h3>
<p>Each expert is typically a feed-forward network (FFN):</p>
<pre><code class="language-python">class Expert(nn.Module):
    """
    Single expert network - typically a two-layer FFN.
    """
    def __init__(self, d_model, d_ff, dropout=0.1):
        super().__init__()
        self.w1 = nn.Linear(d_model, d_ff)
        self.w2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)
        self.activation = nn.GELU()

    def forward(self, x):
        # x: [batch_size, seq_len, d_model]
        # Standard FFN: W2(GELU(W1(x)))
        return self.w2(self.dropout(self.activation(self.w1(x))))
</code></pre>
<p>Experts can also be:</p>
<ul>
<li>Multi-layer networks</li>
<li>Specialized architectures (CNNs, attention layers)</li>
<li>Different sizes (heterogeneous experts)</li>
</ul>
<h3 id="2-routergating-network"><a class="header" href="#2-routergating-network">2. Router/Gating Network</a></h3>
<p>The router determines which experts to activate:</p>
<pre><code class="language-python">class Router(nn.Module):
    """
    Router that selects top-k experts for each token.
    """
    def __init__(self, d_model, num_experts, k=2, noisy_gating=True):
        super().__init__()
        self.num_experts = num_experts
        self.k = k
        self.noisy_gating = noisy_gating

        # Router weights
        self.w_gate = nn.Linear(d_model, num_experts, bias=False)

        # Noise weights (for exploration)
        if noisy_gating:
            self.w_noise = nn.Linear(d_model, num_experts, bias=False)

    def forward(self, x, train=True):
        # x: [batch_size, seq_len, d_model]
        batch_size, seq_len, d_model = x.shape

        # Flatten for routing
        x_flat = x.view(-1, d_model)  # [batch_size * seq_len, d_model]

        # Compute clean logits
        logits = self.w_gate(x_flat)  # [batch_size * seq_len, num_experts]

        # Add noise during training
        if train and self.noisy_gating:
            noise_stddev = F.softplus(self.w_noise(x_flat))
            noise = torch.randn_like(logits) * noise_stddev
            logits = logits + noise

        # Get top-k experts
        top_k_logits, top_k_indices = torch.topk(logits, self.k, dim=-1)
        # top_k_logits: [batch_size * seq_len, k]
        # top_k_indices: [batch_size * seq_len, k]

        # Compute probabilities (softmax over top-k)
        top_k_gates = F.softmax(top_k_logits, dim=-1)

        # Also compute full softmax for load balancing loss
        gates = F.softmax(logits, dim=-1)  # [batch_size * seq_len, num_experts]

        return top_k_gates, top_k_indices, gates, logits
</code></pre>
<h3 id="3-complete-moe-layer"><a class="header" href="#3-complete-moe-layer">3. Complete MoE Layer</a></h3>
<pre><code class="language-python">class MoELayer(nn.Module):
    """
    Complete Mixture of Experts layer.
    """
    def __init__(self, d_model, d_ff, num_experts, k=2,
                 capacity_factor=1.25, dropout=0.1):
        super().__init__()
        self.num_experts = num_experts
        self.k = k
        self.capacity_factor = capacity_factor

        # Create experts
        self.experts = nn.ModuleList([
            Expert(d_model, d_ff, dropout) for _ in range(num_experts)
        ])

        # Create router
        self.router = Router(d_model, num_experts, k)

    def forward(self, x, train=True):
        # x: [batch_size, seq_len, d_model]
        batch_size, seq_len, d_model = x.shape

        # Route tokens
        top_k_gates, top_k_indices, all_gates, logits = self.router(x, train)
        # top_k_gates: [batch_size * seq_len, k]
        # top_k_indices: [batch_size * seq_len, k]

        # Flatten input
        x_flat = x.view(-1, d_model)  # [batch_size * seq_len, d_model]

        # Initialize output
        output = torch.zeros_like(x_flat)

        # Process each expert
        for i in range(self.num_experts):
            # Find tokens routed to this expert
            expert_mask = (top_k_indices == i).any(dim=-1)  # [batch_size * seq_len]

            if expert_mask.any():
                # Get tokens for this expert
                expert_input = x_flat[expert_mask]  # [num_tokens_i, d_model]

                # Process through expert
                expert_output = self.experts[i](expert_input.unsqueeze(1)).squeeze(1)
                # expert_output: [num_tokens_i, d_model]

                # Get gates for tokens assigned to this expert
                # Find which position in top-k this expert is
                expert_positions = (top_k_indices[expert_mask] == i)
                expert_gates = top_k_gates[expert_mask][expert_positions]
                # expert_gates: [num_tokens_i]

                # Add weighted expert output
                output[expert_mask] += expert_gates.unsqueeze(-1) * expert_output

        # Reshape output
        output = output.view(batch_size, seq_len, d_model)

        # Return output and routing info for loss computation
        return output, {
            'gates': all_gates,
            'logits': logits,
            'top_k_indices': top_k_indices,
            'top_k_gates': top_k_gates
        }
</code></pre>
<h3 id="4-moe-in-transformer-architecture"><a class="header" href="#4-moe-in-transformer-architecture">4. MoE in Transformer Architecture</a></h3>
<pre><code>Standard Transformer Block:
┌─────────────────────────┐
│   Input Embedding       │
└───────────┬─────────────┘
            │
    ┌───────▼────────┐
    │ Self-Attention │
    └───────┬────────┘
            │
    ┌───────▼────────┐
    │  Add &amp; Norm    │
    └───────┬────────┘
            │
    ┌───────▼────────┐
    │      FFN       │  &lt;-- Replace with MoE
    └───────┬────────┘
            │
    ┌───────▼────────┐
    │  Add &amp; Norm    │
    └───────┬────────┘
            │
         Output


MoE Transformer Block:
┌─────────────────────────┐
│   Input Embedding       │
└───────────┬─────────────┘
            │
    ┌───────▼────────┐
    │ Self-Attention │
    └───────┬────────┘
            │
    ┌───────▼────────┐
    │  Add &amp; Norm    │
    └───────┬────────┘
            │
    ┌───────▼────────┐
    │   Router       │
    └───┬───┬───┬────┘
        │   │   │
    ┌───▼┐ ┌▼─┐ ┌▼───┐
    │ E1 │ │E2│ │E3  │  &lt;-- Only top-k activated
    └───┬┘ └┬─┘ └┬───┘
        │   │   │
        └───┴───┴────┐
                     │
            ┌────────▼────┐
            │ Combine     │
            └────────┬────┘
                     │
            ┌────────▼────┐
            │  Add &amp; Norm │
            └────────┬────┘
                     │
                  Output
</code></pre>
<h2 id="pytorch-implementation"><a class="header" href="#pytorch-implementation">PyTorch Implementation</a></h2>
<h3 id="complete-working-example"><a class="header" href="#complete-working-example">Complete Working Example</a></h3>
<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Tuple, Dict
import math


class Expert(nn.Module):
    """
    Individual expert network (FFN).

    Args:
        d_model: Model dimension
        d_ff: Hidden dimension of FFN
        dropout: Dropout probability
    """
    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):
        super().__init__()
        self.w1 = nn.Linear(d_model, d_ff)
        self.w2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:
        """
        Args:
            x: [batch, seq_len, d_model]
        Returns:
            output: [batch, seq_len, d_model]
        """
        # FFN: W2(Dropout(GELU(W1(x))))
        hidden = F.gelu(self.w1(x))  # [batch, seq_len, d_ff]
        hidden = self.dropout(hidden)
        output = self.w2(hidden)      # [batch, seq_len, d_model]
        return output


class NoisyTopKRouter(nn.Module):
    """
    Router with noisy top-k gating.

    Args:
        d_model: Model dimension
        num_experts: Number of experts
        k: Number of experts to select per token
        noise_std: Standard deviation of noise (for training)
    """
    def __init__(self, d_model: int, num_experts: int, k: int = 2,
                 noise_std: float = 0.1):
        super().__init__()
        self.num_experts = num_experts
        self.k = k
        self.noise_std = noise_std

        # Gating weights
        self.w_gate = nn.Linear(d_model, num_experts, bias=False)
        # Initialize to small values
        nn.init.normal_(self.w_gate.weight, std=0.01)

    def forward(self, x: torch.Tensor, train: bool = True) -&gt; Tuple:
        """
        Args:
            x: [batch, seq_len, d_model]
            train: Whether in training mode

        Returns:
            top_k_gates: [batch * seq_len, k] - Normalized gates for top-k
            top_k_indices: [batch * seq_len, k] - Expert indices
            all_gates: [batch * seq_len, num_experts] - All gate probabilities
            logits: [batch * seq_len, num_experts] - Raw logits
        """
        batch_size, seq_len, d_model = x.shape

        # Flatten: [batch * seq_len, d_model]
        x_flat = x.reshape(-1, d_model)

        # Compute logits: [batch * seq_len, num_experts]
        logits = self.w_gate(x_flat)

        # Add noise during training for exploration
        if train and self.noise_std &gt; 0:
            noise = torch.randn_like(logits) * self.noise_std
            logits_noisy = logits + noise
        else:
            logits_noisy = logits

        # Get top-k experts
        # top_k_logits: [batch * seq_len, k]
        # top_k_indices: [batch * seq_len, k]
        top_k_logits, top_k_indices = torch.topk(logits_noisy, self.k, dim=-1)

        # Softmax over top-k only
        top_k_gates = F.softmax(top_k_logits, dim=-1)

        # Also compute full softmax for load balancing
        all_gates = F.softmax(logits, dim=-1)

        return top_k_gates, top_k_indices, all_gates, logits


class SparseMoE(nn.Module):
    """
    Sparse Mixture of Experts layer.

    Args:
        d_model: Model dimension
        d_ff: Expert hidden dimension
        num_experts: Number of expert networks
        k: Number of experts to activate per token
        capacity_factor: Capacity factor for expert buffering
        dropout: Dropout probability
    """
    def __init__(self, d_model: int, d_ff: int, num_experts: int,
                 k: int = 2, capacity_factor: float = 1.25, dropout: float = 0.1):
        super().__init__()
        self.d_model = d_model
        self.num_experts = num_experts
        self.k = k
        self.capacity_factor = capacity_factor

        # Create experts
        self.experts = nn.ModuleList([
            Expert(d_model, d_ff, dropout) for _ in range(num_experts)
        ])

        # Create router
        self.router = NoisyTopKRouter(d_model, num_experts, k)

        # For tracking expert usage (debugging)
        self.register_buffer('expert_usage', torch.zeros(num_experts))

    def forward(self, x: torch.Tensor, train: bool = True) -&gt; Tuple[torch.Tensor, Dict]:
        """
        Args:
            x: [batch, seq_len, d_model]
            train: Training mode flag

        Returns:
            output: [batch, seq_len, d_model]
            aux_info: Dictionary with routing information
        """
        batch_size, seq_len, d_model = x.shape
        num_tokens = batch_size * seq_len

        # Get routing decisions
        top_k_gates, top_k_indices, all_gates, logits = self.router(x, train)
        # top_k_gates: [num_tokens, k]
        # top_k_indices: [num_tokens, k]
        # all_gates: [num_tokens, num_experts]

        # Flatten input
        x_flat = x.reshape(num_tokens, d_model)

        # Initialize output
        output_flat = torch.zeros_like(x_flat)

        # Track which experts are used
        if train:
            expert_mask = torch.zeros(self.num_experts, dtype=torch.bool, device=x.device)

        # Process each expert
        for expert_idx in range(self.num_experts):
            # Find all tokens that route to this expert (in any top-k position)
            # expert_mask_tokens: [num_tokens]
            expert_mask_tokens = (top_k_indices == expert_idx).any(dim=-1)

            # Count tokens routed to this expert
            num_expert_tokens = expert_mask_tokens.sum().item()

            if num_expert_tokens == 0:
                continue

            if train:
                expert_mask[expert_idx] = True
                self.expert_usage[expert_idx] += num_expert_tokens

            # Get input for this expert
            # expert_input: [num_expert_tokens, d_model]
            expert_input = x_flat[expert_mask_tokens]

            # Process through expert
            # expert_output: [num_expert_tokens, d_model]
            expert_output = self.experts[expert_idx](expert_input.unsqueeze(1)).squeeze(1)

            # Get gates for this expert
            # For each token routed to this expert, find its gate value
            expert_positions = (top_k_indices[expert_mask_tokens] == expert_idx)
            # expert_positions: [num_expert_tokens, k]

            # Extract gates: [num_expert_tokens]
            expert_gates = top_k_gates[expert_mask_tokens][expert_positions]

            # Accumulate weighted output
            output_flat[expert_mask_tokens] += expert_gates.unsqueeze(-1) * expert_output

        # Reshape output
        output = output_flat.reshape(batch_size, seq_len, d_model)

        # Prepare auxiliary info for loss computation
        aux_info = {
            'gates': all_gates,          # [num_tokens, num_experts]
            'logits': logits,            # [num_tokens, num_experts]
            'top_k_indices': top_k_indices,  # [num_tokens, k]
            'top_k_gates': top_k_gates,      # [num_tokens, k]
            'num_tokens': num_tokens,
        }

        return output, aux_info


def compute_load_balancing_loss(aux_info: Dict, num_experts: int) -&gt; torch.Tensor:
    """
    Compute load balancing auxiliary loss.

    Encourages uniform distribution of tokens across experts.

    Args:
        aux_info: Dictionary from MoE forward pass
        num_experts: Number of experts

    Returns:
        loss: Scalar load balancing loss
    """
    gates = aux_info['gates']  # [num_tokens, num_experts]
    top_k_indices = aux_info['top_k_indices']  # [num_tokens, k]
    num_tokens = aux_info['num_tokens']

    # Compute fraction of tokens assigned to each expert
    # f_i in the paper
    expert_counts = torch.zeros(num_experts, device=gates.device)
    for i in range(num_experts):
        expert_counts[i] = (top_k_indices == i).sum()

    f = expert_counts / num_tokens  # [num_experts]

    # Compute average gate probability for each expert
    # P_i in the paper
    P = gates.mean(dim=0)  # [num_experts]

    # Load balancing loss: encourages f_i * P_i to be uniform
    # loss = N * sum(f_i * P_i)
    loss = num_experts * (f * P).sum()

    return loss


def compute_router_z_loss(aux_info: Dict) -&gt; torch.Tensor:
    """
    Compute router z-loss for numerical stability.

    Penalizes large logits to prevent overflow/underflow.

    Args:
        aux_info: Dictionary from MoE forward pass

    Returns:
        loss: Scalar z-loss
    """
    logits = aux_info['logits']  # [num_tokens, num_experts]

    # Log-sum-exp of logits
    log_z = torch.logsumexp(logits, dim=-1)  # [num_tokens]

    # Square and average
    z_loss = (log_z ** 2).mean()

    return z_loss


# Example usage
def example_moe_forward():
    """
    Demonstrate MoE forward pass with loss computation.
    """
    # Hyperparameters
    batch_size = 2
    seq_len = 4
    d_model = 128
    d_ff = 512
    num_experts = 8
    k = 2

    # Create MoE layer
    moe = SparseMoE(
        d_model=d_model,
        d_ff=d_ff,
        num_experts=num_experts,
        k=k,
        capacity_factor=1.25,
        dropout=0.1
    )

    # Sample input
    x = torch.randn(batch_size, seq_len, d_model)
    print(f"Input shape: {x.shape}")  # [2, 4, 128]

    # Forward pass
    output, aux_info = moe(x, train=True)
    print(f"Output shape: {output.shape}")  # [2, 4, 128]

    # Compute auxiliary losses
    load_balance_loss = compute_load_balancing_loss(aux_info, num_experts)
    z_loss = compute_router_z_loss(aux_info)

    print(f"\nLoad balancing loss: {load_balance_loss.item():.4f}")
    print(f"Router z-loss: {z_loss.item():.4f}")

    # Inspect routing decisions
    top_k_indices = aux_info['top_k_indices']  # [8, 2]
    top_k_gates = aux_info['top_k_gates']      # [8, 2]

    print(f"\nRouting decisions for first 3 tokens:")
    for i in range(3):
        experts = top_k_indices[i].tolist()
        gates = top_k_gates[i].tolist()
        print(f"  Token {i}: Experts {experts} with gates {[f'{g:.3f}' for g in gates]}")

    # Example output:
    # Token 0: Experts [3, 7] with gates ['0.612', '0.388']
    # Token 1: Experts [1, 5] with gates ['0.551', '0.449']
    # Token 2: Experts [2, 4] with gates ['0.723', '0.277']


# Training loop example
def example_training_step():
    """
    Example of a training step with MoE.
    """
    # Model setup
    moe = SparseMoE(d_model=128, d_ff=512, num_experts=8, k=2)
    optimizer = torch.optim.AdamW(moe.parameters(), lr=1e-4)

    # Hyperparameters
    alpha_aux = 0.01  # Load balancing loss weight
    alpha_z = 0.001   # Router z-loss weight

    # Sample batch
    x = torch.randn(4, 10, 128)  # [batch, seq_len, d_model]
    target = torch.randn(4, 10, 128)

    # Forward pass
    output, aux_info = moe(x, train=True)

    # Task loss (e.g., MSE for this example)
    task_loss = F.mse_loss(output, target)

    # Auxiliary losses
    aux_loss = compute_load_balancing_loss(aux_info, num_experts=8)
    z_loss = compute_router_z_loss(aux_info)

    # Total loss
    total_loss = task_loss + alpha_aux * aux_loss + alpha_z * z_loss

    # Backward pass
    optimizer.zero_grad()
    total_loss.backward()
    optimizer.step()

    print(f"Task loss: {task_loss.item():.4f}")
    print(f"Aux loss: {aux_loss.item():.4f}")
    print(f"Z loss: {z_loss.item():.4f}")
    print(f"Total loss: {total_loss.item():.4f}")


if __name__ == "__main__":
    print("=" * 60)
    print("MoE Forward Pass Example")
    print("=" * 60)
    example_moe_forward()

    print("\n" + "=" * 60)
    print("MoE Training Step Example")
    print("=" * 60)
    example_training_step()
</code></pre>
<h3 id="output-example"><a class="header" href="#output-example">Output Example</a></h3>
<pre><code>============================================================
MoE Forward Pass Example
============================================================
Input shape: torch.Size([2, 4, 128])
Output shape: torch.Size([2, 4, 128])

Load balancing loss: 1.2458
Router z-loss: 0.0234

Routing decisions for first 3 tokens:
  Token 0: Experts [3, 7] with gates ['0.612', '0.388']
  Token 1: Experts [1, 5] with gates ['0.551', '0.449']
  Token 2: Experts [2, 4] with gates ['0.723', '0.277']

============================================================
MoE Training Step Example
============================================================
Task loss: 1.0234
Aux loss: 1.1567
Z loss: 0.0198
Total loss: 1.0353
</code></pre>
<h2 id="training-considerations"><a class="header" href="#training-considerations">Training Considerations</a></h2>
<h3 id="1-load-balancing"><a class="header" href="#1-load-balancing">1. Load Balancing</a></h3>
<p><strong>Problem</strong>: Without constraints, all tokens may route to the same few experts, leaving others unused.</p>
<p><strong>Solutions</strong>:</p>
<p>a) <strong>Auxiliary Loss</strong> (most common)</p>
<pre><code class="language-python">loss = task_loss + alpha * load_balance_loss
</code></pre>
<ul>
<li>Encourages uniform expert usage</li>
<li>$\alpha$ typically 0.01 (tune based on task)</li>
</ul>
<p>b) <strong>Expert Capacity</strong></p>
<pre><code class="language-python">capacity = (num_tokens / num_experts) * capacity_factor * k
</code></pre>
<ul>
<li>Hard limit on tokens per expert</li>
<li>Overflow tokens are dropped or sent to alternative</li>
<li>capacity_factor typically 1.25-2.0</li>
</ul>
<p>c) <strong>Random Routing</strong> (during training)</p>
<ul>
<li>With small probability (e.g., 5%), route randomly</li>
<li>Prevents collapse to single expert</li>
</ul>
<p>d) <strong>Curriculum Learning</strong></p>
<ul>
<li>Start with dense model (all experts)</li>
<li>Gradually increase sparsity</li>
<li>Helps establish expert diversity</li>
</ul>
<h3 id="2-router-stability"><a class="header" href="#2-router-stability">2. Router Stability</a></h3>
<p><strong>Problem</strong>: Router logits can grow unbounded, causing numerical issues.</p>
<p><strong>Solution</strong>: Router z-loss</p>
<pre><code class="language-python">z_loss = mean((log_sum_exp(logits)) ^ 2)
loss_total = task_loss + alpha_aux * aux_loss + alpha_z * z_loss
</code></pre>
<p>Typical values:</p>
<ul>
<li>$\alpha_z = 0.001$ to 0.01</li>
</ul>
<h3 id="3-expert-initialization"><a class="header" href="#3-expert-initialization">3. Expert Initialization</a></h3>
<p><strong>Critical</strong>: Experts should start different to enable specialization.</p>
<pre><code class="language-python"># Option 1: Different random seeds per expert
for i, expert in enumerate(experts):
    torch.manual_seed(42 + i)
    expert.apply(init_weights)

# Option 2: Add small random perturbations
for expert in experts:
    for param in expert.parameters():
        param.data += torch.randn_like(param) * 0.01

# Option 3: Pre-train with different data subsets
# Train each expert on different data slice initially
</code></pre>
<h3 id="4-learning-rate-scheduling"><a class="header" href="#4-learning-rate-scheduling">4. Learning Rate Scheduling</a></h3>
<p>Router and experts often benefit from different learning rates:</p>
<pre><code class="language-python">optimizer = torch.optim.AdamW([
    {'params': moe.experts.parameters(), 'lr': 1e-4},
    {'params': moe.router.parameters(), 'lr': 5e-4},  # Higher LR for router
])
</code></pre>
<p>Rationale: Router needs to adapt quickly to find good expert assignments.</p>
<h3 id="5-gradient-clipping"><a class="header" href="#5-gradient-clipping">5. Gradient Clipping</a></h3>
<p>MoE can have unstable gradients, especially early in training:</p>
<pre><code class="language-python">torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
</code></pre>
<h3 id="6-expert-dropout"><a class="header" href="#6-expert-dropout">6. Expert Dropout</a></h3>
<p>During training, randomly drop entire experts to prevent over-reliance:</p>
<pre><code class="language-python">if train and random.random() &lt; expert_dropout_prob:
    # Skip this expert, renormalize gates
    continue
</code></pre>
<p>Typical: expert_dropout_prob = 0.1</p>
<h3 id="7-token-dropping-vs-overflow-handling"><a class="header" href="#7-token-dropping-vs-overflow-handling">7. Token Dropping vs. Overflow Handling</a></h3>
<p>When expert capacity is exceeded:</p>
<p><strong>Option A: Drop tokens</strong> (Switch Transformer)</p>
<ul>
<li>Exceeding tokens get zero output</li>
<li>Simple but loses information</li>
<li>Use high capacity factor (1.5-2.0)</li>
</ul>
<p><strong>Option B: Overflow to next-best expert</strong></p>
<ul>
<li>Send to second-choice expert if first is full</li>
<li>More complex but preserves information</li>
<li>Better for smaller capacity factors</li>
</ul>
<pre><code class="language-python"># Overflow handling
for expert_idx in range(num_experts):
    # Primary assignment
    primary_mask = (top_k_indices[:, 0] == expert_idx)

    # Overflow: check capacity
    if primary_mask.sum() &gt; capacity:
        # Send overflow to second-choice expert
        overflow_mask = primary_mask[capacity:]
        # Process overflow_mask with top_k_indices[:, 1]
</code></pre>
<h2 id="load-balancing-strategies"><a class="header" href="#load-balancing-strategies">Load Balancing Strategies</a></h2>
<h3 id="1-importance-based-load-balancing-switch-transformer"><a class="header" href="#1-importance-based-load-balancing-switch-transformer">1. Importance-based Load Balancing (Switch Transformer)</a></h3>
<p>Minimizes:
$$\mathcal{L}<em>{\text{aux}} = \alpha \cdot N \cdot \sum</em>{i=1}^{N} f_i \cdot P_i$$</p>
<p>Where:</p>
<ul>
<li>$f_i$ = fraction of tokens dispatched to expert $i$</li>
<li>$P_i$ = mean router probability for expert $i$</li>
</ul>
<p><strong>Intuition</strong>: If an expert gets many tokens ($f_i$ high), its router probability should be low ($P_i$ low), and vice versa.</p>
<pre><code class="language-python">def importance_load_balance_loss(gates, top_k_indices, num_experts):
    """Switch Transformer load balancing."""
    num_tokens = gates.shape[0]

    # Fraction of tokens assigned to each expert
    f = torch.zeros(num_experts, device=gates.device)
    for i in range(num_experts):
        f[i] = (top_k_indices == i).sum() / num_tokens

    # Mean router probability
    P = gates.mean(dim=0)  # [num_experts]

    # Loss
    loss = num_experts * (f * P).sum()
    return loss
</code></pre>
<h3 id="2-balance-loss-gshard"><a class="header" href="#2-balance-loss-gshard">2. Balance Loss (GShard)</a></h3>
<p>Encourages even distribution using coefficient of variation:</p>
<p>$$\mathcal{L}_{\text{balance}} = CV(\text{expert_usage})^2 = \left(\frac{\sigma}{\mu}\right)^2$$</p>
<pre><code class="language-python">def balance_loss(top_k_indices, num_experts):
    """GShard balance loss."""
    # Count tokens per expert
    counts = torch.zeros(num_experts, device=top_k_indices.device)
    for i in range(num_experts):
        counts[i] = (top_k_indices == i).sum()

    # Coefficient of variation
    mean = counts.mean()
    std = counts.std()
    cv = std / (mean + 1e-10)

    return cv ** 2
</code></pre>
<h3 id="3-expert-choice-routing"><a class="header" href="#3-expert-choice-routing">3. Expert Choice Routing</a></h3>
<p><strong>Flip the paradigm</strong>: Instead of tokens choosing experts, experts choose tokens!</p>
<pre><code class="language-python">def expert_choice_routing(tokens, experts, capacity_per_expert):
    """
    Each expert selects top-k tokens based on affinity scores.

    Args:
        tokens: [num_tokens, d_model]
        experts: List of expert modules
        capacity_per_expert: Max tokens per expert
    """
    num_experts = len(experts)
    num_tokens = tokens.shape[0]

    # Compute affinity scores
    affinity = torch.zeros(num_experts, num_tokens)
    for i, expert in enumerate(experts):
        affinity[i] = expert.compute_affinity(tokens)  # [num_tokens]

    # Each expert selects top-k tokens
    expert_assignments = []
    for i in range(num_experts):
        top_tokens = torch.topk(affinity[i], capacity_per_expert).indices
        expert_assignments.append(top_tokens)

    return expert_assignments
</code></pre>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Perfect load balancing (each expert gets exactly <code>capacity_per_expert</code> tokens)</li>
<li>No token dropping</li>
<li>No auxiliary loss needed</li>
</ul>
<p><strong>Disadvantages</strong>:</p>
<ul>
<li>Some tokens may be selected by multiple experts (need to handle)</li>
<li>Some tokens may not be selected at all (need default processing)</li>
</ul>
<h3 id="4-dynamic-capacity-adjustment"><a class="header" href="#4-dynamic-capacity-adjustment">4. Dynamic Capacity Adjustment</a></h3>
<p>Adjust expert capacity based on routing distribution:</p>
<pre><code class="language-python">class AdaptiveCapacityMoE(nn.Module):
    def __init__(self, ...):
        super().__init__()
        # Start with high capacity
        self.capacity_factor = 2.0

    def adjust_capacity(self, expert_usage):
        """Adjust capacity based on observed usage."""
        # If load is balanced, can reduce capacity
        cv = coefficient_of_variation(expert_usage)

        if cv &lt; 0.1:  # Well balanced
            self.capacity_factor = max(1.1, self.capacity_factor - 0.1)
        elif cv &gt; 0.5:  # Poorly balanced
            self.capacity_factor = min(3.0, self.capacity_factor + 0.1)
</code></pre>
<h3 id="5-entropy-regularization"><a class="header" href="#5-entropy-regularization">5. Entropy Regularization</a></h3>
<p>Encourage router to have high entropy (uncertainty):</p>
<p>$$\mathcal{L}_{\text{entropy}} = -\lambda \cdot \mathbb{E}[H(G(x))]$$</p>
<p>$$H(p) = -\sum_{i} p_i \log p_i$$</p>
<pre><code class="language-python">def entropy_regularization(gates):
    """Encourage high entropy in routing decisions."""
    # gates: [num_tokens, num_experts]
    entropy = -(gates * (gates + 1e-10).log()).sum(dim=-1)  # [num_tokens]
    mean_entropy = entropy.mean()

    # Negative because we want to maximize entropy (minimize negative entropy)
    return -mean_entropy
</code></pre>
<p>Higher entropy = more exploration = better load balancing.</p>
<p>Typical $\lambda = 0.01$.</p>
<h2 id="moe-variants"><a class="header" href="#moe-variants">MoE Variants</a></h2>
<h3 id="1-switch-transformer-google-2021"><a class="header" href="#1-switch-transformer-google-2021">1. Switch Transformer (Google, 2021)</a></h3>
<p><strong>Key Innovation</strong>: Simplified MoE with k=1 (single expert per token)</p>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Simpler routing (no weighted combination)</li>
<li>Faster training and inference</li>
<li>Scales to 1.6T parameters</li>
</ul>
<p><strong>Architecture</strong>:</p>
<pre><code class="language-python">class SwitchLayer(nn.Module):
    """Switch Transformer: k=1 MoE."""
    def __init__(self, d_model, d_ff, num_experts):
        super().__init__()
        self.experts = nn.ModuleList([Expert(d_model, d_ff) for _ in range(num_experts)])
        self.router = nn.Linear(d_model, num_experts)

    def forward(self, x):
        # x: [batch, seq_len, d_model]
        batch, seq_len, d_model = x.shape
        x_flat = x.view(-1, d_model)

        # Route: select single expert per token
        router_logits = self.router(x_flat)
        expert_indices = router_logits.argmax(dim=-1)  # [batch * seq_len]

        # Process each expert's tokens in batch
        output = torch.zeros_like(x_flat)
        for i in range(len(self.experts)):
            mask = (expert_indices == i)
            if mask.any():
                output[mask] = self.experts[i](x_flat[mask].unsqueeze(1)).squeeze(1)

        return output.view(batch, seq_len, d_model)
</code></pre>
<p><strong>Results</strong>: 7x speedup over T5-XXL with same quality.</p>
<h3 id="2-expert-choice-google-2022"><a class="header" href="#2-expert-choice-google-2022">2. Expert Choice (Google, 2022)</a></h3>
<p>Experts select tokens instead of tokens selecting experts.</p>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Perfect load balancing</li>
<li>No auxiliary loss needed</li>
<li>No token dropping</li>
</ul>
<p><strong>Implementation</strong>:</p>
<pre><code class="language-python">class ExpertChoiceLayer(nn.Module):
    def __init__(self, d_model, d_ff, num_experts, tokens_per_expert):
        super().__init__()
        self.experts = nn.ModuleList([Expert(d_model, d_ff) for _ in range(num_experts)])
        self.tokens_per_expert = tokens_per_expert

        # Affinity scoring
        self.affinity_weights = nn.Parameter(torch.randn(num_experts, d_model))

    def forward(self, x):
        batch, seq_len, d_model = x.shape
        x_flat = x.view(-1, d_model)
        num_tokens = x_flat.shape[0]

        # Compute affinity: [num_experts, num_tokens]
        affinity = torch.matmul(self.affinity_weights, x_flat.T)

        # Each expert selects top-k tokens
        output = torch.zeros_like(x_flat)
        for i in range(len(self.experts)):
            # Expert i selects its top tokens
            top_k_scores, top_k_indices = torch.topk(affinity[i], self.tokens_per_expert)

            # Process selected tokens
            selected_tokens = x_flat[top_k_indices]
            expert_output = self.experts[i](selected_tokens.unsqueeze(1)).squeeze(1)

            # Accumulate (with normalized scores)
            gates = F.softmax(top_k_scores, dim=0)
            output[top_k_indices] += gates.unsqueeze(-1) * expert_output

        return output.view(batch, seq_len, d_model)
</code></pre>
<h3 id="3-sparse-moe-mistral-mixtral-8x7b-2023"><a class="header" href="#3-sparse-moe-mistral-mixtral-8x7b-2023">3. Sparse MoE (Mistral Mixtral 8x7B, 2023)</a></h3>
<p><strong>Configuration</strong>:</p>
<ul>
<li>8 experts, each 7B parameters</li>
<li>k=2 (top-2 routing)</li>
<li>Total: 47B parameters, 13B active per token</li>
</ul>
<p><strong>Key Features</strong>:</p>
<ul>
<li>Expert parallelism across devices</li>
<li>Efficient routing with minimal overhead</li>
<li>Strong performance (matches 70B dense models)</li>
</ul>
<p><strong>Architecture Details</strong>:</p>
<pre><code class="language-python"># Mixtral-style configuration
config = {
    'num_experts': 8,
    'k': 2,
    'd_model': 4096,
    'd_ff': 14336,  # Each expert is 7B params
    'num_layers': 32,
    'replace_every_n_layers': 1,  # MoE every layer
}
</code></pre>
<h3 id="4-soft-moe-meta-2023"><a class="header" href="#4-soft-moe-meta-2023">4. Soft MoE (Meta, 2023)</a></h3>
<p><strong>Key Innovation</strong>: No routing; instead, use learned slot attention.</p>
<p><strong>How it works</strong>:</p>
<ol>
<li>Define $n$ “slots” (similar to experts)</li>
<li>Each slot attends to all tokens (soft assignment)</li>
<li>Process slots through experts</li>
<li>Distribute expert outputs back to tokens</li>
</ol>
<p><strong>Advantages</strong>:</p>
<ul>
<li>No load balancing issues</li>
<li>Fully differentiable</li>
<li>No token dropping</li>
</ul>
<p><strong>Disadvantages</strong>:</p>
<ul>
<li>Not truly sparse (all slots process all information)</li>
<li>Less parameter-efficient</li>
</ul>
<pre><code class="language-python">class SoftMoE(nn.Module):
    def __init__(self, d_model, d_ff, num_slots):
        super().__init__()
        self.slots = nn.Parameter(torch.randn(num_slots, d_model))
        self.expert = Expert(d_model, d_ff)

    def forward(self, x):
        # x: [batch, seq_len, d_model]
        batch, seq_len, d_model = x.shape

        # Compute slot-token affinities
        affinity = torch.matmul(self.slots, x.transpose(1, 2))  # [num_slots, batch, seq_len]

        # Normalize: each slot attends to tokens
        slot_attn = F.softmax(affinity, dim=-1)  # [num_slots, batch, seq_len]

        # Aggregate tokens into slots
        slots_filled = torch.matmul(slot_attn, x)  # [num_slots, batch, d_model]

        # Process slots through expert
        slots_processed = self.expert(slots_filled.unsqueeze(2)).squeeze(2)

        # Distribute back to tokens
        output = torch.matmul(slot_attn.transpose(1, 2), slots_processed)

        return output
</code></pre>
<h3 id="5-hierarchical-moe"><a class="header" href="#5-hierarchical-moe">5. Hierarchical MoE</a></h3>
<p>Multiple levels of routing for very large models:</p>
<pre><code>Input
  │
  ▼
Router L1 (high-level: topic)
  ├──────┬──────┬──────┐
  ▼      ▼      ▼      ▼
Router Router Router Router (L2: subtopic)
  │      │      │      │
Expert Expert Expert Expert (L3: fine-grained)
</code></pre>
<p><strong>Example</strong>:</p>
<ul>
<li>L1: Route by modality (text vs. code vs. math)</li>
<li>L2: Route by subtopic (Python vs. JavaScript, Algebra vs. Calculus)</li>
<li>L3: Fine-grained processing</li>
</ul>
<p><strong>Benefits</strong>:</p>
<ul>
<li>Can scale to thousands of experts</li>
<li>Better specialization</li>
<li>Hierarchical abstractions</li>
</ul>
<h3 id="6-conditional-moe-deepseek-v2"><a class="header" href="#6-conditional-moe-deepseek-v2">6. Conditional MoE (DeepSeek-V2)</a></h3>
<p><strong>Innovation</strong>: MoE applied selectively based on input characteristics.</p>
<pre><code class="language-python">class ConditionalMoE(nn.Module):
    def __init__(self, d_model, d_ff, num_experts):
        super().__init__()
        self.moe = SparseMoE(d_model, d_ff, num_experts)
        self.dense_ffn = Expert(d_model, d_ff)
        self.selector = nn.Linear(d_model, 1)

    def forward(self, x):
        # Decide: MoE or dense?
        use_moe = torch.sigmoid(self.selector(x)) &gt; 0.5  # [batch, seq_len, 1]

        moe_output, _ = self.moe(x)
        dense_output = self.dense_ffn(x)

        # Mix based on selector
        output = torch.where(use_moe, moe_output, dense_output)
        return output
</code></pre>
<p><strong>When to use</strong>: Apply MoE only to challenging tokens, use dense FFN for simple tokens.</p>
<h2 id="advanced-topics"><a class="header" href="#advanced-topics">Advanced Topics</a></h2>
<h3 id="1-expert-parallelism"><a class="header" href="#1-expert-parallelism">1. Expert Parallelism</a></h3>
<p>Distribute experts across multiple devices:</p>
<pre><code class="language-python"># Device assignment
expert_devices = ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3']
for i, expert in enumerate(experts):
    device = expert_devices[i % len(expert_devices)]
    expert.to(device)

# Forward with device routing
def forward_distributed(x, experts, top_k_indices):
    output = torch.zeros_like(x)

    for i, expert in enumerate(experts):
        mask = (top_k_indices == i).any(dim=-1)
        if mask.any():
            # Move tokens to expert's device
            expert_input = x[mask].to(expert.device)
            expert_output = expert(expert_input)
            # Move back to main device
            output[mask] = expert_output.to(x.device)

    return output
</code></pre>
<p><strong>Communication Overhead</strong>: Main bottleneck is moving tokens between devices.</p>
<p><strong>Optimization</strong>: Use expert parallelism with all-to-all communication:</p>
<pre><code class="language-python"># All-to-all gather: collect all tokens for each expert
# Process in batch on expert's device
# All-to-all scatter: return results
</code></pre>
<h3 id="2-fine-tuning-moe-models"><a class="header" href="#2-fine-tuning-moe-models">2. Fine-tuning MoE Models</a></h3>
<p><strong>Challenge</strong>: MoE models can collapse during fine-tuning on small datasets.</p>
<p><strong>Strategies</strong>:</p>
<p>a) <strong>Freeze Router, Fine-tune Experts</strong></p>
<pre><code class="language-python"># Freeze router
for param in model.router.parameters():
    param.requires_grad = False

# Fine-tune experts
for param in model.experts.parameters():
    param.requires_grad = True
</code></pre>
<p>b) <strong>Add Task-Specific Experts</strong></p>
<pre><code class="language-python"># Keep pre-trained experts frozen
# Add new experts for fine-tuning task
moe.experts.append(Expert(d_model, d_ff))  # New expert
</code></pre>
<p>c) <strong>Lower Learning Rate + Stronger Load Balancing</strong></p>
<pre><code class="language-python">optimizer = AdamW(model.parameters(), lr=1e-5)  # 10x lower than pre-training
alpha_aux = 0.1  # 10x higher than pre-training
</code></pre>
<p>d) <strong>LoRA for MoE</strong> (MoE-LoRA)
Apply LoRA to each expert:</p>
<pre><code class="language-python">class ExpertWithLoRA(nn.Module):
    def __init__(self, expert, rank=16):
        super().__init__()
        self.expert = expert
        # Freeze expert
        for param in expert.parameters():
            param.requires_grad = False

        # Add LoRA adapters
        self.lora_A = nn.Linear(d_model, rank, bias=False)
        self.lora_B = nn.Linear(rank, d_model, bias=False)

    def forward(self, x):
        return self.expert(x) + self.lora_B(self.lora_A(x))
</code></pre>
<h3 id="3-distillation-from-moe-to-dense"><a class="header" href="#3-distillation-from-moe-to-dense">3. Distillation from MoE to Dense</a></h3>
<p>Convert sparse MoE to dense model for deployment:</p>
<pre><code class="language-python">def distill_moe_to_dense(moe_model, dense_model, dataloader):
    """
    Distill knowledge from MoE to smaller dense model.
    """
    optimizer = AdamW(dense_model.parameters(), lr=1e-4)

    for batch in dataloader:
        # Get MoE predictions (teacher)
        with torch.no_grad():
            moe_output, _ = moe_model(batch)

        # Dense model predictions (student)
        dense_output = dense_model(batch)

        # Distillation loss (KL divergence)
        loss = F.kl_div(
            F.log_softmax(dense_output / temperature, dim=-1),
            F.softmax(moe_output / temperature, dim=-1),
            reduction='batchmean'
        ) * (temperature ** 2)

        loss.backward()
        optimizer.step()
</code></pre>
<p><strong>Use case</strong>: Train large MoE, distill to deployable dense model.</p>
<h3 id="4-analyzing-expert-specialization"><a class="header" href="#4-analyzing-expert-specialization">4. Analyzing Expert Specialization</a></h3>
<p>Understand what each expert learned:</p>
<pre><code class="language-python">def analyze_expert_specialization(model, dataset, labels):
    """
    Analyze which experts activate for which data types.
    """
    expert_activations = {i: [] for i in range(num_experts)}

    for sample, label in zip(dataset, labels):
        output, aux_info = model(sample)
        top_k_indices = aux_info['top_k_indices']

        for expert_idx in top_k_indices:
            expert_activations[expert_idx].append(label)

    # Analyze distributions
    for i, activations in expert_activations.items():
        counter = Counter(activations)
        print(f"Expert {i}: {counter.most_common(3)}")

# Example output:
# Expert 0: [('code', 456), ('technical', 123), ('math', 45)]
# Expert 1: [('creative', 234), ('narrative', 189), ('poetry', 67)]
# Expert 2: [('science', 345), ('medical', 234), ('biology', 123)]
</code></pre>
<h3 id="5-mixture-of-depths-mod"><a class="header" href="#5-mixture-of-depths-mod">5. Mixture of Depths (MoD)</a></h3>
<p>Combine MoE with dynamic depth (some tokens skip layers):</p>
<pre><code class="language-python">class MixtureOfDepthsLayer(nn.Module):
    def __init__(self, d_model, d_ff, num_experts):
        super().__init__()
        self.moe = SparseMoE(d_model, d_ff, num_experts)
        self.skip_predictor = nn.Linear(d_model, 1)

    def forward(self, x):
        # Predict which tokens should skip this layer
        skip_prob = torch.sigmoid(self.skip_predictor(x))  # [batch, seq_len, 1]
        skip_mask = (skip_prob &gt; 0.5).squeeze(-1)  # [batch, seq_len]

        # Process non-skipped tokens through MoE
        moe_output, _ = self.moe(x)

        # Combine: skip or process
        output = torch.where(skip_mask.unsqueeze(-1), x, moe_output)
        return output
</code></pre>
<p><strong>Benefit</strong>: Some tokens (e.g., stopwords) don’t need deep processing.</p>
<h3 id="6-shared-expert-architecture"><a class="header" href="#6-shared-expert-architecture">6. Shared Expert Architecture</a></h3>
<p>Reserve some experts as “shared” (always active):</p>
<pre><code class="language-python">class SharedExpertMoE(nn.Module):
    def __init__(self, d_model, d_ff, num_routed_experts, num_shared_experts):
        super().__init__()
        # Routed experts (sparse)
        self.routed_experts = nn.ModuleList([
            Expert(d_model, d_ff) for _ in range(num_routed_experts)
        ])

        # Shared experts (always active)
        self.shared_experts = nn.ModuleList([
            Expert(d_model, d_ff) for _ in range(num_shared_experts)
        ])

        self.router = Router(d_model, num_routed_experts)

    def forward(self, x):
        # Route to sparse experts
        routed_output, _ = self.route_to_experts(x)

        # Always process through shared experts
        shared_output = sum(expert(x) for expert in self.shared_experts)

        # Combine
        return routed_output + shared_output / len(self.shared_experts)
</code></pre>
<p><strong>Use case</strong>: Shared experts handle common patterns, routed experts handle specialized cases.</p>
<h2 id="best-practices"><a class="header" href="#best-practices">Best Practices</a></h2>
<h3 id="1-hyperparameter-recommendations"><a class="header" href="#1-hyperparameter-recommendations">1. Hyperparameter Recommendations</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Parameter</th><th>Recommended Range</th><th>Notes</th></tr>
</thead>
<tbody>
<tr><td>Number of experts</td><td>8-64</td><td>Start with 8, scale up</td></tr>
<tr><td>Top-k</td><td>1-2</td><td>k=1 for simplicity, k=2 for quality</td></tr>
<tr><td>Capacity factor</td><td>1.25-2.0</td><td>Higher for k=1, lower for k=2</td></tr>
<tr><td>Load balance loss weight (α)</td><td>0.01-0.1</td><td>Start 0.01, increase if imbalanced</td></tr>
<tr><td>Router z-loss weight (β)</td><td>0.001-0.01</td><td>Usually 0.001</td></tr>
<tr><td>Expert FFN multiplier</td><td>4x</td><td>Same as dense transformers</td></tr>
<tr><td>Router learning rate</td><td>2-5x expert LR</td><td>Router needs faster adaptation</td></tr>
<tr><td>Dropout</td><td>0.1</td><td>Same as dense models</td></tr>
</tbody>
</table>
</div>
<h3 id="2-training-recipe"><a class="header" href="#2-training-recipe">2. Training Recipe</a></h3>
<p><strong>Phase 1: Warmup (first 10% of training)</strong></p>
<ul>
<li>High capacity factor (2.0)</li>
<li>Lower load balance weight (0.005)</li>
<li>Optional: Start with dense routing (no sparsity)</li>
</ul>
<p><strong>Phase 2: Main Training (next 80%)</strong></p>
<ul>
<li>Normal capacity factor (1.25)</li>
<li>Normal load balance weight (0.01)</li>
<li>Full sparse routing</li>
</ul>
<p><strong>Phase 3: Fine-tuning (final 10%)</strong></p>
<ul>
<li>Slightly lower LR</li>
<li>Optionally increase load balance weight (0.02)</li>
<li>Remove routing noise</li>
</ul>
<h3 id="3-debugging-checklist"><a class="header" href="#3-debugging-checklist">3. Debugging Checklist</a></h3>
<p>✅ <strong>Check expert utilization</strong></p>
<pre><code class="language-python"># Should be roughly uniform
expert_counts = torch.bincount(top_k_indices.flatten(), minlength=num_experts)
print(f"Expert usage: {expert_counts.tolist()}")
</code></pre>
<p>✅ <strong>Monitor load balance loss</strong></p>
<pre><code class="language-python"># Should decrease over time
if load_balance_loss &gt; 2.0:
    print("Warning: Poor load balancing!")
</code></pre>
<p>✅ <strong>Verify gradient flow</strong></p>
<pre><code class="language-python"># All experts should receive gradients
for i, expert in enumerate(experts):
    grad_norm = sum(p.grad.norm() for p in expert.parameters() if p.grad is not None)
    print(f"Expert {i} grad norm: {grad_norm:.4f}")
</code></pre>
<p>✅ <strong>Check router logits</strong></p>
<pre><code class="language-python"># Should be moderate (not too large)
logit_magnitude = logits.abs().mean()
if logit_magnitude &gt; 10:
    print("Warning: Router logits too large!")
</code></pre>
<p>✅ <strong>Validate output diversity</strong></p>
<pre><code class="language-python"># Experts should produce different outputs
expert_outputs = [expert(x) for expert in experts]
output_diversity = torch.stack(expert_outputs).std(dim=0).mean()
print(f"Output diversity: {output_diversity:.4f}")
</code></pre>
<h3 id="4-scaling-guidelines"><a class="header" href="#4-scaling-guidelines">4. Scaling Guidelines</a></h3>
<p><strong>Small Scale</strong> (1-8B params)</p>
<ul>
<li>8 experts, k=2</li>
<li>Single-node training</li>
<li>Capacity factor: 1.5</li>
</ul>
<p><strong>Medium Scale</strong> (8-50B params)</p>
<ul>
<li>16-32 experts, k=2</li>
<li>Multi-node with expert parallelism</li>
<li>Capacity factor: 1.25</li>
</ul>
<p><strong>Large Scale</strong> (50B-1T+ params)</p>
<ul>
<li>64-256 experts, k=1 or k=2</li>
<li>Hierarchical expert parallelism</li>
<li>Capacity factor: 1.1</li>
<li>Consider expert choice routing</li>
</ul>
<h3 id="5-inference-optimization"><a class="header" href="#5-inference-optimization">5. Inference Optimization</a></h3>
<p><strong>Batch Size Sensitivity</strong></p>
<ul>
<li>Small batch: Expert utilization may be poor</li>
<li>Large batch: Better expert utilization, but higher latency</li>
<li>Recommendation: Batch size ≥ num_experts * k</li>
</ul>
<p><strong>Expert Caching</strong></p>
<pre><code class="language-python"># Cache expert weights on device for faster access
expert_cache = {i: expert.to('cuda') for i, expert in enumerate(experts)}
</code></pre>
<p><strong>Top-1 Routing for Inference</strong>
Even if trained with k=2, consider k=1 at inference for speed:</p>
<pre><code class="language-python">model.eval()
# Override k for inference
model.moe.k = 1
</code></pre>
<h3 id="6-when-to-replace-ffn-with-moe"><a class="header" href="#6-when-to-replace-ffn-with-moe">6. When to Replace FFN with MoE</a></h3>
<p>In a transformer, you can replace FFNs with MoE at different frequencies:</p>
<ul>
<li><strong>Every Layer</strong>: Maximum capacity, highest communication cost</li>
<li><strong>Every Other Layer</strong>: Good balance (recommended for most cases)</li>
<li><strong>Every 4th Layer</strong>: More efficient, slightly lower capacity</li>
<li><strong>Only Deep Layers</strong>: Shallow layers stay dense, deep layers use MoE</li>
</ul>
<pre><code class="language-python"># Example: MoE every other layer
for i, layer in enumerate(transformer.layers):
    if i % 2 == 0:
        layer.ffn = MoE(...)
    else:
        layer.ffn = DenseFFN(...)
</code></pre>
<h2 id="common-pitfalls"><a class="header" href="#common-pitfalls">Common Pitfalls</a></h2>
<h3 id="1-routing-collapse"><a class="header" href="#1-routing-collapse">1. Routing Collapse</a></h3>
<p><strong>Symptom</strong>: All tokens route to 1-2 experts.</p>
<p><strong>Causes</strong>:</p>
<ul>
<li>Insufficient load balancing loss</li>
<li>Poor expert initialization</li>
<li>Learning rate too high for router</li>
</ul>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-python"># Increase load balance loss
alpha_aux = 0.05  # Higher than default 0.01

# Add entropy regularization
loss += 0.01 * entropy_regularization(gates)

# Lower router learning rate
optimizer = AdamW([
    {'params': experts.parameters(), 'lr': 1e-4},
    {'params': router.parameters(), 'lr': 5e-5},  # Lower
])
</code></pre>
<h3 id="2-expert-underutilization"><a class="header" href="#2-expert-underutilization">2. Expert Underutilization</a></h3>
<p><strong>Symptom</strong>: Some experts receive &lt;1% of tokens.</p>
<p><strong>Causes</strong>:</p>
<ul>
<li>Random initialization too similar</li>
<li>Dataset too small</li>
<li>Too many experts for task complexity</li>
</ul>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-python"># Re-initialize unused experts
if expert_usage[i] &lt; threshold:
    experts[i].apply(init_weights)
    # Add noise to differentiate
    for p in experts[i].parameters():
        p.data += torch.randn_like(p) * 0.1
</code></pre>
<h3 id="3-numerical-instability"><a class="header" href="#3-numerical-instability">3. Numerical Instability</a></h3>
<p><strong>Symptom</strong>: NaN losses, exploding router logits.</p>
<p><strong>Causes</strong>:</p>
<ul>
<li>Router logits too large</li>
<li>No z-loss</li>
<li>Extreme expert outputs</li>
</ul>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-python"># Add router z-loss
z_loss = compute_router_z_loss(aux_info)
loss += 0.001 * z_loss

# Clip router logits
logits = torch.clamp(logits, min=-10, max=10)

# Gradient clipping
torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
</code></pre>
<h3 id="4-memory-overflow"><a class="header" href="#4-memory-overflow">4. Memory Overflow</a></h3>
<p><strong>Symptom</strong>: OOM errors during training.</p>
<p><strong>Cause</strong>: All experts in memory even if not active.</p>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-python"># Expert offloading (for very large models)
expert_on_cpu = [e.cpu() for e in experts]

def forward_with_offloading(x, expert_idx):
    expert = expert_on_cpu[expert_idx].cuda()
    output = expert(x)
    expert.cpu()
    return output

# Or use expert parallelism (distribute across GPUs)
</code></pre>
<h3 id="5-poor-fine-tuning-performance"><a class="header" href="#5-poor-fine-tuning-performance">5. Poor Fine-tuning Performance</a></h3>
<p><strong>Symptom</strong>: Performance degrades when fine-tuning on downstream task.</p>
<p><strong>Causes</strong>:</p>
<ul>
<li>Router overfits to small dataset</li>
<li>Load balancing fails on narrow domain</li>
</ul>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-python"># Freeze router during fine-tuning
for p in model.router.parameters():
    p.requires_grad = False

# Use much higher load balance weight
alpha_aux = 0.1  # vs. 0.01 during pre-training

# Lower learning rate
lr = 1e-5  # vs. 1e-4 during pre-training
</code></pre>
<h3 id="6-communication-bottleneck"><a class="header" href="#6-communication-bottleneck">6. Communication Bottleneck</a></h3>
<p><strong>Symptom</strong>: Slow training despite sparse activation.</p>
<p><strong>Cause</strong>: Too much inter-device communication in expert parallelism.</p>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-python"># Group experts on same device
experts_per_device = num_experts // num_devices

# Use all-to-all collective for efficient communication
# Instead of point-to-point transfers

# Consider expert replication for small experts
</code></pre>
<h2 id="resources"><a class="header" href="#resources">Resources</a></h2>
<h3 id="foundational-papers"><a class="header" href="#foundational-papers">Foundational Papers</a></h3>
<ol>
<li>
<p><strong>Shazeer et al. (2017)</strong> - “Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer”</p>
<ul>
<li>Original scaled MoE architecture</li>
<li>Noisy top-k gating</li>
<li>Load balancing via auxiliary loss</li>
</ul>
</li>
<li>
<p><strong>Lepikhin et al. (2020)</strong> - “GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding”</p>
<ul>
<li>MoE for machine translation</li>
<li>Expert parallelism strategies</li>
<li>600B parameter model</li>
</ul>
</li>
<li>
<p><strong>Fedus et al. (2021)</strong> - “Switch Transformers: Scaling to Trillion Parameter Models”</p>
<ul>
<li>Simplified k=1 routing</li>
<li>1.6T parameter model</li>
<li>Extensive empirical analysis</li>
</ul>
</li>
<li>
<p><strong>Zhou et al. (2022)</strong> - “Mixture-of-Experts with Expert Choice Routing”</p>
<ul>
<li>Flip routing paradigm</li>
<li>Perfect load balancing</li>
<li>No auxiliary loss needed</li>
</ul>
</li>
<li>
<p><strong>Jiang et al. (2024)</strong> - “Mixtral of Experts”</p>
<ul>
<li>Open-source MoE (8x7B)</li>
<li>Practical implementation insights</li>
<li>Competitive with 70B dense models</li>
</ul>
</li>
</ol>
<h3 id="advanced-topics-1"><a class="header" href="#advanced-topics-1">Advanced Topics</a></h3>
<ol start="6">
<li>
<p><strong>Riquelme et al. (2021)</strong> - “Scaling Vision with Sparse Mixture of Experts”</p>
<ul>
<li>MoE for vision transformers</li>
<li>Spatial routing patterns</li>
</ul>
</li>
<li>
<p><strong>Du et al. (2022)</strong> - “GLaM: Efficient Scaling of Language Models with Mixture-of-Experts”</p>
<ul>
<li>1.2T parameters</li>
<li>Energy efficiency analysis</li>
</ul>
</li>
<li>
<p><strong>DeepSeek-AI (2024)</strong> - “DeepSeek-V2”</p>
<ul>
<li>236B params (21B active)</li>
<li>Multi-head latent attention + MoE</li>
<li>Production deployment insights</li>
</ul>
</li>
</ol>
<h3 id="implementation-resources"><a class="header" href="#implementation-resources">Implementation Resources</a></h3>
<ul>
<li><strong>Fairseq MoE</strong>: https://github.com/facebookresearch/fairseq/tree/main/examples/moe</li>
<li><strong>Megatron-LM MoE</strong>: https://github.com/NVIDIA/Megatron-LM</li>
<li><strong>Mixtral (HuggingFace)</strong>: https://huggingface.co/mistralai/Mixtral-8x7B-v0.1</li>
<li><strong>Google Switch Transformers</strong>: https://github.com/google-research/t5x</li>
</ul>
<h3 id="related-topics-in-this-repository"><a class="header" href="#related-topics-in-this-repository">Related Topics in This Repository</a></h3>
<ul>
<li><a href="transformers.html">Transformers</a> - Core architecture that MoE extends</li>
<li><a href="lora.html">LoRA</a> - Parameter-efficient fine-tuning (complementary to MoE)</li>
<li><a href="quantization.html">Quantization</a> - Reducing memory for large MoE models</li>
<li><a href="neural_networks.html">Neural Networks</a> - Foundational concepts</li>
</ul>
<h3 id="tutorials-and-guides"><a class="header" href="#tutorials-and-guides">Tutorials and Guides</a></h3>
<ul>
<li><strong>HuggingFace MoE Tutorial</strong>: Practical implementation guide</li>
<li><strong>PyTorch MoE Examples</strong>: Official examples and best practices</li>
<li><strong>Weights &amp; Biases MoE Guide</strong>: Training and monitoring strategies</li>
</ul>
<hr>
<h2 id="quick-reference"><a class="header" href="#quick-reference">Quick Reference</a></h2>
<h3 id="key-equations"><a class="header" href="#key-equations">Key Equations</a></h3>
<pre><code># MoE Output
y = Σ G(x)_i * E_i(x)

# Gating Function
G(x) = Softmax(x · W_g)

# Top-K Sparse Gating
G_sparse(x)_i = G(x)_i / Σ_{j∈TopK} G(x)_j  if i ∈ TopK(G(x), k)
                0                            otherwise

# Load Balance Loss
L_aux = α · N · Σ f_i · P_i

# Router Z-Loss
L_z = mean((log_sum_exp(logits))²)

# Total Loss
L = L_task + α·L_aux + β·L_z
</code></pre>
<h3 id="typical-hyperparameters"><a class="header" href="#typical-hyperparameters">Typical Hyperparameters</a></h3>
<pre><code class="language-python">config = {
    'num_experts': 8,           # Number of expert networks
    'k': 2,                     # Top-k experts per token
    'capacity_factor': 1.25,    # Expert capacity multiplier
    'alpha_aux': 0.01,          # Load balance loss weight
    'alpha_z': 0.001,           # Router z-loss weight
    'd_model': 512,             # Model dimension
    'd_ff': 2048,               # Expert hidden dimension (4x d_model)
    'dropout': 0.1,             # Dropout rate
    'noise_std': 0.1,           # Router noise (training only)
}
</code></pre>
<h3 id="training-checklist"><a class="header" href="#training-checklist">Training Checklist</a></h3>
<ul>
<li><input disabled="" type="checkbox"> Initialize experts with different random seeds</li>
<li><input disabled="" type="checkbox"> Monitor expert utilization (should be roughly uniform)</li>
<li><input disabled="" type="checkbox"> Use load balancing loss (α ≈ 0.01)</li>
<li><input disabled="" type="checkbox"> Use router z-loss for stability (β ≈ 0.001)</li>
<li><input disabled="" type="checkbox"> Gradient clipping (max_norm = 1.0)</li>
<li><input disabled="" type="checkbox"> Higher LR for router than experts (2-5x)</li>
<li><input disabled="" type="checkbox"> Capacity factor &gt; 1.0 to handle overflow</li>
<li><input disabled="" type="checkbox"> Remove routing noise at inference</li>
<li><input disabled="" type="checkbox"> Track and log routing decisions</li>
<li><input disabled="" type="checkbox"> Validate gradient flow to all experts</li>
</ul>
<h3 id="debugging-commands"><a class="header" href="#debugging-commands">Debugging Commands</a></h3>
<pre><code class="language-python"># Check expert usage distribution
expert_counts = torch.bincount(top_k_indices.flatten())
print(f"Expert usage: {expert_counts}")

# Monitor load balance quality
cv = expert_counts.std() / expert_counts.mean()
print(f"Coefficient of variation: {cv:.4f}")  # Lower is better

# Check router logit magnitude
print(f"Logit magnitude: {logits.abs().mean():.4f}")  # Should be &lt; 10

# Verify gradient flow
for i, expert in enumerate(experts):
    grad_norm = sum(p.grad.norm() for p in expert.parameters() if p.grad is not None)
    print(f"Expert {i} grad: {grad_norm:.4f}")
</code></pre>
<hr>
<p><strong>Last Updated</strong>: 2024
<strong>Version</strong>: 1.0</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../machine_learning/thinking.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256 246.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"/></svg></span>
                            </a>

                            <a rel="next prefetch" href="../machine_learning/jax.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M278.6 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L210.7 256 73.4 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"/></svg></span>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../machine_learning/thinking.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256 246.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"/></svg></span>
                    </a>

                    <a rel="next prefetch" href="../machine_learning/jax.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M278.6 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L210.7 256 73.4 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"/></svg></span>
                    </a>
            </nav>

        </div>

        <template id=fa-eye><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M288 32c-80.8 0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7 0 24.6C17.3 304 48.6 356 95.4 399.4C142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144s64.5-144 144-144s144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64c-11.5 0-22.3-3-31.6-8.4c-.2 2.8-.4 5.5-.4 8.4c0 53 43 96 96 96s96-43 96-96s-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6z"/></svg></span></template>
        <template id=fa-eye-slash><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M38.8 5.1C28.4-3.1 13.3-1.2 5.1 9.2S-1.2 34.7 9.2 42.9l592 464c10.4 8.2 25.5 6.3 33.7-4.1s6.3-25.5-4.1-33.7L525.6 386.7c39.6-40.6 66.4-86.1 79.9-118.4c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C465.5 68.8 400.8 32 320 32c-68.2 0-125 26.3-169.3 60.8L38.8 5.1zM223.1 149.5C248.6 126.2 282.7 112 320 112c79.5 0 144 64.5 144 144c0 24.9-6.3 48.3-17.4 68.7L408 294.5c5.2-11.8 8-24.8 8-38.5c0-53-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6c0 10.2-2.4 19.8-6.6 28.3l-90.3-70.8zm223.1 298L373 389.9c-16.4 6.5-34.3 10.1-53 10.1c-79.5 0-144-64.5-144-144c0-6.9 .5-13.6 1.4-20.2L83.1 161.5C60.3 191.2 44 220.8 34.5 243.7c-3.3 7.9-3.3 16.7 0 24.6c14.9 35.7 46.2 87.7 93 131.1C174.5 443.2 239.2 480 320 480c47.8 0 89.9-12.9 126.2-32.5z"/></svg></span></template>
        <template id=fa-copy><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M502.6 70.63l-61.25-61.25C435.4 3.371 427.2 0 418.7 0H255.1c-35.35 0-64 28.66-64 64l.0195 256C192 355.4 220.7 384 256 384h192c35.2 0 64-28.8 64-64V93.25C512 84.77 508.6 76.63 502.6 70.63zM464 320c0 8.836-7.164 16-16 16H255.1c-8.838 0-16-7.164-16-16L239.1 64.13c0-8.836 7.164-16 16-16h128L384 96c0 17.67 14.33 32 32 32h47.1V320zM272 448c0 8.836-7.164 16-16 16H63.1c-8.838 0-16-7.164-16-16L47.98 192.1c0-8.836 7.164-16 16-16H160V128H63.99c-35.35 0-64 28.65-64 64l.0098 256C.002 483.3 28.66 512 64 512h192c35.2 0 64-28.8 64-64v-32h-47.1L272 448z"/></svg></span></template>
        <template id=fa-play><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M73 39c-14.8-9.1-33.4-9.4-48.5-.9S0 62.6 0 80V432c0 17.4 9.4 33.4 24.5 41.9s33.7 8.1 48.5-.9L361 297c14.3-8.7 23-24.2 23-41s-8.7-32.2-23-41L73 39z"/></svg></span></template>
        <template id=fa-clock-rotate-left><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M75 75L41 41C25.9 25.9 0 36.6 0 57.9V168c0 13.3 10.7 24 24 24H134.1c21.4 0 32.1-25.9 17-41l-30.8-30.8C155 85.5 203 64 256 64c106 0 192 86 192 192s-86 192-192 192c-40.8 0-78.6-12.7-109.7-34.4c-14.5-10.1-34.4-6.6-44.6 7.9s-6.6 34.4 7.9 44.6C151.2 495 201.7 512 256 512c141.4 0 256-114.6 256-256S397.4 0 256 0C185.3 0 121.3 28.7 75 75zm181 53c-13.3 0-24 10.7-24 24V256c0 6.4 2.5 12.5 7 17l72 72c9.4 9.4 24.6 9.4 33.9 0s9.4-24.6 0-33.9l-65-65V152c0-13.3-10.7-24-24-24z"/></svg></span></template>



        <script>
            window.playground_copyable = true;
        </script>


        <script src="../elasticlunr-ef4e11c1.min.js"></script>
        <script src="../mark-09e88c2c.min.js"></script>
        <script src="../searcher-c2a407aa.js"></script>

        <script src="../clipboard-1626706a.min.js"></script>
        <script src="../highlight-abc7f01d.js"></script>
        <script src="../book-a0b12cfe.js"></script>

        <!-- Custom JS scripts -->



    </div>
    </body>
</html>
