<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Lora - My Notes</title>


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon-de23e50b.svg">
        <link rel="shortcut icon" href="../favicon-8114d1fc.png">
        <link rel="stylesheet" href="../css/variables-8adf115d.css">
        <link rel="stylesheet" href="../css/general-2459343d.css">
        <link rel="stylesheet" href="../css/chrome-ae938929.css">
        <link rel="stylesheet" href="../css/print-9e4910d8.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../fonts/fonts-9644e21d.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="mdbook-highlight-css" href="../highlight-493f70e1.css">
        <link rel="stylesheet" id="mdbook-tomorrow-night-css" href="../tomorrow-night-4c0ae647.css">
        <link rel="stylesheet" id="mdbook-ayu-highlight-css" href="../ayu-highlight-3fdfc3ac.css">

        <!-- Custom theme stylesheets -->

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "../";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "../searchindex-a5d335fc.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../toc-cda83475.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="mdbook-body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="mdbook-sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("mdbook-sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="mdbook-sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="../toc.html"></iframe>
            </noscript>
            <div id="mdbook-sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="mdbook-page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="mdbook-menu-bar-hover-placeholder"></div>
                <div id="mdbook-menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="mdbook-sidebar-toggle" class="icon-button" for="mdbook-sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="mdbook-sidebar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"/></svg></span>
                        </label>
                        <button id="mdbook-theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="mdbook-theme-list">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M371.3 367.1c27.3-3.9 51.9-19.4 67.2-42.9L600.2 74.1c12.6-19.5 9.4-45.3-7.6-61.2S549.7-4.4 531.1 9.6L294.4 187.2c-24 18-38.2 46.1-38.4 76.1L371.3 367.1zm-19.6 25.4l-116-104.4C175.9 290.3 128 339.6 128 400c0 3.9 .2 7.8 .6 11.6c1.8 17.5-10.2 36.4-27.8 36.4H96c-17.7 0-32 14.3-32 32s14.3 32 32 32H240c61.9 0 112-50.1 112-112c0-2.5-.1-5-.2-7.5z"/></svg></span>
                        </button>
                        <ul id="mdbook-theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-ayu">Ayu</button></li>
                        </ul>
                        <button id="mdbook-search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="mdbook-searchbar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M416 208c0 45.9-14.9 88.3-40 122.7L502.6 457.4c12.5 12.5 12.5 32.8 0 45.3s-32.8 12.5-45.3 0L330.7 376c-34.4 25.2-76.8 40-122.7 40C93.1 416 0 322.9 0 208S93.1 0 208 0S416 93.1 416 208zM208 352c79.5 0 144-64.5 144-144s-64.5-144-144-144S64 128.5 64 208s64.5 144 144 144z"/></svg></span>
                        </button>
                    </div>

                    <h1 class="menu-title">My Notes</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <span class=fa-svg id="print-button"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M128 0C92.7 0 64 28.7 64 64v96h64V64H354.7L384 93.3V160h64V93.3c0-17-6.7-33.3-18.7-45.3L400 18.7C388 6.7 371.7 0 354.7 0H128zM384 352v32 64H128V384 368 352H384zm64 32h32c17.7 0 32-14.3 32-32V256c0-35.3-28.7-64-64-64H64c-35.3 0-64 28.7-64 64v96c0 17.7 14.3 32 32 32H64v64c0 35.3 28.7 64 64 64H384c35.3 0 64-28.7 64-64V384zm-16-88c-13.3 0-24-10.7-24-24s10.7-24 24-24s24 10.7 24 24s-10.7 24-24 24z"/></svg></span>
                        </a>

                    </div>
                </div>

                <div id="mdbook-search-wrapper" class="hidden">
                    <form id="mdbook-searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="mdbook-searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="mdbook-searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <span class=fa-svg id="fa-spin"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M304 48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zm0 416c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM48 304c26.5 0 48-21.5 48-48s-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48zm464-48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM142.9 437c18.7-18.7 18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zm0-294.2c18.7-18.7 18.7-49.1 0-67.9S93.7 56.2 75 75s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zM369.1 437c18.7 18.7 49.1 18.7 67.9 0s18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9z"/></svg></span>
                            </div>
                        </div>
                    </form>
                    <div id="mdbook-searchresults-outer" class="searchresults-outer hidden">
                        <div id="mdbook-searchresults-header" class="searchresults-header"></div>
                        <ul id="mdbook-searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('mdbook-sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('mdbook-sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#mdbook-sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="mdbook-content" class="content">
                    <main>
                        <h1 id="lora-low-rank-adaptation"><a class="header" href="#lora-low-rank-adaptation">LoRA (Low-Rank Adaptation)</a></h1>
<h2 id="overview"><a class="header" href="#overview">Overview</a></h2>
<p>LoRA (Low-Rank Adaptation of Large Language Models) is a parameter-efficient fine-tuning (PEFT) technique that dramatically reduces the computational and memory requirements for adapting large pre-trained models to downstream tasks. Instead of fine-tuning all model parameters, LoRA freezes the pre-trained weights and injects trainable low-rank decomposition matrices into each layer of the transformer architecture.</p>
<p><strong>Key Advantages:</strong></p>
<ul>
<li><strong>Memory Efficiency</strong>: Reduces trainable parameters by 10,000x for large models</li>
<li><strong>Storage Efficiency</strong>: Adapter weights are tiny (often &lt;100MB vs multi-GB full models)</li>
<li><strong>No Inference Latency</strong>: Adapters can be merged into base weights</li>
<li><strong>Task Switching</strong>: Multiple adapters can be stored and swapped dynamically</li>
<li><strong>Same Performance</strong>: Matches or exceeds full fine-tuning quality on most tasks</li>
</ul>
<p><strong>When to Use LoRA:</strong></p>
<ul>
<li>Fine-tuning large language models (7B+ parameters)</li>
<li>Limited computational resources (consumer GPUs)</li>
<li>Need to maintain multiple task-specific versions</li>
<li>Production deployment with multiple use cases</li>
<li>Rapid experimentation and iteration</li>
</ul>
<h2 id="table-of-contents"><a class="header" href="#table-of-contents">Table of Contents</a></h2>
<ol>
<li><a href="#fundamentals">Fundamentals</a></li>
<li><a href="#mathematical-foundation">Mathematical Foundation</a></li>
<li><a href="#architecture-and-implementation">Architecture and Implementation</a></li>
<li><a href="#qlora-quantized-lora">QLoRA: Quantized LoRA</a></li>
<li><a href="#common-patterns">Common Patterns</a></li>
<li><a href="#operations">Operations</a></li>
<li><a href="#configuration-and-hyperparameters">Configuration and Hyperparameters</a></li>
<li><a href="#implementation-examples">Implementation Examples</a></li>
<li><a href="#advanced-topics">Advanced Topics</a></li>
<li><a href="#best-practices">Best Practices</a></li>
</ol>
<h2 id="fundamentals"><a class="header" href="#fundamentals">Fundamentals</a></h2>
<h3 id="how-lora-works"><a class="header" href="#how-lora-works">How LoRA Works</a></h3>
<p>Traditional fine-tuning updates all parameters of a pre-trained model:</p>
<pre><code>W_finetuned = W_pretrained + ΔW
</code></pre>
<p>LoRA constrains the update ΔW to have a low-rank structure:</p>
<pre><code>W_finetuned = W_pretrained + B·A
</code></pre>
<p>Where:</p>
<ul>
<li><code>W ∈ ℝ^(d×k)</code>: Original pre-trained weight matrix (frozen)</li>
<li><code>B ∈ ℝ^(d×r)</code>: Trainable low-rank matrix</li>
<li><code>A ∈ ℝ^(r×k)</code>: Trainable low-rank matrix</li>
<li><code>r &lt;&lt; min(d,k)</code>: Rank of adaptation (typically 4-64)</li>
</ul>
<p><strong>Key Insight</strong>: The update to pre-trained weights lies in a low-dimensional subspace. Most adaptation can be captured by low-rank matrices.</p>
<h3 id="parameter-reduction"><a class="header" href="#parameter-reduction">Parameter Reduction</a></h3>
<p>For a weight matrix of shape (d × k):</p>
<ul>
<li><strong>Full fine-tuning</strong>: d × k trainable parameters</li>
<li><strong>LoRA</strong>: r × (d + k) trainable parameters</li>
</ul>
<p><strong>Example</strong> (GPT-3 175B):</p>
<ul>
<li>Full fine-tuning: 175B parameters</li>
<li>LoRA (r=4): ~18M parameters (~0.01% of original)</li>
</ul>
<h2 id="mathematical-foundation"><a class="header" href="#mathematical-foundation">Mathematical Foundation</a></h2>
<h3 id="low-rank-decomposition"><a class="header" href="#low-rank-decomposition">Low-Rank Decomposition</a></h3>
<p>LoRA leverages the hypothesis that the update matrix ΔW has a low “intrinsic rank”:</p>
<pre><code>ΔW = BA
</code></pre>
<p>Where:</p>
<ul>
<li>Rank r is chosen such that r &lt;&lt; min(d,k)</li>
<li>B is initialized with random Gaussian</li>
<li>A is initialized to zero (ensuring ΔW = 0 at start)</li>
</ul>
<h3 id="forward-pass"><a class="header" href="#forward-pass">Forward Pass</a></h3>
<p>Original transformation:</p>
<pre><code>h = Wx
</code></pre>
<p>With LoRA:</p>
<pre><code>h = Wx + BAx = Wx + (BA)x
</code></pre>
<p>The scaling factor α/r is applied:</p>
<pre><code>h = Wx + (α/r)·BAx
</code></pre>
<p>Where α is a constant that controls the magnitude of adaptation.</p>
<h3 id="gradient-flow"><a class="header" href="#gradient-flow">Gradient Flow</a></h3>
<p>During backpropagation:</p>
<ul>
<li>W remains frozen (no gradients)</li>
<li>Only A and B receive gradients</li>
<li>Effective learning occurs in low-dimensional subspace</li>
</ul>
<p><strong>Computational Advantage</strong>:</p>
<pre><code>Memory for gradients: O(r·(d+k)) vs O(d·k)
Update computation: O(r·(d+k)) vs O(d·k)
</code></pre>
<h3 id="theoretical-justification"><a class="header" href="#theoretical-justification">Theoretical Justification</a></h3>
<p><strong>Intrinsic Dimensionality</strong>: Research shows that learned adaptations often lie in low-dimensional subspaces. LoRA exploits this by explicitly constraining updates to low-rank matrices.</p>
<p><strong>Connection to SVD</strong>: If we perform SVD on a full fine-tuned ΔW:</p>
<pre><code>ΔW = UΣV^T
</code></pre>
<p>Most singular values are close to zero, suggesting low intrinsic rank.</p>
<h2 id="architecture-and-implementation"><a class="header" href="#architecture-and-implementation">Architecture and Implementation</a></h2>
<h3 id="target-modules"><a class="header" href="#target-modules">Target Modules</a></h3>
<p>LoRA can be applied to various transformer components:</p>
<p><strong>Attention Matrices</strong> (most common):</p>
<ul>
<li><code>q_proj</code>: Query projection</li>
<li><code>k_proj</code>: Key projection</li>
<li><code>v_proj</code>: Value projection</li>
<li><code>o_proj</code>: Output projection</li>
</ul>
<p><strong>Feed-Forward Networks</strong>:</p>
<ul>
<li><code>gate_proj</code>: Gate projection (for architectures like LLaMA)</li>
<li><code>up_proj</code>: Up projection</li>
<li><code>down_proj</code>: Down projection</li>
</ul>
<p><strong>Embedding Layers</strong>:</p>
<ul>
<li>Input embeddings</li>
<li>Output embeddings (LM head)</li>
</ul>
<h3 id="layer-structure"><a class="header" href="#layer-structure">Layer Structure</a></h3>
<pre><code>┌─────────────────────────────────┐
│    Original Transformer Layer    │
├─────────────────────────────────┤
│                                 │
│  ┌──────────────┐               │
│  │ Pre-trained  │ (Frozen)      │
│  │   Weights W  │────┐          │
│  └──────────────┘    │          │
│                      ▼          │
│  ┌──────┐  ┌──────┐  ┌────┐    │
│  │  B   │  │  A   │  │ +  │───▶│ Output
│  │ (d×r)│─▶│ (r×k)│─▶│    │    │
│  └──────┘  └──────┘  └────┘    │
│  Trainable  Trainable    ^      │
│                          │      │
│                        Input    │
│                                 │
└─────────────────────────────────┘
</code></pre>
<h3 id="initialization-strategy"><a class="header" href="#initialization-strategy">Initialization Strategy</a></h3>
<pre><code class="language-python"># LoRA initialization (standard approach)
def init_lora_weights(A, B, r):
    # Initialize A with random Gaussian
    nn.init.kaiming_uniform_(A, a=math.sqrt(5))

    # Initialize B to zero
    nn.init.zeros_(B)

    # This ensures ΔW = BA = 0 at initialization
    # Model starts with pre-trained behavior
</code></pre>
<h2 id="qlora-quantized-lora"><a class="header" href="#qlora-quantized-lora">QLoRA: Quantized LoRA</a></h2>
<p>QLoRA combines quantization with LoRA for even more efficient fine-tuning. It enables training 65B+ parameter models on consumer GPUs.</p>
<h3 id="core-innovations"><a class="header" href="#core-innovations">Core Innovations</a></h3>
<p><strong>4-bit NormalFloat (NF4)</strong>:</p>
<ul>
<li>Custom data type optimized for normally distributed weights</li>
<li>Information-theoretically optimal for Gaussian distributions</li>
<li>Better preservation of model quality than standard INT4</li>
</ul>
<p><strong>Double Quantization</strong>:</p>
<ul>
<li>Quantizes the quantization constants themselves</li>
<li>Saves additional 0.37 bits per parameter on average</li>
</ul>
<p><strong>Paged Optimizers</strong>:</p>
<ul>
<li>Uses unified memory to handle optimizer state spikes</li>
<li>Prevents out-of-memory errors during training</li>
</ul>
<h3 id="qlora-architecture"><a class="header" href="#qlora-architecture">QLoRA Architecture</a></h3>
<pre><code>┌─────────────────────────────────────┐
│         QLoRA Architecture          │
├─────────────────────────────────────┤
│                                     │
│  ┌──────────────────┐               │
│  │ Pre-trained      │               │
│  │ Weights W        │               │
│  │ (Quantized 4-bit)│───┐           │
│  └──────────────────┘   │           │
│        (Frozen)          ▼           │
│                     Dequantize       │
│                          │           │
│                          ▼           │
│  ┌──────┐  ┌──────┐  ┌────┐         │
│  │  B   │  │  A   │  │ +  │────▶    │
│  │(FP16)│─▶│(FP16)│─▶│    │    Output│
│  └──────┘  └──────┘  └────┘         │
│  Trainable  Trainable                │
│                                      │
└──────────────────────────────────────┘
</code></pre>
<h3 id="nf4-quantization"><a class="header" href="#nf4-quantization">NF4 Quantization</a></h3>
<pre><code class="language-python">import torch
import bitsandbytes as bnb

# NF4 quantization process
def quantize_nf4(weights):
    """
    Quantize weights to 4-bit NormalFloat
    """
    # Compute normalization constants
    absmax = torch.max(torch.abs(weights))

    # NF4 quantization bins (optimized for normal distribution)
    nf4_bins = [
        -1.0, -0.6961928009986877, -0.5250730514526367,
        -0.39491748809814453, -0.28444138169288635,
        -0.18477343022823334, -0.09105003625154495,
        0.0, 0.07958029955625534, 0.16093020141124725,
        0.24611230194568634, 0.33791524171829224,
        0.44070982933044434, 0.5626170039176941,
        0.7229568362236023, 1.0
    ]

    # Map weights to nearest bin
    normalized = weights / absmax
    quantized = torch.zeros_like(weights, dtype=torch.uint8)

    for i, val in enumerate(normalized.flatten()):
        # Find closest bin
        idx = min(range(len(nf4_bins)),
                 key=lambda i: abs(nf4_bins[i] - val))
        quantized.view(-1)[i] = idx

    return quantized, absmax

# Dequantization
def dequantize_nf4(quantized, absmax, nf4_bins):
    """
    Dequantize NF4 back to float16
    """
    dequantized = torch.zeros_like(quantized, dtype=torch.float16)

    for i, idx in enumerate(quantized.flatten()):
        dequantized.view(-1)[i] = nf4_bins[idx] * absmax

    return dequantized
</code></pre>
<h3 id="memory-comparison"><a class="header" href="#memory-comparison">Memory Comparison</a></h3>
<p>For a 65B parameter model:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Method</th><th>Memory Required</th><th>Trainable Params</th></tr>
</thead>
<tbody>
<tr><td>Full FP32 Fine-tuning</td><td>~260 GB</td><td>65B</td></tr>
<tr><td>Full FP16 Fine-tuning</td><td>~130 GB</td><td>65B</td></tr>
<tr><td>LoRA (FP16)</td><td>~80 GB</td><td>~84M (r=8)</td></tr>
<tr><td>QLoRA (NF4)</td><td>~48 GB</td><td>~84M (r=8)</td></tr>
</tbody>
</table>
</div>
<p>QLoRA makes 65B model fine-tuning possible on a single 48GB GPU!</p>
<h3 id="qlora-training-process"><a class="header" href="#qlora-training-process">QLoRA Training Process</a></h3>
<ol>
<li>
<p><strong>Load base model in 4-bit</strong>:</p>
<ul>
<li>Weights quantized to NF4</li>
<li>Stored in GPU memory</li>
</ul>
</li>
<li>
<p><strong>Forward pass</strong>:</p>
<ul>
<li>Dequantize weights to FP16 on-the-fly</li>
<li>Compute activations in FP16/BF16</li>
<li>Apply LoRA adapters (FP16)</li>
</ul>
</li>
<li>
<p><strong>Backward pass</strong>:</p>
<ul>
<li>Compute gradients for LoRA adapters only</li>
<li>Base model weights remain frozen and quantized</li>
</ul>
</li>
<li>
<p><strong>Optimizer step</strong>:</p>
<ul>
<li>Update only LoRA parameters</li>
<li>Use paged optimizers for state management</li>
</ul>
</li>
</ol>
<h2 id="common-patterns"><a class="header" href="#common-patterns">Common Patterns</a></h2>
<h3 id="pattern-1-single-task-fine-tuning"><a class="header" href="#pattern-1-single-task-fine-tuning">Pattern 1: Single-Task Fine-Tuning</a></h3>
<p><strong>Use Case</strong>: Adapt a general model to a specific task (e.g., medical Q&amp;A, code generation, sentiment analysis).</p>
<pre><code class="language-python">from peft import LoraConfig, get_peft_model
from transformers import AutoModelForCausalLM

# Load base model
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf")

# Configure LoRA
lora_config = LoraConfig(
    r=16,                      # Rank
    lora_alpha=32,             # Scaling factor
    target_modules=["q_proj", "v_proj"],  # Which layers to adapt
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

# Apply LoRA
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
# Output: trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06%

# Train normally
# ... training loop ...

# Save adapter weights only
model.save_pretrained("./lora_medical_qa")
</code></pre>
<h3 id="pattern-2-multi-task-with-adapter-switching"><a class="header" href="#pattern-2-multi-task-with-adapter-switching">Pattern 2: Multi-Task with Adapter Switching</a></h3>
<p><strong>Use Case</strong>: One base model, multiple task-specific adapters that can be swapped at runtime.</p>
<pre><code class="language-python">from peft import PeftModel

# Load base model
base_model = AutoModelForCausalLM.from_pretrained("base-model")

# Load different adapters for different tasks
medical_model = PeftModel.from_pretrained(base_model, "./lora_medical")
legal_model = PeftModel.from_pretrained(base_model, "./lora_legal")
code_model = PeftModel.from_pretrained(base_model, "./lora_code")

# Use different adapters
def generate_medical(prompt):
    return medical_model.generate(prompt)

def generate_legal(prompt):
    return legal_model.generate(prompt)

# Or dynamically swap adapters
model = PeftModel.from_pretrained(base_model, "./lora_medical")
output1 = model.generate(medical_prompt)

model.set_adapter("legal")  # Switch adapter
output2 = model.generate(legal_prompt)
</code></pre>
<h3 id="pattern-3-progressive-rank-adaptation"><a class="header" href="#pattern-3-progressive-rank-adaptation">Pattern 3: Progressive Rank Adaptation</a></h3>
<p><strong>Use Case</strong>: Start with low rank for fast experimentation, increase for final training.</p>
<pre><code class="language-python"># Phase 1: Quick exploration with low rank
config_phase1 = LoraConfig(r=4, lora_alpha=8, ...)
model = get_peft_model(base_model, config_phase1)
# Train for few epochs to validate approach

# Phase 2: Higher rank for better performance
config_phase2 = LoraConfig(r=32, lora_alpha=64, ...)
model = get_peft_model(base_model, config_phase2)
# Train to convergence
</code></pre>
<h3 id="pattern-4-selective-layer-targeting"><a class="header" href="#pattern-4-selective-layer-targeting">Pattern 4: Selective Layer Targeting</a></h3>
<p><strong>Use Case</strong>: Apply LoRA only to specific layers where adaptation is most beneficial.</p>
<pre><code class="language-python"># Target only attention in middle layers
lora_config = LoraConfig(
    r=16,
    target_modules=[
        "model.layers.12.self_attn.q_proj",
        "model.layers.12.self_attn.v_proj",
        "model.layers.13.self_attn.q_proj",
        "model.layers.13.self_attn.v_proj",
        # ... layers 12-20 only
    ],
    # Or use regex patterns
    # target_modules=r".*layers\.(1[2-9]|20)\.self_attn\.(q|v)_proj",
)
</code></pre>
<h3 id="pattern-5-merge-and-deploy"><a class="header" href="#pattern-5-merge-and-deploy">Pattern 5: Merge and Deploy</a></h3>
<p><strong>Use Case</strong>: Production deployment where you want a single model file without adapter overhead.</p>
<pre><code class="language-python">from peft import PeftModel

# Load base model and adapter
base_model = AutoModelForCausalLM.from_pretrained("base-model")
peft_model = PeftModel.from_pretrained(base_model, "./lora_adapter")

# Merge adapter into base weights
merged_model = peft_model.merge_and_unload()

# Save as standard model (no LoRA dependency)
merged_model.save_pretrained("./merged_model")

# Now can be used without PEFT library
from transformers import AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained("./merged_model")
</code></pre>
<h3 id="pattern-6-multi-adapter-composition"><a class="header" href="#pattern-6-multi-adapter-composition">Pattern 6: Multi-Adapter Composition</a></h3>
<p><strong>Use Case</strong>: Combine multiple adapters for composite capabilities.</p>
<pre><code class="language-python">from peft import PeftModel

base_model = AutoModelForCausalLM.from_pretrained("base-model")

# Load and combine multiple adapters
model = PeftModel.from_pretrained(base_model, "./lora_instruction", adapter_name="instruction")
model.load_adapter("./lora_style", adapter_name="style")
model.load_adapter("./lora_domain", adapter_name="domain")

# Use weighted combination
model.set_adapter(["instruction", "style", "domain"])
model.set_adapter_weights([0.5, 0.3, 0.2])  # Weighted mix

# Generate with combined capabilities
output = model.generate(prompt)
</code></pre>
<h2 id="operations"><a class="header" href="#operations">Operations</a></h2>
<h3 id="training-with-lora"><a class="header" href="#training-with-lora">Training with LoRA</a></h3>
<p><strong>Basic Training Loop</strong>:</p>
<pre><code class="language-python">import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer
from peft import LoraConfig, get_peft_model
from datasets import load_dataset

# Load model and tokenizer
model_name = "meta-llama/Llama-2-7b-hf"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Configure LoRA
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

# Apply LoRA
model = get_peft_model(model, lora_config)

# Load and prepare dataset
dataset = load_dataset("your_dataset")

def tokenize_function(examples):
    return tokenizer(examples["text"], truncation=True, max_length=512)

tokenized_dataset = dataset.map(tokenize_function, batched=True)

# Training arguments
training_args = TrainingArguments(
    output_dir="./lora_output",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    learning_rate=2e-4,
    fp16=True,
    logging_steps=10,
    save_steps=100,
    save_total_limit=2,
)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["validation"],
)

# Train
trainer.train()

# Save LoRA adapter
model.save_pretrained("./final_lora_adapter")
</code></pre>
<h3 id="training-with-qlora"><a class="header" href="#training-with-qlora">Training with QLoRA</a></h3>
<pre><code class="language-python">import torch
from transformers import AutoModelForCausalLM, BitsAndBytesConfig
from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model

# Configure 4-bit quantization
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,      # Double quantization
    bnb_4bit_quant_type="nf4",           # NormalFloat4
    bnb_4bit_compute_dtype=torch.bfloat16  # Compute in BF16
)

# Load model in 4-bit
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-70b-hf",
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True
)

# Prepare for k-bit training
model = prepare_model_for_kbit_training(model)

# Configure LoRA
lora_config = LoraConfig(
    r=64,                      # Higher rank for large models
    lora_alpha=128,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                    "gate_proj", "up_proj", "down_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

# Apply LoRA
model = get_peft_model(model, lora_config)

# Rest of training is identical to standard LoRA
# ... training loop ...
</code></pre>
<h3 id="inference-with-lora"><a class="header" href="#inference-with-lora">Inference with LoRA</a></h3>
<pre><code class="language-python">from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer

# Load base model
base_model = AutoModelForCausalLM.from_pretrained(
    "base-model",
    torch_dtype=torch.float16,
    device_map="auto"
)

# Load LoRA adapter
model = PeftModel.from_pretrained(base_model, "./lora_adapter")

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained("base-model")

# Generate
prompt = "Explain quantum computing in simple terms:"
inputs = tokenizer(prompt, return_tensors="pt").to("cuda")

with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=200,
        temperature=0.7,
        top_p=0.9,
        do_sample=True
    )

response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)
</code></pre>
<h3 id="merging-adapters"><a class="header" href="#merging-adapters">Merging Adapters</a></h3>
<pre><code class="language-python">from peft import PeftModel

# Load model with adapter
base_model = AutoModelForCausalLM.from_pretrained("base-model")
peft_model = PeftModel.from_pretrained(base_model, "./lora_adapter")

# Method 1: Merge and unload (creates new model)
merged_model = peft_model.merge_and_unload()
merged_model.save_pretrained("./merged_model")

# Method 2: Merge in place (modifies base model)
peft_model.merge_adapter()
# Now peft_model has merged weights

# Method 3: Unmerge (reverse the merge)
peft_model.unmerge_adapter()
# Back to base weights + adapter separation
</code></pre>
<h3 id="saving-and-loading"><a class="header" href="#saving-and-loading">Saving and Loading</a></h3>
<pre><code class="language-python"># Save adapter only (efficient)
model.save_pretrained("./my_lora_adapter")
# Creates: adapter_config.json, adapter_model.bin (~10-100 MB)

# Save with optimizer state for resuming training
trainer.save_model("./checkpoint_dir")
# Creates: adapter files + optimizer state + training state

# Load for inference
from peft import PeftModel
model = AutoModelForCausalLM.from_pretrained("base-model")
model = PeftModel.from_pretrained(model, "./my_lora_adapter")

# Load for continued training
model = AutoModelForCausalLM.from_pretrained("base-model")
model = PeftModel.from_pretrained(model, "./checkpoint_dir", is_trainable=True)
</code></pre>
<h3 id="memory-efficient-loading"><a class="header" href="#memory-efficient-loading">Memory-Efficient Loading</a></h3>
<pre><code class="language-python"># For large models, load in 8-bit
from transformers import AutoModelForCausalLM
import torch

model = AutoModelForCausalLM.from_pretrained(
    "large-model",
    load_in_8bit=True,           # 8-bit quantization
    device_map="auto",           # Automatic device placement
    torch_dtype=torch.float16
)

# Or use model sharding across GPUs
model = AutoModelForCausalLM.from_pretrained(
    "large-model",
    device_map="balanced",       # Balance across GPUs
    torch_dtype=torch.float16
)
</code></pre>
<h2 id="configuration-and-hyperparameters"><a class="header" href="#configuration-and-hyperparameters">Configuration and Hyperparameters</a></h2>
<h3 id="loraconfig-parameters"><a class="header" href="#loraconfig-parameters">LoraConfig Parameters</a></h3>
<pre><code class="language-python">from peft import LoraConfig

config = LoraConfig(
    # Core LoRA parameters
    r=8,                           # Rank of adaptation matrices
    lora_alpha=16,                 # Scaling factor (often 2*r)
    target_modules=["q_proj", "v_proj"],  # Modules to apply LoRA
    lora_dropout=0.1,              # Dropout for LoRA layers

    # Bias handling
    bias="none",                   # "none", "all", "lora_only"

    # Task type
    task_type="CAUSAL_LM",         # "CAUSAL_LM", "SEQ_CLS", "SEQ_2_SEQ_LM", etc.

    # Advanced options
    fan_in_fan_out=False,          # For Conv1D layers (GPT-2)
    modules_to_save=None,          # Additional modules to train fully

    # Initialization
    init_lora_weights=True,        # Initialize LoRA weights
    layers_to_transform=None,      # Specific layers (None = all)
    layers_pattern=None,           # Regex pattern for layers
)
</code></pre>
<h3 id="parameter-selection-guide"><a class="header" href="#parameter-selection-guide">Parameter Selection Guide</a></h3>
<p><strong>Rank (r)</strong>:</p>
<ul>
<li><strong>Low (4-8)</strong>: Fast experimentation, simple tasks, limited data</li>
<li><strong>Medium (16-32)</strong>: Most production use cases, good balance</li>
<li><strong>High (64-128)</strong>: Complex tasks, large datasets, maximum quality</li>
<li><strong>Rule of thumb</strong>: Start with 8, increase if underfitting</li>
</ul>
<p><strong>Alpha (lora_alpha)</strong>:</p>
<ul>
<li>Controls effective learning rate: <code>effective_lr = (alpha/r) * lr</code></li>
<li><strong>Common values</strong>: 16, 32, 64</li>
<li><strong>Rule of thumb</strong>: Set to 2×r for stability</li>
<li>Higher alpha = larger updates from adapters</li>
</ul>
<p><strong>Target Modules</strong>:</p>
<pre><code class="language-python"># Minimal (fastest training, least parameters)
target_modules=["q_proj", "v_proj"]

# Balanced (recommended for most cases)
target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]

# Maximum (best performance, more parameters)
target_modules=[
    "q_proj", "k_proj", "v_proj", "o_proj",  # Attention
    "gate_proj", "up_proj", "down_proj"       # FFN
]

# Using patterns (for specific architectures)
target_modules=r".*\.(q_proj|v_proj|k_proj|o_proj)"
</code></pre>
<p><strong>Dropout (lora_dropout)</strong>:</p>
<ul>
<li><strong>0.0</strong>: No regularization, risk of overfitting</li>
<li><strong>0.05-0.1</strong>: Standard choice for most tasks</li>
<li><strong>0.1-0.2</strong>: High regularization for small datasets</li>
<li>Applies dropout to LoRA layers during training</li>
</ul>
<p><strong>Bias</strong>:</p>
<ul>
<li><code>"none"</code>: Don’t train bias terms (most common)</li>
<li><code>"all"</code>: Train all bias parameters</li>
<li><code>"lora_only"</code>: Only train LoRA bias terms</li>
</ul>
<h3 id="training-hyperparameters"><a class="header" href="#training-hyperparameters">Training Hyperparameters</a></h3>
<pre><code class="language-python">from transformers import TrainingArguments

training_args = TrainingArguments(
    # Output
    output_dir="./lora_training",

    # Batch size and accumulation
    per_device_train_batch_size=4,     # Adjust based on GPU memory
    gradient_accumulation_steps=4,      # Effective batch = 4 * 4 = 16

    # Learning rate
    learning_rate=2e-4,                 # LoRA: 1e-4 to 3e-4 typical
    lr_scheduler_type="cosine",         # "linear", "cosine", "constant"
    warmup_ratio=0.03,                  # 3% warmup

    # Training duration
    num_train_epochs=3,
    max_steps=-1,                       # Use epochs instead

    # Optimization
    optim="adamw_torch",                # "adamw_torch", "adamw_8bit", "paged_adamw_8bit"
    weight_decay=0.01,
    max_grad_norm=1.0,                  # Gradient clipping

    # Precision
    fp16=True,                          # Use FP16 (V100, RTX)
    bf16=False,                         # Use BF16 (A100, H100)

    # Logging and saving
    logging_steps=10,
    save_steps=100,
    save_total_limit=2,                 # Keep only 2 checkpoints
    evaluation_strategy="steps",
    eval_steps=100,

    # Performance
    dataloader_num_workers=4,
    dataloader_pin_memory=True,
    group_by_length=True,               # Group similar lengths
)
</code></pre>
<h3 id="recommended-configurations-by-use-case"><a class="header" href="#recommended-configurations-by-use-case">Recommended Configurations by Use Case</a></h3>
<p><strong>Quick Experimentation</strong>:</p>
<pre><code class="language-python">lora_config = LoraConfig(r=4, lora_alpha=8, target_modules=["q_proj", "v_proj"])
training_args = TrainingArguments(num_train_epochs=1, learning_rate=3e-4)
</code></pre>
<p><strong>Production Fine-tuning (7B model)</strong>:</p>
<pre><code class="language-python">lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_dropout=0.05
)
training_args = TrainingArguments(
    num_train_epochs=3,
    learning_rate=2e-4,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4
)
</code></pre>
<p><strong>Large Model (70B+) with QLoRA</strong>:</p>
<pre><code class="language-python">lora_config = LoraConfig(
    r=64,
    lora_alpha=128,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                    "gate_proj", "up_proj", "down_proj"],
    lora_dropout=0.05
)
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)
</code></pre>
<h2 id="implementation-examples"><a class="header" href="#implementation-examples">Implementation Examples</a></h2>
<h3 id="example-1-fine-tune-llama-for-instruction-following"><a class="header" href="#example-1-fine-tune-llama-for-instruction-following">Example 1: Fine-tune LLaMA for Instruction Following</a></h3>
<pre><code class="language-python">import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from datasets import load_dataset
from trl import SFTTrainer

# Load model in 4-bit
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    load_in_4bit=True,
    torch_dtype=torch.float16,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")
tokenizer.pad_token = tokenizer.eos_token

# Prepare for training
model = prepare_model_for_kbit_training(model)

# LoRA configuration
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)

# Load instruction dataset
dataset = load_dataset("timdettmers/openassistant-guanaco")

# Format prompts
def format_instruction(example):
    if example.get("input"):
        return f"""### Instruction:
{example['instruction']}

### Input:
{example['input']}

### Response:
{example['output']}"""
    else:
        return f"""### Instruction:
{example['instruction']}

### Response:
{example['output']}"""

# Training
trainer = SFTTrainer(
    model=model,
    train_dataset=dataset["train"],
    dataset_text_field="text",
    max_seq_length=512,
    tokenizer=tokenizer,
    args=TrainingArguments(
        output_dir="./llama-instruction-lora",
        per_device_train_batch_size=4,
        gradient_accumulation_steps=4,
        warmup_steps=100,
        num_train_epochs=3,
        learning_rate=2e-4,
        fp16=True,
        logging_steps=10,
        save_steps=500,
    ),
)

trainer.train()
model.save_pretrained("./llama-instruction-lora-final")
</code></pre>
<h3 id="example-2-multi-task-adapter-management"><a class="header" href="#example-2-multi-task-adapter-management">Example 2: Multi-Task Adapter Management</a></h3>
<pre><code class="language-python">from peft import PeftModel, LoraConfig, get_peft_model
from transformers import AutoModelForCausalLM
import torch

class MultiTaskLoRAModel:
    def __init__(self, base_model_name):
        self.base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        self.adapters = {}
        self.current_adapter = None

    def add_adapter(self, name, adapter_path=None, config=None):
        """Add a new adapter"""
        if adapter_path:
            # Load existing adapter
            self.adapters[name] = adapter_path
        elif config:
            # Create new adapter for training
            model = get_peft_model(self.base_model, config)
            self.adapters[name] = model

    def switch_adapter(self, name):
        """Switch to a different adapter"""
        if name not in self.adapters:
            raise ValueError(f"Adapter {name} not found")

        adapter_path = self.adapters[name]
        self.current_model = PeftModel.from_pretrained(
            self.base_model,
            adapter_path
        )
        self.current_adapter = name

    def generate(self, prompt, **kwargs):
        """Generate with current adapter"""
        if self.current_adapter is None:
            raise ValueError("No adapter selected")

        inputs = self.tokenizer(prompt, return_tensors="pt").to("cuda")
        outputs = self.current_model.generate(**inputs, **kwargs)
        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)

# Usage
manager = MultiTaskLoRAModel("meta-llama/Llama-2-7b-hf")

# Add adapters
manager.add_adapter("medical", "./lora_medical")
manager.add_adapter("legal", "./lora_legal")
manager.add_adapter("code", "./lora_code")

# Use different adapters
manager.switch_adapter("medical")
medical_response = manager.generate("What are symptoms of diabetes?")

manager.switch_adapter("code")
code_response = manager.generate("Write a Python function to sort a list")
</code></pre>
<h3 id="example-3-lora-with-custom-training-loop"><a class="header" href="#example-3-lora-with-custom-training-loop">Example 3: LoRA with Custom Training Loop</a></h3>
<pre><code class="language-python">import torch
from torch.utils.data import DataLoader
from transformers import AutoModelForCausalLM, AutoTokenizer, get_linear_schedule_with_warmup
from peft import LoraConfig, get_peft_model
from tqdm import tqdm

# Setup
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_name = "gpt2-medium"

# Load model
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# Apply LoRA
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["c_attn"],  # GPT-2 attention
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM"
)
model = get_peft_model(model, lora_config)
model.to(device)

# Prepare data (example)
train_dataset = ...  # Your dataset
train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)

# Optimizer and scheduler
optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=0.01)
num_training_steps = len(train_dataloader) * 3  # 3 epochs
scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=100,
    num_training_steps=num_training_steps
)

# Training loop
model.train()
for epoch in range(3):
    epoch_loss = 0
    progress_bar = tqdm(train_dataloader, desc=f"Epoch {epoch+1}")

    for batch in progress_bar:
        # Prepare inputs
        inputs = {k: v.to(device) for k, v in batch.items()}

        # Forward pass
        outputs = model(**inputs, labels=inputs["input_ids"])
        loss = outputs.loss

        # Backward pass
        loss.backward()

        # Gradient clipping
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        # Optimizer step
        optimizer.step()
        scheduler.step()
        optimizer.zero_grad()

        # Logging
        epoch_loss += loss.item()
        progress_bar.set_postfix({"loss": loss.item(), "lr": scheduler.get_last_lr()[0]})

    print(f"Epoch {epoch+1} - Average Loss: {epoch_loss / len(train_dataloader):.4f}")

# Save adapter
model.save_pretrained("./custom_trained_lora")
</code></pre>
<h3 id="example-4-evaluation-and-comparison"><a class="header" href="#example-4-evaluation-and-comparison">Example 4: Evaluation and Comparison</a></h3>
<pre><code class="language-python">import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from datasets import load_dataset
from tqdm import tqdm
import numpy as np

def evaluate_perplexity(model, tokenizer, dataset, max_samples=100):
    """Evaluate model perplexity on dataset"""
    model.eval()
    total_loss = 0
    total_tokens = 0

    with torch.no_grad():
        for i, example in enumerate(tqdm(dataset)):
            if i &gt;= max_samples:
                break

            inputs = tokenizer(example["text"], return_tensors="pt",
                             truncation=True, max_length=512).to("cuda")

            outputs = model(**inputs, labels=inputs["input_ids"])
            loss = outputs.loss

            total_loss += loss.item() * inputs["input_ids"].size(1)
            total_tokens += inputs["input_ids"].size(1)

    perplexity = np.exp(total_loss / total_tokens)
    return perplexity

# Load base model
base_model = AutoModelForCausalLM.from_pretrained("gpt2", device_map="auto")
tokenizer = AutoTokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token

# Load LoRA model
lora_model = PeftModel.from_pretrained(base_model, "./my_lora_adapter")

# Load test dataset
test_dataset = load_dataset("wikitext", "wikitext-2-raw-v1", split="test")

# Evaluate
print("Evaluating base model...")
base_perplexity = evaluate_perplexity(base_model, tokenizer, test_dataset)
print(f"Base model perplexity: {base_perplexity:.2f}")

print("Evaluating LoRA model...")
lora_perplexity = evaluate_perplexity(lora_model, tokenizer, test_dataset)
print(f"LoRA model perplexity: {lora_perplexity:.2f}")

print(f"Improvement: {((base_perplexity - lora_perplexity) / base_perplexity * 100):.2f}%")
</code></pre>
<h2 id="advanced-topics"><a class="header" href="#advanced-topics">Advanced Topics</a></h2>
<h3 id="dora-weight-decomposed-low-rank-adaptation"><a class="header" href="#dora-weight-decomposed-low-rank-adaptation">DoRA (Weight-Decomposed Low-Rank Adaptation)</a></h3>
<p>DoRA decomposes pre-trained weights into magnitude and direction components, applying LoRA to the directional component.</p>
<pre><code>W' = m · (W_dir + ΔW_dir)
</code></pre>
<p>Where:</p>
<ul>
<li><code>m</code>: Magnitude component (trained)</li>
<li><code>W_dir</code>: Directional component (frozen)</li>
<li><code>ΔW_dir</code>: Low-rank adaptation of direction</li>
</ul>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Better learning capacity than vanilla LoRA</li>
<li>More stable training</li>
<li>Improved performance on complex tasks</li>
</ul>
<pre><code class="language-python">from peft import LoraConfig

# Enable DoRA
config = LoraConfig(
    r=16,
    lora_alpha=32,
    use_dora=True,  # Enable DoRA
    target_modules=["q_proj", "v_proj"]
)
</code></pre>
<h3 id="adalora-adaptive-lora"><a class="header" href="#adalora-adaptive-lora">AdaLoRA (Adaptive LoRA)</a></h3>
<p>Adaptively allocates rank budget across different weight matrices based on importance.</p>
<p><strong>Key Ideas</strong>:</p>
<ul>
<li>Start with budget of total rank across all layers</li>
<li>Prune less important singular values during training</li>
<li>Redistribute rank to more important matrices</li>
</ul>
<pre><code class="language-python">from peft import AdaLoraConfig, get_peft_model

config = AdaLoraConfig(
    init_r=12,              # Initial rank
    target_r=8,             # Target rank after pruning
    beta1=0.85,             # Regularization
    beta2=0.85,
    tinit=200,              # Start pruning after tinit steps
    tfinal=1000,            # Final pruning step
    deltaT=10,              # Pruning frequency
    lora_alpha=32,
    lora_dropout=0.1,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    task_type="CAUSAL_LM"
)

model = get_peft_model(base_model, config)
</code></pre>
<h3 id="lora-improved-optimizer"><a class="header" href="#lora-improved-optimizer">LoRA+ (Improved Optimizer)</a></h3>
<p>Uses different learning rates for A and B matrices.</p>
<p><strong>Insight</strong>: Matrix B (initialized to zero) should learn faster than matrix A.</p>
<pre><code class="language-python"># Manual implementation
def get_lora_plus_optimizer(model, lr_B=1e-3, lr_A=1e-4, weight_decay=0.01):
    """
    Create optimizer with different learning rates for A and B matrices
    """
    param_groups = [
        {
            "params": [p for n, p in model.named_parameters()
                      if "lora_B" in n],
            "lr": lr_B,
            "weight_decay": weight_decay
        },
        {
            "params": [p for n, p in model.named_parameters()
                      if "lora_A" in n],
            "lr": lr_A,
            "weight_decay": weight_decay
        },
        {
            "params": [p for n, p in model.named_parameters()
                      if "lora" not in n and p.requires_grad],
            "lr": lr_A,
            "weight_decay": weight_decay
        }
    ]

    return torch.optim.AdamW(param_groups)

# Usage
optimizer = get_lora_plus_optimizer(model, lr_B=2e-4, lr_A=2e-5)
</code></pre>
<h3 id="vera-vector-based-random-matrix-adaptation"><a class="header" href="#vera-vector-based-random-matrix-adaptation">VeRA (Vector-based Random Matrix Adaptation)</a></h3>
<p>Shares the same low-rank matrices across all layers, using layer-specific scaling vectors.</p>
<p><strong>Benefits</strong>:</p>
<ul>
<li>Even fewer trainable parameters than LoRA</li>
<li>Maintains competitive performance</li>
<li>Faster training</li>
</ul>
<pre><code class="language-python"># Conceptual structure (not in standard PEFT yet)
# Shared matrices: A_shared, B_shared
# Per-layer: scaling vectors d_i, b_i

# Forward pass for layer i:
# h = Wx + (d_i ⊙ (B_shared · A_shared · x)) ⊙ b_i
</code></pre>
<h3 id="lora-with-mixture-of-experts-moe"><a class="header" href="#lora-with-mixture-of-experts-moe">LoRA with Mixture of Experts (MoE)</a></h3>
<p>Combine LoRA with MoE for specialized adapters.</p>
<pre><code class="language-python">import torch.nn as nn

class MoELoRA(nn.Module):
    def __init__(self, base_model, num_experts=4, r=8):
        super().__init__()
        self.base_model = base_model
        self.num_experts = num_experts

        # Multiple LoRA experts
        self.experts = nn.ModuleList([
            get_peft_model(base_model, LoraConfig(r=r))
            for _ in range(num_experts)
        ])

        # Gating network
        self.gate = nn.Linear(hidden_size, num_experts)

    def forward(self, x):
        # Compute gating scores
        gate_scores = torch.softmax(self.gate(x.mean(dim=1)), dim=-1)

        # Weighted combination of expert outputs
        output = 0
        for i, expert in enumerate(self.experts):
            expert_out = expert(x)
            output += gate_scores[:, i:i+1] * expert_out

        return output
</code></pre>
<h3 id="spectral-regularization-for-lora"><a class="header" href="#spectral-regularization-for-lora">Spectral Regularization for LoRA</a></h3>
<p>Regularize the singular values of LoRA updates.</p>
<pre><code class="language-python">def spectral_regularization_loss(lora_A, lora_B, lambda_reg=0.01):
    """
    Regularize singular values of BA to prevent overfitting
    """
    # Compute product
    W_delta = torch.mm(lora_B, lora_A)

    # SVD
    U, S, V = torch.svd(W_delta)

    # Regularization: encourage low-rank structure
    reg_loss = lambda_reg * torch.sum(S)

    return reg_loss

# Add to training loop
loss = model(**inputs).loss + spectral_regularization_loss(model.lora_A, model.lora_B)
</code></pre>
<h3 id="lora-dropout-variants"><a class="header" href="#lora-dropout-variants">LoRA Dropout Variants</a></h3>
<p><strong>Standard Dropout</strong>:</p>
<pre><code class="language-python">config = LoraConfig(lora_dropout=0.1)  # Dropout in LoRA layers
</code></pre>
<p><strong>Stochastic Depth for LoRA</strong>:</p>
<pre><code class="language-python">class StochasticLoRA(nn.Module):
    def __init__(self, lora_layer, drop_prob=0.1):
        super().__init__()
        self.lora = lora_layer
        self.drop_prob = drop_prob

    def forward(self, x):
        if self.training and torch.rand(1).item() &lt; self.drop_prob:
            return 0  # Skip LoRA entirely
        else:
            return self.lora(x)
</code></pre>
<h2 id="best-practices"><a class="header" href="#best-practices">Best Practices</a></h2>
<h3 id="1-choosing-rank"><a class="header" href="#1-choosing-rank">1. Choosing Rank</a></h3>
<p><strong>Start Low, Scale Up</strong>:</p>
<pre><code class="language-python"># Experimentation phase
r = 4  # Quick iterations

# Validation phase
r = 8  # Verify approach works

# Production phase
r = 16-32  # Maximize performance
</code></pre>
<p><strong>Rank vs. Model Size</strong>:</p>
<ul>
<li>Small models (1B-7B): r = 8-16</li>
<li>Medium models (7B-13B): r = 16-32</li>
<li>Large models (30B-70B): r = 32-64</li>
<li>Very large models (70B+): r = 64-128</li>
</ul>
<h3 id="2-target-module-selection"><a class="header" href="#2-target-module-selection">2. Target Module Selection</a></h3>
<p><strong>For Attention-Only Tasks</strong> (Q&amp;A, classification):</p>
<pre><code class="language-python">target_modules=["q_proj", "v_proj"]  # Minimum
</code></pre>
<p><strong>For Generation Tasks</strong> (chat, summarization):</p>
<pre><code class="language-python">target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]  # Recommended
</code></pre>
<p><strong>For Maximum Performance</strong> (complex reasoning):</p>
<pre><code class="language-python">target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                "gate_proj", "up_proj", "down_proj"]  # All
</code></pre>
<h3 id="3-learning-rate-guidelines"><a class="header" href="#3-learning-rate-guidelines">3. Learning Rate Guidelines</a></h3>
<pre><code class="language-python"># LoRA typically needs higher LR than full fine-tuning
learning_rate = 1e-4  # Full fine-tuning typical
learning_rate = 2e-4  # LoRA typical
learning_rate = 3e-4  # LoRA with low rank or complex task
</code></pre>
<h3 id="4-batch-size-and-accumulation"><a class="header" href="#4-batch-size-and-accumulation">4. Batch Size and Accumulation</a></h3>
<pre><code class="language-python"># Goal: Effective batch size of 64-128
per_device_batch_size = 4      # Fit in GPU memory
gradient_accumulation_steps = 16  # Effective batch = 4 * 16 = 64
</code></pre>
<h3 id="5-data-quality-over-quantity"><a class="header" href="#5-data-quality-over-quantity">5. Data Quality over Quantity</a></h3>
<p><strong>LoRA is data-efficient</strong>:</p>
<ul>
<li>1,000 high-quality examples &gt; 10,000 noisy examples</li>
<li>Focus on diverse, representative samples</li>
<li>Remove duplicates and low-quality data</li>
</ul>
<h3 id="6-monitoring-training"><a class="header" href="#6-monitoring-training">6. Monitoring Training</a></h3>
<p><strong>Key Metrics</strong>:</p>
<pre><code class="language-python"># Watch for:
# 1. Training loss decreasing steadily
# 2. Validation loss not increasing (no overfitting)
# 3. Gradient norms stable (no exploding/vanishing)
# 4. Learning rate warmup completed

from transformers import TrainerCallback

class MonitorCallback(TrainerCallback):
    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs:
            print(f"Step {state.global_step}:")
            print(f"  Loss: {logs.get('loss', 'N/A'):.4f}")
            print(f"  LR: {logs.get('learning_rate', 'N/A'):.2e}")
            print(f"  Grad Norm: {logs.get('grad_norm', 'N/A'):.4f}")

trainer = Trainer(..., callbacks=[MonitorCallback()])
</code></pre>
<h3 id="7-preventing-overfitting"><a class="header" href="#7-preventing-overfitting">7. Preventing Overfitting</a></h3>
<pre><code class="language-python"># Multiple strategies:
lora_config = LoraConfig(
    r=8,                    # Lower rank = less capacity
    lora_dropout=0.1,       # Dropout regularization
)

training_args = TrainingArguments(
    weight_decay=0.01,      # L2 regularization
    max_grad_norm=1.0,      # Gradient clipping
    eval_steps=100,         # Frequent evaluation
    save_total_limit=2,     # Don't save too many checkpoints
)

# Early stopping
from transformers import EarlyStoppingCallback
trainer = Trainer(..., callbacks=[EarlyStoppingCallback(patience=3)])
</code></pre>
<h3 id="8-merging-for-production"><a class="header" href="#8-merging-for-production">8. Merging for Production</a></h3>
<pre><code class="language-python"># When deploying, merge for efficiency
from peft import PeftModel

model = PeftModel.from_pretrained(base_model, "./lora_adapter")
merged_model = model.merge_and_unload()

# Quantize merged model for inference
from transformers import AutoModelForCausalLM

merged_model = AutoModelForCausalLM.from_pretrained(
    "./merged_model",
    load_in_8bit=True,  # or load_in_4bit=True
    device_map="auto"
)
</code></pre>
<h3 id="9-version-control-for-adapters"><a class="header" href="#9-version-control-for-adapters">9. Version Control for Adapters</a></h3>
<pre><code>project/
├── base_models/
│   └── llama-2-7b/
├── adapters/
│   ├── v1_medical/
│   │   ├── adapter_config.json
│   │   └── adapter_model.bin
│   ├── v2_medical/
│   └── v1_legal/
├── configs/
│   ├── medical_lora.yaml
│   └── legal_lora.yaml
└── training_logs/
</code></pre>
<h3 id="10-common-pitfalls-to-avoid"><a class="header" href="#10-common-pitfalls-to-avoid">10. Common Pitfalls to Avoid</a></h3>
<p><strong>1. Using rank that’s too high</strong>:</p>
<pre><code class="language-python"># Bad: r=256 (too high, may overfit)
# Good: r=16 (appropriate for most tasks)
</code></pre>
<p><strong>2. Forgetting to set pad_token</strong>:</p>
<pre><code class="language-python"># Bad: tokenizer without pad_token
# Good:
tokenizer.pad_token = tokenizer.eos_token
</code></pre>
<p><strong>3. Not using gradient checkpointing for large models</strong>:</p>
<pre><code class="language-python"># Good: Enable gradient checkpointing
model.gradient_checkpointing_enable()
</code></pre>
<p><strong>4. Training on too little data</strong>:</p>
<pre><code class="language-python"># Minimum: 100-500 examples
# Recommended: 1,000-10,000 examples
# Ideal: 10,000+ high-quality examples
</code></pre>
<p><strong>5. Not testing before merging</strong>:</p>
<pre><code class="language-python"># Always evaluate adapter before merging
eval_results = trainer.evaluate()
if eval_results['eval_loss'] &lt; threshold:
    merged_model = model.merge_and_unload()
</code></pre>
<h3 id="11-testing-and-validation"><a class="header" href="#11-testing-and-validation">11. Testing and Validation</a></h3>
<pre><code class="language-python">def comprehensive_test(model, tokenizer, test_cases):
    """
    Test model on diverse examples
    """
    results = []

    for category, examples in test_cases.items():
        category_results = []

        for example in examples:
            prompt = example['prompt']
            expected = example.get('expected')

            inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
            outputs = model.generate(**inputs, max_new_tokens=100)
            response = tokenizer.decode(outputs[0], skip_special_tokens=True)

            category_results.append({
                'prompt': prompt,
                'response': response,
                'expected': expected,
                'match': expected in response if expected else None
            })

        results.append({
            'category': category,
            'results': category_results,
            'success_rate': sum(1 for r in category_results if r['match']) / len(category_results) if expected else None
        })

    return results

# Usage
test_cases = {
    'medical': [
        {'prompt': 'What is diabetes?', 'expected': 'blood sugar'},
        {'prompt': 'Symptoms of COVID-19?', 'expected': 'fever'}
    ],
    'general': [
        {'prompt': 'Capital of France?', 'expected': 'Paris'}
    ]
}

results = comprehensive_test(model, tokenizer, test_cases)
</code></pre>
<h3 id="12-resource-estimation"><a class="header" href="#12-resource-estimation">12. Resource Estimation</a></h3>
<p><strong>Memory Requirements</strong> (7B model):</p>
<pre><code class="language-python"># Base model (FP16): ~14 GB
# LoRA adapters (r=16): ~50 MB
# Optimizer states: ~100 MB
# Gradients: ~50 MB
# Activations (batch=4): ~2-4 GB
# Total: ~18-20 GB (fits on RTX 4090)

# With 4-bit quantization:
# Base model (NF4): ~3.5 GB
# Total: ~7-9 GB (fits on RTX 3090)
</code></pre>
<p><strong>Training Time</strong> (estimates for 1000 examples):</p>
<ul>
<li>7B model, r=16, 1×A100: ~30 minutes</li>
<li>7B model, r=16, 1×RTX 4090: ~1 hour</li>
<li>70B model, r=64, 1×A100 (QLoRA): ~4-6 hours</li>
</ul>
<hr>
<h2 id="summary"><a class="header" href="#summary">Summary</a></h2>
<p>LoRA revolutionizes fine-tuning by:</p>
<ol>
<li><strong>Efficiency</strong>: Train massive models on consumer hardware</li>
<li><strong>Flexibility</strong>: Multiple adapters for multiple tasks</li>
<li><strong>Performance</strong>: Match or exceed full fine-tuning</li>
<li><strong>Practicality</strong>: Deployable in production</li>
</ol>
<p><strong>Key Takeaways</strong>:</p>
<ul>
<li>Start with r=8-16 for most tasks</li>
<li>Use QLoRA for models &gt;30B parameters</li>
<li>Target attention layers first, add FFN if needed</li>
<li>Monitor for overfitting with small datasets</li>
<li>Merge adapters for production deployment</li>
<li>Version control your adapters</li>
<li>Test thoroughly before deployment</li>
</ul>
<p>LoRA has become the de facto standard for fine-tuning large language models, enabling individuals and small teams to customize state-of-the-art models for their specific needs without massive computational resources.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../machine_learning/interesting_papers.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256 246.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"/></svg></span>
                            </a>

                            <a rel="next prefetch" href="../machine_learning/cuda.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M278.6 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L210.7 256 73.4 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"/></svg></span>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../machine_learning/interesting_papers.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256 246.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"/></svg></span>
                    </a>

                    <a rel="next prefetch" href="../machine_learning/cuda.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M278.6 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L210.7 256 73.4 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"/></svg></span>
                    </a>
            </nav>

        </div>

        <template id=fa-eye><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M288 32c-80.8 0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7 0 24.6C17.3 304 48.6 356 95.4 399.4C142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144s64.5-144 144-144s144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64c-11.5 0-22.3-3-31.6-8.4c-.2 2.8-.4 5.5-.4 8.4c0 53 43 96 96 96s96-43 96-96s-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6z"/></svg></span></template>
        <template id=fa-eye-slash><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M38.8 5.1C28.4-3.1 13.3-1.2 5.1 9.2S-1.2 34.7 9.2 42.9l592 464c10.4 8.2 25.5 6.3 33.7-4.1s6.3-25.5-4.1-33.7L525.6 386.7c39.6-40.6 66.4-86.1 79.9-118.4c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C465.5 68.8 400.8 32 320 32c-68.2 0-125 26.3-169.3 60.8L38.8 5.1zM223.1 149.5C248.6 126.2 282.7 112 320 112c79.5 0 144 64.5 144 144c0 24.9-6.3 48.3-17.4 68.7L408 294.5c5.2-11.8 8-24.8 8-38.5c0-53-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6c0 10.2-2.4 19.8-6.6 28.3l-90.3-70.8zm223.1 298L373 389.9c-16.4 6.5-34.3 10.1-53 10.1c-79.5 0-144-64.5-144-144c0-6.9 .5-13.6 1.4-20.2L83.1 161.5C60.3 191.2 44 220.8 34.5 243.7c-3.3 7.9-3.3 16.7 0 24.6c14.9 35.7 46.2 87.7 93 131.1C174.5 443.2 239.2 480 320 480c47.8 0 89.9-12.9 126.2-32.5z"/></svg></span></template>
        <template id=fa-copy><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M502.6 70.63l-61.25-61.25C435.4 3.371 427.2 0 418.7 0H255.1c-35.35 0-64 28.66-64 64l.0195 256C192 355.4 220.7 384 256 384h192c35.2 0 64-28.8 64-64V93.25C512 84.77 508.6 76.63 502.6 70.63zM464 320c0 8.836-7.164 16-16 16H255.1c-8.838 0-16-7.164-16-16L239.1 64.13c0-8.836 7.164-16 16-16h128L384 96c0 17.67 14.33 32 32 32h47.1V320zM272 448c0 8.836-7.164 16-16 16H63.1c-8.838 0-16-7.164-16-16L47.98 192.1c0-8.836 7.164-16 16-16H160V128H63.99c-35.35 0-64 28.65-64 64l.0098 256C.002 483.3 28.66 512 64 512h192c35.2 0 64-28.8 64-64v-32h-47.1L272 448z"/></svg></span></template>
        <template id=fa-play><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M73 39c-14.8-9.1-33.4-9.4-48.5-.9S0 62.6 0 80V432c0 17.4 9.4 33.4 24.5 41.9s33.7 8.1 48.5-.9L361 297c14.3-8.7 23-24.2 23-41s-8.7-32.2-23-41L73 39z"/></svg></span></template>
        <template id=fa-clock-rotate-left><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M75 75L41 41C25.9 25.9 0 36.6 0 57.9V168c0 13.3 10.7 24 24 24H134.1c21.4 0 32.1-25.9 17-41l-30.8-30.8C155 85.5 203 64 256 64c106 0 192 86 192 192s-86 192-192 192c-40.8 0-78.6-12.7-109.7-34.4c-14.5-10.1-34.4-6.6-44.6 7.9s-6.6 34.4 7.9 44.6C151.2 495 201.7 512 256 512c141.4 0 256-114.6 256-256S397.4 0 256 0C185.3 0 121.3 28.7 75 75zm181 53c-13.3 0-24 10.7-24 24V256c0 6.4 2.5 12.5 7 17l72 72c9.4 9.4 24.6 9.4 33.9 0s9.4-24.6 0-33.9l-65-65V152c0-13.3-10.7-24-24-24z"/></svg></span></template>



        <script>
            window.playground_copyable = true;
        </script>


        <script src="../elasticlunr-ef4e11c1.min.js"></script>
        <script src="../mark-09e88c2c.min.js"></script>
        <script src="../searcher-c2a407aa.js"></script>

        <script src="../clipboard-1626706a.min.js"></script>
        <script src="../highlight-abc7f01d.js"></script>
        <script src="../book-a0b12cfe.js"></script>

        <!-- Custom JS scripts -->



    </div>
    </body>
</html>
