<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>LLAMA - My Notes</title>


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="../highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="../tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "../";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "../searchindex.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../toc.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="../toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">My Notes</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <i class="fa fa-spinner fa-spin"></i>
                            </div>
                        </div>
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="llama-models---meta-ai"><a class="header" href="#llama-models---meta-ai">Llama Models - Meta AI</a></h1>
<p>Complete guide to Meta's Llama family of open-source language models, from setup to fine-tuning and deployment.</p>
<h2 id="table-of-contents"><a class="header" href="#table-of-contents">Table of Contents</a></h2>
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#model-versions">Model Versions</a></li>
<li><a href="#installation--setup">Installation &amp; Setup</a></li>
<li><a href="#basic-usage">Basic Usage</a></li>
<li><a href="#fine-tuning">Fine-tuning</a></li>
<li><a href="#quantization">Quantization</a></li>
<li><a href="#inference-optimization">Inference Optimization</a></li>
<li><a href="#deployment">Deployment</a></li>
<li><a href="#advanced-techniques">Advanced Techniques</a></li>
</ul>
<h2 id="introduction"><a class="header" href="#introduction">Introduction</a></h2>
<p>Llama (Large Language Model Meta AI) is Meta's family of open-source foundation language models. Released as open-weights models, they've become the foundation for countless applications and fine-tuned variants.</p>
<h3 id="key-features"><a class="header" href="#key-features">Key Features</a></h3>
<ul>
<li><strong>Open Source</strong>: Freely available weights</li>
<li><strong>Strong Performance</strong>: Competitive with closed models</li>
<li><strong>Multiple Sizes</strong>: From 1B to 70B+ parameters</li>
<li><strong>Commercial Friendly</strong>: Permissive license</li>
<li><strong>Active Ecosystem</strong>: Huge community support</li>
<li><strong>Efficient</strong>: Optimized for deployment</li>
</ul>
<h3 id="architecture"><a class="header" href="#architecture">Architecture</a></h3>
<ul>
<li><strong>Transformer-based</strong>: Decoder-only architecture</li>
<li><strong>RMSNorm</strong>: Root Mean Square Layer Normalization</li>
<li><strong>SwiGLU</strong>: Activation function</li>
<li><strong>Rotary Embeddings</strong>: Position encoding</li>
<li><strong>Grouped-Query Attention</strong>: Efficient attention mechanism</li>
</ul>
<h2 id="model-versions"><a class="header" href="#model-versions">Model Versions</a></h2>
<h3 id="llama-32-latest"><a class="header" href="#llama-32-latest">Llama 3.2 (Latest)</a></h3>
<p><strong>Released</strong>: September 2024</p>
<h4 id="llama-32-1b3b-edge-models"><a class="header" href="#llama-32-1b3b-edge-models">Llama 3.2 1B/3B (Edge Models)</a></h4>
<pre><code class="language-python">from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model_name = "meta-llama/Llama-3.2-1B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)

# Chat
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "What is Python?"}
]

input_ids = tokenizer.apply_chat_template(
    messages,
    return_tensors="pt"
).to(model.device)

outputs = model.generate(
    input_ids,
    max_new_tokens=256,
    temperature=0.7
)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))
</code></pre>
<p><strong>Features:</strong></p>
<ul>
<li>1B and 3B parameter versions</li>
<li>Optimized for mobile and edge devices</li>
<li>Multilingual support</li>
<li>128K context length</li>
<li>Excellent for on-device inference</li>
</ul>
<h4 id="llama-32-11b90b-vision-models"><a class="header" href="#llama-32-11b90b-vision-models">Llama 3.2 11B/90B (Vision Models)</a></h4>
<pre><code class="language-python">from transformers import MllamaForConditionalGeneration, AutoProcessor
from PIL import Image

model = MllamaForConditionalGeneration.from_pretrained(
    "meta-llama/Llama-3.2-11B-Vision-Instruct",
    torch_dtype=torch.bfloat16,
    device_map="auto"
)
processor = AutoProcessor.from_pretrained("meta-llama/Llama-3.2-11B-Vision-Instruct")

# Load image
image = Image.open("photo.jpg")

# Create prompt
messages = [
    {
        "role": "user",
        "content": [
            {"type": "image"},
            {"type": "text", "text": "What's in this image?"}
        ]
    }
]

# Process and generate
input_text = processor.apply_chat_template(messages, add_generation_prompt=True)
inputs = processor(image, input_text, return_tensors="pt").to(model.device)

output = model.generate(**inputs, max_new_tokens=256)
print(processor.decode(output[0], skip_special_tokens=True))
</code></pre>
<p><strong>Features:</strong></p>
<ul>
<li>Multimodal (text + vision)</li>
<li>11B and 90B variants</li>
<li>Image understanding</li>
<li>Visual question answering</li>
</ul>
<h3 id="llama-31"><a class="header" href="#llama-31">Llama 3.1</a></h3>
<p><strong>Released</strong>: July 2024</p>
<pre><code class="language-python"># 8B - Fast, efficient
model_name = "meta-llama/Llama-3.1-8B-Instruct"

# 70B - High capability
model_name = "meta-llama/Llama-3.1-70B-Instruct"

# 405B - Most capable (requires multiple GPUs)
model_name = "meta-llama/Llama-3.1-405B-Instruct"
</code></pre>
<p><strong>Features:</strong></p>
<ul>
<li>128K context window</li>
<li>Multilingual (8 languages)</li>
<li>Tool use capabilities</li>
<li>Improved reasoning</li>
<li>8B, 70B, and 405B sizes</li>
</ul>
<h3 id="llama-3"><a class="header" href="#llama-3">Llama 3</a></h3>
<p><strong>Released</strong>: April 2024</p>
<pre><code class="language-python"># 8B
model_name = "meta-llama/Meta-Llama-3-8B-Instruct"

# 70B
model_name = "meta-llama/Meta-Llama-3-70B-Instruct"
</code></pre>
<p><strong>Features:</strong></p>
<ul>
<li>8K context window</li>
<li>Strong performance</li>
<li>Better instruction following</li>
<li>8B and 70B sizes</li>
</ul>
<h3 id="llama-2"><a class="header" href="#llama-2">Llama 2</a></h3>
<p><strong>Released</strong>: July 2023</p>
<pre><code class="language-python"># 7B
model_name = "meta-llama/Llama-2-7b-chat-hf"

# 13B
model_name = "meta-llama/Llama-2-13b-chat-hf"

# 70B
model_name = "meta-llama/Llama-2-70b-chat-hf"
</code></pre>
<p><strong>Features:</strong></p>
<ul>
<li>4K context window</li>
<li>7B, 13B, and 70B sizes</li>
<li>Still widely used</li>
</ul>
<h3 id="model-comparison"><a class="header" href="#model-comparison">Model Comparison</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Model</th><th>Parameters</th><th>Context</th><th>VRAM (FP16)</th><th>Use Case</th></tr></thead><tbody>
<tr><td>Llama 3.2 1B</td><td>1B</td><td>128K</td><td>2GB</td><td>Edge/Mobile</td></tr>
<tr><td>Llama 3.2 3B</td><td>3B</td><td>128K</td><td>6GB</td><td>Edge/Desktop</td></tr>
<tr><td>Llama 3.1 8B</td><td>8B</td><td>128K</td><td>16GB</td><td>Standard</td></tr>
<tr><td>Llama 3.2 11B Vision</td><td>11B</td><td>128K</td><td>22GB</td><td>Multimodal</td></tr>
<tr><td>Llama 3.1 70B</td><td>70B</td><td>128K</td><td>140GB</td><td>High-end</td></tr>
<tr><td>Llama 3.2 90B Vision</td><td>90B</td><td>128K</td><td>180GB</td><td>Vision tasks</td></tr>
<tr><td>Llama 3.1 405B</td><td>405B</td><td>128K</td><td>810GB</td><td>Best quality</td></tr>
</tbody></table>
</div>
<h2 id="installation--setup"><a class="header" href="#installation--setup">Installation &amp; Setup</a></h2>
<h3 id="via-hugging-face-transformers"><a class="header" href="#via-hugging-face-transformers">Via Hugging Face Transformers</a></h3>
<pre><code class="language-bash"># Install dependencies
pip install transformers torch accelerate

# For quantization
pip install bitsandbytes

# For training
pip install peft datasets
</code></pre>
<h3 id="via-ollama-easy-local-setup"><a class="header" href="#via-ollama-easy-local-setup">Via Ollama (Easy Local Setup)</a></h3>
<pre><code class="language-bash"># Install Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Pull model
ollama pull llama3.2

# Run
ollama run llama3.2
</code></pre>
<p>Python usage:</p>
<pre><code class="language-python">import requests

def query_ollama(prompt):
    response = requests.post('http://localhost:11434/api/generate', 
        json={
            "model": "llama3.2",
            "prompt": prompt,
            "stream": False
        }
    )
    return response.json()['response']

result = query_ollama("What is machine learning?")
print(result)
</code></pre>
<h3 id="via-llamacpp-efficient-c-implementation"><a class="header" href="#via-llamacpp-efficient-c-implementation">Via llama.cpp (Efficient C++ Implementation)</a></h3>
<pre><code class="language-bash"># Clone and build
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
make

# Download model (GGUF format)
# From Hugging Face or converted locally

# Run inference
./main -m models/llama-3.2-1B-Instruct-Q4_K_M.gguf -p "Hello, how are you?"
</code></pre>
<p>Python bindings:</p>
<pre><code class="language-bash">pip install llama-cpp-python
</code></pre>
<pre><code class="language-python">from llama_cpp import Llama

llm = Llama(
    model_path="models/llama-3.2-3B-Instruct-Q4_K_M.gguf",
    n_ctx=2048,
    n_gpu_layers=35  # Adjust for GPU
)

output = llm(
    "Explain quantum computing",
    max_tokens=256,
    temperature=0.7,
    top_p=0.95,
)

print(output['choices'][0]['text'])
</code></pre>
<h3 id="via-vllm-production-inference"><a class="header" href="#via-vllm-production-inference">Via vLLM (Production Inference)</a></h3>
<pre><code class="language-bash">pip install vllm
</code></pre>
<pre><code class="language-python">from vllm import LLM, SamplingParams

# Load model
llm = LLM(
    model="meta-llama/Llama-3.2-3B-Instruct",
    tensor_parallel_size=1
)

# Sampling parameters
sampling_params = SamplingParams(
    temperature=0.7,
    top_p=0.95,
    max_tokens=256
)

# Generate
prompts = ["What is AI?", "Explain Python"]
outputs = llm.generate(prompts, sampling_params)

for output in outputs:
    print(output.outputs[0].text)
</code></pre>
<h2 id="basic-usage"><a class="header" href="#basic-usage">Basic Usage</a></h2>
<h3 id="simple-text-generation"><a class="header" href="#simple-text-generation">Simple Text Generation</a></h3>
<pre><code class="language-python">from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# Load model
model_name = "meta-llama/Llama-3.2-3B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)

# Generate
prompt = "Write a Python function to calculate factorial:"
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

outputs = model.generate(
    **inputs,
    max_new_tokens=256,
    temperature=0.7,
    top_p=0.9,
    do_sample=True
)

result = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(result)
</code></pre>
<h3 id="chat-format"><a class="header" href="#chat-format">Chat Format</a></h3>
<pre><code class="language-python"># Proper chat formatting
messages = [
    {"role": "system", "content": "You are a helpful AI assistant."},
    {"role": "user", "content": "What is the capital of France?"},
]

# Apply chat template
input_ids = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt=True,
    return_tensors="pt"
).to(model.device)

# Generate
outputs = model.generate(
    input_ids,
    max_new_tokens=256,
    temperature=0.7,
    top_p=0.9,
    do_sample=True,
    pad_token_id=tokenizer.eos_token_id
)

response = tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)
print(response)
</code></pre>
<h3 id="multi-turn-conversation"><a class="header" href="#multi-turn-conversation">Multi-turn Conversation</a></h3>
<pre><code class="language-python">conversation = [
    {"role": "system", "content": "You are a helpful assistant."}
]

def chat(user_message):
    # Add user message
    conversation.append({"role": "user", "content": user_message})
    
    # Generate response
    input_ids = tokenizer.apply_chat_template(
        conversation,
        add_generation_prompt=True,
        return_tensors="pt"
    ).to(model.device)
    
    outputs = model.generate(
        input_ids,
        max_new_tokens=256,
        temperature=0.7,
        pad_token_id=tokenizer.eos_token_id
    )
    
    response = tokenizer.decode(
        outputs[0][input_ids.shape[-1]:],
        skip_special_tokens=True
    )
    
    # Add assistant response
    conversation.append({"role": "assistant", "content": response})
    
    return response

# Use
print(chat("What is Python?"))
print(chat("How do I install it?"))
print(chat("Give me a simple example."))
</code></pre>
<h3 id="streaming-generation"><a class="header" href="#streaming-generation">Streaming Generation</a></h3>
<pre><code class="language-python">from transformers import TextIteratorStreamer
from threading import Thread

streamer = TextIteratorStreamer(tokenizer, skip_special_tokens=True)

# Prepare input
messages = [{"role": "user", "content": "Write a short story about AI"}]
input_ids = tokenizer.apply_chat_template(messages, return_tensors="pt").to(model.device)

# Generate in thread
generation_kwargs = {
    "input_ids": input_ids,
    "max_new_tokens": 512,
    "temperature": 0.8,
    "streamer": streamer
}

thread = Thread(target=model.generate, kwargs=generation_kwargs)
thread.start()

# Stream output
for text in streamer:
    print(text, end="", flush=True)

thread.join()
</code></pre>
<h2 id="fine-tuning"><a class="header" href="#fine-tuning">Fine-tuning</a></h2>
<h3 id="qlora-fine-tuning-most-popular"><a class="header" href="#qlora-fine-tuning-most-popular">QLoRA Fine-tuning (Most Popular)</a></h3>
<p>Efficient fine-tuning with quantization:</p>
<pre><code class="language-bash">pip install transformers peft accelerate bitsandbytes datasets
</code></pre>
<pre><code class="language-python">from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    BitsAndBytesConfig
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from datasets import load_dataset
import torch

# Load model with quantization
model_name = "meta-llama/Llama-3.2-3B-Instruct"

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True
)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# Prepare model
model = prepare_model_for_kbit_training(model)

# LoRA configuration
lora_config = LoraConfig(
    r=16,  # Rank
    lora_alpha=32,  # Scaling factor
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
# Output: trainable params: ~16M / total: 3B (~0.5%)

# Prepare dataset
dataset = load_dataset("your-dataset")

def format_instruction(example):
    text = f"### Instruction:\n{example['instruction']}\n\n### Response:\n{example['response']}"
    return {"text": text}

dataset = dataset.map(format_instruction)

# Tokenize
def tokenize(examples):
    return tokenizer(
        examples["text"],
        truncation=True,
        max_length=512,
        padding="max_length"
    )

tokenized_dataset = dataset.map(tokenize, batched=True)

# Training arguments
training_args = TrainingArguments(
    output_dir="./llama-finetuned",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    fp16=True,
    logging_steps=10,
    save_strategy="epoch",
    optim="paged_adamw_8bit"
)

# Train
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    data_collator=lambda data: {
        'input_ids': torch.stack([f['input_ids'] for f in data]),
        'attention_mask': torch.stack([f['attention_mask'] for f in data]),
        'labels': torch.stack([f['input_ids'] for f in data])
    }
)

trainer.train()

# Save
model.save_pretrained("./llama-lora")
tokenizer.save_pretrained("./llama-lora")
</code></pre>
<h3 id="using-fine-tuned-model"><a class="header" href="#using-fine-tuned-model">Using Fine-tuned Model</a></h3>
<pre><code class="language-python">from peft import PeftModel

# Load base model
base_model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3.2-3B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto"
)

# Load LoRA
model = PeftModel.from_pretrained(base_model, "./llama-lora")

# Generate
tokenizer = AutoTokenizer.from_pretrained("./llama-lora")
inputs = tokenizer("Your prompt here", return_tensors="pt").to(model.device)
outputs = model.generate(**inputs, max_new_tokens=256)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
</code></pre>
<h3 id="full-fine-tuning-requires-more-resources"><a class="header" href="#full-fine-tuning-requires-more-resources">Full Fine-tuning (Requires More Resources)</a></h3>
<pre><code class="language-python">from transformers import Trainer, TrainingArguments

# Load model normally (no quantization)
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3.2-3B-Instruct",
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

# Training arguments
training_args = TrainingArguments(
    output_dir="./llama-fullft",
    num_train_epochs=3,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=8,
    learning_rate=1e-5,
    bf16=True,
    logging_steps=10,
    save_strategy="epoch",
    deepspeed="ds_config.json"  # For multi-GPU
)

# Train
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"]
)

trainer.train()
</code></pre>
<h3 id="using-axolotl-simplified-training"><a class="header" href="#using-axolotl-simplified-training">Using Axolotl (Simplified Training)</a></h3>
<pre><code class="language-bash"># Install
git clone https://github.com/OpenAccess-AI-Collective/axolotl
cd axolotl
pip install -e .
</code></pre>
<p>Create config <code>llama_qlora.yml</code>:</p>
<pre><code class="language-yaml">base_model: meta-llama/Llama-3.2-3B-Instruct
model_type: LlamaForCausalLM

load_in_4bit: true
adapter: qlora
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules:
  - q_proj
  - k_proj
  - v_proj
  - o_proj

datasets:
  - path: your-dataset
    type: alpaca

num_epochs: 3
micro_batch_size: 4
gradient_accumulation_steps: 4
learning_rate: 0.0002

output_dir: ./llama-qlora-out
</code></pre>
<p>Train:</p>
<pre><code class="language-bash">accelerate launch -m axolotl.cli.train llama_qlora.yml
</code></pre>
<h2 id="quantization"><a class="header" href="#quantization">Quantization</a></h2>
<h3 id="bitsandbytes-quantization"><a class="header" href="#bitsandbytes-quantization">BitsAndBytes Quantization</a></h3>
<pre><code class="language-python">from transformers import BitsAndBytesConfig

# 8-bit
bnb_config_8bit = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0
)

# 4-bit (QLoRA)
bnb_config_4bit = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",  # or "fp4"
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True
)

model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3.2-3B-Instruct",
    quantization_config=bnb_config_4bit,
    device_map="auto"
)
</code></pre>
<h3 id="gguf-quantization-llamacpp"><a class="header" href="#gguf-quantization-llamacpp">GGUF Quantization (llama.cpp)</a></h3>
<pre><code class="language-bash"># Convert to GGUF
python convert_hf_to_gguf.py \
    --model-dir models/Llama-3.2-3B-Instruct \
    --outfile llama-3.2-3b-instruct.gguf

# Quantize
./quantize \
    llama-3.2-3b-instruct.gguf \
    llama-3.2-3b-instruct-Q4_K_M.gguf \
    Q4_K_M
</code></pre>
<p>Quantization formats:</p>
<ul>
<li><code>Q4_0</code>: 4-bit, fastest, lowest quality</li>
<li><code>Q4_K_M</code>: 4-bit, good quality (recommended)</li>
<li><code>Q5_K_M</code>: 5-bit, better quality</li>
<li><code>Q8_0</code>: 8-bit, high quality</li>
</ul>
<h3 id="gptq-quantization"><a class="header" href="#gptq-quantization">GPTQ Quantization</a></h3>
<pre><code class="language-bash">pip install auto-gptq
</code></pre>
<pre><code class="language-python">from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig

# Quantize
quantize_config = BaseQuantizeConfig(
    bits=4,
    group_size=128,
    desc_act=False
)

model = AutoGPTQForCausalLM.from_pretrained(
    "meta-llama/Llama-3.2-3B-Instruct",
    quantize_config=quantize_config
)

# Save
model.save_quantized("llama-3.2-3b-gptq")

# Load
model = AutoGPTQForCausalLM.from_quantized(
    "llama-3.2-3b-gptq",
    device_map="auto"
)
</code></pre>
<h3 id="awq-quantization"><a class="header" href="#awq-quantization">AWQ Quantization</a></h3>
<pre><code class="language-bash">pip install autoawq
</code></pre>
<pre><code class="language-python">from awq import AutoAWQForCausalLM

# Quantize
model = AutoAWQForCausalLM.from_pretrained("meta-llama/Llama-3.2-3B-Instruct")
model.quantize(tokenizer, quant_config={"zero_point": True, "q_group_size": 128})
model.save_quantized("llama-3.2-3b-awq")

# Load
model = AutoAWQForCausalLM.from_quantized("llama-3.2-3b-awq")
</code></pre>
<h2 id="inference-optimization"><a class="header" href="#inference-optimization">Inference Optimization</a></h2>
<h3 id="flash-attention-2"><a class="header" href="#flash-attention-2">Flash Attention 2</a></h3>
<pre><code class="language-bash">pip install flash-attn --no-build-isolation
</code></pre>
<pre><code class="language-python">model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3.2-3B-Instruct",
    torch_dtype=torch.float16,
    attn_implementation="flash_attention_2",
    device_map="auto"
)
</code></pre>
<h3 id="batch-inference"><a class="header" href="#batch-inference">Batch Inference</a></h3>
<pre><code class="language-python"># Process multiple prompts efficiently
prompts = [
    "What is Python?",
    "Explain machine learning",
    "How do computers work?"
]

# Tokenize with padding
inputs = tokenizer(
    prompts,
    return_tensors="pt",
    padding=True,
    truncation=True,
    max_length=512
).to(model.device)

# Generate
outputs = model.generate(
    **inputs,
    max_new_tokens=256,
    temperature=0.7,
    do_sample=True,
    pad_token_id=tokenizer.pad_token_id
)

# Decode
results = tokenizer.batch_decode(outputs, skip_special_tokens=True)
for prompt, result in zip(prompts, results):
    print(f"Q: {prompt}\nA: {result}\n")
</code></pre>
<h3 id="kv-cache-optimization"><a class="header" href="#kv-cache-optimization">KV Cache Optimization</a></h3>
<pre><code class="language-python"># Enable static KV cache for faster inference
model.generation_config.cache_implementation = "static"
model.generation_config.max_length = 512

# Or use with generate
outputs = model.generate(
    input_ids,
    max_new_tokens=256,
    use_cache=True,
    cache_implementation="static"
)
</code></pre>
<h3 id="tensorrt-llm"><a class="header" href="#tensorrt-llm">TensorRT-LLM</a></h3>
<pre><code class="language-bash"># Build TensorRT engine
git clone https://github.com/NVIDIA/TensorRT-LLM.git
cd TensorRT-LLM

# Convert and build
python examples/llama/convert_checkpoint.py \
    --model_dir models/Llama-3.2-3B-Instruct \
    --output_dir ./trt_ckpt \
    --dtype float16

trtllm-build \
    --checkpoint_dir ./trt_ckpt \
    --output_dir ./trt_engine \
    --gemm_plugin float16
</code></pre>
<h2 id="deployment"><a class="header" href="#deployment">Deployment</a></h2>
<h3 id="fastapi-server"><a class="header" href="#fastapi-server">FastAPI Server</a></h3>
<pre><code class="language-python">from fastapi import FastAPI
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

app = FastAPI()

# Load model once at startup
model_name = "meta-llama/Llama-3.2-3B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)

class GenerateRequest(BaseModel):
    prompt: str
    max_tokens: int = 256
    temperature: float = 0.7

@app.post("/generate")
async def generate(request: GenerateRequest):
    inputs = tokenizer(request.prompt, return_tensors="pt").to(model.device)
    
    outputs = model.generate(
        **inputs,
        max_new_tokens=request.max_tokens,
        temperature=request.temperature,
        do_sample=True
    )
    
    result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return {"response": result}

# Run: uvicorn server:app --host 0.0.0.0 --port 8000
</code></pre>
<h3 id="vllm-server"><a class="header" href="#vllm-server">vLLM Server</a></h3>
<pre><code class="language-bash"># Start server
vllm serve meta-llama/Llama-3.2-3B-Instruct \
    --host 0.0.0.0 \
    --port 8000 \
    --tensor-parallel-size 1

# Client
curl http://localhost:8000/v1/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "meta-llama/Llama-3.2-3B-Instruct",
        "prompt": "What is AI?",
        "max_tokens": 256
    }'
</code></pre>
<p>Python client:</p>
<pre><code class="language-python">from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="dummy"
)

response = client.completions.create(
    model="meta-llama/Llama-3.2-3B-Instruct",
    prompt="Explain quantum computing",
    max_tokens=256
)

print(response.choices[0].text)
</code></pre>
<h3 id="text-generation-inference-tgi"><a class="header" href="#text-generation-inference-tgi">Text Generation Inference (TGI)</a></h3>
<pre><code class="language-bash"># Docker
docker run --gpus all --shm-size 1g -p 8080:80 \
    -v $PWD/data:/data \
    ghcr.io/huggingface/text-generation-inference:latest \
    --model-id meta-llama/Llama-3.2-3B-Instruct

# Client
curl http://localhost:8080/generate \
    -X POST \
    -d '{"inputs":"What is Python?","parameters":{"max_new_tokens":256}}' \
    -H 'Content-Type: application/json'
</code></pre>
<h3 id="langchain-integration"><a class="header" href="#langchain-integration">LangChain Integration</a></h3>
<pre><code class="language-bash">pip install langchain langchain-community
</code></pre>
<pre><code class="language-python">from langchain_community.llms import HuggingFacePipeline
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

# Load model
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3.2-3B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.2-3B-Instruct")

# Create pipeline
pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=256,
    temperature=0.7
)

# LangChain LLM
llm = HuggingFacePipeline(pipeline=pipe)

# Create chain
template = "Question: {question}\n\nAnswer:"
prompt = PromptTemplate(template=template, input_variables=["question"])
chain = LLMChain(llm=llm, prompt=prompt)

# Use
result = chain.run("What is machine learning?")
print(result)
</code></pre>
<h2 id="advanced-techniques"><a class="header" href="#advanced-techniques">Advanced Techniques</a></h2>
<h3 id="retrieval-augmented-generation-rag"><a class="header" href="#retrieval-augmented-generation-rag">Retrieval-Augmented Generation (RAG)</a></h3>
<pre><code class="language-bash">pip install langchain chromadb sentence-transformers
</code></pre>
<pre><code class="language-python">from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain_community.llms import HuggingFacePipeline

# Load documents
documents = ["Your document text here..."]

# Split text
text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
texts = text_splitter.create_documents(documents)

# Create embeddings
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# Create vector store
vectorstore = Chroma.from_documents(texts, embeddings)

# Create QA chain
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vectorstore.as_retriever()
)

# Query
query = "What does the document say about AI?"
result = qa_chain.run(query)
print(result)
</code></pre>
<h3 id="function-calling"><a class="header" href="#function-calling">Function Calling</a></h3>
<pre><code class="language-python">import json

def get_current_weather(location: str, unit: str = "celsius"):
    """Get current weather for a location"""
    # Simulated function
    return {"location": location, "temperature": 22, "unit": unit}

# Define tools
tools = [
    {
        "name": "get_current_weather",
        "description": "Get the current weather in a given location",
        "parameters": {
            "type": "object",
            "properties": {
                "location": {"type": "string", "description": "City name"},
                "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]}
            },
            "required": ["location"]
        }
    }
]

# System prompt
system_prompt = f"""You are a helpful assistant with access to tools.
Available tools: {json.dumps(tools, indent=2)}

When you need to use a tool, output JSON: {{"tool": "tool_name", "parameters": {{...}}}}
"""

messages = [
    {"role": "system", "content": system_prompt},
    {"role": "user", "content": "What's the weather in Paris?"}
]

# Generate
input_ids = tokenizer.apply_chat_template(messages, return_tensors="pt").to(model.device)
outputs = model.generate(input_ids, max_new_tokens=256)
response = tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)

# Parse and execute tool call
if "tool" in response:
    tool_call = json.loads(response)
    if tool_call["tool"] == "get_current_weather":
        result = get_current_weather(**tool_call["parameters"])
        print(f"Weather: {result}")
</code></pre>
<h3 id="constrained-generation"><a class="header" href="#constrained-generation">Constrained Generation</a></h3>
<pre><code class="language-bash">pip install outlines
</code></pre>
<pre><code class="language-python">import outlines

# Load model
model = outlines.models.transformers("meta-llama/Llama-3.2-1B-Instruct")

# JSON schema constraint
schema = """{
    "type": "object",
    "properties": {
        "name": {"type": "string"},
        "age": {"type": "integer"},
        "skills": {"type": "array", "items": {"type": "string"}}
    }
}"""

generator = outlines.generate.json(model, schema)
result = generator("Generate a person profile:")
print(result)

# Regex constraint
phone_pattern = r"\d{3}-\d{3}-\d{4}"
generator = outlines.generate.regex(model, phone_pattern)
phone = generator("Generate a US phone number:")
print(phone)
</code></pre>
<h2 id="best-practices"><a class="header" href="#best-practices">Best Practices</a></h2>
<h3 id="1-model-selection"><a class="header" href="#1-model-selection">1. Model Selection</a></h3>
<pre><code class="language-python"># Choose based on requirements
model_selection = {
    "mobile/edge": "meta-llama/Llama-3.2-1B-Instruct",
    "desktop/low_vram": "meta-llama/Llama-3.2-3B-Instruct",
    "standard": "meta-llama/Llama-3.1-8B-Instruct",
    "high_quality": "meta-llama/Llama-3.1-70B-Instruct",
    "vision": "meta-llama/Llama-3.2-11B-Vision-Instruct"
}
</code></pre>
<h3 id="2-prompt-templates"><a class="header" href="#2-prompt-templates">2. Prompt Templates</a></h3>
<pre><code class="language-python"># Use consistent templates
SYSTEM_PROMPT = "You are a helpful, respectful and honest assistant."

def format_chat(user_message, system=SYSTEM_PROMPT):
    return [
        {"role": "system", "content": system},
        {"role": "user", "content": user_message}
    ]
</code></pre>
<h3 id="3-memory-management"><a class="header" href="#3-memory-management">3. Memory Management</a></h3>
<pre><code class="language-python">import torch
import gc

def clear_memory():
    gc.collect()
    torch.cuda.empty_cache()

# After large operations
outputs = model.generate(...)
result = tokenizer.decode(outputs[0])
del outputs
clear_memory()
</code></pre>
<h3 id="4-error-handling"><a class="header" href="#4-error-handling">4. Error Handling</a></h3>
<pre><code class="language-python">def safe_generate(prompt, max_retries=3):
    for attempt in range(max_retries):
        try:
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
            outputs = model.generate(**inputs, max_new_tokens=256)
            return tokenizer.decode(outputs[0], skip_special_tokens=True)
        except RuntimeError as e:
            if "out of memory" in str(e) and attempt &lt; max_retries - 1:
                torch.cuda.empty_cache()
                continue
            raise
</code></pre>
<h2 id="resources"><a class="header" href="#resources">Resources</a></h2>
<h3 id="official"><a class="header" href="#official">Official</a></h3>
<ul>
<li><a href="https://llama.meta.com/">Meta Llama</a></li>
<li><a href="https://huggingface.co/meta-llama">Hugging Face Models</a></li>
<li><a href="https://github.com/facebookresearch/llama-recipes">Llama Recipes</a></li>
</ul>
<h3 id="tools"><a class="header" href="#tools">Tools</a></h3>
<ul>
<li><a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a></li>
<li><a href="https://ollama.com/">Ollama</a></li>
<li><a href="https://github.com/vllm-project/vllm">vLLM</a></li>
<li><a href="https://github.com/huggingface/text-generation-inference">Text Generation Inference</a></li>
</ul>
<h3 id="fine-tuning-1"><a class="header" href="#fine-tuning-1">Fine-tuning</a></h3>
<ul>
<li><a href="https://github.com/OpenAccess-AI-Collective/axolotl">Axolotl</a></li>
<li><a href="https://github.com/hiyouga/LLaMA-Factory">LLaMA-Factory</a></li>
<li><a href="https://github.com/huggingface/peft">PEFT</a></li>
</ul>
<h3 id="community"><a class="header" href="#community">Community</a></h3>
<ul>
<li>r/LocalLLaMA</li>
<li>Hugging Face Forums</li>
<li>Discord communities</li>
</ul>
<h2 id="conclusion"><a class="header" href="#conclusion">Conclusion</a></h2>
<p>Llama models provide a powerful, open-source foundation for AI applications. Whether you're running a 1B model on a mobile device or deploying a 70B model in production, the ecosystem offers tools and techniques for every use case.</p>
<p>Key takeaways:</p>
<ul>
<li><strong>Start small</strong>: Test with 1B/3B models first</li>
<li><strong>Quantize</strong>: Use 4-bit for efficient inference</li>
<li><strong>Fine-tune</strong>: QLoRA for custom domains</li>
<li><strong>Optimize</strong>: vLLM/TGI for production</li>
<li><strong>Monitor</strong>: Watch memory and performance</li>
</ul>
<p>The open-source nature and active community make Llama models an excellent choice for both research and production applications.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../ai/prompt_engineering.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../ai/stable_diffusion.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../ai/prompt_engineering.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../ai/stable_diffusion.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="../elasticlunr.min.js"></script>
        <script src="../mark.min.js"></script>
        <script src="../searcher.js"></script>

        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->



    </div>
    </body>
</html>
