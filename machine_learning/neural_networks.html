<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Neural Networks - My Notes</title>


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon-de23e50b.svg">
        <link rel="shortcut icon" href="../favicon-8114d1fc.png">
        <link rel="stylesheet" href="../css/variables-8adf115d.css">
        <link rel="stylesheet" href="../css/general-2459343d.css">
        <link rel="stylesheet" href="../css/chrome-ae938929.css">
        <link rel="stylesheet" href="../css/print-9e4910d8.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../fonts/fonts-9644e21d.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="mdbook-highlight-css" href="../highlight-493f70e1.css">
        <link rel="stylesheet" id="mdbook-tomorrow-night-css" href="../tomorrow-night-4c0ae647.css">
        <link rel="stylesheet" id="mdbook-ayu-highlight-css" href="../ayu-highlight-3fdfc3ac.css">

        <!-- Custom theme stylesheets -->

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "../";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "../searchindex-030848ba.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../toc-c5c10ac8.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="mdbook-body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="mdbook-sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("mdbook-sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="mdbook-sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="../toc.html"></iframe>
            </noscript>
            <div id="mdbook-sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="mdbook-page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="mdbook-menu-bar-hover-placeholder"></div>
                <div id="mdbook-menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="mdbook-sidebar-toggle" class="icon-button" for="mdbook-sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="mdbook-sidebar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"/></svg></span>
                        </label>
                        <button id="mdbook-theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="mdbook-theme-list">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M371.3 367.1c27.3-3.9 51.9-19.4 67.2-42.9L600.2 74.1c12.6-19.5 9.4-45.3-7.6-61.2S549.7-4.4 531.1 9.6L294.4 187.2c-24 18-38.2 46.1-38.4 76.1L371.3 367.1zm-19.6 25.4l-116-104.4C175.9 290.3 128 339.6 128 400c0 3.9 .2 7.8 .6 11.6c1.8 17.5-10.2 36.4-27.8 36.4H96c-17.7 0-32 14.3-32 32s14.3 32 32 32H240c61.9 0 112-50.1 112-112c0-2.5-.1-5-.2-7.5z"/></svg></span>
                        </button>
                        <ul id="mdbook-theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-ayu">Ayu</button></li>
                        </ul>
                        <button id="mdbook-search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="mdbook-searchbar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M416 208c0 45.9-14.9 88.3-40 122.7L502.6 457.4c12.5 12.5 12.5 32.8 0 45.3s-32.8 12.5-45.3 0L330.7 376c-34.4 25.2-76.8 40-122.7 40C93.1 416 0 322.9 0 208S93.1 0 208 0S416 93.1 416 208zM208 352c79.5 0 144-64.5 144-144s-64.5-144-144-144S64 128.5 64 208s64.5 144 144 144z"/></svg></span>
                        </button>
                    </div>

                    <h1 class="menu-title">My Notes</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <span class=fa-svg id="print-button"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M128 0C92.7 0 64 28.7 64 64v96h64V64H354.7L384 93.3V160h64V93.3c0-17-6.7-33.3-18.7-45.3L400 18.7C388 6.7 371.7 0 354.7 0H128zM384 352v32 64H128V384 368 352H384zm64 32h32c17.7 0 32-14.3 32-32V256c0-35.3-28.7-64-64-64H64c-35.3 0-64 28.7-64 64v96c0 17.7 14.3 32 32 32H64v64c0 35.3 28.7 64 64 64H384c35.3 0 64-28.7 64-64V384zm-16-88c-13.3 0-24-10.7-24-24s10.7-24 24-24s24 10.7 24 24s-10.7 24-24 24z"/></svg></span>
                        </a>

                    </div>
                </div>

                <div id="mdbook-search-wrapper" class="hidden">
                    <form id="mdbook-searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="mdbook-searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="mdbook-searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <span class=fa-svg id="fa-spin"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M304 48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zm0 416c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM48 304c26.5 0 48-21.5 48-48s-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48zm464-48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM142.9 437c18.7-18.7 18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zm0-294.2c18.7-18.7 18.7-49.1 0-67.9S93.7 56.2 75 75s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zM369.1 437c18.7 18.7 49.1 18.7 67.9 0s18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9z"/></svg></span>
                            </div>
                        </div>
                    </form>
                    <div id="mdbook-searchresults-outer" class="searchresults-outer hidden">
                        <div id="mdbook-searchresults-header" class="searchresults-header"></div>
                        <ul id="mdbook-searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('mdbook-sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('mdbook-sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#mdbook-sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="mdbook-content" class="content">
                    <main>
                        <h1 id="neural-networks"><a class="header" href="#neural-networks">Neural Networks</a></h1>
<h2 id="overview"><a class="header" href="#overview">Overview</a></h2>
<p>A neural network is a machine learning model inspired by biological brains. It consists of interconnected nodes (neurons) organized in layers that learn patterns from data.</p>
<h2 id="table-of-contents"><a class="header" href="#table-of-contents">Table of Contents</a></h2>
<h3 id="fundamentals"><a class="header" href="#fundamentals">Fundamentals</a></h3>
<ul>
<li><a href="#basic-architecture">Basic Architecture</a></li>
<li><a href="#key-components">Key Components</a></li>
<li><a href="#training-process">Training Process</a></li>
<li><a href="#code-example-pytorch">Code Example</a></li>
<li><a href="#network-types">Network Types</a></li>
<li><a href="#hyperparameters">Hyperparameters</a></li>
</ul>
<h3 id="advanced-topics"><a class="header" href="#advanced-topics">Advanced Topics</a></h3>
<ul>
<li><a href="#modern-architectures">Modern Architectures</a></li>
<li><a href="#advanced-training-techniques">Advanced Training Techniques</a></li>
<li><a href="#practical-considerations">Practical Considerations</a></li>
</ul>
<h3 id="resources"><a class="header" href="#resources">Resources</a></h3>
<ul>
<li><a href="#training-tips">Training Tips</a></li>
<li><a href="#common-issues">Common Issues</a></li>
<li><a href="#quick-reference">Quick Reference</a></li>
<li><a href="#further-resources">Further Resources</a></li>
<li><a href="#eli10">ELI10</a></li>
</ul>
<hr>
<h1 id="fundamentals-1"><a class="header" href="#fundamentals-1">Fundamentals</a></h1>
<h2 id="basic-architecture"><a class="header" href="#basic-architecture">Basic Architecture</a></h2>
<pre><code>Input Layer    Hidden Layers          Output Layer
     o              o                    o
     o              o                    o
     o              o                    o
     o              o
     o              o                    o
     o              o
   [n inputs]   [hidden units]   [output units]
</code></pre>
<h2 id="key-components"><a class="header" href="#key-components">Key Components</a></h2>
<h3 id="neurons"><a class="header" href="#neurons">Neurons</a></h3>
<p>Each neuron applies transformation: $\text{output} = \text{activation}(\text{weights} \cdot \text{inputs} + \text{bias})$</p>
<h3 id="activation-functions"><a class="header" href="#activation-functions">Activation Functions</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Function</th><th>Formula</th><th>Range</th><th>Use Case</th></tr>
</thead>
<tbody>
<tr><td>ReLU</td><td>$\max(0, x)$</td><td>$[0, \infty)$</td><td>Hidden layers</td></tr>
<tr><td>Sigmoid</td><td>$\frac{1}{1+e^{-x}}$</td><td>$(0, 1)$</td><td>Binary classification</td></tr>
<tr><td>Tanh</td><td>$\frac{e^x - e^{-x}}{e^x + e^{-x}}$</td><td>$(-1, 1)$</td><td>Hidden layers</td></tr>
<tr><td>Softmax</td><td>$\frac{e^{x_i}}{\sum_j e^{x_j}}$</td><td>$(0, 1)$ probabilities</td><td>Multi-class output</td></tr>
<tr><td>Linear</td><td>$x$</td><td>$(-\infty, \infty)$</td><td>Regression output</td></tr>
</tbody>
</table>
</div>
<h3 id="layers"><a class="header" href="#layers">Layers</a></h3>
<ol>
<li><strong>Input Layer</strong>: Raw data (28x28 pixels, word embeddings, etc.)</li>
<li><strong>Hidden Layers</strong>: Learn complex patterns through non-linear transformations</li>
<li><strong>Output Layer</strong>: Final predictions</li>
</ol>
<h2 id="training-process"><a class="header" href="#training-process">Training Process</a></h2>
<h3 id="forward-pass"><a class="header" href="#forward-pass">Forward Pass</a></h3>
<p>Input flows through network:</p>
<pre><code>x → w1 + b1 → activation → ... → output
</code></pre>
<h3 id="loss-function"><a class="header" href="#loss-function">Loss Function</a></h3>
<p>Measures prediction error:</p>
<ul>
<li><strong>MSE</strong> (regression): Mean squared error</li>
<li><strong>Cross-Entropy</strong> (classification): Measures probability difference</li>
</ul>
<h3 id="backpropagation"><a class="header" href="#backpropagation">Backpropagation</a></h3>
<p>Calculates gradients and updates weights:</p>
<pre><code>1. Compute loss
2. Calculate gradients: ∂(loss)/∂(weights)
3. Update weights: w = w - learning_rate × gradient
4. Repeat
</code></pre>
<h3 id="optimizers"><a class="header" href="#optimizers">Optimizers</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Optimizer</th><th>Learning</th><th>Best For</th></tr>
</thead>
<tbody>
<tr><td>SGD</td><td>Fixed or decaying</td><td>Simple tasks</td></tr>
<tr><td>Momentum</td><td>Accelerated</td><td>Faster convergence</td></tr>
<tr><td>Adam</td><td>Adaptive</td><td>Most modern tasks</td></tr>
<tr><td>RMSprop</td><td>Adaptive</td><td>Deep networks</td></tr>
</tbody>
</table>
</div>
<h2 id="code-examples"><a class="header" href="#code-examples">Code Examples</a></h2>
<h3 id="basic-neural-network"><a class="header" href="#basic-neural-network">Basic Neural Network</a></h3>
<h4 id="pytorch-implementation"><a class="header" href="#pytorch-implementation">PyTorch Implementation</a></h4>
<pre><code class="language-python">import torch
import torch.nn as nn
from torch.optim import Adam

# Define network
class NeuralNetwork(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super().__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

# Create network
model = NeuralNetwork(input_size=784, hidden_size=128, output_size=10)
criterion = nn.CrossEntropyLoss()
optimizer = Adam(model.parameters(), lr=0.001)

# Training loop
for epoch in range(10):
    for batch_x, batch_y in train_loader:
        # Forward pass
        outputs = model(batch_x)
        loss = criterion(outputs, batch_y)

        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
</code></pre>
<h4 id="tensorflowkeras-implementation"><a class="header" href="#tensorflowkeras-implementation">TensorFlow/Keras Implementation</a></h4>
<pre><code class="language-python">import tensorflow as tf
from tensorflow import keras

# Define network (Sequential API)
model = keras.Sequential([
    keras.layers.Dense(128, activation='relu', input_shape=(784,)),
    keras.layers.Dense(10, activation='softmax')
])

# Compile model
model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=0.001),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Training
history = model.fit(
    train_dataset,
    epochs=10,
    validation_data=val_dataset
)

# Functional API (for complex architectures)
inputs = keras.Input(shape=(784,))
x = keras.layers.Dense(128, activation='relu')(inputs)
outputs = keras.layers.Dense(10, activation='softmax')(x)
model = keras.Model(inputs=inputs, outputs=outputs)
</code></pre>
<h3 id="complete-training-pipeline"><a class="header" href="#complete-training-pipeline">Complete Training Pipeline</a></h3>
<pre><code class="language-python">import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR
import numpy as np

# Custom Dataset
class CustomDataset(Dataset):
    def __init__(self, data, labels, transform=None):
        self.data = data
        self.labels = labels
        self.transform = transform

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        x = self.data[idx]
        y = self.labels[idx]
        if self.transform:
            x = self.transform(x)
        return x, y

# Model with dropout and batch norm
class AdvancedNN(nn.Module):
    def __init__(self, input_size, hidden_sizes, output_size, dropout=0.3):
        super().__init__()
        layers = []
        prev_size = input_size

        for hidden_size in hidden_sizes:
            layers.extend([
                nn.Linear(prev_size, hidden_size),
                nn.BatchNorm1d(hidden_size),
                nn.ReLU(),
                nn.Dropout(dropout)
            ])
            prev_size = hidden_size

        layers.append(nn.Linear(prev_size, output_size))
        self.network = nn.Sequential(*layers)

    def forward(self, x):
        return self.network(x)

# Training function
def train_epoch(model, loader, criterion, optimizer, device):
    model.train()
    total_loss = 0
    correct = 0
    total = 0

    for batch_x, batch_y in loader:
        batch_x, batch_y = batch_x.to(device), batch_y.to(device)

        # Forward pass
        outputs = model(batch_x)
        loss = criterion(outputs, batch_y)

        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()

        # Metrics
        total_loss += loss.item()
        _, predicted = outputs.max(1)
        total += batch_y.size(0)
        correct += predicted.eq(batch_y).sum().item()

    return total_loss / len(loader), 100. * correct / total

# Validation function
def validate(model, loader, criterion, device):
    model.eval()
    total_loss = 0
    correct = 0
    total = 0

    with torch.no_grad():
        for batch_x, batch_y in loader:
            batch_x, batch_y = batch_x.to(device), batch_y.to(device)
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)

            total_loss += loss.item()
            _, predicted = outputs.max(1)
            total += batch_y.size(0)
            correct += predicted.eq(batch_y).sum().item()

    return total_loss / len(loader), 100. * correct / total

# Main training loop
def train_model(model, train_loader, val_loader, epochs=50, device='cuda'):
    criterion = nn.CrossEntropyLoss()
    optimizer = AdamW(model.parameters(), lr=1e-3, weight_decay=0.01)
    scheduler = CosineAnnealingLR(optimizer, T_max=epochs)

    best_val_acc = 0
    patience = 10
    patience_counter = 0

    for epoch in range(epochs):
        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)
        val_loss, val_acc = validate(model, val_loader, criterion, device)
        scheduler.step()

        print(f'Epoch {epoch+1}/{epochs}:')
        print(f'  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')
        print(f'  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')

        # Early stopping
        if val_acc &gt; best_val_acc:
            best_val_acc = val_acc
            patience_counter = 0
            torch.save(model.state_dict(), 'best_model.pt')
        else:
            patience_counter += 1
            if patience_counter &gt;= patience:
                print(f'Early stopping at epoch {epoch+1}')
                break

    # Load best model
    model.load_state_dict(torch.load('best_model.pt'))
    return model

# Usage
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = AdvancedNN(input_size=784, hidden_sizes=[512, 256, 128], output_size=10).to(device)

train_dataset = CustomDataset(train_data, train_labels)
val_dataset = CustomDataset(val_data, val_labels)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=64)

model = train_model(model, train_loader, val_loader, epochs=50, device=device)
</code></pre>
<h3 id="custom-layer-example"><a class="header" href="#custom-layer-example">Custom Layer Example</a></h3>
<pre><code class="language-python">class CustomAttentionLayer(nn.Module):
    """Simple self-attention layer"""
    def __init__(self, embed_dim):
        super().__init__()
        self.query = nn.Linear(embed_dim, embed_dim)
        self.key = nn.Linear(embed_dim, embed_dim)
        self.value = nn.Linear(embed_dim, embed_dim)
        self.scale = embed_dim ** 0.5

    def forward(self, x):
        # x shape: (batch, seq_len, embed_dim)
        Q = self.query(x)
        K = self.key(x)
        V = self.value(x)

        # Attention scores
        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale
        attn_weights = torch.softmax(scores, dim=-1)

        # Apply attention to values
        output = torch.matmul(attn_weights, V)
        return output, attn_weights
</code></pre>
<h3 id="inference-example"><a class="header" href="#inference-example">Inference Example</a></h3>
<pre><code class="language-python">def inference(model, input_data, device='cuda'):
    """Run inference with proper preprocessing"""
    model.eval()

    # Preprocess
    input_tensor = torch.from_numpy(input_data).float()
    input_tensor = input_tensor.to(device)

    # Inference
    with torch.no_grad():
        output = model(input_tensor)
        probabilities = torch.softmax(output, dim=-1)
        predicted_class = output.argmax(dim=-1)

    return predicted_class.cpu().numpy(), probabilities.cpu().numpy()

# Batch inference
predictions, probs = inference(model, test_data)

# Single sample
single_pred, single_probs = inference(model, test_data[0:1])
print(f"Predicted: {single_pred[0]}, Confidence: {single_probs[0][single_pred[0]]:.2%}")
</code></pre>
<h2 id="network-types"><a class="header" href="#network-types">Network Types</a></h2>
<h3 id="feedforward-neural-networks-fnn"><a class="header" href="#feedforward-neural-networks-fnn">Feedforward Neural Networks (FNN)</a></h3>
<ul>
<li>Data flows one direction only</li>
<li>Simplest type, works for structured data</li>
</ul>
<h3 id="convolutional-neural-networks-cnn"><a class="header" href="#convolutional-neural-networks-cnn">Convolutional Neural Networks (CNN)</a></h3>
<ul>
<li>Specialized for image processing</li>
<li>Uses filters to extract spatial features</li>
<li>Reduces parameters through weight sharing</li>
</ul>
<h3 id="recurrent-neural-networks-rnn"><a class="header" href="#recurrent-neural-networks-rnn">Recurrent Neural Networks (RNN)</a></h3>
<ul>
<li>Processes sequences (text, time series)</li>
<li>Maintains hidden state between inputs</li>
<li>Variants: LSTM, GRU (better long-term memory)</li>
</ul>
<h3 id="transformers"><a class="header" href="#transformers">Transformers</a></h3>
<ul>
<li>Attention-based architecture</li>
<li>Parallel processing of sequences</li>
<li>Powers modern LLMs (GPT, BERT)</li>
</ul>
<h2 id="hyperparameters"><a class="header" href="#hyperparameters">Hyperparameters</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Parameter</th><th>Impact</th><th>Typical Values</th></tr>
</thead>
<tbody>
<tr><td>Learning Rate</td><td>Convergence speed, stability</td><td>0.001 - 0.1</td></tr>
<tr><td>Batch Size</td><td>Memory, stability</td><td>32 - 256</td></tr>
<tr><td>Hidden Units</td><td>Capacity</td><td>64 - 2048</td></tr>
<tr><td>Epochs</td><td>Training duration</td><td>10 - 100</td></tr>
<tr><td>Dropout</td><td>Regularization</td><td>0.3 - 0.5</td></tr>
</tbody>
</table>
</div>
<h2 id="training-tips"><a class="header" href="#training-tips">Training Tips</a></h2>
<h3 id="1-data-preprocessing"><a class="header" href="#1-data-preprocessing">1. Data Preprocessing</a></h3>
<pre><code class="language-python"># Normalize inputs
mean = X_train.mean()
std = X_train.std()
X_train = (X_train - mean) / std
</code></pre>
<h3 id="2-early-stopping"><a class="header" href="#2-early-stopping">2. Early Stopping</a></h3>
<pre><code class="language-python"># Stop if validation loss doesn't improve
if val_loss &gt; best_loss:
    patience -= 1
    if patience == 0:
        break
best_loss = min(best_loss, val_loss)
</code></pre>
<h3 id="3-learning-rate-scheduling"><a class="header" href="#3-learning-rate-scheduling">3. Learning Rate Scheduling</a></h3>
<pre><code class="language-python"># Decrease learning rate over time
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)
for epoch in range(100):
    # train...
    scheduler.step()
</code></pre>
<h3 id="4-regularization"><a class="header" href="#4-regularization">4. Regularization</a></h3>
<ul>
<li><strong>L1/L2</strong>: Penalize large weights</li>
<li><strong>Dropout</strong>: Randomly disable neurons</li>
<li><strong>Batch Normalization</strong>: Normalize activations</li>
</ul>
<h2 id="common-issues"><a class="header" href="#common-issues">Common Issues</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Problem</th><th>Cause</th><th>Solution</th></tr>
</thead>
<tbody>
<tr><td>Underfitting</td><td>Model too simple</td><td>Increase hidden units, epochs</td></tr>
<tr><td>Overfitting</td><td>Model too complex</td><td>Add dropout, L2 regularization</td></tr>
<tr><td>Vanishing Gradients</td><td>Gradients $\to$ 0</td><td>Use ReLU, batch norm</td></tr>
<tr><td>Exploding Gradients</td><td>Gradients $\to \infty$</td><td>Gradient clipping</td></tr>
</tbody>
</table>
</div>
<hr>
<h1 id="advanced-topics-1"><a class="header" href="#advanced-topics-1">Advanced Topics</a></h1>
<h2 id="modern-architectures"><a class="header" href="#modern-architectures">Modern Architectures</a></h2>
<h3 id="resnet-residual-networks"><a class="header" href="#resnet-residual-networks">ResNet (Residual Networks)</a></h3>
<p><strong>Key Innovation</strong>: Skip connections (residual connections)</p>
<pre><code>x → Conv → ReLU → Conv → (+) → ReLU
                          ↑
                          x (skip connection)
</code></pre>
<p><strong>Benefits</strong>:</p>
<ul>
<li>Enables training of very deep networks (100+ layers)</li>
<li>Prevents vanishing gradients</li>
<li>Easier optimization landscape</li>
</ul>
<p><strong>Use Cases</strong>: Image classification, feature extraction backbone</p>
<h3 id="transformers-1"><a class="header" href="#transformers-1">Transformers</a></h3>
<p><strong>Key Innovation</strong>: Self-attention mechanism replaces recurrence</p>
<p><strong>Architecture</strong>:</p>
<pre><code>Input Embeddings + Positional Encoding
         ↓
Multi-Head Self-Attention
         ↓
Feed-Forward Network
         ↓
Output Predictions
</code></pre>
<p><strong>Attention Formula</strong>: $\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$</p>
<p><strong>Variants</strong>:</p>
<ul>
<li><strong>BERT</strong>: Bidirectional, masked language modeling</li>
<li><strong>GPT</strong>: Autoregressive, next-token prediction</li>
<li><strong>T5</strong>: Encoder-decoder, text-to-text framework</li>
<li><strong>Vision Transformers (ViT)</strong>: Apply to image patches</li>
</ul>
<p><strong>Use Cases</strong>: NLP, computer vision, multimodal AI</p>
<h3 id="generative-adversarial-networks-gans"><a class="header" href="#generative-adversarial-networks-gans">Generative Adversarial Networks (GANs)</a></h3>
<p><strong>Key Innovation</strong>: Two networks compete in a game</p>
<p><strong>Architecture</strong>:</p>
<ul>
<li><strong>Generator</strong>: Creates fake samples from noise</li>
<li><strong>Discriminator</strong>: Distinguishes real from fake</li>
</ul>
<pre><code class="language-python"># Training loop
for epoch in epochs:
    # Train Discriminator
    real_loss = criterion(D(real_data), ones)
    fake_loss = criterion(D(G(noise)), zeros)
    d_loss = real_loss + fake_loss

    # Train Generator
    g_loss = criterion(D(G(noise)), ones)  # Fool discriminator
</code></pre>
<p><strong>Variants</strong>:</p>
<ul>
<li><strong>DCGAN</strong>: Convolutional architecture</li>
<li><strong>StyleGAN</strong>: High-quality image generation with style control</li>
<li><strong>CycleGAN</strong>: Unpaired image-to-image translation</li>
<li><strong>Pix2Pix</strong>: Paired image translation</li>
</ul>
<p><strong>Use Cases</strong>: Image generation, data augmentation, style transfer</p>
<h3 id="diffusion-models"><a class="header" href="#diffusion-models">Diffusion Models</a></h3>
<p><strong>Key Innovation</strong>: Learn to denoise images through iterative refinement</p>
<p><strong>Forward Process</strong>: Add noise gradually over T steps
$$q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{1-\beta_t} x_{t-1}, \beta_t I)$$</p>
<p><strong>Reverse Process</strong>: Neural network learns to denoise
$$p_\theta(x_{t-1} | x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t))$$</p>
<p><strong>Key Models</strong>:</p>
<ul>
<li><strong>DDPM</strong> (Denoising Diffusion Probabilistic Models): Foundation</li>
<li><strong>DDIM</strong>: Faster sampling with fewer steps</li>
<li><strong>Stable Diffusion</strong>: Text-to-image generation with latent diffusion</li>
<li><strong>Imagen/DALL-E</strong>: High-quality text-to-image synthesis</li>
</ul>
<p><strong>Advantages</strong>:</p>
<ul>
<li>More stable training than GANs</li>
<li>High-quality generation</li>
<li>Easy to condition on text, class labels, etc.</li>
</ul>
<p><strong>Use Cases</strong>: Image generation, inpainting, super-resolution, text-to-image</p>
<h3 id="state-space-models"><a class="header" href="#state-space-models">State-Space Models</a></h3>
<p><strong>Key Innovation</strong>: Efficient sequence modeling with linear complexity</p>
<p><strong>Classical State-Space</strong>:
$$h_t = Ah_{t-1} + Bx_t$$
$$y_t = Ch_t + Dx_t$$</p>
<p><strong>S4 (Structured State Spaces)</strong>:</p>
<ul>
<li>Handles long sequences efficiently (10k+ tokens)</li>
<li>Structured matrices for efficient computation</li>
<li>Better than Transformers for very long contexts</li>
</ul>
<p><strong>Mamba</strong>:</p>
<ul>
<li>Selective state-space model</li>
<li>Input-dependent state transitions</li>
<li>5x faster than Transformers for long sequences</li>
<li>O(n) complexity vs O(n²) for attention</li>
</ul>
<p><strong>Use Cases</strong>: Long document processing, time series, genomics, audio</p>
<h3 id="neural-radiance-fields-nerf"><a class="header" href="#neural-radiance-fields-nerf">Neural Radiance Fields (NeRF)</a></h3>
<p><strong>Key Innovation</strong>: Represent 3D scenes as continuous functions</p>
<p><strong>Architecture</strong>:</p>
<ul>
<li>Input: 5D coordinates (x, y, z, θ, φ)</li>
<li>Output: Color (RGB) and volume density (σ)</li>
<li>Network: MLP with positional encoding</li>
</ul>
<p><strong>Training</strong>:</p>
<ul>
<li>Render images by ray marching through scene</li>
<li>Minimize difference with actual photographs</li>
<li>Learns implicit 3D representation</li>
</ul>
<p><strong>Variants</strong>:</p>
<ul>
<li><strong>Instant-NGP</strong>: 1000x faster training with hash encoding</li>
<li><strong>Mip-NeRF</strong>: Anti-aliasing and multi-scale representation</li>
<li><strong>NeRF-W</strong>: Wild scenes with varying conditions</li>
</ul>
<p><strong>Use Cases</strong>: 3D reconstruction, novel view synthesis, virtual reality</p>
<h3 id="graph-neural-networks-gnns"><a class="header" href="#graph-neural-networks-gnns">Graph Neural Networks (GNNs)</a></h3>
<p><strong>Key Innovation</strong>: Process graph-structured data</p>
<p><strong>Message Passing</strong>:
$$h_v^{(k+1)} = \text{UPDATE}\left(h_v^{(k)}, \text{AGGREGATE}({h_u^{(k)} : u \in N(v)})\right)$$</p>
<p><strong>Variants</strong>:</p>
<ul>
<li><strong>GCN</strong> (Graph Convolutional Networks)</li>
<li><strong>GraphSAGE</strong>: Scalable inductive learning</li>
<li><strong>GAT</strong> (Graph Attention Networks): Attention-based aggregation</li>
<li><strong>GIN</strong> (Graph Isomorphism Networks): Maximum discriminative power</li>
</ul>
<p><strong>Use Cases</strong>: Social networks, molecules, recommendation systems, knowledge graphs</p>
<h2 id="advanced-training-techniques"><a class="header" href="#advanced-training-techniques">Advanced Training Techniques</a></h2>
<h3 id="modern-optimizers"><a class="header" href="#modern-optimizers">Modern Optimizers</a></h3>
<h4 id="adamw-adam-with-weight-decay"><a class="header" href="#adamw-adam-with-weight-decay">AdamW (Adam with Weight Decay)</a></h4>
<p><strong>Improvement over Adam</strong>: Decouples weight decay from gradient updates</p>
<pre><code class="language-python">optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=1e-3,
    betas=(0.9, 0.999),
    weight_decay=0.01  # L2 regularization
)
</code></pre>
<p><strong>Why it’s better</strong>: Fixes weight decay implementation in Adam, improves generalization</p>
<h4 id="lion-evolved-sign-momentum"><a class="header" href="#lion-evolved-sign-momentum">Lion (Evolved Sign Momentum)</a></h4>
<p><strong>Key Feature</strong>: Memory-efficient, uses only sign of gradients</p>
<pre><code class="language-python">from lion_pytorch import Lion

optimizer = Lion(
    model.parameters(),
    lr=1e-4,  # Use ~3-10x smaller LR than Adam
    weight_decay=0.1
)
</code></pre>
<p><strong>Benefits</strong>: 2x memory reduction, often matches or beats AdamW</p>
<h4 id="adafactor"><a class="header" href="#adafactor">Adafactor</a></h4>
<p><strong>Key Feature</strong>: Adaptive learning rates with reduced memory</p>
<p><strong>Benefits</strong>:</p>
<ul>
<li>Reduces optimizer memory from O(parameters) to O(√parameters)</li>
<li>Good for large language models</li>
<li>No need to tune learning rate as carefully</li>
</ul>
<h4 id="optimizer-comparison"><a class="header" href="#optimizer-comparison">Optimizer Comparison</a></h4>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Optimizer</th><th>Memory</th><th>Speed</th><th>Stability</th><th>Best For</th></tr>
</thead>
<tbody>
<tr><td>SGD</td><td>Low</td><td>Fast</td><td>High</td><td>Simple tasks, fine-tuning</td></tr>
<tr><td>Adam</td><td>High</td><td>Fast</td><td>Medium</td><td>General purpose</td></tr>
<tr><td>AdamW</td><td>High</td><td>Fast</td><td>High</td><td>Most modern tasks</td></tr>
<tr><td>Lion</td><td>Medium</td><td>Fast</td><td>High</td><td>Large models, limited memory</td></tr>
<tr><td>Adafactor</td><td>Low</td><td>Medium</td><td>Medium</td><td>Very large models (LLMs)</td></tr>
</tbody>
</table>
</div>
<h3 id="learning-rate-schedules"><a class="header" href="#learning-rate-schedules">Learning Rate Schedules</a></h3>
<h4 id="warmup"><a class="header" href="#warmup">Warmup</a></h4>
<p><strong>Purpose</strong>: Prevent instability in early training</p>
<pre><code class="language-python">from torch.optim.lr_scheduler import LambdaLR

def warmup_lambda(epoch):
    if epoch &lt; warmup_epochs:
        return epoch / warmup_epochs
    return 1.0

scheduler = LambdaLR(optimizer, lr_lambda=warmup_lambda)
</code></pre>
<h4 id="cosine-annealing"><a class="header" href="#cosine-annealing">Cosine Annealing</a></h4>
<p><strong>Purpose</strong>: Smoothly decrease learning rate to fine-tune at end</p>
<pre><code class="language-python">from torch.optim.lr_scheduler import CosineAnnealingLR

scheduler = CosineAnnealingLR(
    optimizer,
    T_max=num_epochs,
    eta_min=1e-6  # Minimum LR
)

# With warmup
from torch.optim.lr_scheduler import SequentialLR

warmup_scheduler = LinearLR(optimizer, start_factor=0.1, total_iters=5)
cosine_scheduler = CosineAnnealingLR(optimizer, T_max=95)
scheduler = SequentialLR(
    optimizer,
    schedulers=[warmup_scheduler, cosine_scheduler],
    milestones=[5]
)
</code></pre>
<h4 id="one-cycle-policy"><a class="header" href="#one-cycle-policy">One Cycle Policy</a></h4>
<p><strong>Purpose</strong>: Fast training with super-convergence</p>
<pre><code class="language-python">from torch.optim.lr_scheduler import OneCycleLR

scheduler = OneCycleLR(
    optimizer,
    max_lr=0.1,
    epochs=num_epochs,
    steps_per_epoch=len(train_loader)
)
</code></pre>
<h3 id="mixed-precision-training"><a class="header" href="#mixed-precision-training">Mixed Precision Training</a></h3>
<p><strong>Purpose</strong>: Faster training, reduced memory (2x speedup typical)</p>
<h4 id="automatic-mixed-precision-amp"><a class="header" href="#automatic-mixed-precision-amp">Automatic Mixed Precision (AMP)</a></h4>
<pre><code class="language-python">from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

for epoch in range(num_epochs):
    for batch_x, batch_y in train_loader:
        optimizer.zero_grad()

        # Forward pass in mixed precision
        with autocast():
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)

        # Backward pass with gradient scaling
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
</code></pre>
<p><strong>How it works</strong>:</p>
<ul>
<li>FP16 for most operations (faster, less memory)</li>
<li>FP32 for stability-critical operations</li>
<li>Gradient scaling prevents underflow</li>
</ul>
<p><strong>Typical speedup</strong>: 1.5-3x on modern GPUs (A100, V100, RTX 30xx+)</p>
<h4 id="bfloat16"><a class="header" href="#bfloat16">BFloat16</a></h4>
<p><strong>Alternative to FP16</strong>: Same range as FP32, less precision</p>
<pre><code class="language-python">model = model.to(torch.bfloat16)  # Convert model
# or
with autocast(dtype=torch.bfloat16):
    outputs = model(batch_x)
</code></pre>
<p><strong>When to use</strong>: TPUs, newer GPUs (A100, H100), more stable than FP16</p>
<h3 id="gradient-accumulation"><a class="header" href="#gradient-accumulation">Gradient Accumulation</a></h3>
<p><strong>Purpose</strong>: Simulate larger batch sizes without memory</p>
<pre><code class="language-python">accumulation_steps = 4  # Effective batch = batch_size × 4

optimizer.zero_grad()
for i, (batch_x, batch_y) in enumerate(train_loader):
    outputs = model(batch_x)
    loss = criterion(outputs, batch_y)
    loss = loss / accumulation_steps  # Normalize loss

    loss.backward()

    # Update only every N steps
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
</code></pre>
<p><strong>Trade-off</strong>: More memory efficient, but slower training</p>
<h3 id="gradient-clipping"><a class="header" href="#gradient-clipping">Gradient Clipping</a></h3>
<p><strong>Purpose</strong>: Prevent exploding gradients</p>
<pre><code class="language-python"># Clip by norm
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

# Clip by value
torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=0.5)
</code></pre>
<p><strong>When to use</strong>: RNNs, very deep networks, unstable training</p>
<h3 id="transfer-learning--fine-tuning"><a class="header" href="#transfer-learning--fine-tuning">Transfer Learning &amp; Fine-Tuning</a></h3>
<h4 id="feature-extraction"><a class="header" href="#feature-extraction">Feature Extraction</a></h4>
<p><strong>Strategy</strong>: Freeze pretrained layers, train only new layers</p>
<pre><code class="language-python"># Load pretrained model
model = torchvision.models.resnet50(pretrained=True)

# Freeze all layers
for param in model.parameters():
    param.requires_grad = False

# Replace final layer
model.fc = nn.Linear(model.fc.in_features, num_classes)

# Only train new layer
optimizer = torch.optim.Adam(model.fc.parameters(), lr=1e-3)
</code></pre>
<h4 id="fine-tuning"><a class="header" href="#fine-tuning">Fine-Tuning</a></h4>
<p><strong>Strategy</strong>: Train entire model with small learning rate</p>
<pre><code class="language-python"># Load pretrained model
model = torchvision.models.resnet50(pretrained=True)
model.fc = nn.Linear(model.fc.in_features, num_classes)

# Different learning rates for different layers
optimizer = torch.optim.Adam([
    {'params': model.fc.parameters(), 'lr': 1e-3},
    {'params': model.layer4.parameters(), 'lr': 1e-4},
    {'params': model.layer1.parameters(), 'lr': 1e-5}
])
</code></pre>
<h4 id="discriminative-learning-rates"><a class="header" href="#discriminative-learning-rates">Discriminative Learning Rates</a></h4>
<p><strong>Strategy</strong>: Lower layers learn slower (more general features)</p>
<pre><code class="language-python">def get_layer_lr(layer_depth, base_lr=1e-3, decay=0.9):
    return base_lr * (decay ** layer_depth)

param_groups = []
for i, layer in enumerate(model.layers):
    param_groups.append({
        'params': layer.parameters(),
        'lr': get_layer_lr(i)
    })
optimizer = torch.optim.Adam(param_groups)
</code></pre>
<h3 id="progressive-resizing"><a class="header" href="#progressive-resizing">Progressive Resizing</a></h3>
<p><strong>Strategy</strong>: Train on small images first, then larger</p>
<pre><code class="language-python"># Start with 128x128
train_dataset = ImageDataset(transform=resize_to(128))
train_model(model, train_dataset, epochs=5)

# Increase to 224x224
train_dataset = ImageDataset(transform=resize_to(224))
train_model(model, train_dataset, epochs=5)

# Final size 384x384
train_dataset = ImageDataset(transform=resize_to(384))
train_model(model, train_dataset, epochs=5)
</code></pre>
<p><strong>Benefits</strong>: Faster training, acts as regularization, better accuracy</p>
<h2 id="practical-considerations"><a class="header" href="#practical-considerations">Practical Considerations</a></h2>
<h3 id="hardware--performance"><a class="header" href="#hardware--performance">Hardware &amp; Performance</a></h3>
<h4 id="gpu-memory-management"><a class="header" href="#gpu-memory-management">GPU Memory Management</a></h4>
<p><strong>Check available memory</strong>:</p>
<pre><code class="language-python">import torch

print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
print(f"Allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB")
print(f"Cached: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB")
</code></pre>
<p><strong>Memory optimization strategies</strong>:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Technique</th><th>Memory Savings</th><th>Speed Impact</th><th>When to Use</th></tr>
</thead>
<tbody>
<tr><td>Reduce batch size</td><td>High</td><td>Slower</td><td>Out of memory errors</td></tr>
<tr><td>Mixed precision (FP16)</td><td>2x</td><td>Faster</td><td>Always (modern GPUs)</td></tr>
<tr><td>Gradient accumulation</td><td>High</td><td>Slower</td><td>Need large effective batch</td></tr>
<tr><td>Gradient checkpointing</td><td>High</td><td>20-30% slower</td><td>Very deep networks</td></tr>
<tr><td>Model parallelism</td><td>Scales</td><td>Depends</td><td>Model &gt; single GPU</td></tr>
</tbody>
</table>
</div>
<p><strong>Gradient Checkpointing</strong>:</p>
<pre><code class="language-python">from torch.utils.checkpoint import checkpoint

class MyModel(nn.Module):
    def forward(self, x):
        # Recompute activations during backward pass
        x = checkpoint(self.layer1, x)
        x = checkpoint(self.layer2, x)
        return x
</code></pre>
<p><strong>When to use</strong>: Very deep networks (ResNet-200+, GPT-3), trades compute for memory</p>
<h4 id="hardware-selection"><a class="header" href="#hardware-selection">Hardware Selection</a></h4>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Use Case</th><th>Recommended GPU</th><th>VRAM Needed</th><th>Notes</th></tr>
</thead>
<tbody>
<tr><td>Small models (&lt;100M params)</td><td>RTX 3060, T4</td><td>6-8 GB</td><td>Good for learning</td></tr>
<tr><td>Medium models (100M-1B)</td><td>RTX 4090, A10</td><td>12-24 GB</td><td>Most research</td></tr>
<tr><td>Large models (1B-10B)</td><td>A100 (40GB)</td><td>40-80 GB</td><td>Professional use</td></tr>
<tr><td>Very large (10B+)</td><td>A100 (80GB), H100</td><td>80+ GB</td><td>Multi-GPU required</td></tr>
</tbody>
</table>
</div>
<p><strong>Cloud options</strong>:</p>
<ul>
<li><strong>AWS</strong>: p3.2xlarge (V100), p4d.24xlarge (A100)</li>
<li><strong>GCP</strong>: a2-highgpu-1g (A100)</li>
<li><strong>Azure</strong>: NC-series (V100, A100)</li>
<li><strong>Lambda Labs</strong>, <strong>RunPod</strong>: Cost-effective alternatives</li>
</ul>
<h3 id="model-deployment"><a class="header" href="#model-deployment">Model Deployment</a></h3>
<h4 id="export-to-onnx"><a class="header" href="#export-to-onnx">Export to ONNX</a></h4>
<p><strong>Purpose</strong>: Framework-agnostic format for inference</p>
<pre><code class="language-python">import torch.onnx

# Export PyTorch model
dummy_input = torch.randn(1, 3, 224, 224)
torch.onnx.export(
    model,
    dummy_input,
    "model.onnx",
    input_names=['input'],
    output_names=['output'],
    dynamic_axes={'input': {0: 'batch_size'}}
)

# Load with ONNX Runtime (faster inference)
import onnxruntime as ort
session = ort.InferenceSession("model.onnx")
output = session.run(None, {'input': input_array})
</code></pre>
<p><strong>Benefits</strong>: 2-5x faster inference, works across frameworks</p>
<h4 id="quantization"><a class="header" href="#quantization">Quantization</a></h4>
<p><strong>Purpose</strong>: Reduce model size and inference time</p>
<p><strong>Post-Training Quantization</strong> (easiest):</p>
<pre><code class="language-python">import torch.quantization

# PyTorch dynamic quantization
model_quantized = torch.quantization.quantize_dynamic(
    model,
    {nn.Linear},  # Quantize Linear layers
    dtype=torch.qint8
)

# Size reduction: 4x smaller, 2-3x faster
</code></pre>
<p><strong>Quantization-Aware Training</strong> (best accuracy):</p>
<pre><code class="language-python">model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')
model_prepared = torch.quantization.prepare_qat(model)

# Train as normal
for epoch in range(num_epochs):
    train_one_epoch(model_prepared)

# Convert to quantized model
model_quantized = torch.quantization.convert(model_prepared)
</code></pre>
<p><strong>Results</strong>:</p>
<ul>
<li><strong>INT8</strong>: 4x smaller, 2-4x faster, ~1% accuracy loss</li>
<li><strong>INT4</strong>: 8x smaller, 3-6x faster, ~2-3% accuracy loss</li>
</ul>
<h4 id="tensorrt-optimization"><a class="header" href="#tensorrt-optimization">TensorRT Optimization</a></h4>
<p><strong>Purpose</strong>: Maximum inference speed on NVIDIA GPUs</p>
<pre><code class="language-python">import torch_tensorrt

# Compile with TensorRT
trt_model = torch_tensorrt.compile(
    model,
    inputs=[torch_tensorrt.Input((1, 3, 224, 224))],
    enabled_precisions={torch.float16}
)

# Save compiled model
torch.jit.save(trt_model, "model_trt.ts")
</code></pre>
<p><strong>Typical speedup</strong>: 2-10x faster than native PyTorch</p>
<h4 id="model-serving"><a class="header" href="#model-serving">Model Serving</a></h4>
<p><strong>TorchServe</strong>:</p>
<pre><code class="language-bash"># Create model archive
torch-model-archiver --model-name resnet \
    --version 1.0 \
    --serialized-file model.pt \
    --handler image_classifier

# Start server
torchserve --start --model-store model_store --models resnet=resnet.mar
</code></pre>
<p><strong>FastAPI</strong> (simple custom server):</p>
<pre><code class="language-python">from fastapi import FastAPI
import torch

app = FastAPI()
model = torch.jit.load("model.pt")

@app.post("/predict")
def predict(data: dict):
    input_tensor = preprocess(data)
    with torch.no_grad():
        output = model(input_tensor)
    return {"prediction": output.tolist()}
</code></pre>
<h3 id="distributed-training"><a class="header" href="#distributed-training">Distributed Training</a></h3>
<h4 id="data-parallelism-ddp"><a class="header" href="#data-parallelism-ddp">Data Parallelism (DDP)</a></h4>
<p><strong>Strategy</strong>: Replicate model on multiple GPUs, split data</p>
<pre><code class="language-python">import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

# Initialize process group
dist.init_process_group(backend='nccl')
local_rank = int(os.environ['LOCAL_RANK'])

# Wrap model
model = model.to(local_rank)
model = DDP(model, device_ids=[local_rank])

# Use DistributedSampler
train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)
train_loader = DataLoader(train_dataset, sampler=train_sampler)

# Training loop
for epoch in range(num_epochs):
    train_sampler.set_epoch(epoch)  # Shuffle differently each epoch
    for batch in train_loader:
        # Training code...
</code></pre>
<p><strong>Launch</strong>:</p>
<pre><code class="language-bash">torchrun --nproc_per_node=4 train.py
</code></pre>
<p><strong>Scaling</strong>: Near-linear speedup (4 GPUs → ~3.8x faster)</p>
<h4 id="fully-sharded-data-parallel-fsdp"><a class="header" href="#fully-sharded-data-parallel-fsdp">Fully Sharded Data Parallel (FSDP)</a></h4>
<p><strong>Strategy</strong>: Shard model parameters across GPUs</p>
<pre><code class="language-python">from torch.distributed.fsdp import FullyShardedDataParallel as FSDP

model = FSDP(
    model,
    auto_wrap_policy=default_auto_wrap_policy,
    mixed_precision=MixedPrecision(param_dtype=torch.float16)
)
</code></pre>
<p><strong>When to use</strong>: Model doesn’t fit on single GPU (LLMs, very deep networks)</p>
<h4 id="deepspeed"><a class="header" href="#deepspeed">DeepSpeed</a></h4>
<p><strong>Framework</strong>: Advanced distributed training optimizations</p>
<pre><code class="language-python">import deepspeed

model_engine, optimizer, _, _ = deepspeed.initialize(
    model=model,
    model_parameters=model.parameters(),
    config="ds_config.json"
)

for batch in train_loader:
    loss = model_engine(batch)
    model_engine.backward(loss)
    model_engine.step()
</code></pre>
<p><strong>ZeRO stages</strong>:</p>
<ul>
<li><strong>ZeRO-1</strong>: Shard optimizer states (4x memory reduction)</li>
<li><strong>ZeRO-2</strong>: + Shard gradients (8x reduction)</li>
<li><strong>ZeRO-3</strong>: + Shard parameters (linear scaling to 1000s of GPUs)</li>
</ul>
<h3 id="production-best-practices"><a class="header" href="#production-best-practices">Production Best Practices</a></h3>
<h4 id="model-versioning"><a class="header" href="#model-versioning">Model Versioning</a></h4>
<pre><code class="language-python"># Save with metadata
torch.save({
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'epoch': epoch,
    'loss': loss,
    'hyperparameters': config,
    'timestamp': datetime.now()
}, f'model_v{version}.pt')
</code></pre>
<h4 id="monitoring--logging"><a class="header" href="#monitoring--logging">Monitoring &amp; Logging</a></h4>
<pre><code class="language-python">import wandb  # or tensorboard

wandb.init(project="my-project")
wandb.config.update(hyperparameters)

for epoch in range(num_epochs):
    train_loss = train_one_epoch()
    val_loss = validate()

    wandb.log({
        'train_loss': train_loss,
        'val_loss': val_loss,
        'learning_rate': optimizer.param_groups[0]['lr']
    })
</code></pre>
<h4 id="input-validation"><a class="header" href="#input-validation">Input Validation</a></h4>
<pre><code class="language-python">def validate_input(tensor):
    # Check shape
    assert tensor.shape[1:] == (3, 224, 224), f"Expected (3,224,224), got {tensor.shape[1:]}"

    # Check value range
    assert tensor.min() &gt;= 0 and tensor.max() &lt;= 1, "Input must be normalized to [0,1]"

    # Check for NaN/Inf
    assert not torch.isnan(tensor).any(), "Input contains NaN"
    assert not torch.isinf(tensor).any(), "Input contains Inf"
</code></pre>
<h4 id="ab-testing"><a class="header" href="#ab-testing">A/B Testing</a></h4>
<pre><code class="language-python">def predict_with_ab_test(input_data):
    model_version = random.choice(['v1', 'v2'], p=[0.9, 0.1])  # 90/10 split

    if model_version == 'v1':
        return model_v1(input_data), 'v1'
    else:
        return model_v2(input_data), 'v2'
</code></pre>
<h2 id="eli10"><a class="header" href="#eli10">ELI10</a></h2>
<p>Think of a neural network like learning to draw:</p>
<ol>
<li><strong>Input Layer</strong>: You see a cat</li>
<li><strong>Hidden Layers</strong>: Brain recognizes ears -&gt; whiskers -&gt; tail (learns patterns)</li>
<li><strong>Output Layer</strong>: Brain says “This is a cat!”</li>
</ol>
<p>The network learns by:</p>
<ul>
<li>Making predictions (forward pass)</li>
<li>Checking if wrong (loss)</li>
<li>Adjusting “how to recognize cats” (backprop)</li>
<li>Repeating until accurate</li>
</ul>
<p>More hidden layers = learns more complex patterns!</p>
<hr>
<h1 id="quick-reference"><a class="header" href="#quick-reference">Quick Reference</a></h1>
<h2 id="common-patterns-cheat-sheet"><a class="header" href="#common-patterns-cheat-sheet">Common Patterns Cheat Sheet</a></h2>
<h3 id="model-initialization"><a class="header" href="#model-initialization">Model Initialization</a></h3>
<pre><code class="language-python"># PyTorch
model = MyModel().to(device)
model.load_state_dict(torch.load('model.pt'))  # Load weights
torch.save(model.state_dict(), 'model.pt')     # Save weights

# TensorFlow
model = MyModel()
model.load_weights('model.h5')     # Load weights
model.save_weights('model.h5')     # Save weights
model.save('full_model.keras')     # Save full model
</code></pre>
<h3 id="training-loop-pattern"><a class="header" href="#training-loop-pattern">Training Loop Pattern</a></h3>
<pre><code class="language-python"># PyTorch standard training loop
model.train()  # Set to training mode
for batch_x, batch_y in train_loader:
    batch_x, batch_y = batch_x.to(device), batch_y.to(device)
    optimizer.zero_grad()           # Clear gradients
    outputs = model(batch_x)        # Forward pass
    loss = criterion(outputs, batch_y)
    loss.backward()                 # Compute gradients
    optimizer.step()                # Update weights

# TensorFlow/Keras
model.fit(train_dataset, epochs=10, validation_data=val_dataset)
</code></pre>
<h3 id="validationinference-pattern"><a class="header" href="#validationinference-pattern">Validation/Inference Pattern</a></h3>
<pre><code class="language-python"># PyTorch
model.eval()  # Set to evaluation mode
with torch.no_grad():  # Disable gradient computation
    for batch_x, batch_y in val_loader:
        batch_x = batch_x.to(device)
        outputs = model(batch_x)
        # Compute metrics...

# TensorFlow
model.evaluate(test_dataset)
predictions = model.predict(test_data)
</code></pre>
<h3 id="layer-types-quick-reference"><a class="header" href="#layer-types-quick-reference">Layer Types Quick Reference</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Task</th><th>Layer Type</th><th>PyTorch</th><th>TensorFlow</th></tr>
</thead>
<tbody>
<tr><td>Fully Connected</td><td>Dense/Linear</td><td><code>nn.Linear(in, out)</code></td><td><code>Dense(units)</code></td></tr>
<tr><td>Convolution 2D</td><td>Conv</td><td><code>nn.Conv2d(in, out, kernel)</code></td><td><code>Conv2D(filters, kernel)</code></td></tr>
<tr><td>Max Pooling</td><td>Pooling</td><td><code>nn.MaxPool2d(kernel)</code></td><td><code>MaxPool2D(pool_size)</code></td></tr>
<tr><td>Dropout</td><td>Regularization</td><td><code>nn.Dropout(p)</code></td><td><code>Dropout(rate)</code></td></tr>
<tr><td>Batch Norm</td><td>Normalization</td><td><code>nn.BatchNorm1d(features)</code></td><td><code>BatchNormalization()</code></td></tr>
<tr><td>ReLU</td><td>Activation</td><td><code>nn.ReLU()</code></td><td><code>Activation('relu')</code></td></tr>
<tr><td>LSTM</td><td>Recurrent</td><td><code>nn.LSTM(in, hidden)</code></td><td><code>LSTM(units)</code></td></tr>
</tbody>
</table>
</div>
<h3 id="loss-functions"><a class="header" href="#loss-functions">Loss Functions</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Task</th><th>Loss Function</th><th>PyTorch</th><th>TensorFlow</th></tr>
</thead>
<tbody>
<tr><td>Binary Classification</td><td>BCE</td><td><code>nn.BCELoss()</code></td><td><code>'binary_crossentropy'</code></td></tr>
<tr><td>Multi-class</td><td>CrossEntropy</td><td><code>nn.CrossEntropyLoss()</code></td><td><code>'categorical_crossentropy'</code></td></tr>
<tr><td>Multi-class (sparse)</td><td>CrossEntropy</td><td><code>nn.CrossEntropyLoss()</code></td><td><code>'sparse_categorical_crossentropy'</code></td></tr>
<tr><td>Regression</td><td>MSE</td><td><code>nn.MSELoss()</code></td><td><code>'mse'</code></td></tr>
<tr><td>Regression</td><td>MAE</td><td><code>nn.L1Loss()</code></td><td><code>'mae'</code></td></tr>
</tbody>
</table>
</div>
<h3 id="optimizer-selection-guide"><a class="header" href="#optimizer-selection-guide">Optimizer Selection Guide</a></h3>
<pre><code class="language-python"># Starting point for most tasks
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.01)

# Fine-tuning pretrained models
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)

# Memory constrained
optimizer = Lion(model.parameters(), lr=1e-4, weight_decay=0.1)

# Simple tasks, maximum control
optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
</code></pre>
<h3 id="learning-rate-schedule-pattern"><a class="header" href="#learning-rate-schedule-pattern">Learning Rate Schedule Pattern</a></h3>
<pre><code class="language-python"># Warmup + Cosine Annealing (recommended)
from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingLR

warmup = LinearLR(optimizer, start_factor=0.1, total_iters=5)
cosine = CosineAnnealingLR(optimizer, T_max=45, eta_min=1e-6)
scheduler = SequentialLR(optimizer, [warmup, cosine], milestones=[5])

# In training loop
for epoch in range(epochs):
    train_one_epoch()
    scheduler.step()  # Update learning rate
</code></pre>
<h3 id="debugging-checklist"><a class="header" href="#debugging-checklist">Debugging Checklist</a></h3>
<p><strong>Loss not decreasing?</strong></p>
<ol>
<li>Check learning rate (try 1e-3, 1e-4, 1e-5)</li>
<li>Verify data preprocessing (normalization, scaling)</li>
<li>Check loss function matches task</li>
<li>Ensure labels are correct format</li>
<li>Try simpler model first</li>
</ol>
<p><strong>Loss is NaN/Inf?</strong></p>
<ol>
<li>Reduce learning rate</li>
<li>Add gradient clipping: <code>torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)</code></li>
<li>Check for division by zero</li>
<li>Use mixed precision carefully</li>
<li>Check input data for NaN/Inf values</li>
</ol>
<p><strong>Overfitting (train acc &gt;&gt; val acc)?</strong></p>
<ol>
<li>Add dropout: <code>nn.Dropout(0.3)</code></li>
<li>Add weight decay: <code>weight_decay=0.01</code></li>
<li>Reduce model size</li>
<li>Get more training data</li>
<li>Add data augmentation</li>
<li>Early stopping</li>
</ol>
<p><strong>Underfitting (both accuracies low)?</strong></p>
<ol>
<li>Increase model capacity (more layers/units)</li>
<li>Train longer</li>
<li>Reduce regularization</li>
<li>Check data quality</li>
<li>Try different architecture</li>
</ol>
<h3 id="common-hyperparameter-ranges"><a class="header" href="#common-hyperparameter-ranges">Common Hyperparameter Ranges</a></h3>
<pre><code class="language-python">hyperparameters = {
    'learning_rate': [1e-5, 1e-4, 1e-3, 1e-2],      # Most critical
    'batch_size': [16, 32, 64, 128, 256],            # Memory dependent
    'hidden_units': [64, 128, 256, 512, 1024],       # Task dependent
    'dropout': [0.1, 0.2, 0.3, 0.4, 0.5],           # Regularization
    'weight_decay': [0, 1e-5, 1e-4, 1e-3, 1e-2],    # L2 regularization
    'epochs': [10, 20, 50, 100],                     # Use early stopping
}
</code></pre>
<h3 id="data-preprocessing-template"><a class="header" href="#data-preprocessing-template">Data Preprocessing Template</a></h3>
<pre><code class="language-python">from torchvision import transforms

# Image preprocessing (standard)
transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet stats
                        std=[0.229, 0.224, 0.225])
])

# Numerical data preprocessing
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_val = scaler.transform(X_val)  # Use same scaler!
</code></pre>
<h3 id="model-architecture-rules-of-thumb"><a class="header" href="#model-architecture-rules-of-thumb">Model Architecture Rules of Thumb</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Guideline</th><th>Recommendation</th></tr>
</thead>
<tbody>
<tr><td><strong>Depth</strong></td><td>Start with 2-3 hidden layers, add more if underfitting</td></tr>
<tr><td><strong>Width</strong></td><td>Hidden size typically 50-500% of input size</td></tr>
<tr><td><strong>Output layer</strong></td><td>No activation for regression, softmax for classification</td></tr>
<tr><td><strong>Hidden activation</strong></td><td>ReLU for most cases, GELU for transformers</td></tr>
<tr><td><strong>Batch norm</strong></td><td>After linear layer, before activation</td></tr>
<tr><td><strong>Dropout</strong></td><td>After activation, 0.3-0.5 typical</td></tr>
<tr><td><strong>Residual connections</strong></td><td>For networks &gt;10 layers</td></tr>
</tbody>
</table>
</div>
<h3 id="gpu-memory-estimation"><a class="header" href="#gpu-memory-estimation">GPU Memory Estimation</a></h3>
<pre><code>Memory = Model Params × (4 bytes if FP32, 2 bytes if FP16)
        + Activations × Batch Size × 4 bytes
        + Gradients ≈ 2 × Model Memory
        + Optimizer State ≈ 2 × Model Memory (Adam)

Total ≈ 4-6× Model Size for training (Adam, FP32)
Total ≈ 2-3× Model Size for training (Adam, FP16)
</code></pre>
<p><strong>Example</strong>: 100M parameter model</p>
<ul>
<li>FP32: 400 MB model + overhead = ~2 GB training</li>
<li>FP16: 200 MB model + overhead = ~1 GB training</li>
</ul>
<h3 id="performance-optimization-priority"><a class="header" href="#performance-optimization-priority">Performance Optimization Priority</a></h3>
<ol>
<li><strong>Mixed precision training</strong> (FP16) - 2x speedup, 2x memory reduction</li>
<li><strong>Increase batch size</strong> - Better GPU utilization</li>
<li><strong>Use DataLoader with num_workers</strong> - Parallel data loading</li>
<li><strong>Pin memory</strong> - <code>DataLoader(..., pin_memory=True)</code></li>
<li><strong>Compile model</strong> (PyTorch 2.0+) - <code>model = torch.compile(model)</code></li>
<li><strong>Profile code</strong> - Find bottlenecks before optimizing</li>
</ol>
<h2 id="further-resources"><a class="header" href="#further-resources">Further Resources</a></h2>
<ul>
<li><a href="https://playground.tensorflow.org/">Neural Networks Visualization</a></li>
<li><a href="https://www.youtube.com/watch?v=aircAruvnKk">3Blue1Brown Neural Networks Series</a></li>
<li><a href="https://pytorch.org/tutorials/">PyTorch Tutorials</a></li>
<li><a href="https://www.deeplearningbook.org/">Deep Learning Book</a></li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../machine_learning/deep_learning.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256 246.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"/></svg></span>
                            </a>

                            <a rel="next prefetch" href="../machine_learning/supervised_learning.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M278.6 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L210.7 256 73.4 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"/></svg></span>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../machine_learning/deep_learning.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256 246.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"/></svg></span>
                    </a>

                    <a rel="next prefetch" href="../machine_learning/supervised_learning.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M278.6 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L210.7 256 73.4 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"/></svg></span>
                    </a>
            </nav>

        </div>

        <template id=fa-eye><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M288 32c-80.8 0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7 0 24.6C17.3 304 48.6 356 95.4 399.4C142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144s64.5-144 144-144s144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64c-11.5 0-22.3-3-31.6-8.4c-.2 2.8-.4 5.5-.4 8.4c0 53 43 96 96 96s96-43 96-96s-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6z"/></svg></span></template>
        <template id=fa-eye-slash><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M38.8 5.1C28.4-3.1 13.3-1.2 5.1 9.2S-1.2 34.7 9.2 42.9l592 464c10.4 8.2 25.5 6.3 33.7-4.1s6.3-25.5-4.1-33.7L525.6 386.7c39.6-40.6 66.4-86.1 79.9-118.4c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C465.5 68.8 400.8 32 320 32c-68.2 0-125 26.3-169.3 60.8L38.8 5.1zM223.1 149.5C248.6 126.2 282.7 112 320 112c79.5 0 144 64.5 144 144c0 24.9-6.3 48.3-17.4 68.7L408 294.5c5.2-11.8 8-24.8 8-38.5c0-53-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6c0 10.2-2.4 19.8-6.6 28.3l-90.3-70.8zm223.1 298L373 389.9c-16.4 6.5-34.3 10.1-53 10.1c-79.5 0-144-64.5-144-144c0-6.9 .5-13.6 1.4-20.2L83.1 161.5C60.3 191.2 44 220.8 34.5 243.7c-3.3 7.9-3.3 16.7 0 24.6c14.9 35.7 46.2 87.7 93 131.1C174.5 443.2 239.2 480 320 480c47.8 0 89.9-12.9 126.2-32.5z"/></svg></span></template>
        <template id=fa-copy><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M502.6 70.63l-61.25-61.25C435.4 3.371 427.2 0 418.7 0H255.1c-35.35 0-64 28.66-64 64l.0195 256C192 355.4 220.7 384 256 384h192c35.2 0 64-28.8 64-64V93.25C512 84.77 508.6 76.63 502.6 70.63zM464 320c0 8.836-7.164 16-16 16H255.1c-8.838 0-16-7.164-16-16L239.1 64.13c0-8.836 7.164-16 16-16h128L384 96c0 17.67 14.33 32 32 32h47.1V320zM272 448c0 8.836-7.164 16-16 16H63.1c-8.838 0-16-7.164-16-16L47.98 192.1c0-8.836 7.164-16 16-16H160V128H63.99c-35.35 0-64 28.65-64 64l.0098 256C.002 483.3 28.66 512 64 512h192c35.2 0 64-28.8 64-64v-32h-47.1L272 448z"/></svg></span></template>
        <template id=fa-play><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M73 39c-14.8-9.1-33.4-9.4-48.5-.9S0 62.6 0 80V432c0 17.4 9.4 33.4 24.5 41.9s33.7 8.1 48.5-.9L361 297c14.3-8.7 23-24.2 23-41s-8.7-32.2-23-41L73 39z"/></svg></span></template>
        <template id=fa-clock-rotate-left><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M75 75L41 41C25.9 25.9 0 36.6 0 57.9V168c0 13.3 10.7 24 24 24H134.1c21.4 0 32.1-25.9 17-41l-30.8-30.8C155 85.5 203 64 256 64c106 0 192 86 192 192s-86 192-192 192c-40.8 0-78.6-12.7-109.7-34.4c-14.5-10.1-34.4-6.6-44.6 7.9s-6.6 34.4 7.9 44.6C151.2 495 201.7 512 256 512c141.4 0 256-114.6 256-256S397.4 0 256 0C185.3 0 121.3 28.7 75 75zm181 53c-13.3 0-24 10.7-24 24V256c0 6.4 2.5 12.5 7 17l72 72c9.4 9.4 24.6 9.4 33.9 0s9.4-24.6 0-33.9l-65-65V152c0-13.3-10.7-24-24-24z"/></svg></span></template>



        <script>
            window.playground_copyable = true;
        </script>


        <script src="../elasticlunr-ef4e11c1.min.js"></script>
        <script src="../mark-09e88c2c.min.js"></script>
        <script src="../searcher-c2a407aa.js"></script>

        <script src="../clipboard-1626706a.min.js"></script>
        <script src="../highlight-abc7f01d.js"></script>
        <script src="../book-a0b12cfe.js"></script>

        <!-- Custom JS scripts -->



    </div>
    </body>
</html>
