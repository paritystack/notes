<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Whisper - My Notes</title>


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon-de23e50b.svg">
        <link rel="shortcut icon" href="../favicon-8114d1fc.png">
        <link rel="stylesheet" href="../css/variables-8adf115d.css">
        <link rel="stylesheet" href="../css/general-2459343d.css">
        <link rel="stylesheet" href="../css/chrome-ae938929.css">
        <link rel="stylesheet" href="../css/print-9e4910d8.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../fonts/fonts-9644e21d.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="mdbook-highlight-css" href="../highlight-493f70e1.css">
        <link rel="stylesheet" id="mdbook-tomorrow-night-css" href="../tomorrow-night-4c0ae647.css">
        <link rel="stylesheet" id="mdbook-ayu-highlight-css" href="../ayu-highlight-3fdfc3ac.css">

        <!-- Custom theme stylesheets -->

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "../";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "../searchindex-5e706ac8.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../toc-82510463.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="mdbook-body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="mdbook-sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("mdbook-sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="mdbook-sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="../toc.html"></iframe>
            </noscript>
            <div id="mdbook-sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="mdbook-page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="mdbook-menu-bar-hover-placeholder"></div>
                <div id="mdbook-menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="mdbook-sidebar-toggle" class="icon-button" for="mdbook-sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="mdbook-sidebar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"/></svg></span>
                        </label>
                        <button id="mdbook-theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="mdbook-theme-list">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M371.3 367.1c27.3-3.9 51.9-19.4 67.2-42.9L600.2 74.1c12.6-19.5 9.4-45.3-7.6-61.2S549.7-4.4 531.1 9.6L294.4 187.2c-24 18-38.2 46.1-38.4 76.1L371.3 367.1zm-19.6 25.4l-116-104.4C175.9 290.3 128 339.6 128 400c0 3.9 .2 7.8 .6 11.6c1.8 17.5-10.2 36.4-27.8 36.4H96c-17.7 0-32 14.3-32 32s14.3 32 32 32H240c61.9 0 112-50.1 112-112c0-2.5-.1-5-.2-7.5z"/></svg></span>
                        </button>
                        <ul id="mdbook-theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-ayu">Ayu</button></li>
                        </ul>
                        <button id="mdbook-search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="mdbook-searchbar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M416 208c0 45.9-14.9 88.3-40 122.7L502.6 457.4c12.5 12.5 12.5 32.8 0 45.3s-32.8 12.5-45.3 0L330.7 376c-34.4 25.2-76.8 40-122.7 40C93.1 416 0 322.9 0 208S93.1 0 208 0S416 93.1 416 208zM208 352c79.5 0 144-64.5 144-144s-64.5-144-144-144S64 128.5 64 208s64.5 144 144 144z"/></svg></span>
                        </button>
                    </div>

                    <h1 class="menu-title">My Notes</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <span class=fa-svg id="print-button"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M128 0C92.7 0 64 28.7 64 64v96h64V64H354.7L384 93.3V160h64V93.3c0-17-6.7-33.3-18.7-45.3L400 18.7C388 6.7 371.7 0 354.7 0H128zM384 352v32 64H128V384 368 352H384zm64 32h32c17.7 0 32-14.3 32-32V256c0-35.3-28.7-64-64-64H64c-35.3 0-64 28.7-64 64v96c0 17.7 14.3 32 32 32H64v64c0 35.3 28.7 64 64 64H384c35.3 0 64-28.7 64-64V384zm-16-88c-13.3 0-24-10.7-24-24s10.7-24 24-24s24 10.7 24 24s-10.7 24-24 24z"/></svg></span>
                        </a>

                    </div>
                </div>

                <div id="mdbook-search-wrapper" class="hidden">
                    <form id="mdbook-searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="mdbook-searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="mdbook-searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <span class=fa-svg id="fa-spin"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M304 48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zm0 416c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM48 304c26.5 0 48-21.5 48-48s-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48zm464-48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM142.9 437c18.7-18.7 18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zm0-294.2c18.7-18.7 18.7-49.1 0-67.9S93.7 56.2 75 75s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zM369.1 437c18.7 18.7 49.1 18.7 67.9 0s18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9z"/></svg></span>
                            </div>
                        </div>
                    </form>
                    <div id="mdbook-searchresults-outer" class="searchresults-outer hidden">
                        <div id="mdbook-searchresults-header" class="searchresults-header"></div>
                        <ul id="mdbook-searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('mdbook-sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('mdbook-sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#mdbook-sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="mdbook-content" class="content">
                    <main>
                        <h1 id="whisper---openai-speech-recognition"><a class="header" href="#whisper---openai-speech-recognition">Whisper - OpenAI Speech Recognition</a></h1>
<p>Complete guide to OpenAI’s Whisper, a robust automatic speech recognition (ASR) system trained on 680,000 hours of multilingual data.</p>
<h2 id="table-of-contents"><a class="header" href="#table-of-contents">Table of Contents</a></h2>
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#model-versions">Model Versions</a></li>
<li><a href="#installation--setup">Installation &amp; Setup</a></li>
<li><a href="#basic-usage">Basic Usage</a></li>
<li><a href="#fine-tuning">Fine-tuning</a></li>
<li><a href="#common-patterns">Common Patterns</a></li>
<li><a href="#advanced-operations">Advanced Operations</a></li>
<li><a href="#optimization">Optimization</a></li>
<li><a href="#deployment">Deployment</a></li>
<li><a href="#advanced-techniques">Advanced Techniques</a></li>
<li><a href="#integration">Integration</a></li>
<li><a href="#best-practices">Best Practices</a></li>
</ul>
<h2 id="introduction"><a class="header" href="#introduction">Introduction</a></h2>
<p>Whisper is OpenAI’s state-of-the-art automatic speech recognition (ASR) model, released in September 2022. It’s trained on 680,000 hours of multilingual and multitask supervised data collected from the web, making it robust to accents, background noise, and technical language.</p>
<h3 id="key-features"><a class="header" href="#key-features">Key Features</a></h3>
<ul>
<li><strong>Multilingual</strong>: Supports 99 languages</li>
<li><strong>Robust</strong>: Works with noisy audio, accents, technical terms</li>
<li><strong>Multitask</strong>: Transcription, translation, language identification, timestamp generation</li>
<li><strong>Open Source</strong>: Available under MIT license</li>
<li><strong>Multiple Sizes</strong>: From 39M (tiny) to 1550M (large) parameters</li>
<li><strong>High Accuracy</strong>: Near-human level performance on clean audio</li>
<li><strong>Zero-shot</strong>: Works without fine-tuning</li>
</ul>
<h3 id="architecture"><a class="header" href="#architecture">Architecture</a></h3>
<ul>
<li><strong>Encoder-Decoder Transformer</strong>: Based on sequence-to-sequence architecture</li>
<li><strong>Mel Spectrogram Input</strong>: 80-channel log-mel spectrogram</li>
<li><strong>Multi-head Attention</strong>: Self and cross-attention mechanisms</li>
<li><strong>Positional Encoding</strong>: Sinusoidal positional embeddings</li>
<li><strong>Special Tokens</strong>: Task-specific tokens for control</li>
<li><strong>Byte Pair Encoding</strong>: Multilingual tokenizer</li>
</ul>
<h3 id="supported-languages"><a class="header" href="#supported-languages">Supported Languages</a></h3>
<pre><code class="language-python"># Major languages supported (99 total)
languages = [
    "English", "Chinese", "Spanish", "French", "German", "Japanese",
    "Portuguese", "Russian", "Korean", "Arabic", "Hindi", "Italian",
    "Dutch", "Polish", "Turkish", "Vietnamese", "Indonesian", "Thai",
    "Hebrew", "Greek", "Czech", "Romanian", "Swedish", "Hungarian"
    # ... and 75 more
]
</code></pre>
<h3 id="tasks"><a class="header" href="#tasks">Tasks</a></h3>
<ol>
<li><strong>Transcription</strong>: Audio → text in same language</li>
<li><strong>Translation</strong>: Audio → English text</li>
<li><strong>Language Detection</strong>: Identify spoken language</li>
<li><strong>Timestamp Generation</strong>: Word-level or segment-level timing</li>
<li><strong>Voice Activity Detection</strong>: Detect speech regions</li>
</ol>
<h2 id="model-versions"><a class="header" href="#model-versions">Model Versions</a></h2>
<h3 id="overview"><a class="header" href="#overview">Overview</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Model</th><th>Parameters</th><th>VRAM (FP16)</th><th>Relative Speed</th><th>English WER</th></tr>
</thead>
<tbody>
<tr><td>tiny</td><td>39M</td><td>1GB</td><td>32x</td><td>7.5%</td></tr>
<tr><td>base</td><td>74M</td><td>1GB</td><td>16x</td><td>5.5%</td></tr>
<tr><td>small</td><td>244M</td><td>2GB</td><td>6x</td><td>3.5%</td></tr>
<tr><td>medium</td><td>769M</td><td>5GB</td><td>2x</td><td>2.8%</td></tr>
<tr><td>large</td><td>1550M</td><td>10GB</td><td>1x</td><td>2.3%</td></tr>
<tr><td>large-v2</td><td>1550M</td><td>10GB</td><td>1x</td><td>2.1%</td></tr>
<tr><td>large-v3</td><td>1550M</td><td>10GB</td><td>1x</td><td>1.8%</td></tr>
</tbody>
</table>
</div>
<h3 id="tiny-model"><a class="header" href="#tiny-model">Tiny Model</a></h3>
<p><strong>Best for: Real-time applications, edge devices, quick prototyping</strong></p>
<pre><code class="language-python">import whisper

model = whisper.load_model("tiny")
result = model.transcribe("audio.mp3")
print(result["text"])
</code></pre>
<p><strong>Characteristics:</strong></p>
<ul>
<li>Fastest inference</li>
<li>Lowest memory usage</li>
<li>Good for English</li>
<li>Lower accuracy on noisy audio</li>
<li>32x faster than large</li>
</ul>
<h3 id="base-model"><a class="header" href="#base-model">Base Model</a></h3>
<p><strong>Best for: Balanced speed and accuracy</strong></p>
<pre><code class="language-python">model = whisper.load_model("base")
result = model.transcribe("audio.mp3", language="en")
</code></pre>
<p><strong>Characteristics:</strong></p>
<ul>
<li>Fast inference</li>
<li>Decent accuracy</li>
<li>Good multilingual support</li>
<li>16x faster than large</li>
<li>Low resource requirements</li>
</ul>
<h3 id="small-model"><a class="header" href="#small-model">Small Model</a></h3>
<p><strong>Best for: Production applications with reasonable accuracy</strong></p>
<pre><code class="language-python">model = whisper.load_model("small")
result = model.transcribe(
    "audio.mp3",
    language="en",
    task="transcribe"
)
</code></pre>
<p><strong>Characteristics:</strong></p>
<ul>
<li>Balanced speed/accuracy</li>
<li>Good multilingual performance</li>
<li>Handles accents well</li>
<li>6x faster than large</li>
<li>Popular choice for APIs</li>
</ul>
<h3 id="medium-model"><a class="header" href="#medium-model">Medium Model</a></h3>
<p><strong>Best for: High accuracy without extreme resources</strong></p>
<pre><code class="language-python">model = whisper.load_model("medium")
result = model.transcribe(
    "audio.mp3",
    language="es",
    verbose=True
)
</code></pre>
<p><strong>Characteristics:</strong></p>
<ul>
<li>High accuracy</li>
<li>Good for difficult audio</li>
<li>Better punctuation</li>
<li>2x faster than large</li>
<li>Good multilingual performance</li>
</ul>
<h3 id="large-models-v1-v2-v3"><a class="header" href="#large-models-v1-v2-v3">Large Models (v1, v2, v3)</a></h3>
<p><strong>Best for: Maximum accuracy, research, difficult audio</strong></p>
<pre><code class="language-python"># Large-v3 (recommended)
model = whisper.load_model("large-v3")

# Large-v2
model = whisper.load_model("large-v2")

# Large (original)
model = whisper.load_model("large")

result = model.transcribe(
    "audio.mp3",
    task="transcribe",
    language="en"
)
</code></pre>
<p><strong>Characteristics:</strong></p>
<ul>
<li>Highest accuracy</li>
<li>Best multilingual support</li>
<li>Handles noise, accents, dialects</li>
<li>Slower inference</li>
<li>Large-v3 is most recent and accurate</li>
</ul>
<p><strong>Version Differences:</strong></p>
<ul>
<li><strong>v1</strong>: Original release</li>
<li><strong>v2</strong>: Improved for difficult audio, better timestamps</li>
<li><strong>v3</strong>: Best overall, improved low-resource languages</li>
</ul>
<h3 id="model-selection-guide"><a class="header" href="#model-selection-guide">Model Selection Guide</a></h3>
<pre><code class="language-python">def select_model(use_case):
    models = {
        "realtime": "tiny",           # Live transcription
        "mobile": "tiny",              # Mobile apps
        "chatbot": "base",             # Voice assistants
        "subtitles": "small",          # Video subtitles
        "meetings": "medium",          # Meeting transcription
        "medical": "large-v3",         # Medical dictation
        "legal": "large-v3",           # Legal transcription
        "research": "large-v3",        # Academic research
        "multilingual": "large-v3",    # Multiple languages
        "low_latency": "tiny",         # &lt; 1s response
        "high_accuracy": "large-v3",   # Best quality
    }
    return models.get(use_case, "small")

# Usage
model_name = select_model("meetings")
model = whisper.load_model(model_name)
</code></pre>
<h2 id="installation--setup"><a class="header" href="#installation--setup">Installation &amp; Setup</a></h2>
<h3 id="method-1-official-openai-whisper"><a class="header" href="#method-1-official-openai-whisper">Method 1: Official OpenAI Whisper</a></h3>
<pre><code class="language-bash">pip install -U openai-whisper

# Install ffmpeg (required for audio processing)
# macOS
brew install ffmpeg

# Ubuntu/Debian
sudo apt update &amp;&amp; sudo apt install ffmpeg

# Windows (use chocolatey)
choco install ffmpeg
</code></pre>
<p>Basic usage:</p>
<pre><code class="language-python">import whisper

# Load model
model = whisper.load_model("base")

# Transcribe
result = model.transcribe("audio.mp3")
print(result["text"])
</code></pre>
<h3 id="method-2-hugging-face-transformers"><a class="header" href="#method-2-hugging-face-transformers">Method 2: Hugging Face Transformers</a></h3>
<pre><code class="language-bash">pip install transformers torch accelerate
</code></pre>
<pre><code class="language-python">from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor
import torch

device = "cuda:0" if torch.cuda.is_available() else "cpu"
torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32

model_id = "openai/whisper-large-v3"

# Load model
model = AutoModelForSpeechSeq2Seq.from_pretrained(
    model_id,
    torch_dtype=torch_dtype,
    low_cpu_mem_usage=True,
    use_safetensors=True
)
model.to(device)

processor = AutoProcessor.from_pretrained(model_id)

# Transcribe
import librosa

audio, sr = librosa.load("audio.mp3", sr=16000)
inputs = processor(audio, sampling_rate=16000, return_tensors="pt")
inputs = inputs.to(device)

generated_ids = model.generate(inputs["input_features"])
transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(transcription)
</code></pre>
<h3 id="method-3-faster-whisper-recommended-for-production"><a class="header" href="#method-3-faster-whisper-recommended-for-production">Method 3: faster-whisper (Recommended for Production)</a></h3>
<p><strong>CTranslate2-based implementation, 4x faster with same accuracy</strong></p>
<pre><code class="language-bash">pip install faster-whisper
</code></pre>
<pre><code class="language-python">from faster_whisper import WhisperModel

# Load model (runs on GPU by default)
model = WhisperModel("large-v3", device="cuda", compute_type="float16")

# Or CPU
# model = WhisperModel("large-v3", device="cpu", compute_type="int8")

# Transcribe
segments, info = model.transcribe("audio.mp3", language="en")

print(f"Detected language: {info.language} (probability: {info.language_probability})")

for segment in segments:
    print(f"[{segment.start:.2f}s -&gt; {segment.end:.2f}s] {segment.text}")
</code></pre>
<p><strong>Benefits:</strong></p>
<ul>
<li>4x faster than openai-whisper</li>
<li>Lower memory usage</li>
<li>Same accuracy</li>
<li>Better batching</li>
<li>Production-ready</li>
</ul>
<h3 id="method-4-whisperx-state-of-the-art-alignment"><a class="header" href="#method-4-whisperx-state-of-the-art-alignment">Method 4: whisperX (State-of-the-art alignment)</a></h3>
<p><strong>Adds word-level timestamps and speaker diarization</strong></p>
<pre><code class="language-bash">pip install whisperx
</code></pre>
<pre><code class="language-python">import whisperx

device = "cuda"
batch_size = 16
compute_type = "float16"

# Load model
model = whisperx.load_model("large-v3", device, compute_type=compute_type)

# Transcribe
audio = whisperx.load_audio("audio.mp3")
result = model.transcribe(audio, batch_size=batch_size)

# Align (word-level timestamps)
model_a, metadata = whisperx.load_align_model(
    language_code=result["language"],
    device=device
)
result = whisperx.align(
    result["segments"],
    model_a,
    metadata,
    audio,
    device
)

# Diarization (speaker identification)
diarize_model = whisperx.DiarizationPipeline(
    use_auth_token="YOUR_HF_TOKEN",
    device=device
)
diarize_segments = diarize_model(audio)
result = whisperx.assign_word_speakers(diarize_segments, result)

# Print with speakers
for segment in result["segments"]:
    print(f"[{segment['start']:.2f}s - {segment['end']:.2f}s] Speaker {segment.get('speaker', 'Unknown')}: {segment['text']}")
</code></pre>
<h3 id="method-5-openai-api-cloud"><a class="header" href="#method-5-openai-api-cloud">Method 5: OpenAI API (Cloud)</a></h3>
<pre><code class="language-bash">pip install openai
</code></pre>
<pre><code class="language-python">from openai import OpenAI

client = OpenAI(api_key="your-api-key")

# Transcribe
with open("audio.mp3", "rb") as audio_file:
    transcript = client.audio.transcriptions.create(
        model="whisper-1",
        file=audio_file,
        response_format="text"
    )
    print(transcript)

# With timestamps
transcript = client.audio.transcriptions.create(
    model="whisper-1",
    file=audio_file,
    response_format="verbose_json",
    timestamp_granularities=["word", "segment"]
)

# Translation (to English)
translation = client.audio.translations.create(
    model="whisper-1",
    file=audio_file
)
</code></pre>
<h3 id="method-6-command-line"><a class="header" href="#method-6-command-line">Method 6: Command Line</a></h3>
<pre><code class="language-bash"># Install
pip install openai-whisper

# Transcribe
whisper audio.mp3 --model medium --language en

# With options
whisper audio.mp3 \
  --model large-v3 \
  --language en \
  --task transcribe \
  --output_format srt \
  --output_dir ./transcripts

# Multiple files
whisper *.mp3 --model small --language auto
</code></pre>
<h2 id="basic-usage"><a class="header" href="#basic-usage">Basic Usage</a></h2>
<h3 id="simple-transcription"><a class="header" href="#simple-transcription">Simple Transcription</a></h3>
<pre><code class="language-python">import whisper

# Load model
model = whisper.load_model("base")

# Transcribe
result = model.transcribe("audio.mp3")

# Get text
print(result["text"])

# Get segments with timestamps
for segment in result["segments"]:
    print(f"[{segment['start']:.2f}s - {segment['end']:.2f}s]: {segment['text']}")
</code></pre>
<h3 id="with-language-specification"><a class="header" href="#with-language-specification">With Language Specification</a></h3>
<pre><code class="language-python"># Specify language (faster than auto-detection)
result = model.transcribe("audio.mp3", language="en")

# Spanish
result = model.transcribe("audio.mp3", language="es")

# Japanese
result = model.transcribe("audio.mp3", language="ja")
</code></pre>
<h3 id="translation-to-english"><a class="header" href="#translation-to-english">Translation to English</a></h3>
<pre><code class="language-python"># Translate any language to English
result = model.transcribe("audio_spanish.mp3", task="translate")
print(result["text"])  # Output in English
</code></pre>
<h3 id="from-different-audio-sources"><a class="header" href="#from-different-audio-sources">From Different Audio Sources</a></h3>
<pre><code class="language-python">import whisper

model = whisper.load_model("base")

# From file
result = model.transcribe("audio.mp3")

# From URL
import urllib.request
url = "https://example.com/audio.mp3"
urllib.request.urlretrieve(url, "temp.mp3")
result = model.transcribe("temp.mp3")

# From numpy array
import numpy as np
audio_array = np.load("audio.npy")
result = model.transcribe(audio_array)

# From microphone (real-time)
import sounddevice as sd

duration = 5  # seconds
sample_rate = 16000
audio = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=1)
sd.wait()
result = model.transcribe(audio.flatten())
</code></pre>
<h3 id="detailed-output"><a class="header" href="#detailed-output">Detailed Output</a></h3>
<pre><code class="language-python">result = model.transcribe("audio.mp3", verbose=True)

# Access detailed information
print(f"Language: {result['language']}")
print(f"Text: {result['text']}")

# Segments with more info
for segment in result['segments']:
    print(f"ID: {segment['id']}")
    print(f"Start: {segment['start']:.2f}s")
    print(f"End: {segment['end']:.2f}s")
    print(f"Text: {segment['text']}")
    print(f"Tokens: {segment['tokens']}")
    print(f"Temperature: {segment['temperature']}")
    print(f"Avg Logprob: {segment['avg_logprob']}")
    print(f"Compression Ratio: {segment['compression_ratio']}")
    print(f"No Speech Prob: {segment['no_speech_prob']}")
    print("---")
</code></pre>
<h3 id="output-formats"><a class="header" href="#output-formats">Output Formats</a></h3>
<pre><code class="language-python"># Plain text
result = model.transcribe("audio.mp3")
text = result["text"]

# SRT (SubRip)
def to_srt(result):
    srt = ""
    for i, segment in enumerate(result["segments"], start=1):
        start = format_timestamp(segment["start"])
        end = format_timestamp(segment["end"])
        text = segment["text"].strip()
        srt += f"{i}\n{start} --&gt; {end}\n{text}\n\n"
    return srt

def format_timestamp(seconds):
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    millis = int((seconds % 1) * 1000)
    return f"{hours:02d}:{minutes:02d}:{secs:02d},{millis:03d}"

srt_output = to_srt(result)
with open("subtitles.srt", "w") as f:
    f.write(srt_output)

# VTT (WebVTT)
def to_vtt(result):
    vtt = "WEBVTT\n\n"
    for segment in result["segments"]:
        start = format_vtt_timestamp(segment["start"])
        end = format_vtt_timestamp(segment["end"])
        text = segment["text"].strip()
        vtt += f"{start} --&gt; {end}\n{text}\n\n"
    return vtt

# JSON
import json
with open("transcript.json", "w") as f:
    json.dump(result, f, indent=2)
</code></pre>
<h2 id="fine-tuning"><a class="header" href="#fine-tuning">Fine-tuning</a></h2>
<h3 id="when-to-fine-tune"><a class="header" href="#when-to-fine-tune">When to Fine-tune</a></h3>
<p>✅ <strong>Good use cases:</strong></p>
<ul>
<li>Domain-specific vocabulary (medical, legal, technical)</li>
<li>Specific accents or dialects</li>
<li>Low-resource languages</li>
<li>Custom output format</li>
<li>Improved accuracy for your use case</li>
</ul>
<p>❌ <strong>Not needed for:</strong></p>
<ul>
<li>General transcription</li>
<li>Standard languages/accents</li>
<li>When base model works well</li>
</ul>
<h3 id="prepare-dataset"><a class="header" href="#prepare-dataset">Prepare Dataset</a></h3>
<pre><code class="language-python"># Dataset format: audio files + transcripts
# Directory structure:
# data/
#   train/
#     audio1.mp3
#     audio1.txt
#     audio2.mp3
#     audio2.txt
#   test/
#     audio1.mp3
#     audio1.txt

from datasets import Dataset, Audio
import os

def load_data(data_dir):
    audio_files = []
    transcripts = []

    for filename in os.listdir(data_dir):
        if filename.endswith('.mp3'):
            audio_path = os.path.join(data_dir, filename)
            txt_path = audio_path.replace('.mp3', '.txt')

            if os.path.exists(txt_path):
                audio_files.append(audio_path)
                with open(txt_path, 'r') as f:
                    transcripts.append(f.read().strip())

    return Dataset.from_dict({
        "audio": audio_files,
        "transcription": transcripts
    }).cast_column("audio", Audio(sampling_rate=16000))

train_dataset = load_data("data/train")
test_dataset = load_data("data/test")
</code></pre>
<h3 id="fine-tune-with-hugging-face"><a class="header" href="#fine-tune-with-hugging-face">Fine-tune with Hugging Face</a></h3>
<pre><code class="language-bash">pip install transformers datasets accelerate evaluate jiwer
</code></pre>
<pre><code class="language-python">from transformers import (
    WhisperForConditionalGeneration,
    WhisperProcessor,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer
)
from dataclasses import dataclass
from typing import Any, Dict, List, Union
import torch

# Load model and processor
model_id = "openai/whisper-small"
model = WhisperForConditionalGeneration.from_pretrained(model_id)
processor = WhisperProcessor.from_pretrained(model_id)

# Prepare data
def prepare_dataset(batch):
    audio = batch["audio"]

    # Compute input features
    batch["input_features"] = processor(
        audio["array"],
        sampling_rate=audio["sampling_rate"]
    ).input_features[0]

    # Encode transcription
    batch["labels"] = processor.tokenizer(batch["transcription"]).input_ids

    return batch

train_dataset = train_dataset.map(
    prepare_dataset,
    remove_columns=train_dataset.column_names
)

# Data collator
@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    processor: Any

    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -&gt; Dict[str, torch.Tensor]:
        input_features = [{"input_features": feature["input_features"]} for feature in features]
        batch = self.processor.feature_extractor.pad(input_features, return_tensors="pt")

        label_features = [{"input_ids": feature["labels"]} for feature in features]
        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors="pt")

        labels = labels_batch["input_ids"].masked_fill(
            labels_batch.attention_mask.ne(1), -100
        )

        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():
            labels = labels[:, 1:]

        batch["labels"] = labels
        return batch

data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)

# Training arguments
training_args = Seq2SeqTrainingArguments(
    output_dir="./whisper-finetuned",
    per_device_train_batch_size=8,
    gradient_accumulation_steps=2,
    learning_rate=1e-5,
    warmup_steps=50,
    num_train_epochs=3,
    evaluation_strategy="steps",
    eval_steps=100,
    save_steps=100,
    logging_steps=25,
    load_best_model_at_end=True,
    metric_for_best_model="wer",
    greater_is_better=False,
    push_to_hub=False,
    fp16=True,
    predict_with_generate=True,
    generation_max_length=225,
)

# Metrics
import evaluate
wer_metric = evaluate.load("wer")

def compute_metrics(pred):
    pred_ids = pred.predictions
    label_ids = pred.label_ids

    # Replace -100 with pad_token_id
    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id

    # Decode
    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)

    # Compute WER
    wer = 100 * wer_metric.compute(predictions=pred_str, references=label_str)

    return {"wer": wer}

# Trainer
trainer = Seq2SeqTrainer(
    args=training_args,
    model=model,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    tokenizer=processor.feature_extractor,
)

# Train
trainer.train()

# Save
model.save_pretrained("./whisper-finetuned-final")
processor.save_pretrained("./whisper-finetuned-final")
</code></pre>
<h3 id="lora-fine-tuning-memory-efficient"><a class="header" href="#lora-fine-tuning-memory-efficient">LoRA Fine-tuning (Memory Efficient)</a></h3>
<pre><code class="language-bash">pip install peft
</code></pre>
<pre><code class="language-python">from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

# LoRA configuration
lora_config = LoraConfig(
    r=32,
    lora_alpha=64,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
)

# Prepare model
model = prepare_model_for_kbit_training(model)
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
# trainable params: 2.6M / total: 244M (~1%)

# Train as before
trainer = Seq2SeqTrainer(...)
trainer.train()

# Merge and save
model = model.merge_and_unload()
model.save_pretrained("./whisper-lora-merged")
</code></pre>
<h3 id="use-fine-tuned-model"><a class="header" href="#use-fine-tuned-model">Use Fine-tuned Model</a></h3>
<pre><code class="language-python">from transformers import pipeline

# Load fine-tuned model
pipe = pipeline(
    "automatic-speech-recognition",
    model="./whisper-finetuned-final",
    device="cuda:0"
)

# Transcribe
result = pipe("audio.mp3")
print(result["text"])
</code></pre>
<h2 id="common-patterns"><a class="header" href="#common-patterns">Common Patterns</a></h2>
<h3 id="pattern-1-language-detection"><a class="header" href="#pattern-1-language-detection">Pattern 1: Language Detection</a></h3>
<pre><code class="language-python">import whisper

model = whisper.load_model("base")

# Detect language
audio = whisper.load_audio("audio.mp3")
audio = whisper.pad_or_trim(audio)

mel = whisper.log_mel_spectrogram(audio).to(model.device)
_, probs = model.detect_language(mel)

detected_language = max(probs, key=probs.get)
print(f"Detected language: {detected_language} (confidence: {probs[detected_language]:.2%})")

# All probabilities
for lang, prob in sorted(probs.items(), key=lambda x: x[1], reverse=True)[:5]:
    print(f"{lang}: {prob:.2%}")
</code></pre>
<h3 id="pattern-2-batch-processing"><a class="header" href="#pattern-2-batch-processing">Pattern 2: Batch Processing</a></h3>
<pre><code class="language-python">import whisper
import os
from pathlib import Path

model = whisper.load_model("small")

def transcribe_directory(input_dir, output_dir, language="en"):
    Path(output_dir).mkdir(parents=True, exist_ok=True)

    audio_files = list(Path(input_dir).glob("*.mp3")) + \
                  list(Path(input_dir).glob("*.wav"))

    for audio_file in audio_files:
        print(f"Transcribing: {audio_file.name}")

        result = model.transcribe(
            str(audio_file),
            language=language,
            verbose=False
        )

        # Save transcript
        output_file = Path(output_dir) / f"{audio_file.stem}.txt"
        with open(output_file, "w") as f:
            f.write(result["text"])

        # Save SRT
        srt_file = Path(output_dir) / f"{audio_file.stem}.srt"
        with open(srt_file, "w") as f:
            f.write(to_srt(result))

        print(f"✓ Saved: {output_file.name}")

# Usage
transcribe_directory("./audio_files", "./transcripts", language="en")
</code></pre>
<h3 id="pattern-3-real-time-streaming"><a class="header" href="#pattern-3-real-time-streaming">Pattern 3: Real-time Streaming</a></h3>
<pre><code class="language-python">import whisper
import pyaudio
import numpy as np
import queue
import threading

model = whisper.load_model("base")

# Audio settings
CHUNK = 1024
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 16000
RECORD_SECONDS = 5  # Process every 5 seconds

audio_queue = queue.Queue()

def audio_callback(in_data, frame_count, time_info, status):
    audio_queue.put(in_data)
    return (in_data, pyaudio.paContinue)

def transcribe_stream():
    p = pyaudio.PyAudio()

    stream = p.open(
        format=FORMAT,
        channels=CHANNELS,
        rate=RATE,
        input=True,
        frames_per_buffer=CHUNK,
        stream_callback=audio_callback
    )

    stream.start_stream()

    audio_buffer = []

    try:
        while stream.is_active():
            if not audio_queue.empty():
                data = audio_queue.get()
                audio_buffer.append(np.frombuffer(data, dtype=np.int16))

                # Process when buffer reaches target length
                if len(audio_buffer) &gt;= (RATE * RECORD_SECONDS) // CHUNK:
                    audio_data = np.concatenate(audio_buffer).astype(np.float32) / 32768.0

                    result = model.transcribe(audio_data)
                    print(f"Transcription: {result['text']}")

                    audio_buffer = []

    except KeyboardInterrupt:
        print("Stopping...")

    finally:
        stream.stop_stream()
        stream.close()
        p.terminate()

# Run
transcribe_stream()
</code></pre>
<h3 id="pattern-4-video-subtitles"><a class="header" href="#pattern-4-video-subtitles">Pattern 4: Video Subtitles</a></h3>
<pre><code class="language-python">import whisper
import subprocess
import os

def generate_subtitles(video_file, output_srt, model_size="small", language="en"):
    # Extract audio from video
    audio_file = "temp_audio.mp3"
    subprocess.run([
        "ffmpeg", "-i", video_file,
        "-vn", "-acodec", "mp3",
        "-y", audio_file
    ], check=True)

    # Transcribe
    model = whisper.load_model(model_size)
    result = model.transcribe(audio_file, language=language)

    # Generate SRT
    with open(output_srt, "w") as f:
        f.write(to_srt(result))

    # Clean up
    os.remove(audio_file)

    print(f"✓ Subtitles saved to: {output_srt}")

# Burn subtitles into video
def burn_subtitles(video_file, srt_file, output_file):
    subprocess.run([
        "ffmpeg", "-i", video_file,
        "-vf", f"subtitles={srt_file}",
        "-c:a", "copy",
        "-y", output_file
    ], check=True)

    print(f"✓ Video with subtitles: {output_file}")

# Usage
generate_subtitles("video.mp4", "subtitles.srt", model_size="medium", language="en")
burn_subtitles("video.mp4", "subtitles.srt", "video_with_subs.mp4")
</code></pre>
<h3 id="pattern-5-timestamp-based-search"><a class="header" href="#pattern-5-timestamp-based-search">Pattern 5: Timestamp-based Search</a></h3>
<pre><code class="language-python">def search_in_transcript(audio_file, search_terms, model_size="base"):
    model = whisper.load_model(model_size)
    result = model.transcribe(audio_file)

    matches = []

    for segment in result["segments"]:
        text = segment["text"].lower()

        for term in search_terms:
            if term.lower() in text:
                matches.append({
                    "term": term,
                    "timestamp": segment["start"],
                    "end": segment["end"],
                    "text": segment["text"]
                })

    return matches

# Usage
results = search_in_transcript(
    "meeting.mp3",
    ["budget", "deadline", "milestone"]
)

for match in results:
    print(f"[{match['timestamp']:.1f}s] Found '{match['term']}': {match['text']}")
</code></pre>
<h3 id="pattern-6-meeting-transcription"><a class="header" href="#pattern-6-meeting-transcription">Pattern 6: Meeting Transcription</a></h3>
<pre><code class="language-python">import whisper
from datetime import datetime, timedelta

def transcribe_meeting(audio_file, meeting_name=None):
    model = whisper.load_model("medium")

    print("Transcribing meeting...")
    result = model.transcribe(audio_file, language="en")

    # Generate formatted transcript
    meeting_name = meeting_name or "Meeting"
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    transcript = f"# {meeting_name}\n"
    transcript += f"Date: {timestamp}\n"
    transcript += f"Duration: {result['segments'][-1]['end']:.0f} seconds\n\n"
    transcript += "## Transcript\n\n"

    for segment in result["segments"]:
        time_str = str(timedelta(seconds=int(segment["start"])))
        transcript += f"**[{time_str}]** {segment['text']}\n\n"

    # Save
    output_file = f"{meeting_name}_{datetime.now().strftime('%Y%m%d')}.md"
    with open(output_file, "w") as f:
        f.write(transcript)

    print(f"✓ Meeting transcript saved: {output_file}")
    return result

# Usage
transcribe_meeting("team_meeting.mp3", "Weekly Team Sync")
</code></pre>
<h2 id="advanced-operations"><a class="header" href="#advanced-operations">Advanced Operations</a></h2>
<h3 id="voice-activity-detection-vad"><a class="header" href="#voice-activity-detection-vad">Voice Activity Detection (VAD)</a></h3>
<pre><code class="language-python">from faster_whisper import WhisperModel
import torch

model = WhisperModel("base", device="cuda")

# Transcribe with VAD
segments, info = model.transcribe(
    "audio.mp3",
    vad_filter=True,
    vad_parameters=dict(
        threshold=0.5,
        min_speech_duration_ms=250,
        max_speech_duration_s=float('inf'),
        min_silence_duration_ms=2000,
        window_size_samples=1024,
        speech_pad_ms=400
    )
)

for segment in segments:
    print(f"[{segment.start:.2f}s - {segment.end:.2f}s] {segment.text}")
</code></pre>
<h3 id="initial-prompt-engineering"><a class="header" href="#initial-prompt-engineering">Initial Prompt Engineering</a></h3>
<pre><code class="language-python"># Use initial_prompt to guide transcription style
result = model.transcribe(
    "audio.mp3",
    initial_prompt="This is a technical discussion about machine learning, neural networks, and artificial intelligence."
)

# For proper nouns and terminology
result = model.transcribe(
    "medical.mp3",
    initial_prompt="Medical terminology: MRI, CT scan, diagnosis, prognosis, pharmacology"
)

# For formatting
result = model.transcribe(
    "interview.mp3",
    initial_prompt="Q: Question text\nA: Answer text"
)

# For specific style
result = model.transcribe(
    "presentation.mp3",
    initial_prompt="Professional presentation with proper punctuation and capitalization."
)
</code></pre>
<h3 id="conditioning-on-previous-text"><a class="header" href="#conditioning-on-previous-text">Conditioning on Previous Text</a></h3>
<pre><code class="language-python"># Process long audio in chunks with context
def transcribe_with_context(audio_file, chunk_duration=30, overlap=5):
    model = whisper.load_model("medium")
    audio = whisper.load_audio(audio_file)
    sample_rate = 16000

    chunk_samples = chunk_duration * sample_rate
    overlap_samples = overlap * sample_rate

    transcripts = []
    previous_text = ""

    for i in range(0, len(audio), chunk_samples - overlap_samples):
        chunk = audio[i:i + chunk_samples]

        # Use previous text as context
        result = model.transcribe(
            chunk,
            initial_prompt=previous_text[-200:] if previous_text else None
        )

        transcripts.append(result["text"])
        previous_text = result["text"]

    return " ".join(transcripts)

# Usage
full_transcript = transcribe_with_context("long_audio.mp3")
</code></pre>
<h3 id="temperature-fallback"><a class="header" href="#temperature-fallback">Temperature Fallback</a></h3>
<pre><code class="language-python"># Use multiple temperatures for difficult audio
result = model.transcribe(
    "difficult_audio.mp3",
    temperature=(0.0, 0.2, 0.4, 0.6, 0.8, 1.0)
)

# Check which temperature was used
for segment in result["segments"]:
    print(f"Temperature used: {segment['temperature']}")
    print(f"Text: {segment['text']}")
</code></pre>
<h3 id="beam-search-tuning"><a class="header" href="#beam-search-tuning">Beam Search Tuning</a></h3>
<pre><code class="language-python"># Adjust beam search for better accuracy
result = model.transcribe(
    "audio.mp3",
    beam_size=10,          # Default: 5
    best_of=5,             # Default: 5
    patience=2.0,          # Default: 1.0
)

# For faster inference with acceptable quality
result = model.transcribe(
    "audio.mp3",
    beam_size=1,  # Greedy decoding
)
</code></pre>
<h3 id="compression-ratio-filtering"><a class="header" href="#compression-ratio-filtering">Compression Ratio Filtering</a></h3>
<pre><code class="language-python"># Filter out hallucinations using compression ratio
def transcribe_with_filtering(audio_file, compression_threshold=2.4):
    model = whisper.load_model("medium")
    result = model.transcribe(audio_file)

    filtered_segments = []

    for segment in result["segments"]:
        if segment["compression_ratio"] &lt; compression_threshold:
            filtered_segments.append(segment)
        else:
            print(f"Filtered segment (compression: {segment['compression_ratio']:.2f}): {segment['text']}")

    return filtered_segments

segments = transcribe_with_filtering("audio.mp3")
</code></pre>
<h3 id="no-speech-probability-filtering"><a class="header" href="#no-speech-probability-filtering">No Speech Probability Filtering</a></h3>
<pre><code class="language-python"># Remove segments without speech
def filter_no_speech(result, threshold=0.6):
    return [
        segment for segment in result["segments"]
        if segment["no_speech_prob"] &lt; threshold
    ]

result = model.transcribe("audio.mp3")
speech_segments = filter_no_speech(result, threshold=0.5)
</code></pre>
<h3 id="word-level-timestamps"><a class="header" href="#word-level-timestamps">Word-level Timestamps</a></h3>
<pre><code class="language-python"># Using faster-whisper for word-level timestamps
from faster_whisper import WhisperModel

model = WhisperModel("medium", device="cuda")

segments, info = model.transcribe(
    "audio.mp3",
    word_timestamps=True
)

for segment in segments:
    print(f"[{segment.start:.2f}s - {segment.end:.2f}s] {segment.text}")

    for word in segment.words:
        print(f"  [{word.start:.2f}s - {word.end:.2f}s] {word.word}")
</code></pre>
<h2 id="optimization"><a class="header" href="#optimization">Optimization</a></h2>
<h3 id="speed-optimization"><a class="header" href="#speed-optimization">Speed Optimization</a></h3>
<pre><code class="language-python"># 1. Use faster-whisper (4x faster)
from faster_whisper import WhisperModel

model = WhisperModel("base", device="cuda", compute_type="float16")
segments, _ = model.transcribe("audio.mp3")

# 2. Use smaller model
model = whisper.load_model("tiny")  # 32x faster than large

# 3. Reduce beam size
result = model.transcribe("audio.mp3", beam_size=1)

# 4. Skip language detection
result = model.transcribe("audio.mp3", language="en")

# 5. Lower temperature
result = model.transcribe("audio.mp3", temperature=0.0)

# 6. Batch processing with faster-whisper
model = WhisperModel("base")
audio_files = ["audio1.mp3", "audio2.mp3", "audio3.mp3"]

for audio_file in audio_files:
    segments, _ = model.transcribe(audio_file)
    for segment in segments:
        print(segment.text)
</code></pre>
<h3 id="memory-optimization"><a class="header" href="#memory-optimization">Memory Optimization</a></h3>
<pre><code class="language-python">import torch
import gc

# 1. Use smaller model
model = whisper.load_model("small")

# 2. Process in chunks
def transcribe_large_file(audio_file, chunk_duration=30):
    model = whisper.load_model("base")
    audio = whisper.load_audio(audio_file)
    sample_rate = 16000

    transcripts = []
    chunk_samples = chunk_duration * sample_rate

    for i in range(0, len(audio), chunk_samples):
        chunk = audio[i:i + chunk_samples]
        result = model.transcribe(chunk)
        transcripts.append(result["text"])

        # Clear cache
        torch.cuda.empty_cache()
        gc.collect()

    return " ".join(transcripts)

# 3. Use int8 quantization (CPU)
from faster_whisper import WhisperModel

model = WhisperModel("medium", device="cpu", compute_type="int8")

# 4. Enable gradient checkpointing (for training)
model.gradient_checkpointing_enable()
</code></pre>
<h3 id="quality-vs-speed-trade-offs"><a class="header" href="#quality-vs-speed-trade-offs">Quality vs Speed Trade-offs</a></h3>
<pre><code class="language-python">import time

def benchmark_models(audio_file):
    models = ["tiny", "base", "small", "medium", "large-v3"]
    results = []

    for model_name in models:
        print(f"Testing {model_name}...")
        model = whisper.load_model(model_name)

        start = time.time()
        result = model.transcribe(audio_file)
        duration = time.time() - start

        results.append({
            "model": model_name,
            "duration": duration,
            "text": result["text"]
        })

        del model
        torch.cuda.empty_cache()

    return results

# Analyze results
results = benchmark_models("test_audio.mp3")
for r in results:
    print(f"{r['model']}: {r['duration']:.2f}s")
</code></pre>
<h3 id="batched-inference"><a class="header" href="#batched-inference">Batched Inference</a></h3>
<pre><code class="language-python">from faster_whisper import WhisperModel
import concurrent.futures

model = WhisperModel("base", device="cuda")

def transcribe_file(audio_file):
    segments, _ = model.transcribe(audio_file)
    return " ".join([segment.text for segment in segments])

# Parallel processing
audio_files = ["audio1.mp3", "audio2.mp3", "audio3.mp3"]

with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
    results = list(executor.map(transcribe_file, audio_files))

for audio_file, result in zip(audio_files, results):
    print(f"{audio_file}: {result[:100]}...")
</code></pre>
<h3 id="gpu-optimization"><a class="header" href="#gpu-optimization">GPU Optimization</a></h3>
<pre><code class="language-python">import torch

# 1. Use float16 (half precision)
model = whisper.load_model("medium").half().cuda()

# 2. Enable TensorFloat32 (Ampere GPUs)
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True

# 3. Use faster-whisper with optimal settings
from faster_whisper import WhisperModel

model = WhisperModel(
    "large-v3",
    device="cuda",
    compute_type="float16",
    num_workers=4
)

# 4. Pin memory for faster data transfer
# (Handled automatically by faster-whisper)
</code></pre>
<h2 id="deployment"><a class="header" href="#deployment">Deployment</a></h2>
<h3 id="rest-api-with-fastapi"><a class="header" href="#rest-api-with-fastapi">REST API with FastAPI</a></h3>
<pre><code class="language-python">from fastapi import FastAPI, File, UploadFile, Form
from fastapi.responses import JSONResponse
import whisper
import tempfile
import os

app = FastAPI()

# Load model at startup
model = whisper.load_model("base")

@app.post("/transcribe")
async def transcribe_audio(
    file: UploadFile = File(...),
    language: str = Form("en"),
    task: str = Form("transcribe")
):
    # Save uploaded file temporarily
    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as temp_file:
        content = await file.read()
        temp_file.write(content)
        temp_path = temp_file.name

    try:
        # Transcribe
        result = model.transcribe(
            temp_path,
            language=language,
            task=task
        )

        return JSONResponse({
            "text": result["text"],
            "language": result["language"],
            "segments": [
                {
                    "start": seg["start"],
                    "end": seg["end"],
                    "text": seg["text"]
                }
                for seg in result["segments"]
            ]
        })

    finally:
        # Clean up
        os.unlink(temp_path)

@app.get("/health")
async def health_check():
    return {"status": "healthy"}

# Run: uvicorn app:app --host 0.0.0.0 --port 8000
</code></pre>
<h3 id="docker-deployment"><a class="header" href="#docker-deployment">Docker Deployment</a></h3>
<pre><code class="language-dockerfile"># Dockerfile
FROM nvidia/cuda:11.8.0-base-ubuntu22.04

# Install Python and ffmpeg
RUN apt-get update &amp;&amp; apt-get install -y \
    python3 python3-pip ffmpeg \
    &amp;&amp; rm -rf /var/lib/apt/lists/*

# Install dependencies
COPY requirements.txt .
RUN pip3 install -r requirements.txt

# Copy application
COPY app.py .

# Download model at build time (optional)
RUN python3 -c "import whisper; whisper.load_model('base')"

# Expose port
EXPOSE 8000

# Run
CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "8000"]
</code></pre>
<pre><code class="language-bash"># Build and run
docker build -t whisper-api .
docker run -p 8000:8000 --gpus all whisper-api
</code></pre>
<h3 id="worker-queue-with-celery"><a class="header" href="#worker-queue-with-celery">Worker Queue with Celery</a></h3>
<pre><code class="language-python"># tasks.py
from celery import Celery
import whisper

app = Celery('tasks', broker='redis://localhost:6379/0')

model = whisper.load_model("base")

@app.task
def transcribe_task(audio_path, language="en"):
    result = model.transcribe(audio_path, language=language)
    return {
        "text": result["text"],
        "segments": result["segments"]
    }

# client.py
from tasks import transcribe_task

# Submit task
task = transcribe_task.delay("audio.mp3", language="en")

# Get result
result = task.get(timeout=300)
print(result["text"])
</code></pre>
<h3 id="serverless-aws-lambda"><a class="header" href="#serverless-aws-lambda">Serverless (AWS Lambda)</a></h3>
<pre><code class="language-python"># lambda_function.py
import json
import boto3
import whisper
import tempfile

s3 = boto3.client('s3')
model = whisper.load_model("tiny")  # Use tiny for lambda

def lambda_handler(event, context):
    # Get audio from S3
    bucket = event['bucket']
    key = event['key']

    with tempfile.NamedTemporaryFile(suffix=".mp3") as temp_file:
        s3.download_fileobj(bucket, key, temp_file)
        temp_file.flush()

        # Transcribe
        result = model.transcribe(temp_file.name)

        return {
            'statusCode': 200,
            'body': json.dumps({
                'text': result['text']
            })
        }
</code></pre>
<h3 id="kubernetes-deployment"><a class="header" href="#kubernetes-deployment">Kubernetes Deployment</a></h3>
<pre><code class="language-yaml"># deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: whisper-api
spec:
  replicas: 3
  selector:
    matchLabels:
      app: whisper-api
  template:
    metadata:
      labels:
        app: whisper-api
    spec:
      containers:
      - name: whisper
        image: whisper-api:latest
        ports:
        - containerPort: 8000
        resources:
          limits:
            nvidia.com/gpu: 1
          requests:
            memory: "8Gi"
            cpu: "2"
---
apiVersion: v1
kind: Service
metadata:
  name: whisper-service
spec:
  selector:
    app: whisper-api
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8000
  type: LoadBalancer
</code></pre>
<h2 id="advanced-techniques"><a class="header" href="#advanced-techniques">Advanced Techniques</a></h2>
<h3 id="speaker-diarization"><a class="header" href="#speaker-diarization">Speaker Diarization</a></h3>
<pre><code class="language-python"># Using WhisperX with pyannote
import whisperx

device = "cuda"
audio_file = "meeting.mp3"

# 1. Transcribe
model = whisperx.load_model("large-v3", device, compute_type="float16")
audio = whisperx.load_audio(audio_file)
result = model.transcribe(audio, batch_size=16)

# 2. Align
model_a, metadata = whisperx.load_align_model(
    language_code=result["language"],
    device=device
)
result = whisperx.align(result["segments"], model_a, metadata, audio, device)

# 3. Diarize
from pyannote.audio import Pipeline

diarize_pipeline = Pipeline.from_pretrained(
    "pyannote/speaker-diarization",
    use_auth_token="YOUR_HF_TOKEN"
)
diarize_segments = diarize_pipeline(audio_file)

# 4. Assign speakers
result = whisperx.assign_word_speakers(diarize_segments, result)

# 5. Format output
for segment in result["segments"]:
    speaker = segment.get("speaker", "UNKNOWN")
    print(f"[{segment['start']:.1f}s - {segment['end']:.1f}s] {speaker}: {segment['text']}")
</code></pre>
<h3 id="multi-language-detection-and-switching"><a class="header" href="#multi-language-detection-and-switching">Multi-language Detection and Switching</a></h3>
<pre><code class="language-python">def transcribe_multilingual(audio_file):
    model = whisper.load_model("large-v3")

    # Initial language detection
    audio = whisper.load_audio(audio_file)
    audio_segment = whisper.pad_or_trim(audio)
    mel = whisper.log_mel_spectrogram(audio_segment).to(model.device)

    _, probs = model.detect_language(mel)
    primary_language = max(probs, key=probs.get)

    print(f"Primary language: {primary_language}")

    # Transcribe with language switching detection
    result = model.transcribe(
        audio_file,
        task="transcribe",
        verbose=True
    )

    # Detect language per segment
    segments_with_language = []

    for segment in result["segments"]:
        segment_audio = audio[int(segment["start"] * 16000):int(segment["end"] * 16000)]
        segment_audio = whisper.pad_or_trim(segment_audio)
        mel = whisper.log_mel_spectrogram(segment_audio).to(model.device)

        _, seg_probs = model.detect_language(mel)
        seg_language = max(seg_probs, key=seg_probs.get)

        segments_with_language.append({
            "start": segment["start"],
            "end": segment["end"],
            "text": segment["text"],
            "language": seg_language,
            "confidence": seg_probs[seg_language]
        })

    return segments_with_language

# Usage
segments = transcribe_multilingual("multilingual_audio.mp3")
for seg in segments:
    print(f"[{seg['language']}] {seg['text']}")
</code></pre>
<h3 id="custom-vocabulary-and-spelling"><a class="header" href="#custom-vocabulary-and-spelling">Custom Vocabulary and Spelling</a></h3>
<pre><code class="language-python">def transcribe_with_vocabulary(audio_file, vocabulary):
    """
    Use initial_prompt to guide recognition of specific terms
    """
    model = whisper.load_model("medium")

    # Create prompt with vocabulary
    vocab_prompt = "Vocabulary: " + ", ".join(vocabulary) + "."

    result = model.transcribe(
        audio_file,
        initial_prompt=vocab_prompt
    )

    return result

# Usage
custom_vocab = [
    "TensorFlow", "PyTorch", "CUDA", "GPU",
    "Kubernetes", "Docker", "CI/CD",
    "API", "REST", "GraphQL"
]

result = transcribe_with_vocabulary("tech_talk.mp3", custom_vocab)
</code></pre>
<h3 id="noise-reduction-preprocessing"><a class="header" href="#noise-reduction-preprocessing">Noise Reduction Preprocessing</a></h3>
<pre><code class="language-python">import noisereduce as nr
import librosa
import soundfile as sf
import whisper

def transcribe_with_noise_reduction(audio_file):
    # Load audio
    audio, sr = librosa.load(audio_file, sr=16000)

    # Reduce noise
    reduced_noise = nr.reduce_noise(y=audio, sr=sr, prop_decrease=0.8)

    # Save temporarily
    temp_file = "temp_cleaned.wav"
    sf.write(temp_file, reduced_noise, sr)

    # Transcribe
    model = whisper.load_model("base")
    result = model.transcribe(temp_file)

    # Clean up
    import os
    os.remove(temp_file)

    return result

# Usage
result = transcribe_with_noise_reduction("noisy_audio.mp3")
</code></pre>
<h3 id="audio-normalization"><a class="header" href="#audio-normalization">Audio Normalization</a></h3>
<pre><code class="language-python">from pydub import AudioSegment
from pydub.effects import normalize
import whisper

def transcribe_with_normalization(audio_file):
    # Load and normalize
    audio = AudioSegment.from_file(audio_file)
    normalized_audio = normalize(audio)

    # Export
    temp_file = "temp_normalized.mp3"
    normalized_audio.export(temp_file, format="mp3")

    # Transcribe
    model = whisper.load_model("base")
    result = model.transcribe(temp_file)

    # Clean up
    import os
    os.remove(temp_file)

    return result
</code></pre>
<h3 id="transcript-post-processing"><a class="header" href="#transcript-post-processing">Transcript Post-processing</a></h3>
<pre><code class="language-python">import re

def post_process_transcript(text):
    """
    Clean up and format transcript
    """
    # Remove multiple spaces
    text = re.sub(r'\s+', ' ', text)

    # Capitalize sentences
    text = '. '.join(sentence.capitalize() for sentence in text.split('. '))

    # Fix common errors
    replacements = {
        ' i ': ' I ',
        "im ": "I'm ",
        "ive ": "I've ",
        "youre ": "you're ",
    }

    for old, new in replacements.items():
        text = text.replace(old, new)

    # Remove filler words (optional)
    fillers = ['um', 'uh', 'er', 'ah']
    for filler in fillers:
        text = re.sub(rf'\b{filler}\b', '', text, flags=re.IGNORECASE)

    # Clean up spacing
    text = re.sub(r'\s+', ' ', text).strip()

    return text

# Usage
result = model.transcribe("audio.mp3")
clean_text = post_process_transcript(result["text"])
</code></pre>
<h3 id="confidence-scoring"><a class="header" href="#confidence-scoring">Confidence Scoring</a></h3>
<pre><code class="language-python">def transcribe_with_confidence(audio_file):
    model = whisper.load_model("medium")
    result = model.transcribe(audio_file, verbose=False)

    segments_with_confidence = []

    for segment in result["segments"]:
        # Average log probability as confidence
        avg_logprob = segment["avg_logprob"]
        confidence = np.exp(avg_logprob)  # Convert to probability

        segments_with_confidence.append({
            "text": segment["text"],
            "start": segment["start"],
            "end": segment["end"],
            "confidence": confidence,
            "no_speech_prob": segment["no_speech_prob"]
        })

    return segments_with_confidence

# Usage
segments = transcribe_with_confidence("audio.mp3")
for seg in segments:
    if seg["confidence"] &gt; 0.8:
        print(f"HIGH CONF: {seg['text']}")
    else:
        print(f"LOW CONF: {seg['text']} (review needed)")
</code></pre>
<h2 id="integration"><a class="header" href="#integration">Integration</a></h2>
<h3 id="with-langchain"><a class="header" href="#with-langchain">With LangChain</a></h3>
<pre><code class="language-python">from langchain.document_loaders import WhisperAudioLoader
from langchain.chains.summarize import load_summarize_chain
from langchain.llms import OpenAI

# Transcribe audio
loader = WhisperAudioLoader("meeting.mp3")
documents = loader.load()

# Summarize transcript
llm = OpenAI(temperature=0)
chain = load_summarize_chain(llm, chain_type="map_reduce")
summary = chain.run(documents)

print(summary)
</code></pre>
<h3 id="with-streamlit"><a class="header" href="#with-streamlit">With Streamlit</a></h3>
<pre><code class="language-python">import streamlit as st
import whisper
import tempfile

st.title("Whisper Transcription App")

# Upload audio
uploaded_file = st.file_uploader("Choose an audio file", type=["mp3", "wav", "m4a"])

if uploaded_file is not None:
    # Model selection
    model_size = st.selectbox("Model size", ["tiny", "base", "small", "medium", "large-v3"])

    # Language selection
    language = st.selectbox("Language", ["auto", "en", "es", "fr", "de", "ja", "zh"])

    if st.button("Transcribe"):
        with st.spinner("Loading model..."):
            model = whisper.load_model(model_size)

        # Save uploaded file
        with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as temp_file:
            temp_file.write(uploaded_file.read())
            temp_path = temp_file.name

        with st.spinner("Transcribing..."):
            result = model.transcribe(
                temp_path,
                language=None if language == "auto" else language
            )

        # Display results
        st.subheader("Transcript")
        st.write(result["text"])

        st.subheader("Segments")
        for segment in result["segments"]:
            st.write(f"**[{segment['start']:.1f}s - {segment['end']:.1f}s]** {segment['text']}")

        # Download button
        st.download_button(
            "Download Transcript",
            result["text"],
            file_name="transcript.txt"
        )
</code></pre>
<h3 id="with-flask"><a class="header" href="#with-flask">With Flask</a></h3>
<pre><code class="language-python">from flask import Flask, request, jsonify, render_template
import whisper
import os

app = Flask(__name__)
model = whisper.load_model("base")

UPLOAD_FOLDER = "uploads"
os.makedirs(UPLOAD_FOLDER, exist_ok=True)

@app.route("/")
def index():
    return render_template("index.html")

@app.route("/transcribe", methods=["POST"])
def transcribe():
    if "file" not in request.files:
        return jsonify({"error": "No file provided"}), 400

    file = request.files["file"]
    language = request.form.get("language", "en")

    # Save file
    filepath = os.path.join(UPLOAD_FOLDER, file.filename)
    file.save(filepath)

    try:
        # Transcribe
        result = model.transcribe(filepath, language=language)

        return jsonify({
            "text": result["text"],
            "segments": result["segments"]
        })

    finally:
        # Clean up
        os.remove(filepath)

if __name__ == "__main__":
    app.run(debug=True)
</code></pre>
<h3 id="with-discord-bot"><a class="header" href="#with-discord-bot">With Discord Bot</a></h3>
<pre><code class="language-python">import discord
from discord.ext import commands
import whisper
import os

bot = commands.Bot(command_prefix="!")
model = whisper.load_model("base")

@bot.command()
async def transcribe(ctx):
    """Transcribe an attached audio file"""
    if not ctx.message.attachments:
        await ctx.send("Please attach an audio file!")
        return

    attachment = ctx.message.attachments[0]

    # Download file
    filepath = f"temp_{attachment.filename}"
    await attachment.save(filepath)

    await ctx.send("Transcribing...")

    try:
        # Transcribe
        result = model.transcribe(filepath)

        # Send result (split if too long)
        text = result["text"]
        if len(text) &gt; 2000:
            for i in range(0, len(text), 2000):
                await ctx.send(text[i:i+2000])
        else:
            await ctx.send(text)

    finally:
        os.remove(filepath)

bot.run("YOUR_BOT_TOKEN")
</code></pre>
<h3 id="with-telegram-bot"><a class="header" href="#with-telegram-bot">With Telegram Bot</a></h3>
<pre><code class="language-python">from telegram import Update
from telegram.ext import Application, CommandHandler, MessageHandler, filters
import whisper
import os

model = whisper.load_model("base")

async def transcribe_audio(update: Update, context):
    """Handle voice messages"""
    if update.message.voice:
        file = await update.message.voice.get_file()
    elif update.message.audio:
        file = await update.message.audio.get_file()
    else:
        await update.message.reply_text("Please send an audio file or voice message!")
        return

    # Download
    filepath = "temp_audio.ogg"
    await file.download_to_drive(filepath)

    await update.message.reply_text("Transcribing...")

    try:
        # Transcribe
        result = model.transcribe(filepath)
        await update.message.reply_text(result["text"])

    finally:
        os.remove(filepath)

# Create application
app = Application.builder().token("YOUR_BOT_TOKEN").build()

# Add handlers
app.add_handler(MessageHandler(filters.VOICE | filters.AUDIO, transcribe_audio))

# Run
app.run_polling()
</code></pre>
<h2 id="best-practices"><a class="header" href="#best-practices">Best Practices</a></h2>
<h3 id="1-model-selection"><a class="header" href="#1-model-selection">1. Model Selection</a></h3>
<pre><code class="language-python"># Production decision tree
def choose_model(requirements):
    if requirements["latency"] == "realtime":
        return "tiny"
    elif requirements["accuracy"] == "high" and requirements["resources"] == "available":
        return "large-v3"
    elif requirements["accuracy"] == "medium":
        return "small" if requirements["latency"] == "fast" else "medium"
    else:
        return "base"

# Example
model_name = choose_model({
    "latency": "moderate",
    "accuracy": "high",
    "resources": "available"
})
</code></pre>
<h3 id="2-error-handling"><a class="header" href="#2-error-handling">2. Error Handling</a></h3>
<pre><code class="language-python">def safe_transcribe(audio_file, model_size="base", max_retries=3):
    """Robust transcription with error handling"""
    import logging

    for attempt in range(max_retries):
        try:
            model = whisper.load_model(model_size)
            result = model.transcribe(audio_file)
            return result

        except FileNotFoundError:
            logging.error(f"Audio file not found: {audio_file}")
            raise

        except RuntimeError as e:
            if "out of memory" in str(e):
                logging.warning(f"OOM on attempt {attempt + 1}, trying smaller model")
                model_size = {
                    "large-v3": "medium",
                    "medium": "small",
                    "small": "base",
                    "base": "tiny"
                }.get(model_size, "tiny")
                continue
            raise

        except Exception as e:
            logging.error(f"Transcription failed on attempt {attempt + 1}: {e}")
            if attempt == max_retries - 1:
                raise

    return None
</code></pre>
<h3 id="3-audio-preprocessing"><a class="header" href="#3-audio-preprocessing">3. Audio Preprocessing</a></h3>
<pre><code class="language-python">def preprocess_audio(audio_file, output_file="processed.wav"):
    """Prepare audio for optimal transcription"""
    from pydub import AudioSegment

    # Load audio
    audio = AudioSegment.from_file(audio_file)

    # Convert to mono
    audio = audio.set_channels(1)

    # Set sample rate to 16kHz
    audio = audio.set_frame_rate(16000)

    # Normalize volume
    from pydub.effects import normalize
    audio = normalize(audio)

    # Remove silence from start/end
    from pydub.silence import detect_leading_silence

    start_trim = detect_leading_silence(audio)
    end_trim = detect_leading_silence(audio.reverse())

    duration = len(audio)
    audio = audio[start_trim:duration-end_trim]

    # Export
    audio.export(output_file, format="wav")

    return output_file
</code></pre>
<h3 id="4-monitoring-and-logging"><a class="header" href="#4-monitoring-and-logging">4. Monitoring and Logging</a></h3>
<pre><code class="language-python">import logging
import time
from functools import wraps

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def monitor_transcription(func):
    """Decorator to monitor transcription performance"""
    @wraps(func)
    def wrapper(audio_file, *args, **kwargs):
        logger.info(f"Starting transcription: {audio_file}")
        start_time = time.time()

        try:
            result = func(audio_file, *args, **kwargs)

            duration = time.time() - start_time
            text_length = len(result.get("text", ""))

            logger.info(f"Transcription completed in {duration:.2f}s")
            logger.info(f"Generated {text_length} characters")

            return result

        except Exception as e:
            logger.error(f"Transcription failed: {e}")
            raise

    return wrapper

@monitor_transcription
def transcribe(audio_file, model_size="base"):
    model = whisper.load_model(model_size)
    return model.transcribe(audio_file)
</code></pre>
<h3 id="5-caching"><a class="header" href="#5-caching">5. Caching</a></h3>
<pre><code class="language-python">import hashlib
import json
import os

CACHE_DIR = ".whisper_cache"
os.makedirs(CACHE_DIR, exist_ok=True)

def get_file_hash(filepath):
    """Get MD5 hash of file"""
    hasher = hashlib.md5()
    with open(filepath, 'rb') as f:
        hasher.update(f.read())
    return hasher.hexdigest()

def transcribe_with_cache(audio_file, model_size="base"):
    """Transcribe with result caching"""
    # Generate cache key
    file_hash = get_file_hash(audio_file)
    cache_key = f"{file_hash}_{model_size}"
    cache_file = os.path.join(CACHE_DIR, f"{cache_key}.json")

    # Check cache
    if os.path.exists(cache_file):
        print("Loading from cache...")
        with open(cache_file, 'r') as f:
            return json.load(f)

    # Transcribe
    print("Transcribing...")
    model = whisper.load_model(model_size)
    result = model.transcribe(audio_file)

    # Save to cache
    with open(cache_file, 'w') as f:
        json.dump(result, f)

    return result
</code></pre>
<h3 id="6-parallel-processing"><a class="header" href="#6-parallel-processing">6. Parallel Processing</a></h3>
<pre><code class="language-python">from concurrent.futures import ProcessPoolExecutor
import multiprocessing

def transcribe_single(args):
    """Transcribe single file (for parallel processing)"""
    audio_file, model_size = args
    import whisper

    model = whisper.load_model(model_size)
    result = model.transcribe(audio_file)

    return {
        "file": audio_file,
        "text": result["text"]
    }

def transcribe_parallel(audio_files, model_size="base", max_workers=None):
    """Transcribe multiple files in parallel"""
    if max_workers is None:
        max_workers = multiprocessing.cpu_count()

    args = [(f, model_size) for f in audio_files]

    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        results = list(executor.map(transcribe_single, args))

    return results

# Usage
audio_files = ["audio1.mp3", "audio2.mp3", "audio3.mp3"]
results = transcribe_parallel(audio_files, model_size="base")
</code></pre>
<h3 id="7-quality-assurance"><a class="header" href="#7-quality-assurance">7. Quality Assurance</a></h3>
<pre><code class="language-python">def validate_transcription(result, min_confidence=0.7):
    """Validate transcription quality"""
    issues = []

    # Check for hallucination indicators
    for segment in result["segments"]:
        # High compression ratio = possible hallucination
        if segment["compression_ratio"] &gt; 2.4:
            issues.append(f"High compression at {segment['start']:.1f}s")

        # High no_speech_prob = not actually speech
        if segment["no_speech_prob"] &gt; 0.6:
            issues.append(f"Low speech probability at {segment['start']:.1f}s")

        # Low confidence
        confidence = np.exp(segment["avg_logprob"])
        if confidence &lt; min_confidence:
            issues.append(f"Low confidence at {segment['start']:.1f}s: {confidence:.2%}")

    if issues:
        print("⚠️  Quality issues detected:")
        for issue in issues:
            print(f"  - {issue}")
    else:
        print("✓ Quality check passed")

    return len(issues) == 0
</code></pre>
<h2 id="resources"><a class="header" href="#resources">Resources</a></h2>
<h3 id="official"><a class="header" href="#official">Official</a></h3>
<ul>
<li><a href="https://github.com/openai/whisper">Whisper GitHub</a></li>
<li><a href="https://openai.com/blog/whisper/">OpenAI Blog Post</a></li>
<li><a href="https://arxiv.org/abs/2212.04356">Whisper Paper</a></li>
<li><a href="https://github.com/openai/whisper/blob/main/model-card.md">Model Card</a></li>
</ul>
<h3 id="alternative-implementations"><a class="header" href="#alternative-implementations">Alternative Implementations</a></h3>
<ul>
<li><a href="https://github.com/guillaumekln/faster-whisper">faster-whisper</a> - CTranslate2 implementation (4x faster)</li>
<li><a href="https://github.com/ggerganov/whisper.cpp">whisper.cpp</a> - C++ implementation</li>
<li><a href="https://github.com/m-bain/whisperX">whisperX</a> - Word-level timestamps and diarization</li>
<li><a href="https://github.com/Vaibhavs10/insanely-fast-whisper">insanely-fast-whisper</a> - Optimized with Flash Attention</li>
</ul>
<h3 id="tools"><a class="header" href="#tools">Tools</a></h3>
<ul>
<li><a href="https://huggingface.co/spaces/openai/whisper">Hugging Face Space</a></li>
<li><a href="https://replicate.com/openai/whisper">Replicate API</a></li>
<li><a href="https://whisper.ggerganov.com/">Whisper Web</a> - Browser-based</li>
</ul>
<h3 id="fine-tuning-1"><a class="header" href="#fine-tuning-1">Fine-tuning</a></h3>
<ul>
<li><a href="https://huggingface.co/blog/fine-tune-whisper">Hugging Face Tutorial</a></li>
<li><a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/speech_recognition.ipynb">Fine-tuning notebook</a></li>
</ul>
<h3 id="community"><a class="header" href="#community">Community</a></h3>
<ul>
<li><a href="https://github.com/openai/whisper/discussions">Whisper Discussions</a></li>
<li>r/OpenAI</li>
<li>Hugging Face Forums</li>
</ul>
<h3 id="benchmarks"><a class="header" href="#benchmarks">Benchmarks</a></h3>
<ul>
<li><a href="https://github.com/openai/whisper#available-models-and-languages">WER Benchmarks</a></li>
<li><a href="https://github.com/openai/whisper/blob/main/language-breakdown.svg">Language Performance</a></li>
</ul>
<h2 id="conclusion"><a class="header" href="#conclusion">Conclusion</a></h2>
<p>Whisper represents a breakthrough in automatic speech recognition, offering:</p>
<ul>
<li><strong>Robustness</strong>: Works across accents, noise, and technical language</li>
<li><strong>Multilingual</strong>: 99 languages with strong performance</li>
<li><strong>Flexibility</strong>: Multiple model sizes for different requirements</li>
<li><strong>Open Source</strong>: MIT license enables wide adoption</li>
</ul>
<h3 id="key-takeaways"><a class="header" href="#key-takeaways">Key Takeaways</a></h3>
<ol>
<li>
<p><strong>Start with the right model</strong>:</p>
<ul>
<li>Tiny/Base: Prototyping, real-time</li>
<li>Small/Medium: Production balance</li>
<li>Large-v3: Maximum accuracy</li>
</ul>
</li>
<li>
<p><strong>Optimize for production</strong>:</p>
<ul>
<li>Use faster-whisper for 4x speedup</li>
<li>Implement caching and batching</li>
<li>Add error handling and monitoring</li>
</ul>
</li>
<li>
<p><strong>Fine-tune when needed</strong>:</p>
<ul>
<li>Domain-specific vocabulary</li>
<li>Specialized accents</li>
<li>Improved accuracy</li>
</ul>
</li>
<li>
<p><strong>Leverage advanced features</strong>:</p>
<ul>
<li>Word-level timestamps</li>
<li>Speaker diarization</li>
<li>Language detection</li>
</ul>
</li>
<li>
<p><strong>Consider trade-offs</strong>:</p>
<ul>
<li>Speed vs accuracy</li>
<li>Memory vs quality</li>
<li>Cost vs performance</li>
</ul>
</li>
</ol>
<p>Whisper has democratized speech recognition, making state-of-the-art ASR accessible to everyone. Whether building a voice assistant, transcription service, or accessibility tool, Whisper provides the foundation for robust speech-to-text applications.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../ai/deepseek_r1.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256 246.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"/></svg></span>
                            </a>

                            <a rel="next prefetch" href="../ai/phi.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M278.6 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L210.7 256 73.4 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"/></svg></span>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../ai/deepseek_r1.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256 246.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"/></svg></span>
                    </a>

                    <a rel="next prefetch" href="../ai/phi.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M278.6 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L210.7 256 73.4 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"/></svg></span>
                    </a>
            </nav>

        </div>

        <template id=fa-eye><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M288 32c-80.8 0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7 0 24.6C17.3 304 48.6 356 95.4 399.4C142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144s64.5-144 144-144s144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64c-11.5 0-22.3-3-31.6-8.4c-.2 2.8-.4 5.5-.4 8.4c0 53 43 96 96 96s96-43 96-96s-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6z"/></svg></span></template>
        <template id=fa-eye-slash><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M38.8 5.1C28.4-3.1 13.3-1.2 5.1 9.2S-1.2 34.7 9.2 42.9l592 464c10.4 8.2 25.5 6.3 33.7-4.1s6.3-25.5-4.1-33.7L525.6 386.7c39.6-40.6 66.4-86.1 79.9-118.4c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C465.5 68.8 400.8 32 320 32c-68.2 0-125 26.3-169.3 60.8L38.8 5.1zM223.1 149.5C248.6 126.2 282.7 112 320 112c79.5 0 144 64.5 144 144c0 24.9-6.3 48.3-17.4 68.7L408 294.5c5.2-11.8 8-24.8 8-38.5c0-53-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6c0 10.2-2.4 19.8-6.6 28.3l-90.3-70.8zm223.1 298L373 389.9c-16.4 6.5-34.3 10.1-53 10.1c-79.5 0-144-64.5-144-144c0-6.9 .5-13.6 1.4-20.2L83.1 161.5C60.3 191.2 44 220.8 34.5 243.7c-3.3 7.9-3.3 16.7 0 24.6c14.9 35.7 46.2 87.7 93 131.1C174.5 443.2 239.2 480 320 480c47.8 0 89.9-12.9 126.2-32.5z"/></svg></span></template>
        <template id=fa-copy><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M502.6 70.63l-61.25-61.25C435.4 3.371 427.2 0 418.7 0H255.1c-35.35 0-64 28.66-64 64l.0195 256C192 355.4 220.7 384 256 384h192c35.2 0 64-28.8 64-64V93.25C512 84.77 508.6 76.63 502.6 70.63zM464 320c0 8.836-7.164 16-16 16H255.1c-8.838 0-16-7.164-16-16L239.1 64.13c0-8.836 7.164-16 16-16h128L384 96c0 17.67 14.33 32 32 32h47.1V320zM272 448c0 8.836-7.164 16-16 16H63.1c-8.838 0-16-7.164-16-16L47.98 192.1c0-8.836 7.164-16 16-16H160V128H63.99c-35.35 0-64 28.65-64 64l.0098 256C.002 483.3 28.66 512 64 512h192c35.2 0 64-28.8 64-64v-32h-47.1L272 448z"/></svg></span></template>
        <template id=fa-play><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M73 39c-14.8-9.1-33.4-9.4-48.5-.9S0 62.6 0 80V432c0 17.4 9.4 33.4 24.5 41.9s33.7 8.1 48.5-.9L361 297c14.3-8.7 23-24.2 23-41s-8.7-32.2-23-41L73 39z"/></svg></span></template>
        <template id=fa-clock-rotate-left><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M75 75L41 41C25.9 25.9 0 36.6 0 57.9V168c0 13.3 10.7 24 24 24H134.1c21.4 0 32.1-25.9 17-41l-30.8-30.8C155 85.5 203 64 256 64c106 0 192 86 192 192s-86 192-192 192c-40.8 0-78.6-12.7-109.7-34.4c-14.5-10.1-34.4-6.6-44.6 7.9s-6.6 34.4 7.9 44.6C151.2 495 201.7 512 256 512c141.4 0 256-114.6 256-256S397.4 0 256 0C185.3 0 121.3 28.7 75 75zm181 53c-13.3 0-24 10.7-24 24V256c0 6.4 2.5 12.5 7 17l72 72c9.4 9.4 24.6 9.4 33.9 0s9.4-24.6 0-33.9l-65-65V152c0-13.3-10.7-24-24-24z"/></svg></span></template>



        <script>
            window.playground_copyable = true;
        </script>


        <script src="../elasticlunr-ef4e11c1.min.js"></script>
        <script src="../mark-09e88c2c.min.js"></script>
        <script src="../searcher-c2a407aa.js"></script>

        <script src="../clipboard-1626706a.min.js"></script>
        <script src="../highlight-abc7f01d.js"></script>
        <script src="../book-a0b12cfe.js"></script>

        <!-- Custom JS scripts -->



    </div>
    </body>
</html>
