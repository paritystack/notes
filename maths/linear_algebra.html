<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Linear Algebra - My Notes</title>


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon-de23e50b.svg">
        <link rel="shortcut icon" href="../favicon-8114d1fc.png">
        <link rel="stylesheet" href="../css/variables-8adf115d.css">
        <link rel="stylesheet" href="../css/general-2459343d.css">
        <link rel="stylesheet" href="../css/chrome-ae938929.css">
        <link rel="stylesheet" href="../css/print-9e4910d8.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../fonts/fonts-9644e21d.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="mdbook-highlight-css" href="../highlight-493f70e1.css">
        <link rel="stylesheet" id="mdbook-tomorrow-night-css" href="../tomorrow-night-4c0ae647.css">
        <link rel="stylesheet" id="mdbook-ayu-highlight-css" href="../ayu-highlight-3fdfc3ac.css">

        <!-- Custom theme stylesheets -->

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "../";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "../searchindex-0298451c.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../toc-3ff3ea79.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="mdbook-body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="mdbook-sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("mdbook-sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="mdbook-sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="../toc.html"></iframe>
            </noscript>
            <div id="mdbook-sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="mdbook-page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="mdbook-menu-bar-hover-placeholder"></div>
                <div id="mdbook-menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="mdbook-sidebar-toggle" class="icon-button" for="mdbook-sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="mdbook-sidebar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"/></svg></span>
                        </label>
                        <button id="mdbook-theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="mdbook-theme-list">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M371.3 367.1c27.3-3.9 51.9-19.4 67.2-42.9L600.2 74.1c12.6-19.5 9.4-45.3-7.6-61.2S549.7-4.4 531.1 9.6L294.4 187.2c-24 18-38.2 46.1-38.4 76.1L371.3 367.1zm-19.6 25.4l-116-104.4C175.9 290.3 128 339.6 128 400c0 3.9 .2 7.8 .6 11.6c1.8 17.5-10.2 36.4-27.8 36.4H96c-17.7 0-32 14.3-32 32s14.3 32 32 32H240c61.9 0 112-50.1 112-112c0-2.5-.1-5-.2-7.5z"/></svg></span>
                        </button>
                        <ul id="mdbook-theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-ayu">Ayu</button></li>
                        </ul>
                        <button id="mdbook-search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="mdbook-searchbar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M416 208c0 45.9-14.9 88.3-40 122.7L502.6 457.4c12.5 12.5 12.5 32.8 0 45.3s-32.8 12.5-45.3 0L330.7 376c-34.4 25.2-76.8 40-122.7 40C93.1 416 0 322.9 0 208S93.1 0 208 0S416 93.1 416 208zM208 352c79.5 0 144-64.5 144-144s-64.5-144-144-144S64 128.5 64 208s64.5 144 144 144z"/></svg></span>
                        </button>
                    </div>

                    <h1 class="menu-title">My Notes</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <span class=fa-svg id="print-button"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M128 0C92.7 0 64 28.7 64 64v96h64V64H354.7L384 93.3V160h64V93.3c0-17-6.7-33.3-18.7-45.3L400 18.7C388 6.7 371.7 0 354.7 0H128zM384 352v32 64H128V384 368 352H384zm64 32h32c17.7 0 32-14.3 32-32V256c0-35.3-28.7-64-64-64H64c-35.3 0-64 28.7-64 64v96c0 17.7 14.3 32 32 32H64v64c0 35.3 28.7 64 64 64H384c35.3 0 64-28.7 64-64V384zm-16-88c-13.3 0-24-10.7-24-24s10.7-24 24-24s24 10.7 24 24s-10.7 24-24 24z"/></svg></span>
                        </a>

                    </div>
                </div>

                <div id="mdbook-search-wrapper" class="hidden">
                    <form id="mdbook-searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="mdbook-searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="mdbook-searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <span class=fa-svg id="fa-spin"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M304 48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zm0 416c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM48 304c26.5 0 48-21.5 48-48s-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48zm464-48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM142.9 437c18.7-18.7 18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zm0-294.2c18.7-18.7 18.7-49.1 0-67.9S93.7 56.2 75 75s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zM369.1 437c18.7 18.7 49.1 18.7 67.9 0s18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9z"/></svg></span>
                            </div>
                        </div>
                    </form>
                    <div id="mdbook-searchresults-outer" class="searchresults-outer hidden">
                        <div id="mdbook-searchresults-header" class="searchresults-header"></div>
                        <ul id="mdbook-searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('mdbook-sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('mdbook-sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#mdbook-sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="mdbook-content" class="content">
                    <main>
                        <h1 id="linear-algebra"><a class="header" href="#linear-algebra">Linear Algebra</a></h1>
<p>A comprehensive guide to linear algebra from fundamentals to advanced applications in machine learning, computer graphics, scientific computing, and algorithmic theory.</p>
<h2 id="table-of-contents"><a class="header" href="#table-of-contents">Table of Contents</a></h2>
<ol>
<li>
<p><a href="#introduction">Introduction</a></p>
<ul>
<li><a href="#what-is-linear-algebra">What is Linear Algebra?</a></li>
<li><a href="#why-linear-algebra-matters">Why Linear Algebra Matters</a></li>
<li><a href="#prerequisites-and-reading-guide">Prerequisites and Reading Guide</a></li>
</ul>
</li>
<li>
<p><a href="#vectors---fundamentals">Vectors - Fundamentals</a></p>
<ul>
<li><a href="#vector-basics">Vector Basics</a></li>
<li><a href="#vector-operations">Vector Operations</a></li>
<li><a href="#dot-product">Dot Product</a></li>
<li><a href="#cross-product">Cross Product</a></li>
<li><a href="#vector-norms">Vector Norms</a></li>
</ul>
</li>
<li>
<p><a href="#matrices---core-concepts">Matrices - Core Concepts</a></p>
<ul>
<li><a href="#matrix-fundamentals">Matrix Fundamentals</a></li>
<li><a href="#matrix-operations">Matrix Operations</a></li>
<li><a href="#matrix-transpose">Matrix Transpose</a></li>
<li><a href="#special-matrix-types">Special Matrix Types</a></li>
</ul>
</li>
<li>
<p><a href="#linear-systems">Linear Systems</a></p>
<ul>
<li><a href="#systems-of-linear-equations">Systems of Linear Equations</a></li>
<li><a href="#gaussian-elimination">Gaussian Elimination</a></li>
<li><a href="#lu-decomposition">LU Decomposition</a></li>
</ul>
</li>
<li>
<p><a href="#vector-spaces">Vector Spaces</a></p>
<ul>
<li><a href="#vector-space-axioms">Vector Space Axioms</a></li>
<li><a href="#subspaces">Subspaces</a></li>
<li><a href="#span-and-linear-independence">Span and Linear Independence</a></li>
<li><a href="#basis-and-dimension">Basis and Dimension</a></li>
</ul>
</li>
<li>
<p><a href="#linear-transformations">Linear Transformations</a></p>
<ul>
<li><a href="#definition-and-properties">Definition and Properties</a></li>
<li><a href="#geometric-transformations">Geometric Transformations</a></li>
<li><a href="#composition-of-transformations">Composition of Transformations</a></li>
<li><a href="#inverse-transformations">Inverse Transformations</a></li>
</ul>
</li>
<li>
<p><a href="#determinants">Determinants</a></p>
<ul>
<li><a href="#geometric-interpretation-determinants">Geometric Interpretation</a></li>
<li><a href="#computing-determinants">Computing Determinants</a></li>
<li><a href="#properties-of-determinants">Properties of Determinants</a></li>
</ul>
</li>
<li>
<p><a href="#eigenvalues-and-eigenvectors">Eigenvalues and Eigenvectors</a></p>
<ul>
<li><a href="#fundamental-concepts-eigenvalues">Fundamental Concepts</a></li>
<li><a href="#computing-eigenvalues">Computing Eigenvalues</a></li>
<li><a href="#diagonalization">Diagonalization</a></li>
<li><a href="#applications-of-eigenvalues">Applications of Eigenvalues</a></li>
</ul>
</li>
<li>
<p><a href="#orthogonality">Orthogonality</a></p>
<ul>
<li><a href="#orthogonal-vectors">Orthogonal Vectors</a></li>
<li><a href="#gram-schmidt-process">Gram-Schmidt Process</a></li>
<li><a href="#orthogonal-projections">Orthogonal Projections</a></li>
</ul>
</li>
<li>
<p><a href="#matrix-decompositions">Matrix Decompositions</a></p>
<ul>
<li><a href="#lu-decomposition-extended">LU Decomposition (Extended)</a></li>
<li><a href="#qr-decomposition">QR Decomposition</a></li>
<li><a href="#cholesky-decomposition">Cholesky Decomposition</a></li>
<li><a href="#singular-value-decomposition-svd">Singular Value Decomposition (SVD)</a></li>
</ul>
</li>
<li>
<p><a href="#least-squares">Least Squares</a></p>
<ul>
<li><a href="#linear-regression-framework">Linear Regression Framework</a></li>
<li><a href="#solution-methods-least-squares">Solution Methods</a></li>
<li><a href="#regularization">Regularization</a></li>
</ul>
</li>
<li>
<p><a href="#advanced-topics">Advanced Topics</a></p>
<ul>
<li><a href="#matrix-calculus">Matrix Calculus</a></li>
<li><a href="#matrix-norms">Matrix Norms</a></li>
<li><a href="#tensor-operations">Tensor Operations</a></li>
<li><a href="#sparse-linear-algebra">Sparse Linear Algebra</a></li>
</ul>
</li>
<li>
<p><a href="#numerical-considerations">Numerical Considerations</a></p>
<ul>
<li><a href="#conditioning-and-stability">Conditioning and Stability</a></li>
<li><a href="#computational-complexity">Computational Complexity</a></li>
</ul>
</li>
<li>
<p><a href="#applications">Applications</a></p>
<ul>
<li><a href="#machine-learning-and-data-science">Machine Learning and Data Science</a></li>
<li><a href="#computer-graphics-and-vision">Computer Graphics and Vision</a></li>
<li><a href="#scientific-computing">Scientific Computing</a></li>
<li><a href="#algorithms-and-theory">Algorithms and Theory</a></li>
</ul>
</li>
<li>
<p><a href="#practical-implementation-guide">Practical Implementation Guide</a></p>
<ul>
<li><a href="#numpy-best-practices">NumPy Best Practices</a></li>
<li><a href="#common-pitfalls">Common Pitfalls</a></li>
<li><a href="#performance-optimization">Performance Optimization</a></li>
<li><a href="#quick-reference">Quick Reference</a></li>
<li><a href="#further-reading">Further Reading</a></li>
</ul>
</li>
</ol>
<hr>
<h2 id="introduction"><a class="header" href="#introduction">Introduction</a></h2>
<h3 id="what-is-linear-algebra"><a class="header" href="#what-is-linear-algebra">What is Linear Algebra?</a></h3>
<p>Linear algebra is the branch of mathematics that studies vectors, vector spaces, linear transformations, and systems of linear equations. At its core, it’s about understanding and manipulating collections of numbers in structured ways.</p>
<p><strong>Intuitive Understanding:</strong>
Think of linear algebra as the mathematics of:</p>
<ul>
<li><strong>Arrows in space</strong> (vectors): representing direction and magnitude</li>
<li><strong>Grids and tables</strong> (matrices): organizing and transforming data</li>
<li><strong>Linear relationships</strong>: where doubling the input doubles the output</li>
</ul>
<h3 id="why-linear-algebra-matters"><a class="header" href="#why-linear-algebra-matters">Why Linear Algebra Matters</a></h3>
<p>Linear algebra is the mathematical foundation of modern technology:</p>
<p><strong>Machine Learning &amp; AI:</strong></p>
<ul>
<li>Neural networks are built from matrix multiplications</li>
<li>PCA reduces data dimensions using eigenvalues</li>
<li>Recommender systems use matrix factorization</li>
<li>Image recognition processes pixels as vectors</li>
</ul>
<p><strong>Computer Graphics:</strong></p>
<ul>
<li>3D transformations (rotation, scaling, translation)</li>
<li>Camera projections for rendering</li>
<li>Animation and skeletal systems</li>
<li>Lighting and shading calculations</li>
</ul>
<p><strong>Data Science:</strong></p>
<ul>
<li>PageRank algorithm powers Google search</li>
<li>Image compression via SVD</li>
<li>Natural language processing with word embeddings</li>
<li>Statistical analysis and regression</li>
</ul>
<p><strong>Scientific Computing:</strong></p>
<ul>
<li>Solving differential equations</li>
<li>Optimization algorithms</li>
<li>Physics simulations</li>
<li>Quantum mechanics calculations</li>
</ul>
<h3 id="prerequisites-and-reading-guide"><a class="header" href="#prerequisites-and-reading-guide">Prerequisites and Reading Guide</a></h3>
<p><strong>Prerequisites:</strong></p>
<ul>
<li>Basic algebra and arithmetic</li>
<li>Understanding of functions and graphs</li>
<li>Familiarity with Python is helpful but not required</li>
</ul>
<p><strong>Reading Guide:</strong>
This guide follows a beginner-friendly approach:</p>
<ol>
<li><strong>Intuition first</strong>: Every concept starts with geometric/visual understanding</li>
<li><strong>Progressive complexity</strong>: Informal → concrete examples → formal definitions</li>
<li><strong>Multiple perspectives</strong>: Algebraic, geometric, and computational views</li>
<li><strong>Practical code</strong>: NumPy implementations you can run and experiment with</li>
</ol>
<hr>
<h2 id="vectors---fundamentals"><a class="header" href="#vectors---fundamentals">Vectors - Fundamentals</a></h2>
<h3 id="vector-basics"><a class="header" href="#vector-basics">Vector Basics</a></h3>
<p><strong>Intuitive Understanding:</strong>
Imagine you’re giving directions: “Walk 3 blocks east and 4 blocks north.” This describes a <strong>vector</strong> - it has both direction (northeast) and magnitude (5 blocks total). Vectors are the fundamental building blocks of linear algebra.</p>
<p><strong>Two Views of Vectors:</strong></p>
<ol>
<li>
<p><strong>Geometric View</strong>: A vector is an arrow in space with:</p>
<ul>
<li>Direction: where it points</li>
<li>Magnitude: how long it is</li>
<li>Starting at the origin (0, 0)</li>
</ul>
</li>
<li>
<p><strong>Algebraic View</strong>: A vector is an ordered list of numbers:</p>
</li>
</ol>
<p>$$\mathbf{v} = \begin{bmatrix} 3 \ 4 \end{bmatrix}$$</p>
<p>where the first number is the x-coordinate (3 blocks east) and the second is the y-coordinate (4 blocks north).</p>
<p><strong>Formal Definition:</strong>
A vector in $\mathbb{R}^n$ is an n-tuple of real numbers, represented as a column:</p>
<p>$$\mathbf{v} = \begin{bmatrix} v_1 \ v_2 \ \vdots \ v_n \end{bmatrix} \in \mathbb{R}^n$$</p>
<p><strong>Python Implementation:</strong></p>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

# Creating vectors
v = np.array([3, 4])  # 2D vector
u = np.array([1, 2])

print(f"Vector v: {v}")
print(f"Vector u: {u}")

# Visualizing vectors
def plot_vectors(vectors, colors, labels):
    """Visualize 2D vectors as arrows from origin"""
    plt.figure(figsize=(8, 8))
    for vec, color, label in zip(vectors, colors, labels):
        plt.quiver(0, 0, vec[0], vec[1],
                   angles='xy', scale_units='xy', scale=1,
                   color=color, width=0.006, label=label)

    plt.xlim(-1, 5)
    plt.ylim(-1, 5)
    plt.axhline(y=0, color='k', linewidth=0.5)
    plt.axvline(x=0, color='k', linewidth=0.5)
    plt.grid(True, alpha=0.3)
    plt.legend()
    plt.axis('equal')
    plt.title('Vector Visualization')
    plt.xlabel('x')
    plt.ylabel('y')
    plt.show()

# Visualize our vectors
plot_vectors([v, u], ['blue', 'red'], ['v = [3, 4]', 'u = [1, 2]'])
</code></pre>
<p><strong>Column vs Row Vectors:</strong></p>
<ul>
<li>Column vector (default): $\mathbf{v} = \begin{bmatrix} 3 \ 4 \end{bmatrix}$</li>
<li>Row vector (transpose): $\mathbf{v}^T = \begin{bmatrix} 3 &amp; 4 \end{bmatrix}$</li>
</ul>
<h3 id="vector-operations"><a class="header" href="#vector-operations">Vector Operations</a></h3>
<h4 id="vector-addition"><a class="header" href="#vector-addition">Vector Addition</a></h4>
<p><strong>Intuition</strong>: Vector addition is like combining two sets of directions. If you walk 3 east and 4 north, then 1 more east and 2 more north, you end up 4 east and 6 north total.</p>
<p><strong>Geometric View</strong>: Place the tail of the second vector at the head of the first. The sum is the vector from the origin to the final point (parallelogram rule).</p>
<p><strong>Algebraic Definition</strong>:</p>
<p>$$\mathbf{u} + \mathbf{v} = \begin{bmatrix} u_1 \ u_2 \ \vdots \ u_n \end{bmatrix} + \begin{bmatrix} v_1 \ v_2 \ \vdots \ v_n \end{bmatrix} = \begin{bmatrix} u_1 + v_1 \ u_2 + v_2 \ \vdots \ u_n + v_n \end{bmatrix}$$</p>
<p><strong>Python Example:</strong></p>
<pre><code class="language-python"># Vector addition
u = np.array([1, 2])
v = np.array([3, 4])
w = u + v

print(f"u + v = {w}")  # [4, 6]

# Visualize addition
plt.figure(figsize=(8, 8))
# Original vectors
plt.quiver(0, 0, u[0], u[1], angles='xy', scale_units='xy', scale=1,
           color='red', width=0.006, label='u')
plt.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1,
           color='blue', width=0.006, label='v')
# Sum vector
plt.quiver(0, 0, w[0], w[1], angles='xy', scale_units='xy', scale=1,
           color='green', width=0.008, label='u + v')
# Parallelogram visualization
plt.quiver(u[0], u[1], v[0], v[1], angles='xy', scale_units='xy', scale=1,
           color='blue', width=0.004, alpha=0.5, linestyle='dashed')
plt.quiver(v[0], v[1], u[0], u[1], angles='xy', scale_units='xy', scale=1,
           color='red', width=0.004, alpha=0.5, linestyle='dashed')

plt.xlim(-1, 6)
plt.ylim(-1, 7)
plt.grid(True, alpha=0.3)
plt.legend()
plt.axis('equal')
plt.title('Vector Addition (Parallelogram Rule)')
plt.show()
</code></pre>
<h4 id="scalar-multiplication"><a class="header" href="#scalar-multiplication">Scalar Multiplication</a></h4>
<p><strong>Intuition</strong>: Multiplying a vector by a scalar (number) stretches or shrinks it. Multiplying by 2 doubles its length; multiplying by -1 reverses its direction.</p>
<p><strong>Algebraic Definition</strong>:</p>
<p>$$c \cdot \mathbf{v} = c \cdot \begin{bmatrix} v_1 \ v_2 \ \vdots \ v_n \end{bmatrix} = \begin{bmatrix} c \cdot v_1 \ c \cdot v_2 \ \vdots \ c \cdot v_n \end{bmatrix}$$</p>
<p><strong>Python Example:</strong></p>
<pre><code class="language-python">v = np.array([2, 1])

# Scalar multiplication
v_scaled = 2 * v      # [4, 2] - doubled
v_half = 0.5 * v      # [1, 0.5] - halved
v_reversed = -1 * v   # [-2, -1] - reversed direction

print(f"Original: {v}")
print(f"Doubled: {v_scaled}")
print(f"Halved: {v_half}")
print(f"Reversed: {v_reversed}")
</code></pre>
<h4 id="linear-combinations"><a class="header" href="#linear-combinations">Linear Combinations</a></h4>
<p><strong>Intuition</strong>: A linear combination is like mixing ingredients with different amounts. You take some amount of one vector plus some amount of another.</p>
<p><strong>Definition</strong>: A linear combination of vectors $\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_k$ is:</p>
<p>$$c_1\mathbf{v}_1 + c_2\mathbf{v}_2 + \cdots + c_k\mathbf{v}_k$$</p>
<p>where $c_1, c_2, \ldots, c_k$ are scalars (called coefficients).</p>
<p><strong>Python Example:</strong></p>
<pre><code class="language-python">v1 = np.array([1, 0])
v2 = np.array([0, 1])

# Linear combination: 3*v1 + 2*v2
result = 3*v1 + 2*v2
print(f"3*v1 + 2*v2 = {result}")  # [3, 2]

# Any 2D vector can be written as a linear combination of v1 and v2!
target = np.array([5, 7])
# target = 5*v1 + 7*v2
reconstructed = 5*v1 + 7*v2
print(f"Target: {target}")
print(f"Reconstructed: {reconstructed}")
print(f"Match: {np.allclose(target, reconstructed)}")
</code></pre>
<h3 id="dot-product"><a class="header" href="#dot-product">Dot Product</a></h3>
<p><strong>Intuition</strong>: The dot product measures how much two vectors point in the same direction. If they’re perfectly aligned, the dot product is large; if they’re perpendicular, it’s zero; if they point opposite directions, it’s negative.</p>
<p><strong>Algebraic Definition</strong>:</p>
<p>$$\mathbf{u} \cdot \mathbf{v} = u_1v_1 + u_2v_2 + \cdots + u_nv_n = \sum_{i=1}^{n} u_iv_i$$</p>
<p><strong>Geometric Interpretation</strong>:</p>
<p>$$\mathbf{u} \cdot \mathbf{v} = ||\mathbf{u}|| \cdot ||\mathbf{v}|| \cdot \cos\theta$$</p>
<p>where $\theta$ is the angle between the vectors.</p>
<p><strong>Key Properties:</strong></p>
<ol>
<li><strong>Commutative</strong>: $\mathbf{u} \cdot \mathbf{v} = \mathbf{v} \cdot \mathbf{u}$</li>
<li><strong>Distributive</strong>: $\mathbf{u} \cdot (\mathbf{v} + \mathbf{w}) = \mathbf{u} \cdot \mathbf{v} + \mathbf{u} \cdot \mathbf{w}$</li>
<li><strong>Orthogonality</strong>: $\mathbf{u} \cdot \mathbf{v} = 0 \iff \mathbf{u} \perp \mathbf{v}$</li>
</ol>
<p><strong>Python Example:</strong></p>
<pre><code class="language-python">u = np.array([3, 4])
v = np.array([1, 2])

# Dot product - multiple ways
dot1 = np.dot(u, v)           # NumPy function
dot2 = u @ v                   # Matrix multiplication operator
dot3 = np.sum(u * v)          # Manual: sum of element-wise products

print(f"Dot product u·v = {dot1}")  # 3*1 + 4*2 = 11
print(f"All methods agree: {dot1 == dot2 == dot3}")

# Computing angle between vectors
def angle_between(u, v):
    """Compute angle in radians between vectors u and v"""
    cos_theta = np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))
    # Clamp to avoid numerical errors
    cos_theta = np.clip(cos_theta, -1.0, 1.0)
    return np.arccos(cos_theta)

theta = angle_between(u, v)
print(f"Angle between u and v: {np.degrees(theta):.2f} degrees")

# Check orthogonality
u_ortho = np.array([1, 0])
v_ortho = np.array([0, 1])
print(f"u_ortho · v_ortho = {np.dot(u_ortho, v_ortho)}")  # 0 - perpendicular!
</code></pre>
<p><strong>Application: Cosine Similarity (Used in ML)</strong></p>
<pre><code class="language-python">def cosine_similarity(u, v):
    """
    Measure similarity between vectors (-1 to 1)
    1 = same direction, 0 = perpendicular, -1 = opposite
    """
    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))

# Example: document similarity
doc1 = np.array([3, 2, 0, 5])  # word frequencies
doc2 = np.array([2, 1, 0, 3])  # similar document
doc3 = np.array([0, 0, 4, 0])  # different topic

print(f"Similarity(doc1, doc2) = {cosine_similarity(doc1, doc2):.3f}")  # High
print(f"Similarity(doc1, doc3) = {cosine_similarity(doc1, doc3):.3f}")  # Low
</code></pre>
<h3 id="cross-product"><a class="header" href="#cross-product">Cross Product</a></h3>
<p><strong>Note</strong>: The cross product is only defined for 3D vectors.</p>
<p><strong>Intuition</strong>: The cross product of two vectors produces a third vector that’s perpendicular to both. Imagine two arrows in space - the cross product points in the direction perpendicular to the plane they form.</p>
<p><strong>Algebraic Definition</strong> (3D only):</p>
<p>$$\mathbf{u} \times \mathbf{v} = \begin{bmatrix} u_2v_3 - u_3v_2 \ u_3v_1 - u_1v_3 \ u_1v_2 - u_2v_1 \end{bmatrix}$$</p>
<p><strong>Geometric Properties:</strong></p>
<ol>
<li>$\mathbf{u} \times \mathbf{v}$ is perpendicular to both $\mathbf{u}$ and $\mathbf{v}$</li>
<li>$||\mathbf{u} \times \mathbf{v}|| = ||\mathbf{u}|| \cdot ||\mathbf{v}|| \cdot \sin\theta$ (area of parallelogram)</li>
<li>Direction given by right-hand rule</li>
<li><strong>Anti-commutative</strong>: $\mathbf{u} \times \mathbf{v} = -(\mathbf{v} \times \mathbf{u})$</li>
</ol>
<p><strong>Python Example:</strong></p>
<pre><code class="language-python"># 3D vectors
u = np.array([1, 0, 0])  # x-axis
v = np.array([0, 1, 0])  # y-axis

# Cross product
w = np.cross(u, v)
print(f"u × v = {w}")  # [0, 0, 1] - z-axis (right-hand rule)

# Verify perpendicularity
print(f"(u × v) · u = {np.dot(w, u)}")  # 0 - perpendicular
print(f"(u × v) · v = {np.dot(w, v)}")  # 0 - perpendicular

# Anti-commutativity
w_reversed = np.cross(v, u)
print(f"v × u = {w_reversed}")  # [0, 0, -1] - opposite direction

# Application: surface normal in graphics
def surface_normal(p1, p2, p3):
    """
    Compute normal vector to a triangle defined by 3 points
    Used in 3D graphics for lighting calculations
    """
    # Two edges of the triangle
    edge1 = p2 - p1
    edge2 = p3 - p1

    # Normal is perpendicular to both edges
    normal = np.cross(edge1, edge2)

    # Normalize to unit length
    return normal / np.linalg.norm(normal)

# Example triangle
p1 = np.array([0, 0, 0])
p2 = np.array([1, 0, 0])
p3 = np.array([0, 1, 0])

normal = surface_normal(p1, p2, p3)
print(f"Triangle normal: {normal}")  # Points in z-direction
</code></pre>
<h3 id="vector-norms"><a class="header" href="#vector-norms">Vector Norms</a></h3>
<p><strong>Intuition</strong>: A norm measures the “size” or “length” of a vector. Think of it as the distance from the origin to the point the vector represents.</p>
<p><strong>L2 Norm (Euclidean Distance)</strong> - Most Common:</p>
<p>$$||\mathbf{v}||<em>2 = \sqrt{v_1^2 + v_2^2 + \cdots + v_n^2} = \sqrt{\sum</em>{i=1}^{n} v_i^2}$$</p>
<p>This is the “straight-line” distance, like measuring with a ruler.</p>
<p><strong>L1 Norm (Manhattan Distance)</strong>:</p>
<p>$$||\mathbf{v}||<em>1 = |v_1| + |v_2| + \cdots + |v_n| = \sum</em>{i=1}^{n} |v_i|$$</p>
<p>This is the “city block” distance, like walking along a grid of streets.</p>
<p><strong>L∞ Norm (Maximum Norm)</strong>:</p>
<p>$$||\mathbf{v}||_\infty = \max(|v_1|, |v_2|, \ldots, |v_n|)$$</p>
<p><strong>General Lp Norm</strong>:</p>
<p>$$||\mathbf{v}||<em>p = \left(\sum</em>{i=1}^{n} |v_i|^p\right)^{1/p}$$</p>
<p><strong>Python Example:</strong></p>
<pre><code class="language-python">v = np.array([3, 4])

# Different norms
l2_norm = np.linalg.norm(v, ord=2)     # Default: Euclidean
l1_norm = np.linalg.norm(v, ord=1)     # Manhattan
linf_norm = np.linalg.norm(v, ord=np.inf)  # Maximum

print(f"Vector: {v}")
print(f"L2 norm (Euclidean): {l2_norm}")     # sqrt(3² + 4²) = 5
print(f"L1 norm (Manhattan): {l1_norm}")     # |3| + |4| = 7
print(f"L∞ norm (Maximum): {linf_norm}")     # max(3, 4) = 4

# Unit vectors (normalized)
def normalize(v, ord=2):
    """Return unit vector in same direction as v"""
    norm = np.linalg.norm(v, ord=ord)
    if norm == 0:
        return v
    return v / norm

v_unit = normalize(v)
print(f"\nOriginal vector: {v}, length: {np.linalg.norm(v)}")
print(f"Unit vector: {v_unit}, length: {np.linalg.norm(v_unit)}")

# Distance between points (vectors)
p1 = np.array([1, 2])
p2 = np.array([4, 6])

euclidean_dist = np.linalg.norm(p2 - p1, ord=2)
manhattan_dist = np.linalg.norm(p2 - p1, ord=1)

print(f"\nDistance from {p1} to {p2}:")
print(f"  Euclidean: {euclidean_dist:.2f}")
print(f"  Manhattan: {manhattan_dist:.2f}")
</code></pre>
<hr>
<h2 id="matrices---core-concepts"><a class="header" href="#matrices---core-concepts">Matrices - Core Concepts</a></h2>
<h3 id="matrix-fundamentals"><a class="header" href="#matrix-fundamentals">Matrix Fundamentals</a></h3>
<p><strong>Intuition</strong>: A matrix is a rectangular grid of numbers. Think of it as:</p>
<ul>
<li>A spreadsheet organizing data in rows and columns</li>
<li>A transformation that changes vectors</li>
<li>A compact way to write multiple equations at once</li>
</ul>
<p><strong>Definition</strong>: An $m \times n$ matrix has $m$ rows and $n$ columns:</p>
<p>$$A = \begin{bmatrix} a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \ a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \ a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn} \end{bmatrix}$$</p>
<p>where $a_{ij}$ is the element in row $i$, column $j$.</p>
<p><strong>Python Example:</strong></p>
<pre><code class="language-python"># Creating matrices
A = np.array([[1, 2, 3],
              [4, 5, 6]])  # 2×3 matrix

print(f"Matrix A:\n{A}")
print(f"Shape: {A.shape}")  # (2, 3) means 2 rows, 3 columns
print(f"Element a_12 (row 0, col 1): {A[0, 1]}")  # 2 (0-indexed)

# Different ways to create matrices
zeros = np.zeros((3, 3))       # 3×3 matrix of zeros
ones = np.ones((2, 4))         # 2×4 matrix of ones
identity = np.eye(4)           # 4×4 identity matrix
diagonal = np.diag([1, 2, 3]) # Diagonal matrix
random = np.random.rand(3, 2)  # 3×2 random matrix

print(f"\nIdentity matrix:\n{identity}")
print(f"\nDiagonal matrix:\n{diagonal}")
</code></pre>
<p><strong>Special Matrices:</strong></p>
<ol>
<li>
<p><strong>Zero Matrix</strong>: All elements are 0</p>
</li>
<li>
<p><strong>Identity Matrix</strong> $I$: 1s on diagonal, 0s elsewhere
$$I_3 = \begin{bmatrix} 1 &amp; 0 &amp; 0 \ 0 &amp; 1 &amp; 0 \ 0 &amp; 0 &amp; 1 \end{bmatrix}$$</p>
</li>
<li>
<p><strong>Diagonal Matrix</strong>: Non-zero elements only on diagonal</p>
</li>
<li>
<p><strong>Triangular Matrix</strong>: Non-zero elements only above (upper) or below (lower) diagonal</p>
</li>
</ol>
<h3 id="matrix-operations"><a class="header" href="#matrix-operations">Matrix Operations</a></h3>
<h4 id="matrix-addition-and-subtraction"><a class="header" href="#matrix-addition-and-subtraction">Matrix Addition and Subtraction</a></h4>
<p><strong>Intuition</strong>: Add corresponding elements, just like vector addition.</p>
<p><strong>Definition</strong> (matrices must have same dimensions):</p>
<p>$$A + B = \begin{bmatrix} a_{11} + b_{11} &amp; a_{12} + b_{12} \ a_{21} + b_{21} &amp; a_{22} + b_{22} \end{bmatrix}$$</p>
<p><strong>Python Example:</strong></p>
<pre><code class="language-python">A = np.array([[1, 2],
              [3, 4]])

B = np.array([[5, 6],
              [7, 8]])

C = A + B
print(f"A + B =\n{C}")  # [[6, 8], [10, 12]]

D = A - B
print(f"A - B =\n{D}")  # [[-4, -4], [-4, -4]]
</code></pre>
<h4 id="matrix-multiplication"><a class="header" href="#matrix-multiplication">Matrix Multiplication</a></h4>
<p><strong>Intuition</strong>: This is the most important operation! Matrix multiplication combines transformations. Think of it as:</p>
<ul>
<li>Applying one transformation, then another</li>
<li>Computing dot products of rows and columns</li>
<li>Combining multiple linear equations</li>
</ul>
<p><strong>Key Insight</strong>: For $A$ ($m \times n$) and $B$ ($n \times p$):</p>
<ul>
<li>The <strong>number of columns in $A$</strong> must equal the <strong>number of rows in $B$</strong></li>
<li>Result $AB$ is $m \times p$</li>
</ul>
<p><strong>Definition</strong>: Element $(AB)_{ij}$ is the dot product of row $i$ of $A$ with column $j$ of $B$:</p>
<p>$$(AB)<em>{ij} = \sum</em>{k=1}^{n} a_{ik}b_{kj}$$</p>
<p><strong>Example</strong> (2×3 matrix times 3×2 matrix):</p>
<p>$$\begin{bmatrix} 1 &amp; 2 &amp; 3 \ 4 &amp; 5 &amp; 6 \end{bmatrix} \begin{bmatrix} 7 &amp; 8 \ 9 &amp; 10 \ 11 &amp; 12 \end{bmatrix} = \begin{bmatrix} 58 &amp; 64 \ 139 &amp; 154 \end{bmatrix}$$</p>
<p>First element: $1 \cdot 7 + 2 \cdot 9 + 3 \cdot 11 = 7 + 18 + 33 = 58$</p>
<p><strong>Python Example:</strong></p>
<pre><code class="language-python">A = np.array([[1, 2, 3],
              [4, 5, 6]])  # 2×3

B = np.array([[7, 8],
              [9, 10],
              [11, 12]])  # 3×2

# Matrix multiplication
C = A @ B  # Recommended: @ operator
C_alt = np.dot(A, B)  # Alternative
C_matmul = np.matmul(A, B)  # Also works

print(f"A @ B =\n{C}")
print(f"Shape: {A.shape} @ {B.shape} = {C.shape}")  # (2,3) @ (3,2) = (2,2)

# Element-wise multiplication (Hadamard product) - DIFFERENT!
A_square = np.array([[1, 2], [3, 4]])
B_square = np.array([[5, 6], [7, 8]])

hadamard = A_square * B_square  # Element-wise
matrix_mult = A_square @ B_square  # Matrix multiplication

print(f"\nElement-wise A * B:\n{hadamard}")
print(f"\nMatrix multiplication A @ B:\n{matrix_mult}")

# Matrix-vector multiplication (special case)
M = np.array([[1, 2],
              [3, 4]])
v = np.array([5, 6])

result = M @ v  # Transforms vector v
print(f"\nM @ v = {result}")  # [1*5 + 2*6, 3*5 + 4*6] = [17, 39]
</code></pre>
<p><strong>Critical Properties:</strong></p>
<ol>
<li><strong>NOT commutative</strong>: $AB \neq BA$ (usually)</li>
<li><strong>Associative</strong>: $(AB)C = A(BC)$</li>
<li><strong>Distributive</strong>: $A(B + C) = AB + AC$</li>
<li><strong>Identity</strong>: $AI = IA = A$</li>
</ol>
<p><strong>Why AB ≠ BA:</strong></p>
<pre><code class="language-python">A = np.array([[1, 2],
              [3, 4]])

B = np.array([[0, 1],
              [1, 0]])  # Swaps coordinates

AB = A @ B
BA = B @ A

print(f"AB =\n{AB}")
print(f"BA =\n{BA}")
print(f"AB == BA: {np.array_equal(AB, BA)}")  # False!
</code></pre>
<h3 id="matrix-transpose"><a class="header" href="#matrix-transpose">Matrix Transpose</a></h3>
<p><strong>Intuition</strong>: Flip the matrix over its diagonal. Rows become columns, columns become rows.</p>
<p><strong>Definition</strong>:</p>
<p>$$(A^T)<em>{ij} = A</em>{ji}$$</p>
<p>$$A = \begin{bmatrix} 1 &amp; 2 &amp; 3 \ 4 &amp; 5 &amp; 6 \end{bmatrix} \implies A^T = \begin{bmatrix} 1 &amp; 4 \ 2 &amp; 5 \ 3 &amp; 6 \end{bmatrix}$$</p>
<p><strong>Properties:</strong></p>
<ol>
<li>$(A^T)^T = A$</li>
<li>$(A + B)^T = A^T + B^T$</li>
<li>$(AB)^T = B^T A^T$ (order reverses!)</li>
<li>$(cA)^T = cA^T$</li>
</ol>
<p><strong>Python Example:</strong></p>
<pre><code class="language-python">A = np.array([[1, 2, 3],
              [4, 5, 6]])

A_T = A.T  # Transpose

print(f"Original A ({A.shape}):\n{A}")
print(f"Transpose A^T ({A_T.shape}):\n{A_T}")

# Verify transpose property
B = np.array([[1, 2],
              [3, 4]])

C = np.array([[5, 6],
              [7, 8]])

# (BC)^T = C^T B^T
left = (B @ C).T
right = C.T @ B.T

print(f"(BC)^T =\n{left}")
print(f"C^T B^T =\n{right}")
print(f"Equal: {np.allclose(left, right)}")
</code></pre>
<h3 id="special-matrix-types"><a class="header" href="#special-matrix-types">Special Matrix Types</a></h3>
<h4 id="symmetric-matrix"><a class="header" href="#symmetric-matrix">Symmetric Matrix</a></h4>
<p><strong>Definition</strong>: A matrix equals its transpose: $A = A^T$</p>
<p>$$A = \begin{bmatrix} 1 &amp; 2 &amp; 3 \ 2 &amp; 4 &amp; 5 \ 3 &amp; 5 &amp; 6 \end{bmatrix}$$</p>
<p><strong>Properties:</strong></p>
<ul>
<li>Must be square</li>
<li>Appear in optimization, physics, statistics</li>
<li>Have real eigenvalues</li>
<li>Eigenvectors are orthogonal</li>
</ul>
<pre><code class="language-python"># Symmetric matrix
A_sym = np.array([[1, 2, 3],
                  [2, 4, 5],
                  [3, 5, 6]])

is_symmetric = np.allclose(A_sym, A_sym.T)
print(f"Is symmetric: {is_symmetric}")

# Create symmetric matrix from any square matrix
A = np.random.rand(3, 3)
A_sym = (A + A.T) / 2  # Guaranteed symmetric
print(f"Symmetrized:\n{A_sym}")
</code></pre>
<h4 id="orthogonal-matrix"><a class="header" href="#orthogonal-matrix">Orthogonal Matrix</a></h4>
<p><strong>Definition</strong>: $Q^T Q = QQ^T = I$ (transpose equals inverse)</p>
<p><strong>Geometric Meaning</strong>: Preserves lengths and angles (rotation or reflection)</p>
<p><strong>Properties:</strong></p>
<ul>
<li>Columns are orthonormal vectors</li>
<li>Rows are orthonormal vectors</li>
<li>Determinant is ±1</li>
<li>Numerically stable</li>
</ul>
<pre><code class="language-python"># Rotation matrix (orthogonal)
theta = np.pi / 4  # 45 degrees
Q = np.array([[np.cos(theta), -np.sin(theta)],
              [np.sin(theta),  np.cos(theta)]])

print(f"Rotation matrix Q:\n{Q}")

# Verify Q^T Q = I
I = Q.T @ Q
print(f"Q^T Q:\n{I}")
print(f"Is identity: {np.allclose(I, np.eye(2))}")

# Orthogonal matrices preserve length
v = np.array([3, 4])
v_rotated = Q @ v

print(f"Original length: {np.linalg.norm(v)}")
print(f"Rotated length: {np.linalg.norm(v_rotated)}")  # Same!
</code></pre>
<h4 id="positive-definite-matrix"><a class="header" href="#positive-definite-matrix">Positive Definite Matrix</a></h4>
<p><strong>Intuition</strong>: All eigenvalues are positive. Appears in optimization as “bowl-shaped” functions.</p>
<p><strong>Definition</strong>: For all non-zero vectors $\mathbf{x}$:</p>
<p>$$\mathbf{x}^T A \mathbf{x} &gt; 0$$</p>
<p><strong>Properties:</strong></p>
<ul>
<li>Symmetric</li>
<li>Used in optimization (local minima)</li>
<li>Cholesky decomposition exists</li>
<li>Covariance matrices are positive semi-definite</li>
</ul>
<pre><code class="language-python"># Positive definite matrix
A = np.array([[2, 1],
              [1, 2]])

# Check: all eigenvalues positive
eigenvalues = np.linalg.eigvals(A)
is_positive_def = np.all(eigenvalues &gt; 0)

print(f"Eigenvalues: {eigenvalues}")
print(f"Is positive definite: {is_positive_def}")

# Test with random vector
x = np.random.rand(2)
quad_form = x.T @ A @ x
print(f"x^T A x = {quad_form} &gt; 0")  # Positive!
</code></pre>
<hr>
<h2 id="linear-systems"><a class="header" href="#linear-systems">Linear Systems</a></h2>
<h3 id="systems-of-linear-equations"><a class="header" href="#systems-of-linear-equations">Systems of Linear Equations</a></h3>
<p><strong>Intuition</strong>: A system of linear equations asks: “Where do multiple lines/planes intersect?” In 2D, two lines might intersect at a point, be parallel (no solution), or be the same line (infinite solutions).</p>
<p><strong>General Form</strong>:</p>
<p>$$\begin{cases} a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n = b_1 \ a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n = b_2 \ \vdots \ a_{m1}x_1 + a_{m2}x_2 + \cdots + a_{mn}x_n = b_m \end{cases}$$</p>
<p><strong>Matrix Form</strong>: $A\mathbf{x} = \mathbf{b}$</p>
<p>$$\begin{bmatrix} a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \ a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \ a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn} \end{bmatrix} \begin{bmatrix} x_1 \ x_2 \ \vdots \ x_n \end{bmatrix} = \begin{bmatrix} b_1 \ b_2 \ \vdots \ b_m \end{bmatrix}$$</p>
<p><strong>Geometric Interpretation:</strong></p>
<p>2D Example: Two lines</p>
<ul>
<li>Unique solution: Lines intersect at one point</li>
<li>No solution: Lines are parallel</li>
<li>Infinite solutions: Lines are identical</li>
</ul>
<p><strong>Python Example:</strong></p>
<pre><code class="language-python"># System: 2x + 3y = 8
#         x - y = 1
#
# Matrix form: [2  3] [x] = [8]
#              [1 -1] [y]   [1]

A = np.array([[2, 3],
              [1, -1]])

b = np.array([8, 1])

# Solve using NumPy
x = np.linalg.solve(A, b)

print(f"Solution: x = {x}")  # [2.5, 1.5]

# Verify solution
print(f"Verification A@x = {A @ x}")  # Should equal b
print(f"Equals b: {np.allclose(A @ x, b)}")

# Visualize the system
x_vals = np.linspace(-1, 5, 100)

# Line 1: 2x + 3y = 8  =&gt;  y = (8 - 2x) / 3
y1 = (8 - 2*x_vals) / 3

# Line 2: x - y = 1  =&gt;  y = x - 1
y2 = x_vals - 1

plt.figure(figsize=(8, 6))
plt.plot(x_vals, y1, label='2x + 3y = 8')
plt.plot(x_vals, y2, label='x - y = 1')
plt.plot(x[0], x[1], 'ro', markersize=10, label=f'Solution ({x[0]:.1f}, {x[1]:.1f})')
plt.grid(True, alpha=0.3)
plt.legend()
plt.xlabel('x')
plt.ylabel('y')
plt.title('System of Linear Equations')
plt.xlim(-1, 5)
plt.ylim(-1, 5)
plt.show()
</code></pre>
<h3 id="gaussian-elimination"><a class="header" href="#gaussian-elimination">Gaussian Elimination</a></h3>
<p><strong>Intuition</strong>: Systematically eliminate variables to solve the system. Like solving puzzles by substitution, but organized.</p>
<p><strong>Algorithm</strong>:</p>
<ol>
<li><strong>Forward elimination</strong>: Create zeros below diagonal (upper triangular form)</li>
<li><strong>Back substitution</strong>: Solve from bottom to top</li>
</ol>
<p><strong>Row Operations</strong> (don’t change the solution):</p>
<ol>
<li>Swap two rows</li>
<li>Multiply a row by a non-zero constant</li>
<li>Add a multiple of one row to another</li>
</ol>
<p><strong>Example</strong>: Solve manually</p>
<p>$$\begin{align*} x + 2y + z &amp;= 9 \ 2x + 4y + 3z &amp;= 21 \ 3x + 2y + z &amp;= 13 \end{align*}$$</p>
<p><strong>Python Implementation:</strong></p>
<pre><code class="language-python">def gaussian_elimination(A, b):
    """
    Solve Ax = b using Gaussian elimination with partial pivoting

    Returns: solution vector x
    """
    # Make copies to avoid modifying inputs
    A = A.astype(float)
    b = b.astype(float).reshape(-1, 1)
    n = len(b)

    # Create augmented matrix [A|b]
    Ab = np.column_stack([A, b])

    # Forward elimination
    for col in range(n):
        # Partial pivoting: find row with largest value in column
        max_row = np.argmax(np.abs(Ab[col:, col])) + col

        # Swap rows
        Ab[[col, max_row]] = Ab[[max_row, col]]

        # Make pivot 1
        pivot = Ab[col, col]
        if abs(pivot) &lt; 1e-10:
            raise ValueError("Matrix is singular")

        Ab[col] = Ab[col] / pivot

        # Eliminate column below pivot
        for row in range(col + 1, n):
            factor = Ab[row, col]
            Ab[row] = Ab[row] - factor * Ab[col]

    # Back substitution
    x = np.zeros(n)
    for i in range(n - 1, -1, -1):
        x[i] = Ab[i, -1] - np.dot(Ab[i, i+1:n], x[i+1:n])

    return x

# Test
A = np.array([[1, 2, 1],
              [2, 4, 3],
              [3, 2, 1]], dtype=float)

b = np.array([9, 21, 13], dtype=float)

x = gaussian_elimination(A, b)
print(f"Solution: {x}")

# Verify
print(f"Verification: A@x = {A @ x}")
print(f"Should be b = {b}")
print(f"Match: {np.allclose(A @ x, b)}")

# Compare with NumPy
x_numpy = np.linalg.solve(A, b)
print(f"NumPy solution: {x_numpy}")
print(f"Solutions match: {np.allclose(x, x_numpy)}")
</code></pre>
<p><strong>Complexity</strong>: $O(n^3)$ for an $n \times n$ system</p>
<h3 id="lu-decomposition"><a class="header" href="#lu-decomposition">LU Decomposition</a></h3>
<p><strong>Intuition</strong>: Factor matrix into Lower × Upper triangular matrices. Like factoring 12 = 3 × 4, but for matrices. Once factored, solving $A\mathbf{x} = \mathbf{b}$ becomes much faster for multiple right-hand sides.</p>
<p><strong>Decomposition</strong>: $A = LU$</p>
<p>$$\begin{bmatrix} a_{11} &amp; a_{12} &amp; a_{13} \ a_{21} &amp; a_{22} &amp; a_{23} \ a_{31} &amp; a_{32} &amp; a_{33} \end{bmatrix} = \begin{bmatrix} 1 &amp; 0 &amp; 0 \ l_{21} &amp; 1 &amp; 0 \ l_{31} &amp; l_{32} &amp; 1 \end{bmatrix} \begin{bmatrix} u_{11} &amp; u_{12} &amp; u_{13} \ 0 &amp; u_{22} &amp; u_{23} \ 0 &amp; 0 &amp; u_{33} \end{bmatrix}$$</p>
<p><strong>Solving $A\mathbf{x} = \mathbf{b}$ with LU</strong>:</p>
<ol>
<li>Decompose: $A = LU$ (once, $O(n^3)$)</li>
<li>Solve $L\mathbf{y} = \mathbf{b}$ (forward substitution, $O(n^2)$)</li>
<li>Solve $U\mathbf{x} = \mathbf{y}$ (back substitution, $O(n^2)$)</li>
</ol>
<p><strong>Benefit</strong>: Solve for multiple $\mathbf{b}$ vectors efficiently!</p>
<p><strong>Python Example:</strong></p>
<pre><code class="language-python">from scipy.linalg import lu

A = np.array([[2, 1, 1],
              [4, 3, 3],
              [8, 7, 9]], dtype=float)

b = np.array([1, 2, 3], dtype=float)

# LU decomposition with partial pivoting
P, L, U = lu(A)

print(f"Original matrix A:\n{A}\n")
print(f"Permutation matrix P:\n{P}\n")
print(f"Lower triangular L:\n{L}\n")
print(f"Upper triangular U:\n{U}\n")

# Verify: PA = LU
print(f"PA:\n{P @ A}")
print(f"LU:\n{L @ U}")
print(f"PA = LU: {np.allclose(P @ A, L @ U)}\n")

# Solving Ax = b using LU
# Step 1: Solve Ly = Pb
y = np.linalg.solve(L, P @ b)

# Step 2: Solve Ux = y
x = np.linalg.solve(U, y)

print(f"Solution x: {x}")
print(f"Verification A@x: {A @ x}")
print(f"Matches b: {np.allclose(A @ x, b)}")

# Solving for multiple right-hand sides
b1 = np.array([1, 2, 3])
b2 = np.array([4, 5, 6])
b3 = np.array([7, 8, 9])

# LU decomposition done once!
for b in [b1, b2, b3]:
    y = np.linalg.solve(L, P @ b)
    x = np.linalg.solve(U, y)
    print(f"b = {b}, x = {x}")
</code></pre>
<p><strong>Computational Comparison:</strong></p>
<pre><code class="language-python">import time

# Create large system
n = 1000
A_large = np.random.rand(n, n)
bs = [np.random.rand(n) for _ in range(10)]

# Method 1: Solve each system independently
start = time.time()
for b in bs:
    x = np.linalg.solve(A_large, b)
time_direct = time.time() - start

# Method 2: LU decomposition once, then solve
start = time.time()
P, L, U = lu(A_large)
for b in bs:
    y = np.linalg.solve(L, P @ b)
    x = np.linalg.solve(U, y)
time_lu = time.time() - start

print(f"Direct solve (10 systems): {time_direct:.3f}s")
print(f"LU decomposition method: {time_lu:.3f}s")
print(f"Speedup: {time_direct/time_lu:.2f}x")
</code></pre>
<hr>
<h2 id="vector-spaces"><a class="header" href="#vector-spaces">Vector Spaces</a></h2>
<h3 id="vector-space-axioms"><a class="header" href="#vector-space-axioms">Vector Space Axioms</a></h3>
<p><strong>Intuition</strong>: A vector space is a collection of objects (vectors) where you can add them and multiply by scalars, and everything behaves nicely. Think of it as a “playground” where vectors live and play by specific rules.</p>
<p><strong>Formal Definition</strong>: A vector space $V$ over a field $\mathbb{F}$ (usually $\mathbb{R}$) is a set with two operations:</p>
<ul>
<li>Vector addition: $\mathbf{u} + \mathbf{v} \in V$</li>
<li>Scalar multiplication: $c\mathbf{v} \in V$</li>
</ul>
<p>satisfying these <strong>10 axioms</strong>:</p>
<p><strong>Axioms</strong> (for vectors $\mathbf{u}, \mathbf{v}, \mathbf{w} \in V$ and scalars $c, d \in \mathbb{R}$):</p>
<ol>
<li><strong>Closure under addition</strong>: $\mathbf{u} + \mathbf{v} \in V$</li>
<li><strong>Commutativity</strong>: $\mathbf{u} + \mathbf{v} = \mathbf{v} + \mathbf{u}$</li>
<li><strong>Associativity</strong>: $(\mathbf{u} + \mathbf{v}) + \mathbf{w} = \mathbf{u} + (\mathbf{v} + \mathbf{w})$</li>
<li><strong>Zero vector exists</strong>: $\exists \mathbf{0}$ such that $\mathbf{v} + \mathbf{0} = \mathbf{v}$</li>
<li><strong>Additive inverse exists</strong>: $\exists (-\mathbf{v})$ such that $\mathbf{v} + (-\mathbf{v}) = \mathbf{0}$</li>
<li><strong>Closure under scalar multiplication</strong>: $c\mathbf{v} \in V$</li>
<li><strong>Distributivity (scalar)</strong>: $c(\mathbf{u} + \mathbf{v}) = c\mathbf{u} + c\mathbf{v}$</li>
<li><strong>Distributivity (vector)</strong>: $(c + d)\mathbf{v} = c\mathbf{v} + d\mathbf{v}$</li>
<li><strong>Associativity (scalar)</strong>: $c(d\mathbf{v}) = (cd)\mathbf{v}$</li>
<li><strong>Identity</strong>: $1\mathbf{v} = \mathbf{v}$</li>
</ol>
<p><strong>Examples of Vector Spaces:</strong></p>
<ul>
<li>$\mathbb{R}^n$: n-dimensional Euclidean space</li>
<li>$\mathbb{C}^n$: Complex vectors</li>
<li>$P_n$: Polynomials of degree ≤ n</li>
<li>$M_{m \times n}$: m×n matrices</li>
<li>Function spaces: continuous functions on [a, b]</li>
</ul>
<p><strong>Non-Examples</strong> (fail one or more axioms):</p>
<ul>
<li>Natural numbers $\mathbb{N}$ (no additive inverse)</li>
<li>First quadrant of $\mathbb{R}^2$ (not closed under scalar multiplication by negatives)</li>
</ul>
<h3 id="subspaces"><a class="header" href="#subspaces">Subspaces</a></h3>
<p><strong>Intuition</strong>: A subspace is a vector space living inside another vector space. Like a plane through the origin in 3D space, or a line through the origin in 2D.</p>
<p><strong>Definition</strong>: A subset $W \subseteq V$ is a subspace if:</p>
<ol>
<li>$\mathbf{0} \in W$ (contains zero vector)</li>
<li>Closed under addition: if $\mathbf{u}, \mathbf{v} \in W$, then $\mathbf{u} + \mathbf{v} \in W$</li>
<li>Closed under scalar multiplication: if $\mathbf{v} \in W$ and $c \in \mathbb{R}$, then $c\mathbf{v} \in W$</li>
</ol>
<p><strong>Important Subspaces of Matrix $A$ (m×n):</strong></p>
<ol>
<li>
<p><strong>Column Space</strong> $C(A)$: All linear combinations of columns</p>
<ul>
<li>Span of column vectors</li>
<li>Range of transformation $\mathbf{x} \mapsto A\mathbf{x}$</li>
</ul>
</li>
<li>
<p><strong>Row Space</strong> $C(A^T)$: All linear combinations of rows</p>
<ul>
<li>Column space of $A^T$</li>
</ul>
</li>
<li>
<p><strong>Null Space</strong> $N(A)$: Solutions to $A\mathbf{x} = \mathbf{0}$</p>
<ul>
<li>Kernel of transformation</li>
</ul>
</li>
<li>
<p><strong>Left Null Space</strong> $N(A^T)$: Solutions to $A^T\mathbf{y} = \mathbf{0}$</p>
</li>
</ol>
<p><strong>Python Example:</strong></p>
<pre><code class="language-python">def is_in_column_space(A, b):
    """
    Check if b is in the column space of A
    (i.e., does Ax = b have a solution?)
    """
    try:
        x = np.linalg.lstsq(A, b, rcond=None)[0]
        # Check if Ax ≈ b
        return np.allclose(A @ x, b)
    except:
        return False

A = np.array([[1, 2],
              [2, 4],
              [3, 6]])

# Vectors in column space
b1 = np.array([3, 6, 9])  # b1 = 3 * column1
print(f"b1 in C(A): {is_in_column_space(A, b1)}")  # True

# Vector NOT in column space (columns are parallel, so column space is a line)
b2 = np.array([1, 2, 4])  # Not a multiple of column1
print(f"b2 in C(A): {is_in_column_space(A, b2)}")  # False

# Find null space (solutions to Ax = 0)
def null_space(A, tol=1e-10):
    """
    Compute orthonormal basis for null space of A
    """
    U, s, Vt = np.linalg.svd(A, full_matrices=True)
    # Null space spanned by columns of V corresponding to zero singular values
    null_mask = s &lt; tol
    null_space_basis = Vt[len(s):].T  # Last columns of V
    return null_space_basis

A = np.array([[1, 2, 3],
              [2, 4, 6]])

ns = null_space(A)
print(f"\nNull space basis:\n{ns}")

# Verify: A @ ns should be zero
for i in range(ns.shape[1]):
    result = A @ ns[:, i]
    print(f"A @ null_vector_{i} = {result} (≈ 0: {np.allclose(result, 0)})")
</code></pre>
<h3 id="span-and-linear-independence"><a class="header" href="#span-and-linear-independence">Span and Linear Independence</a></h3>
<p><strong>Span - Intuition</strong>: The span is all possible linear combinations of vectors. If you have two non-parallel vectors in 2D, their span is the entire plane.</p>
<p><strong>Definition</strong>: The span of vectors $\mathbf{v}_1, \ldots, \mathbf{v}_k$ is:</p>
<p>$$\text{span}(\mathbf{v}_1, \ldots, \mathbf{v}_k) = {c_1\mathbf{v}_1 + \cdots + c_k\mathbf{v}_k : c_i \in \mathbb{R}}$$</p>
<p><strong>Linear Independence - Intuition</strong>: Vectors are linearly independent if none can be written as a combination of the others. In 2D, two vectors are independent if they’re not parallel.</p>
<p><strong>Definition</strong>: Vectors $\mathbf{v}_1, \ldots, \mathbf{v}_k$ are linearly independent if:</p>
<p>$$c_1\mathbf{v}_1 + \cdots + c_k\mathbf{v}_k = \mathbf{0} \implies c_1 = \cdots = c_k = 0$$</p>
<p>(only the trivial combination gives zero)</p>
<p><strong>Python Example:</strong></p>
<pre><code class="language-python">def are_linearly_independent(vectors):
    """
    Check if a set of vectors is linearly independent
    """
    # Stack vectors as columns
    A = np.column_stack(vectors)

    # Vectors are independent if rank equals number of vectors
    rank = np.linalg.matrix_rank(A)
    n_vectors = len(vectors)

    return rank == n_vectors

# Independent vectors
v1 = np.array([1, 0])
v2 = np.array([0, 1])
print(f"v1, v2 independent: {are_linearly_independent([v1, v2])}")  # True

# Dependent vectors (v3 = 2*v1 + 3*v2)
v3 = 2*v1 + 3*v2
print(f"v1, v2, v3 independent: {are_linearly_independent([v1, v2, v3])}")  # False

# Visualize span
def visualize_span_2d(v1, v2):
    """Visualize span of two 2D vectors"""
    plt.figure(figsize=(8, 8))

    # Plot original vectors
    plt.quiver(0, 0, v1[0], v1[1], angles='xy', scale_units='xy', scale=1,
               color='red', width=0.01, label='v1')
    plt.quiver(0, 0, v2[0], v2[1], angles='xy', scale_units='xy', scale=1,
               color='blue', width=0.01, label='v2')

    # Plot linear combinations
    for c1 in np.linspace(-2, 2, 20):
        for c2 in np.linspace(-2, 2, 20):
            combo = c1*v1 + c2*v2
            plt.plot(combo[0], combo[1], 'g.', markersize=2, alpha=0.3)

    plt.xlim(-5, 5)
    plt.ylim(-5, 5)
    plt.grid(True, alpha=0.3)
    plt.axhline(y=0, color='k', linewidth=0.5)
    plt.axvline(x=0, color='k', linewidth=0.5)
    plt.legend()
    plt.title('Span of v1 and v2')
    plt.axis('equal')
    plt.show()

v1 = np.array([1, 0])
v2 = np.array([0, 1])
visualize_span_2d(v1, v2)  # Fills entire plane

# Parallel vectors (dependent)
v1_parallel = np.array([1, 1])
v2_parallel = np.array([2, 2])  # Parallel to v1
visualize_span_2d(v1_parallel, v2_parallel)  # Only fills a line
</code></pre>
<h3 id="basis-and-dimension"><a class="header" href="#basis-and-dimension">Basis and Dimension</a></h3>
<p><strong>Basis - Intuition</strong>: A basis is a minimal set of vectors that spans the space. Like a coordinate system - just enough vectors to reach anywhere, with no redundancy.</p>
<p><strong>Definition</strong>: A basis for vector space $V$ is a set of vectors that:</p>
<ol>
<li><strong>Spans</strong> $V$: Every vector in $V$ can be written as a linear combination</li>
<li>Is <strong>linearly independent</strong>: No redundancy</li>
</ol>
<p><strong>Dimension</strong>: The number of vectors in a basis (all bases have the same size!)</p>
<p><strong>Standard Basis for $\mathbb{R}^n$</strong>:</p>
<p>$$\mathbf{e}_1 = \begin{bmatrix} 1 \ 0 \ \vdots \ 0 \end{bmatrix}, \mathbf{e}_2 = \begin{bmatrix} 0 \ 1 \ \vdots \ 0 \end{bmatrix}, \ldots, \mathbf{e}_n = \begin{bmatrix} 0 \ 0 \ \vdots \ 1 \end{bmatrix}$$</p>
<p><strong>Python Example:</strong></p>
<pre><code class="language-python"># Standard basis for R^3
e1 = np.array([1, 0, 0])
e2 = np.array([0, 1, 0])
e3 = np.array([0, 0, 1])

print("Standard basis for R^3:")
print(f"e1 = {e1}")
print(f"e2 = {e2}")
print(f"e3 = {e3}")

# Any vector is a linear combination of basis vectors
v = np.array([5, 7, 3])
print(f"\nVector v = {v}")
print(f"v = {v[0]}*e1 + {v[1]}*e2 + {v[2]}*e3")
reconstructed = v[0]*e1 + v[1]*e2 + v[2]*e3
print(f"Reconstructed: {reconstructed}")

# Different basis (still spans R^2)
b1 = np.array([1, 1])
b2 = np.array([1, -1])

print(f"\nAlternative basis for R^2:")
print(f"b1 = {b1}, b2 = {b2}")
print(f"Independent: {are_linearly_independent([b1, b2])}")

# Express vector in new basis
def coordinates_in_basis(v, basis):
    """
    Find coordinates of v in given basis
    """
    B = np.column_stack(basis)  # Basis vectors as columns
    coords = np.linalg.solve(B, v)
    return coords

v = np.array([3, 1])
coords = coordinates_in_basis(v, [b1, b2])
print(f"\nVector {v} in standard basis")
print(f"Coordinates in (b1, b2) basis: {coords}")
print(f"Verification: {coords[0]}*b1 + {coords[1]}*b2 = {coords[0]*b1 + coords[1]*b2}")

# Find basis for column space
def column_space_basis(A):
    """
    Find basis vectors for column space of A
    """
    Q, R = np.linalg.qr(A)
    # Number of independent columns = rank
    rank = np.linalg.matrix_rank(A)
    return Q[:, :rank]

A = np.array([[1, 2, 3],
              [2, 4, 5],
              [3, 6, 7]])

basis = column_space_basis(A)
print(f"\nColumn space basis:\n{basis}")
print(f"Dimension of column space: {basis.shape[1]}")
</code></pre>
<hr>
<h2 id="linear-transformations"><a class="header" href="#linear-transformations">Linear Transformations</a></h2>
<h3 id="definition-and-properties"><a class="header" href="#definition-and-properties">Definition and Properties</a></h3>
<p><strong>Intuition</strong>: A linear transformation is a function that maps vectors to vectors while preserving vector addition and scalar multiplication. Think of it as stretching, rotating, or projecting space in a predictable way.</p>
<p><strong>Definition</strong>: A function $T: V \to W$ is a linear transformation if for all vectors $\mathbf{u}, \mathbf{v}$ and scalar $c$:</p>
<ol>
<li>$T(\mathbf{u} + \mathbf{v}) = T(\mathbf{u}) + T(\mathbf{v})$ (additivity)</li>
<li>$T(c\mathbf{v}) = cT(\mathbf{v})$ (homogeneity)</li>
</ol>
<p><strong>Combined</strong>: $T(c_1\mathbf{u} + c_2\mathbf{v}) = c_1T(\mathbf{u}) + c_2T(\mathbf{v})$</p>
<p><strong>Matrix Representation</strong>: Every linear transformation $T: \mathbb{R}^n \to \mathbb{R}^m$ can be represented as:</p>
<p>$$T(\mathbf{x}) = A\mathbf{x}$$</p>
<p>where $A$ is an $m \times n$ matrix.</p>
<p><strong>Key Properties</strong>:</p>
<ul>
<li>Always maps zero to zero: $T(\mathbf{0}) = \mathbf{0}$</li>
<li>Preserves lines and planes</li>
<li>Preserves parallelism</li>
<li>Maps grid lines to grid lines (possibly skewed)</li>
</ul>
<p><strong>Python Example:</strong></p>
<pre><code class="language-python"># Define transformation by its matrix
A = np.array([[2, 1],
              [1, 2]])

def transform(v):
    """Apply linear transformation defined by A"""
    return A @ v

# Test linearity properties
u = np.array([1, 0])
v = np.array([0, 1])
c = 3

# Property 1: T(u + v) = T(u) + T(v)
left = transform(u + v)
right = transform(u) + transform(v)
print(f"T(u+v) = {left}")
print(f"T(u) + T(v) = {right}")
print(f"Additive: {np.allclose(left, right)}")

# Property 2: T(cv) = cT(v)
left = transform(c * v)
right = c * transform(v)
print(f"\nT(cv) = {left}")
print(f"cT(v) = {right}")
print(f"Homogeneous: {np.allclose(left, right)}")
</code></pre>
<h3 id="geometric-transformations"><a class="header" href="#geometric-transformations">Geometric Transformations</a></h3>
<p>Common 2D transformations and their matrices:</p>
<p><strong>1. Scaling</strong> (stretch/shrink):</p>
<p>$$S = \begin{bmatrix} s_x &amp; 0 \ 0 &amp; s_y \end{bmatrix}$$</p>
<p><strong>2. Rotation</strong> (counterclockwise by θ):</p>
<p>$$R(\theta) = \begin{bmatrix} \cos\theta &amp; -\sin\theta \ \sin\theta &amp; \cos\theta \end{bmatrix}$$</p>
<p><strong>3. Reflection</strong> (across x-axis):</p>
<p>$$F_x = \begin{bmatrix} 1 &amp; 0 \ 0 &amp; -1 \end{bmatrix}$$</p>
<p><strong>4. Shear</strong> (horizontal):</p>
<p>$$H = \begin{bmatrix} 1 &amp; k \ 0 &amp; 1 \end{bmatrix}$$</p>
<p><strong>5. Projection</strong> (onto x-axis):</p>
<p>$$P_x = \begin{bmatrix} 1 &amp; 0 \ 0 &amp; 0 \end{bmatrix}$$</p>
<p><strong>Python Examples:</strong></p>
<pre><code class="language-python">def visualize_transformation(A, title="Transformation"):
    """
    Visualize how transformation A affects a grid of points
    """
    # Create grid of points
    x = np.linspace(-2, 2, 20)
    y = np.linspace(-2, 2, 20)
    X, Y = np.meshgrid(x, y)

    # Original points
    points_orig = np.vstack([X.ravel(), Y.ravel()])

    # Transformed points
    points_trans = A @ points_orig

    plt.figure(figsize=(12, 5))

    # Original
    plt.subplot(1, 2, 1)
    plt.scatter(points_orig[0], points_orig[1], c='blue', alpha=0.3, s=10)
    plt.grid(True, alpha=0.3)
    plt.axis('equal')
    plt.xlim(-4, 4)
    plt.ylim(-4, 4)
    plt.title('Original')

    # Transformed
    plt.subplot(1, 2, 2)
    plt.scatter(points_trans[0], points_trans[1], c='red', alpha=0.3, s=10)
    plt.grid(True, alpha=0.3)
    plt.axis('equal')
    plt.xlim(-4, 4)
    plt.ylim(-4, 4)
    plt.title(f'After {title}')

    plt.tight_layout()
    plt.show()

# 1. Scaling
S = np.array([[2, 0],
              [0, 0.5]])  # Double x, halve y
visualize_transformation(S, "Scaling")

# 2. Rotation (45 degrees)
theta = np.pi / 4
R = np.array([[np.cos(theta), -np.sin(theta)],
              [np.sin(theta),  np.cos(theta)]])
visualize_transformation(R, "Rotation 45°")

# 3. Reflection (across y = x)
F = np.array([[0, 1],
              [1, 0]])
visualize_transformation(F, "Reflection")

# 4. Shear
H = np.array([[1, 0.5],
              [0, 1]])
visualize_transformation(H, "Shear")

# 5. Projection (onto x-axis)
P = np.array([[1, 0],
              [0, 0]])
visualize_transformation(P, "Projection")

# Transform specific shapes
def transform_shape(A, shape_points):
    """Transform a shape defined by points"""
    return A @ shape_points

# Create a square
square = np.array([[0, 1, 1, 0, 0],   # x-coordinates
                   [0, 0, 1, 1, 0]])  # y-coordinates

# Apply rotation
theta = np.pi / 6  # 30 degrees
R = np.array([[np.cos(theta), -np.sin(theta)],
              [np.sin(theta),  np.cos(theta)]])

square_rotated = R @ square

plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.plot(square[0], square[1], 'b-', linewidth=2)
plt.grid(True)
plt.axis('equal')
plt.title('Original Square')

plt.subplot(1, 2, 2)
plt.plot(square_rotated[0], square_rotated[1], 'r-', linewidth=2)
plt.grid(True)
plt.axis('equal')
plt.title('Rotated Square')
plt.show()
</code></pre>
<h3 id="composition-of-transformations"><a class="header" href="#composition-of-transformations">Composition of Transformations</a></h3>
<p><strong>Intuition</strong>: Applying one transformation after another is like function composition, represented by matrix multiplication.</p>
<p><strong>Key Insight</strong>: $T_2(T_1(\mathbf{x})) = T_2(A_1\mathbf{x}) = A_2(A_1\mathbf{x}) = (A_2A_1)\mathbf{x}$</p>
<p><strong>Order Matters!</strong> $A_2A_1 \neq A_1A_2$ (usually)</p>
<p><strong>Python Example:</strong></p>
<pre><code class="language-python"># Rotate then scale
theta = np.pi / 4
R = np.array([[np.cos(theta), -np.sin(theta)],
              [np.sin(theta),  np.cos(theta)]])

S = np.array([[2, 0],
              [0, 0.5]])

# Composition 1: Rotate THEN scale
M1 = S @ R  # Apply R first, then S

# Composition 2: Scale THEN rotate
M2 = R @ S  # Apply S first, then R

print(f"Rotate then scale:\n{M1}\n")
print(f"Scale then rotate:\n{M2}\n")
print(f"Same result: {np.allclose(M1, M2)}")  # False - order matters!

# Visualize difference
v = np.array([1, 0])

result1 = M1 @ v  # Rotate then scale
result2 = M2 @ v  # Scale then rotate

print(f"\nOriginal vector: {v}")
print(f"Rotate→Scale: {result1}")
print(f"Scale→Rotate: {result2}")

# 3D Rotation around axis
def rotation_x(theta):
    """Rotation around x-axis"""
    return np.array([[1, 0, 0],
                     [0, np.cos(theta), -np.sin(theta)],
                     [0, np.sin(theta),  np.cos(theta)]])

def rotation_y(theta):
    """Rotation around y-axis"""
    return np.array([[np.cos(theta), 0, np.sin(theta)],
                     [0, 1, 0],
                     [-np.sin(theta), 0, np.cos(theta)]])

def rotation_z(theta):
    """Rotation around z-axis"""
    return np.array([[np.cos(theta), -np.sin(theta), 0],
                     [np.sin(theta),  np.cos(theta), 0],
                     [0, 0, 1]])

# Compose rotations
angle = np.pi / 6
Rx = rotation_x(angle)
Ry = rotation_y(angle)
Rz = rotation_z(angle)

# Combined rotation
R_combined = Rz @ Ry @ Rx  # Apply Rx, then Ry, then Rz

print(f"\nCombined 3D rotation:\n{R_combined}")
</code></pre>
<h3 id="inverse-transformations"><a class="header" href="#inverse-transformations">Inverse Transformations</a></h3>
<p><strong>Intuition</strong>: An inverse transformation “undoes” the original. If $T$ rotates 45°, then $T^{-1}$ rotates -45°.</p>
<p><strong>Definition</strong>: $T^{-1}$ is the inverse if:</p>
<p>$$T^{-1}(T(\mathbf{x})) = \mathbf{x}$$
$$T(T^{-1}(\mathbf{x})) = \mathbf{x}$$</p>
<p><strong>Matrix Form</strong>: $A^{-1}A = AA^{-1} = I$</p>
<p><strong>Existence</strong>: Inverse exists if and only if:</p>
<ul>
<li>Matrix is square ($n \times n$)</li>
<li>Determinant $\det(A) \neq 0$</li>
<li>Columns are linearly independent</li>
<li>Transformation is one-to-one and onto</li>
</ul>
<p><strong>Python Example:</strong></p>
<pre><code class="language-python"># Rotation matrix (always invertible)
theta = np.pi / 3  # 60 degrees
R = np.array([[np.cos(theta), -np.sin(theta)],
              [np.sin(theta),  np.cos(theta)]])

# Inverse rotation
R_inv = np.linalg.inv(R)

print(f"Rotation matrix R:\n{R}\n")
print(f"Inverse R^(-1):\n{R_inv}\n")

# For rotation, inverse = transpose!
print(f"Transpose R^T:\n{R.T}\n")
print(f"R^(-1) = R^T: {np.allclose(R_inv, R.T)}")

# Verify inverse property
I = R @ R_inv
print(f"\nR @ R^(-1) =\n{I}")
print(f"Is identity: {np.allclose(I, np.eye(2))}")

# Apply transformation and undo it
v = np.array([3, 4])
v_rotated = R @ v
v_restored = R_inv @ v_rotated

print(f"\nOriginal: {v}")
print(f"After rotation: {v_rotated}")
print(f"After inverse: {v_restored}")
print(f"Restored: {np.allclose(v, v_restored)}")

# Non-invertible transformation (projection)
P = np.array([[1, 0],
              [0, 0]])  # Project to x-axis

try:
    P_inv = np.linalg.inv(P)
except np.linalg.LinAlgError:
    print("\nProjection matrix is not invertible!")
    print("(Information is lost - can't recover y-coordinate)")

# Check determinant
print(f"det(R) = {np.linalg.det(R):.6f} (invertible)")
print(f"det(P) = {np.linalg.det(P):.6f} (not invertible)")
</code></pre>
<hr>
<h2 id="determinants"><a class="header" href="#determinants">Determinants</a></h2>
<h3 id="geometric-interpretation-determinants"><a class="header" href="#geometric-interpretation-determinants">Geometric Interpretation (Determinants)</a></h3>
<p><strong>Intuition</strong>: The determinant measures the “signed volume” of the transformation. For 2D, it’s the area of the parallelogram formed by the matrix columns; for 3D, it’s the volume of the parallelepiped.</p>
<p><strong>Key Insights:</strong></p>
<ul>
<li>$|\det(A)| =$ volume scaling factor</li>
<li>$\det(A) &gt; 0$: preserves orientation (no flip)</li>
<li>$\det(A) &lt; 0$: reverses orientation (flip/reflection)</li>
<li>$\det(A) = 0$: collapses space (not invertible)</li>
</ul>
<p><strong>2D Example:</strong></p>
<p>$$\det\begin{bmatrix} a &amp; b \ c &amp; d \end{bmatrix} = ad - bc$$</p>
<p>This is the area of the parallelogram with sides $\begin{bmatrix} a \ c \end{bmatrix}$ and $\begin{bmatrix} b \ d \end{bmatrix}$.</p>
<p><strong>Python Visualization:</strong></p>
<pre><code class="language-python">def visualize_determinant_2d(A):
    """Visualize determinant as area"""
    # Column vectors
    v1 = A[:, 0]
    v2 = A[:, 1]

    # Create parallelogram
    parallelogram = np.array([[0, v1[0], v1[0]+v2[0], v2[0], 0],
                              [0, v1[1], v1[1]+v2[1], v2[1], 0]])

    det = np.linalg.det(A)

    plt.figure(figsize=(8, 8))
    plt.fill(parallelogram[0], parallelogram[1], alpha=0.3, color='blue')
    plt.plot(parallelogram[0], parallelogram[1], 'b-', linewidth=2)

    # Draw column vectors
    plt.quiver(0, 0, v1[0], v1[1], angles='xy', scale_units='xy', scale=1,
               color='red', width=0.01, label=f'col1')
    plt.quiver(0, 0, v2[0], v2[1], angles='xy', scale_units='xy', scale=1,
               color='green', width=0.01, label=f'col2')

    plt.grid(True, alpha=0.3)
    plt.axis('equal')
    plt.legend()
    plt.title(f'Determinant = {det:.2f} (Area of parallelogram)')
    plt.axhline(y=0, color='k', linewidth=0.5)
    plt.axvline(x=0, color='k', linewidth=0.5)
    plt.show()

# Example matrices
A1 = np.array([[2, 1],
               [1, 2]])  # det = 3
A2 = np.array([[2, 4],
               [1, 2]])  # det = 0 (parallel columns)

visualize_determinant_2d(A1)
</code></pre>
<h3 id="computing-determinants"><a class="header" href="#computing-determinants">Computing Determinants</a></h3>
<p><strong>2×2 Matrix:</strong></p>
<p>$$\det\begin{bmatrix} a &amp; b \ c &amp; d \end{bmatrix} = ad - bc$$</p>
<p><strong>3×3 Matrix</strong> (cofactor expansion along first row):</p>
<p>$$\det\begin{bmatrix} a &amp; b &amp; c \ d &amp; e &amp; f \ g &amp; h &amp; i \end{bmatrix} = a\det\begin{bmatrix} e &amp; f \ h &amp; i \end{bmatrix} - b\det\begin{bmatrix} d &amp; f \ g &amp; i \end{bmatrix} + c\det\begin{bmatrix} d &amp; e \ g &amp; h \end{bmatrix}$$</p>
<p><strong>General n×n Matrix</strong> (cofactor expansion):</p>
<p>$$\det(A) = \sum_{j=1}^{n} (-1)^{i+j} a_{ij} M_{ij}$$</p>
<p>where $M_{ij}$ is the minor (determinant of submatrix with row $i$ and column $j$ removed).</p>
<p><strong>Python Examples:</strong></p>
<pre><code class="language-python"># 2×2 determinant
A = np.array([[3, 8],
              [4, 6]])

det_manual = A[0,0]*A[1,1] - A[0,1]*A[1,0]
det_numpy = np.linalg.det(A)

print(f"2×2 Matrix:\n{A}")
print(f"Manual calculation: {det_manual}")
print(f"NumPy: {det_numpy}")

# 3×3 determinant
A = np.array([[1, 2, 3],
              [4, 5, 6],
              [7, 8, 9]])

det = np.linalg.det(A)
print(f"\n3×3 Matrix:\n{A}")
print(f"Determinant: {det:.6f}")  # ≈ 0 (columns are linearly dependent!)

# Cofactor expansion (educational)
def determinant_cofactor(A):
    """
    Compute determinant using cofactor expansion
    (Slow: O(n!), only for small matrices)
    """
    n = A.shape[0]

    # Base case: 1×1
    if n == 1:
        return A[0, 0]

    # Base case: 2×2
    if n == 2:
        return A[0,0]*A[1,1] - A[0,1]*A[1,0]

    # Recursive case: expand along first row
    det = 0
    for j in range(n):
        # Create minor (remove row 0, column j)
        minor = np.delete(np.delete(A, 0, axis=0), j, axis=1)
        cofactor = ((-1) ** j) * determinant_cofactor(minor)
        det += A[0, j] * cofactor

    return det

A_small = np.array([[1, 2, 3],
                    [0, 4, 5],
                    [1, 0, 6]])

print(f"\nCofactor expansion: {determinant_cofactor(A_small)}")
print(f"NumPy (LU-based): {np.linalg.det(A_small)}")
</code></pre>
<h3 id="properties-of-determinants"><a class="header" href="#properties-of-determinants">Properties of Determinants</a></h3>
<p><strong>Key Properties:</strong></p>
<ol>
<li><strong>Multiplicative</strong>: $\det(AB) = \det(A)\det(B)$</li>
<li><strong>Transpose</strong>: $\det(A^T) = \det(A)$</li>
<li><strong>Inverse</strong>: $\det(A^{-1}) = \frac{1}{\det(A)}$</li>
<li><strong>Scalar multiple</strong>: $\det(cA) = c^n\det(A)$ for $n \times n$ matrix</li>
<li><strong>Row operations</strong>:
<ul>
<li>Swap rows: determinant changes sign</li>
<li>Multiply row by $c$: determinant multiplied by $c$</li>
<li>Add multiple of one row to another: determinant unchanged</li>
</ul>
</li>
</ol>
<p><strong>Python Examples:</strong></p>
<pre><code class="language-python">A = np.array([[1, 2],
              [3, 4]])

B = np.array([[2, 0],
              [1, 2]])

# Property 1: det(AB) = det(A)det(B)
det_AB = np.linalg.det(A @ B)
det_A_times_det_B = np.linalg.det(A) * np.linalg.det(B)

print(f"det(AB) = {det_AB:.6f}")
print(f"det(A)·det(B) = {det_A_times_det_B:.6f}")
print(f"Equal: {np.isclose(det_AB, det_A_times_det_B)}")

# Property 2: det(A^T) = det(A)
det_A = np.linalg.det(A)
det_AT = np.linalg.det(A.T)
print(f"\ndet(A) = {det_A:.6f}")
print(f"det(A^T) = {det_AT:.6f}")

# Property 3: det(A^-1) = 1/det(A)
if det_A != 0:
    A_inv = np.linalg.inv(A)
    det_A_inv = np.linalg.det(A_inv)
    print(f"\ndet(A^-1) = {det_A_inv:.6f}")
    print(f"1/det(A) = {1/det_A:.6f}")

# Determinant test for invertibility
matrices = [
    np.array([[1, 2], [3, 4]]),      # Invertible
    np.array([[1, 2], [2, 4]]),      # Not invertible (parallel rows)
    np.array([[2, 0], [0, 3]])       # Invertible (diagonal)
]

for i, M in enumerate(matrices):
    det = np.linalg.det(M)
    invertible = abs(det) &gt; 1e-10
    print(f"\nMatrix {i+1}: det = {det:.6f}, Invertible: {invertible}")
</code></pre>
<hr>
<h2 id="eigenvalues-and-eigenvectors"><a class="header" href="#eigenvalues-and-eigenvectors">Eigenvalues and Eigenvectors</a></h2>
<h3 id="fundamental-concepts-eigenvalues"><a class="header" href="#fundamental-concepts-eigenvalues">Fundamental Concepts (Eigenvalues)</a></h3>
<p><strong>Intuition</strong>: Eigenvectors are special vectors that don’t change direction when a transformation is applied - they only get scaled. The scaling factor is the eigenvalue.</p>
<p>Think of stretching a rubber sheet: most points move in complicated ways, but points along certain directions just move straight in or out. Those special directions are eigenvectors.</p>
<p><strong>Definition</strong>: For matrix $A$ and vector $\mathbf{v} \neq \mathbf{0}$:</p>
<p>$$A\mathbf{v} = \lambda\mathbf{v}$$</p>
<p>where:</p>
<ul>
<li>$\mathbf{v}$ is an <strong>eigenvector</strong></li>
<li>$\lambda$ is an <strong>eigenvalue</strong> (can be negative or complex)</li>
</ul>
<p><strong>Geometric Meaning:</strong></p>
<ul>
<li>Eigenvector: direction preserved by transformation</li>
<li>Eigenvalue: how much it’s stretched ($\lambda &gt; 1$), shrunk ($|\lambda| &lt; 1$), or flipped ($\lambda &lt; 0$)</li>
</ul>
<p><strong>Characteristic Equation</strong>:</p>
<p>$$\det(A - \lambda I) = 0$$</p>
<p>This polynomial equation gives us all eigenvalues.</p>
<p><strong>Python Example:</strong></p>
<pre><code class="language-python"># Simple 2×2 example
A = np.array([[4, 2],
              [1, 3]])

# Compute eigenvalues and eigenvectors
eigenvalues, eigenvectors = np.linalg.eig(A)

print(f"Matrix A:\n{A}\n")
print(f"Eigenvalues: {eigenvalues}")
print(f"Eigenvectors:\n{eigenvectors}\n")

# Verify Av = λv for each eigenvalue
for i in range(len(eigenvalues)):
    λ = eigenvalues[i]
    v = eigenvectors[:, i]  # i-th column is i-th eigenvector

    Av = A @ v
    λv = λ * v

    print(f"Eigenvalue λ_{i+1} = {λ:.4f}")
    print(f"  Av = {Av}")
    print(f"  λv = {λv}")
    print(f"  Match: {np.allclose(Av, λv)}\n")

# Visualize eigenvectors
def visualize_eigenvectors(A):
    """Visualize how matrix A affects space and its eigenvectors"""
    eigenvalues, eigenvectors = np.linalg.eig(A)

    # Create unit circle
    theta = np.linspace(0, 2*np.pi, 100)
    circle = np.array([np.cos(theta), np.sin(theta)])

    # Transform circle
    ellipse = A @ circle

    plt.figure(figsize=(10, 5))

    # Original circle
    plt.subplot(1, 2, 1)
    plt.plot(circle[0], circle[1], 'b-', label='Unit circle')
    plt.axis('equal')
    plt.grid(True, alpha=0.3)
    plt.legend()
    plt.title('Original')

    # Transformed ellipse with eigenvectors
    plt.subplot(1, 2, 2)
    plt.plot(ellipse[0], ellipse[1], 'r-', label='Transformed')

    # Plot eigenvectors
    for i in range(len(eigenvalues)):
        λ = eigenvalues[i].real
        v = eigenvectors[:, i].real
        # Eigenvector is scaled by λ
        plt.quiver(0, 0, λ*v[0], λ*v[1],
                   angles='xy', scale_units='xy', scale=1,
                   width=0.01, label=f'λ={λ:.2f}')

    plt.axis('equal')
    plt.grid(True, alpha=0.3)
    plt.legend()
    plt.title('After transformation A')
    plt.tight_layout()
    plt.show()

visualize_eigenvectors(A)
</code></pre>
<h3 id="computing-eigenvalues"><a class="header" href="#computing-eigenvalues">Computing Eigenvalues</a></h3>
<p><strong>Step-by-Step Process:</strong></p>
<ol>
<li><strong>Form characteristic equation</strong>: $\det(A - \lambda I) = 0$</li>
<li><strong>Solve for</strong> $\lambda$ (roots of polynomial)</li>
<li><strong>For each</strong> $\lambda$, <strong>solve</strong> $(A - \lambda I)\mathbf{v} = \mathbf{0}$ to find eigenvectors</li>
</ol>
<p><strong>Example</strong>: Find eigenvalues of $A = \begin{bmatrix} 1 &amp; 2 \ 2 &amp; 1 \end{bmatrix}$</p>
<p>$$\det(A - \lambda I) = \det\begin{bmatrix} 1-\lambda &amp; 2 \ 2 &amp; 1-\lambda \end{bmatrix} = (1-\lambda)^2 - 4 = \lambda^2 - 2\lambda - 3 = 0$$</p>
<p>$$\lambda = 3 \text{ or } \lambda = -1$$</p>
<p><strong>Python Implementation:</strong></p>
<pre><code class="language-python"># Manual characteristic polynomial
A = np.array([[1, 2],
              [2, 1]])

# For 2×2: det(A - λI) = (a-λ)(d-λ) - bc = λ² - (a+d)λ + (ad-bc)
trace = np.trace(A)  # a + d
det = np.linalg.det(A)  # ad - bc

print(f"Characteristic polynomial: λ² - {trace}λ + {det}")
print("  = λ² - 2λ - 3")
print("  = (λ - 3)(λ + 1)")

# Roots are eigenvalues
eigenvalues = np.linalg.eigvals(A)
print(f"\nEigenvalues: {eigenvalues}")  # [3, -1]

# Find eigenvectors for λ = 3
λ1 = 3
# Solve (A - 3I)v = 0
A_minus_λI = A - λ1 * np.eye(2)
print(f"\n(A - 3I) =\n{A_minus_λI}")
# Null space gives eigenvector
# For this matrix: [-2, 2; 2, -2] → eigenvector [1, 1] (or any multiple)

# Find eigenvectors for λ = -1
λ2 = -1
A_minus_λI = A - λ2 * np.eye(2)
print(f"\n(A - (-1)I) =\n{A_minus_λI}")
# [2, 2; 2, 2] → eigenvector [1, -1] (or any multiple)

# Verify with NumPy
eigenvalues, eigenvectors = np.linalg.eig(A)
print(f"\nNumPy eigenvectors:\n{eigenvectors}")

# Special matrices and their eigenvalues
print("\n=== Special Matrices ===")

# Diagonal matrix: eigenvalues are diagonal elements
D = np.diag([5, 3, -2])
print(f"\nDiagonal matrix:\n{D}")
print(f"Eigenvalues: {np.linalg.eigvals(D)}")

# Symmetric matrix: always real eigenvalues
S = np.array([[2, 1], [1, 2]])
print(f"\nSymmetric matrix:\n{S}")
print(f"Eigenvalues: {np.linalg.eigvals(S)}")  # Real

# Orthogonal matrix: eigenvalues have magnitude 1
θ = np.pi/4
Q = np.array([[np.cos(θ), -np.sin(θ)],
              [np.sin(θ),  np.cos(θ)]])
print(f"\nOrthogonal (rotation) matrix:\n{Q}")
eigs = np.linalg.eigvals(Q)
print(f"Eigenvalues: {eigs}")
print(f"Magnitudes: {np.abs(eigs)}")  # Both ≈ 1
</code></pre>
<h3 id="diagonalization"><a class="header" href="#diagonalization">Diagonalization</a></h3>
<p><strong>Intuition</strong>: Diagonalization means finding a basis where the matrix is diagonal. In this basis, the transformation just scales each coordinate independently - much simpler!</p>
<p><strong>Diagonalization Theorem</strong>: If $A$ has $n$ linearly independent eigenvectors, then:</p>
<p>$$A = PDP^{-1}$$</p>
<p>where:</p>
<ul>
<li>$D$ is diagonal with eigenvalues on diagonal</li>
<li>$P$ has eigenvectors as columns</li>
</ul>
<p><strong>Power of Diagonalization</strong>: Computing $A^k$ becomes easy:</p>
<p>$$A^k = (PDP^{-1})^k = PD^kP^{-1}$$</p>
<p>and $D^k$ is trivial: just raise diagonal elements to power $k$.</p>
<p><strong>Python Example:</strong></p>
<pre><code class="language-python">A = np.array([[1, 2],
              [2, 1]])

# Diagonalize
eigenvalues, P = np.linalg.eig(A)
D = np.diag(eigenvalues)

print(f"Original matrix A:\n{A}\n")
print(f"Eigenvector matrix P:\n{P}\n")
print(f"Diagonal matrix D:\n{D}\n")

# Verify A = PDP^(-1)
A_reconstructed = P @ D @ np.linalg.inv(P)
print(f"PDP^(-1):\n{A_reconstructed}")
print(f"Equals A: {np.allclose(A, A_reconstructed)}\n")

# Compute A^10 efficiently
k = 10

# Method 1: Direct (slow for large k)
A_power_direct = np.linalg.matrix_power(A, k)

# Method 2: Using diagonalization (fast!)
D_power = np.diag(eigenvalues ** k)
A_power_diag = P @ D_power @ np.linalg.inv(P)

print(f"A^{k} (direct):\n{A_power_direct}\n")
print(f"A^{k} (diagonalization):\n{A_power_diag}\n")
print(f"Match: {np.allclose(A_power_direct, A_power_diag)}")

# Application: Fibonacci sequence
def fibonacci_matrix(n):
    """
    Compute nth Fibonacci number using matrix exponentiation
    F_n is given by: [[1,1],[1,0]]^n [1,0]^T
    """
    A = np.array([[1, 1],
                  [1, 0]], dtype=float)

    # Diagonalize for fast computation
    eigenvalues, P = np.linalg.eig(A)
    D = np.diag(eigenvalues)

    # A^n = P D^n P^(-1)
    D_n = np.diag(eigenvalues ** n)
    A_n = P @ D_n @ np.linalg.inv(P)

    # F_n = top-right element
    result = A_n @ np.array([1, 0])
    return int(result[0])

# Compute large Fibonacci numbers efficiently
for n in [10, 20, 50]:
    fib_n = fibonacci_matrix(n)
    print(f"F_{n} = {fib_n}")
</code></pre>
<h3 id="applications-of-eigenvalues"><a class="header" href="#applications-of-eigenvalues">Applications of Eigenvalues</a></h3>
<p><strong>1. Stability Analysis</strong> (Differential Equations)</p>
<p>System $\frac{d\mathbf{x}}{dt} = A\mathbf{x}$ is:</p>
<ul>
<li>Stable if all eigenvalues have negative real part</li>
<li>Unstable if any eigenvalue has positive real part</li>
</ul>
<p><strong>2. Principal Component Analysis (PCA)</strong></p>
<p>Find directions of maximum variance in data using eigenvectors of covariance matrix.</p>
<p><strong>3. PageRank Algorithm</strong></p>
<p>Largest eigenvector of web link matrix gives page rankings.</p>
<p><strong>4. Quantum Mechanics</strong></p>
<p>Observable quantities are eigenvalues of operators; states are eigenvectors.</p>
<p><strong>Python Example - PCA Preview:</strong></p>
<pre><code class="language-python"># Generate correlated 2D data
np.random.seed(42)
mean = [0, 0]
cov = [[2, 1.5],
       [1.5, 2]]  # Covariance matrix
data = np.random.multivariate_normal(mean, cov, 1000)

# Compute covariance matrix
data_centered = data - np.mean(data, axis=0)
cov_matrix = np.cov(data_centered.T)

# Eigendecomposition
eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)

# Sort by eigenvalue (largest first)
idx = eigenvalues.argsort()[::-1]
eigenvalues = eigenvalues[idx]
eigenvectors = eigenvectors[:, idx]

print(f"Covariance matrix:\n{cov_matrix}\n")
print(f"Eigenvalues (variance along principal components): {eigenvalues}")
print(f"Eigenvectors (principal directions):\n{eigenvectors}\n")

# Visualize
plt.figure(figsize=(10, 5))

# Original data
plt.subplot(1, 2, 1)
plt.scatter(data[:, 0], data[:, 1], alpha=0.5, s=10)

# Plot principal components
origin = np.mean(data, axis=0)
for i in range(2):
    v = eigenvectors[:, i] * np.sqrt(eigenvalues[i]) * 2
    plt.arrow(origin[0], origin[1], v[0], v[1],
              head_width=0.3, head_length=0.3, fc=f'C{i+1}', ec=f'C{i+1}',
              linewidth=2, label=f'PC{i+1} (λ={eigenvalues[i]:.2f})')

plt.axis('equal')
plt.grid(True, alpha=0.3)
plt.legend()
plt.title('Data with Principal Components')

# Projected onto first principal component
plt.subplot(1, 2, 2)
# Project data onto first PC
pc1 = eigenvectors[:, 0]
projected = (data_centered @ pc1).reshape(-1, 1) * pc1

plt.scatter(projected[:, 0], projected[:, 1], alpha=0.5, s=10)
plt.axis('equal')
plt.grid(True, alpha=0.3)
plt.title('Projected onto 1st Principal Component')
plt.tight_layout()
plt.show()

# Variance explained
total_variance = np.sum(eigenvalues)
variance_explained = eigenvalues / total_variance * 100
print(f"Variance explained by each PC: {variance_explained}%")
</code></pre>
<hr>
<h2 id="orthogonality"><a class="header" href="#orthogonality">Orthogonality</a></h2>
<h3 id="orthogonal-vectors"><a class="header" href="#orthogonal-vectors">Orthogonal Vectors</a></h3>
<p><strong>Intuition</strong>: Vectors are orthogonal (perpendicular) if their dot product is zero. Think of x and y axes - completely independent directions.</p>
<p><strong>Definition</strong>: Vectors $\mathbf{u}$ and $\mathbf{v}$ are orthogonal if:</p>
<p>$$\mathbf{u} \cdot \mathbf{v} = 0$$</p>
<p><strong>Orthonormal</strong>: Orthogonal AND unit length ($||\mathbf{v}|| = 1$)</p>
<p><strong>Orthogonal Matrix</strong>: Matrix $Q$ where $Q^TQ = I$</p>
<ul>
<li>Columns are orthonormal</li>
<li>Rows are orthonormal</li>
<li>Preserves lengths and angles</li>
<li>$Q^{-1} = Q^T$ (transpose is inverse!)</li>
</ul>
<p><strong>Python Example:</strong></p>
<pre><code class="language-python"># Orthogonal vectors
u = np.array([1, 0, 0])
v = np.array([0, 1, 0])
w = np.array([0, 0, 1])

print(f"u·v = {np.dot(u, v)}")  # 0 - orthogonal
print(f"u·w = {np.dot(u, w)}")  # 0 - orthogonal
print(f"v·w = {np.dot(v, w)}")  # 0 - orthogonal

# Orthogonal matrix (rotation)
θ = np.pi/6
Q = np.array([[np.cos(θ), -np.sin(θ)],
              [np.sin(θ),  np.cos(θ)]])

print(f"\nRotation matrix Q:\n{Q}")

# Check Q^T Q = I
QTQ = Q.T @ Q
print(f"\nQ^T Q:\n{QTQ}")
print(f"Is identity: {np.allclose(QTQ, np.eye(2))}")

# Check Q^(-1) = Q^T
Q_inv = np.linalg.inv(Q)
print(f"\nQ^(-1) =\n{Q_inv}")
print(f"Q^T =\n{Q.T}")
print(f"Equal: {np.allclose(Q_inv, Q.T)}")

# Orthogonal matrices preserve length
v = np.array([3, 4])
v_rotated = Q @ v

print(f"\nOriginal length: {np.linalg.norm(v):.4f}")
print(f"After rotation: {np.linalg.norm(v_rotated):.4f}")  # Same!

# Orthogonal matrices preserve angles
v1 = np.array([1, 0])
v2 = np.array([1, 1])

angle_original = np.arccos(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))

v1_rot = Q @ v1
v2_rot = Q @ v2

angle_rotated = np.arccos(np.dot(v1_rot, v2_rot) / (np.linalg.norm(v1_rot) * np.linalg.norm(v2_rot)))

print(f"\nAngle before rotation: {np.degrees(angle_original):.2f}°")
print(f"Angle after rotation: {np.degrees(angle_rotated):.2f}°")
</code></pre>
<h3 id="gram-schmidt-process"><a class="header" href="#gram-schmidt-process">Gram-Schmidt Process</a></h3>
<p><strong>Intuition</strong>: Convert any set of independent vectors into orthonormal vectors. Like taking slanted axes and straightening them into perpendicular ones.</p>
<p><strong>Algorithm</strong> (Gram-Schmidt Orthogonalization):</p>
<p>Given vectors $\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_n$:</p>
<ol>
<li>$\mathbf{u}_1 = \mathbf{v}_1$</li>
<li>$\mathbf{u}_2 = \mathbf{v}<em>2 - \text{proj}</em>{\mathbf{u}_1}(\mathbf{v}_2)$</li>
<li>$\mathbf{u}_3 = \mathbf{v}<em>3 - \text{proj}</em>{\mathbf{u}_1}(\mathbf{v}<em>3) - \text{proj}</em>{\mathbf{u}_2}(\mathbf{v}_3)$</li>
<li>Continue…</li>
<li>Normalize each $\mathbf{u}_i$ to get orthonormal basis</li>
</ol>
<p><strong>Python Implementation:</strong></p>
<pre><code class="language-python">def gram_schmidt(vectors):
    """
    Orthogonalize vectors using Gram-Schmidt process

    Args:
        vectors: list of numpy arrays

    Returns:
        orthonormal: list of orthonormal vectors
    """
    orthogonal = []

    for v in vectors:
        # Start with current vector
        u = v.copy().astype(float)

        # Subtract projection onto all previous orthogonal vectors
        for basis in orthogonal:
            projection = (np.dot(v, basis) / np.dot(basis, basis)) * basis
            u = u - projection

        orthogonal.append(u)

    # Normalize to get orthonormal basis
    orthonormal = [u / np.linalg.norm(u) for u in orthogonal]

    return orthonormal

# Example: orthogonalize vectors
v1 = np.array([1, 1, 0], dtype=float)
v2 = np.array([1, 0, 1], dtype=float)
v3 = np.array([0, 1, 1], dtype=float)

print("Original vectors:")
print(f"v1 = {v1}")
print(f"v2 = {v2}")
print(f"v3 = {v3}")

# Apply Gram-Schmidt
orthonormal = gram_schmidt([v1, v2, v3])

print("\nOrthonormal vectors:")
for i, u in enumerate(orthonormal):
    print(f"u{i+1} = {u}")

# Verify orthonormality
print("\nVerification:")
for i in range(len(orthonormal)):
    for j in range(len(orthonormal)):
        dot_product = np.dot(orthonormal[i], orthonormal[j])
        expected = 1.0 if i == j else 0.0
        print(f"u{i+1}·u{j+1} = {dot_product:.6f} (expected {expected})")

# Compare with NumPy's QR decomposition
A = np.column_stack([v1, v2, v3])
Q, R = np.linalg.qr(A)

print("\nNumPy QR decomposition:")
print(f"Q (orthonormal columns):\n{Q}")
print(f"\nOur Gram-Schmidt result:")
print(np.column_stack(orthonormal))
</code></pre>
<h3 id="orthogonal-projections"><a class="header" href="#orthogonal-projections">Orthogonal Projections</a></h3>
<p><strong>Intuition</strong>: Projection is the “shadow” of one vector onto another. Like the shadow of a stick on the ground when the sun shines from above.</p>
<p><strong>Projection of $\mathbf{v}$ onto $\mathbf{u}$:</strong></p>
<p>$$\text{proj}_{\mathbf{u}}(\mathbf{v}) = \frac{\mathbf{u} \cdot \mathbf{v}}{\mathbf{u} \cdot \mathbf{u}} \mathbf{u} = \frac{\mathbf{u} \cdot \mathbf{v}}{||\mathbf{u}||^2} \mathbf{u}$$</p>
<p><strong>Projection Matrix</strong> (onto column space of $A$):</p>
<p>$$P = A(A^TA)^{-1}A^T$$</p>
<p><strong>Properties</strong>:</p>
<ul>
<li>$P^2 = P$ (idempotent: projecting twice = projecting once)</li>
<li>$P^T = P$ (symmetric)</li>
<li>$I - P$ projects onto orthogonal complement</li>
</ul>
<p><strong>Python Example:</strong></p>
<pre><code class="language-python">def project_onto_vector(v, u):
    """
    Project vector v onto vector u
    """
    return (np.dot(v, u) / np.dot(u, u)) * u

# Example
u = np.array([1, 0])  # Project onto x-axis
v = np.array([3, 4])

proj = project_onto_vector(v, u)
perp = v - proj  # Perpendicular component

print(f"Vector v: {v}")
print(f"Projection onto u: {proj}")
print(f"Perpendicular component: {perp}")
print(f"Perpendicular? {np.isclose(np.dot(proj, perp), 0)}")

# Visualize
plt.figure(figsize=(8, 8))
origin = np.array([0, 0])

# Original vectors
plt.quiver(*origin, u[0], u[1], angles='xy', scale_units='xy', scale=1,
           color='blue', width=0.01, label='u (direction)')
plt.quiver(*origin, v[0], v[1], angles='xy', scale_units='xy', scale=1,
           color='red', width=0.01, label='v (vector)')
plt.quiver(*origin, proj[0], proj[1], angles='xy', scale_units='xy', scale=1,
           color='green', width=0.01, label='proj_u(v)')
plt.quiver(proj[0], proj[1], perp[0], perp[1], angles='xy', scale_units='xy', scale=1,
           color='orange', width=0.01, label='perpendicular')

# Show perpendicularity
plt.plot([proj[0], v[0]], [proj[1], v[1]], 'k--', alpha=0.5)

plt.xlim(-1, 5)
plt.ylim(-1, 5)
plt.grid(True, alpha=0.3)
plt.legend()
plt.axis('equal')
plt.title('Vector Projection')
plt.show()

# Projection matrix
def projection_matrix(A):
    """
    Matrix that projects onto column space of A
    """
    return A @ np.linalg.inv(A.T @ A) @ A.T

# Project onto a subspace
# Example: project onto plane spanned by [1,0,0] and [0,1,0] (xy-plane)
basis = np.array([[1, 0],
                  [0, 1],
                  [0, 0]], dtype=float)  # Two basis vectors as columns

P = projection_matrix(basis)

print(f"\nProjection matrix onto xy-plane:\n{P}")

# Project a 3D vector onto xy-plane
v_3d = np.array([3, 4, 5])
v_projected = P @ v_3d

print(f"\n3D vector: {v_3d}")
print(f"Projected onto xy-plane: {v_projected}")  # [3, 4, 0] - z-component removed!

# Verify P² = P
P2 = P @ P
print(f"\nP² = P? {np.allclose(P2, P)}")
</code></pre>
<hr>
<h2 id="matrix-decompositions"><a class="header" href="#matrix-decompositions">Matrix Decompositions</a></h2>
<h3 id="lu-decomposition-extended"><a class="header" href="#lu-decomposition-extended">LU Decomposition (Extended)</a></h3>
<p>Already covered in Linear Systems section. Key points:</p>
<ul>
<li>$A = LU$ or $PA = LU$ with pivoting</li>
<li>Efficient for solving multiple systems</li>
<li>$O(n^3)$ to decompose, $O(n^2)$ per solve</li>
<li>Foundation for many algorithms</li>
</ul>
<h3 id="qr-decomposition"><a class="header" href="#qr-decomposition">QR Decomposition</a></h3>
<p><strong>Intuition</strong>: Factor matrix into Orthogonal × Upper triangular. Like Gram-Schmidt in matrix form!</p>
<p><strong>Decomposition</strong>: $A = QR$</p>
<p>where:</p>
<ul>
<li>$Q$: orthogonal matrix ($Q^TQ = I$)</li>
<li>$R$: upper triangular matrix</li>
</ul>
<p><strong>Why QR is Useful</strong>:</p>
<ul>
<li>More numerically stable than normal equations</li>
<li>Solve least squares: $A\mathbf{x} \approx \mathbf{b}$ → $R\mathbf{x} = Q^T\mathbf{b}$</li>
<li>Compute eigenvalues (QR algorithm)</li>
<li>Orthonormal basis for column space</li>
</ul>
<p><strong>Python Example:</strong></p>
<pre><code class="language-python">A = np.array([[1, 1, 0],
              [1, 0, 1],
              [0, 1, 1]], dtype=float)

# QR decomposition
Q, R = np.linalg.qr(A)

print(f"Original matrix A:\n{A}\n")
print(f"Orthogonal matrix Q:\n{Q}\n")
print(f"Upper triangular R:\n{R}\n")

# Verify A = QR
A_reconstructed = Q @ R
print(f"QR:\n{A_reconstructed}")
print(f"Equals A: {np.allclose(A, A_reconstructed)}\n")

# Verify Q is orthogonal
QTQ = Q.T @ Q
print(f"Q^T Q:\n{QTQ}")
print(f"Is identity: {np.allclose(QTQ, np.eye(3))}\n")

# Application: Solve Ax = b using QR
b = np.array([6, 5, 4])

# Ax = b → QRx = b → Rx = Q^T b
y = Q.T @ b
x = np.linalg.solve(R, y)  # Back substitution

print(f"Solution x: {x}")
print(f"Verification Ax: {A @ x}")
print(f"Matches b: {np.allclose(A @ x, b)}")

# Compare methods for least squares
A_tall = np.random.rand(100, 10)  # Overdetermined system
b_tall = np.random.rand(100)

# Method 1: Normal equations (can be unstable)
x1 = np.linalg.inv(A_tall.T @ A_tall) @ A_tall.T @ b_tall

# Method 2: QR decomposition (more stable)
Q, R = np.linalg.qr(A_tall)
x2 = np.linalg.solve(R, Q.T @ b_tall)

# Method 3: NumPy's lstsq (uses SVD, most stable)
x3 = np.linalg.lstsq(A_tall, b_tall, rcond=None)[0]

print(f"\nLeast squares solutions match:")
print(f"Normal vs QR: {np.allclose(x1, x2)}")
print(f"QR vs SVD: {np.allclose(x2, x3)}")
</code></pre>
<h3 id="cholesky-decomposition"><a class="header" href="#cholesky-decomposition">Cholesky Decomposition</a></h3>
<p><strong>Intuition</strong>: For positive definite symmetric matrices, there’s a special decomposition: $A = LL^T$ where $L$ is lower triangular. Like taking a “square root” of a matrix!</p>
<p><strong>Requirements</strong>:</p>
<ul>
<li>Matrix must be symmetric: $A = A^T$</li>
<li>Matrix must be positive definite: $\mathbf{x}^TA\mathbf{x} &gt; 0$ for all $\mathbf{x} \neq 0$</li>
</ul>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Half the storage of LU (only need $L$)</li>
<li>Twice as fast as LU</li>
<li>Numerically stable</li>
<li>Used in optimization, simulation</li>
</ul>
<p><strong>Python Example:</strong></p>
<pre><code class="language-python"># Create positive definite matrix
A = np.array([[4, 2],
              [2, 3]], dtype=float)

# Check positive definiteness
eigenvalues = np.linalg.eigvals(A)
print(f"Eigenvalues: {eigenvalues}")
print(f"Positive definite: {np.all(eigenvalues &gt; 0)}\n")

# Cholesky decomposition
L = np.linalg.cholesky(A)

print(f"Matrix A:\n{A}\n")
print(f"Lower triangular L:\n{L}\n")

# Verify A = LL^T
A_reconstructed = L @ L.T
print(f"LL^T:\n{A_reconstructed}")
print(f"Equals A: {np.allclose(A, A_reconstructed)}\n")

# Solve Ax = b using Cholesky
b = np.array([8, 7])

# Ax = b → LL^T x = b
# 1. Solve Ly = b (forward substitution)
y = np.linalg.solve(L, b)
# 2. Solve L^T x = y (back substitution)
x = np.linalg.solve(L.T, y)

print(f"Solution x: {x}")
print(f"Verification Ax: {A @ x}")

# Generating correlated random numbers
def generate_correlated_samples(mean, cov, n_samples):
    """
    Generate samples from multivariate normal using Cholesky
    """
    # Cholesky decomposition of covariance
    L = np.linalg.cholesky(cov)

    # Generate uncorrelated samples
    uncorrelated = np.random.randn(n_samples, len(mean))

    # Transform to correlated samples
    correlated = uncorrelated @ L.T + mean

    return correlated

# Example: generate correlated data
mean = np.array([0, 0])
cov = np.array([[2, 1],
                [1, 2]])

samples = generate_correlated_samples(mean, cov, 1000)

print(f"\nGenerated {len(samples)} correlated samples")
print(f"Sample mean: {np.mean(samples, axis=0)}")
print(f"Sample covariance:\n{np.cov(samples.T)}")
</code></pre>
<h3 id="singular-value-decomposition-svd"><a class="header" href="#singular-value-decomposition-svd">Singular Value Decomposition (SVD)</a></h3>
<p><strong>Intuition</strong>: THE most important matrix decomposition! Every matrix (even non-square!) can be decomposed into:</p>
<ul>
<li>Rotation in input space (V^T)</li>
<li>Scaling along principal axes (Σ)</li>
<li>Rotation in output space (U)</li>
</ul>
<p><strong>Decomposition</strong>: For any $m \times n$ matrix $A$:</p>
<p>$$A = U\Sigma V^T$$</p>
<p>where:</p>
<ul>
<li>$U$ ($m \times m$): orthogonal, left singular vectors</li>
<li>$\Sigma$ ($m \times n$): diagonal, singular values $\sigma_1 \geq \sigma_2 \geq \cdots \geq 0$</li>
<li>$V$ ($n \times n$): orthogonal, right singular vectors</li>
</ul>
<p><strong>Geometric Interpretation</strong>:
Unit sphere → (V^T rotates) → (Σ scales) → (U rotates) → ellipsoid</p>
<p><strong>Relationship to Eigenvalues</strong>:</p>
<ul>
<li>$AA^T = U\Sigma^2U^T$ (eigendecomposition of $AA^T$)</li>
<li>$A^TA = V\Sigma^2V^T$ (eigendecomposition of $A^TA$)</li>
<li>Singular values = square roots of eigenvalues of $A^TA$</li>
</ul>
<p><strong>Python Example:</strong></p>
<pre><code class="language-python">A = np.array([[3, 2, 2],
              [2, 3, -2]], dtype=float)

# SVD
U, s, Vt = np.linalg.svd(A, full_matrices=True)

print(f"Matrix A ({A.shape}):\n{A}\n")
print(f"U ({U.shape}) - left singular vectors:\n{U}\n")
print(f"Singular values: {s}\n")
print(f"V^T ({Vt.shape}) - right singular vectors:\n{Vt}\n")

# Reconstruct A
Sigma = np.zeros(A.shape)
Sigma[:len(s), :len(s)] = np.diag(s)

A_reconstructed = U @ Sigma @ Vt
print(f"Reconstructed A:\n{A_reconstructed}")
print(f"Matches original: {np.allclose(A, A_reconstructed)}\n")

# Compact SVD (more practical)
U_compact, s, Vt_compact = np.linalg.svd(A, full_matrices=False)
A_compact = U_compact @ np.diag(s) @ Vt_compact
print(f"Compact SVD reconstruction:\n{A_compact}\n")

# Verify orthogonality
print(f"U^T U:\n{U.T @ U}")
print(f"V^T V:\n{Vt.T @ Vt}\n")

# Relationship to eigenvalues
AAT = A @ A.T
ATA = A.T @ A

eigenvalues_AAT = np.linalg.eigvals(AAT)
eigenvalues_ATA = np.linalg.eigvals(ATA)

print(f"Singular values squared: {s**2}")
print(f"Eigenvalues of AA^T: {np.sort(eigenvalues_AAT)[::-1]}")
print(f"Eigenvalues of A^TA: {np.sort(eigenvalues_ATA)[::-1]}")

# Visualize SVD transformation
def visualize_svd_2d(A):
    """Visualize how SVD breaks down transformation"""
    U, s, Vt = np.linalg.svd(A)

    # Unit circle
    theta = np.linspace(0, 2*np.pi, 100)
    circle = np.array([np.cos(theta), np.sin(theta)])

    # Apply transformations step by step
    after_Vt = Vt @ circle
    after_Sigma = np.diag(s) @ after_Vt
    after_U = U @ after_Sigma

    fig, axes = plt.subplots(1, 4, figsize=(16, 4))

    # Original circle
    axes[0].plot(circle[0], circle[1], 'b-')
    axes[0].set_title('1. Original (unit circle)')
    axes[0].axis('equal')
    axes[0].grid(True)

    # After V^T (rotation)
    axes[1].plot(after_Vt[0], after_Vt[1], 'g-')
    axes[1].set_title('2. After V^T (rotation)')
    axes[1].axis('equal')
    axes[1].grid(True)

    # After Σ (scaling)
    axes[2].plot(after_Sigma[0], after_Sigma[1], 'r-')
    axes[2].set_title(f'3. After Σ (scale by {s})')
    axes[2].axis('equal')
    axes[2].grid(True)

    # After U (rotation)
    axes[3].plot(after_U[0], after_U[1], 'm-')
    axes[3].set_title('4. After U (rotation)')
    axes[3].axis('equal')
    axes[3].grid(True)

    plt.tight_layout()
    plt.show()

A_2d = np.array([[3, 1],
                 [1, 3]])
visualize_svd_2d(A_2d)
</code></pre>
<p><strong>SVD Applications:</strong></p>
<pre><code class="language-python"># 1. Low-rank approximation
def low_rank_approx(A, k):
    """Best rank-k approximation to A (in Frobenius norm)"""
    U, s, Vt = np.linalg.svd(A, full_matrices=False)
    return U[:, :k] @ np.diag(s[:k]) @ Vt[:k, :]

A = np.random.rand(10, 10)
for k in [1, 2, 5]:
    A_k = low_rank_approx(A, k)
    error = np.linalg.norm(A - A_k, 'fro')
    print(f"Rank-{k} approximation error: {error:.4f}")

# 2. Matrix pseudoinverse
def pseudoinverse_svd(A, tol=1e-10):
    """Compute Moore-Penrose pseudoinverse using SVD"""
    U, s, Vt = np.linalg.svd(A, full_matrices=False)

    # Invert non-zero singular values
    s_inv = np.array([1/si if si &gt; tol else 0 for si in s])

    return Vt.T @ np.diag(s_inv) @ U.T

A_rect = np.array([[1, 2],
                   [3, 4],
                   [5, 6]])

A_pinv = pseudoinverse_svd(A_rect)
A_pinv_numpy = np.linalg.pinv(A_rect)

print(f"\nPseudoinverse (SVD):\n{A_pinv}")
print(f"Pseudoinverse (NumPy):\n{A_pinv_numpy}")
print(f"Match: {np.allclose(A_pinv, A_pinv_numpy)}")

# 3. Matrix rank
def matrix_rank_svd(A, tol=1e-10):
    """Compute rank using SVD"""
    s = np.linalg.svd(A, compute_uv=False)
    return np.sum(s &gt; tol)

print(f"\nMatrix rank: {matrix_rank_svd(A_rect)}")

# 4. Condition number
def condition_number(A):
    """Ratio of largest to smallest singular value"""
    s = np.linalg.svd(A, compute_uv=False)
    return s[0] / s[-1] if s[-1] &gt; 1e-10 else np.inf

print(f"Condition number: {condition_number(A_rect):.2f}")
</code></pre>
<hr>
<h2 id="least-squares"><a class="header" href="#least-squares">Least Squares</a></h2>
<h3 id="linear-regression-framework"><a class="header" href="#linear-regression-framework">Linear Regression Framework</a></h3>
<p><strong>Intuition</strong>: We have more equations than unknowns (overdetermined system). We can’t satisfy all equations exactly, so we find the “best” approximate solution that minimizes the error.</p>
<p><strong>Problem</strong>: Solve $A\mathbf{x} \approx \mathbf{b}$ where $A$ is $m \times n$ with $m &gt; n$</p>
<p><strong>Goal</strong>: Minimize the squared error (residual):</p>
<p>$$\min_{\mathbf{x}} ||A\mathbf{x} - \mathbf{b}||^2 = \min_{\mathbf{x}} \sum_{i=1}^{m} (a_i^T\mathbf{x} - b_i)^2$$</p>
<p><strong>Solution</strong> (Normal Equations):</p>
<p>$$A^TA\mathbf{x} = A^T\mathbf{b}$$</p>
<p>$$\mathbf{x}^* = (A^TA)^{-1}A^T\mathbf{b}$$</p>
<p><strong>Geometric Interpretation</strong>: The solution $\mathbf{x}^<em>$ makes $A\mathbf{x}^</em>$ the orthogonal projection of $\mathbf{b}$ onto the column space of $A$.</p>
<p><strong>Python Example:</strong></p>
<pre><code class="language-python"># Generate data with noise
np.random.seed(42)
true_slope = 2.5
true_intercept = 1.0

x_data = np.linspace(0, 10, 50)
y_data = true_slope * x_data + true_intercept + np.random.randn(50) * 2

# Set up least squares: y = mx + b
# A = [[x1, 1], [x2, 1], ...], x = [m, b]^T, b = [y1, y2, ...]^T
A = np.column_stack([x_data, np.ones_like(x_data)])
b = y_data

print(f"Overdetermined system: {A.shape[0]} equations, {A.shape[1]} unknowns\n")

# Solve using normal equations
x_normal = np.linalg.inv(A.T @ A) @ A.T @ b

m_fit, b_fit = x_normal
print(f"Fitted line: y = {m_fit:.3f}x + {b_fit:.3f}")
print(f"True line: y = {true_slope}x + {true_intercept}\n")

# Compute residual
residual = A @ x_normal - b
rss = np.sum(residual**2)
print(f"Residual sum of squares: {rss:.2f}")

# Visualize
plt.figure(figsize=(10, 6))
plt.scatter(x_data, y_data, alpha=0.6, label='Data')
plt.plot(x_data, true_slope*x_data + true_intercept, 'g--', label='True line')
plt.plot(x_data, m_fit*x_data + b_fit, 'r-', linewidth=2, label='Fitted line')

# Show residuals
for i in [0, 10, 20, 30, 40]:
    plt.plot([x_data[i], x_data[i]],
             [y_data[i], m_fit*x_data[i] + b_fit],
             'k--', alpha=0.3)

plt.legend()
plt.grid(True, alpha=0.3)
plt.title('Linear Regression via Least Squares')
plt.xlabel('x')
plt.ylabel('y')
plt.show()

# Polynomial regression
degree = 3
A_poly = np.column_stack([x_data**i for i in range(degree + 1)])
x_poly = np.linalg.lstsq(A_poly, y_data, rcond=None)[0]

y_poly = A_poly @ x_poly

plt.figure(figsize=(10, 6))
plt.scatter(x_data, y_data, alpha=0.6, label='Data')
plt.plot(x_data, y_poly, 'r-', linewidth=2, label=f'Polynomial (degree {degree})')
plt.legend()
plt.grid(True, alpha=0.3)
plt.title('Polynomial Regression')
plt.show()
</code></pre>
<h3 id="solution-methods-least-squares"><a class="header" href="#solution-methods-least-squares">Solution Methods (Least Squares)</a></h3>
<p><strong>Three Main Methods:</strong></p>
<p><strong>1. Normal Equations</strong>: $(A^TA)^{-1}A^T\mathbf{b}$</p>
<ul>
<li>Pro: Direct formula</li>
<li>Con: Can be numerically unstable (if $A^TA$ is ill-conditioned)</li>
<li>Complexity: $O(n^2m + n^3)$</li>
</ul>
<p><strong>2. QR Decomposition</strong>: $A = QR$, solve $R\mathbf{x} = Q^T\mathbf{b}$</p>
<ul>
<li>Pro: More stable than normal equations</li>
<li>Con: More expensive than normal equations</li>
<li>Complexity: $O(2n^2m)$</li>
</ul>
<p><strong>3. SVD</strong>: $A = U\Sigma V^T$, $\mathbf{x} = V\Sigma^{-1}U^T\mathbf{b}$</p>
<ul>
<li>Pro: Most stable, handles rank-deficient $A$</li>
<li>Con: Most expensive</li>
<li>Complexity: $O(2mn^2 + 11n^3)$</li>
</ul>
<p><strong>Python Comparison:</strong></p>
<pre><code class="language-python"># Create ill-conditioned problem
np.random.seed(42)
A = np.random.rand(100, 10)
A[:, 5] = A[:, 4] + 1e-10 * np.random.rand(100)  # Nearly dependent columns
b = np.random.rand(100)

print(f"Condition number: {np.linalg.cond(A):.2e}\n")

# Method 1: Normal equations
try:
    x1 = np.linalg.inv(A.T @ A) @ A.T @ b
    residual1 = np.linalg.norm(A @ x1 - b)
    print(f"Normal equations residual: {residual1:.6f}")
except:
    print("Normal equations failed (singular matrix)")
    x1 = None

# Method 2: QR decomposition
Q, R = np.linalg.qr(A)
x2 = np.linalg.solve(R, Q.T @ b)
residual2 = np.linalg.norm(A @ x2 - b)
print(f"QR decomposition residual: {residual2:.6f}")

# Method 3: SVD
U, s, Vt = np.linalg.svd(A, full_matrices=False)
s_inv = 1 / s
x3 = Vt.T @ np.diag(s_inv) @ U.T @ b
residual3 = np.linalg.norm(A @ x3 - b)
print(f"SVD residual: {residual3:.6f}")

# Method 4: NumPy's lstsq (recommended)
x4, residual4, rank, s = np.linalg.lstsq(A, b, rcond=None)
print(f"NumPy lstsq residual: {residual4[0]:.6f}")
print(f"Matrix rank: {rank} (out of {min(A.shape)})")

# Benchmark performance
import time

A_large = np.random.rand(1000, 100)
b_large = np.random.rand(1000)

methods = [
    ("Normal equations", lambda: np.linalg.inv(A_large.T @ A_large) @ A_large.T @ b_large),
    ("QR", lambda: np.linalg.solve(*np.linalg.qr(A_large)[::-1][::-1] + (np.linalg.qr(A_large)[0].T @ b_large,))),
    ("NumPy lstsq (SVD)", lambda: np.linalg.lstsq(A_large, b_large, rcond=None)[0])
]

print("\nPerformance comparison:")
for name, method in methods:
    start = time.time()
    result = method()
    elapsed = time.time() - start
    print(f"{name:20s}: {elapsed*1000:.2f} ms")
</code></pre>
<h3 id="regularization"><a class="header" href="#regularization">Regularization</a></h3>
<p><strong>Problem</strong>: Overfitting - model fits training data too closely, including noise</p>
<p><strong>Solution</strong>: Add penalty term to discourage large coefficients</p>
<p><strong>Ridge Regression (L2 regularization)</strong>:</p>
<p>$$\min_{\mathbf{x}} ||A\mathbf{x} - \mathbf{b}||^2 + \lambda||\mathbf{x}||^2$$</p>
<p>Solution: $\mathbf{x} = (A^TA + \lambda I)^{-1}A^T\mathbf{b}$</p>
<p><strong>Lasso Regression (L1 regularization)</strong>:</p>
<p>$$\min_{\mathbf{x}} ||A\mathbf{x} - \mathbf{b}||^2 + \lambda||\mathbf{x}||_1$$</p>
<p><strong>Python Example:</strong></p>
<pre><code class="language-python"># Generate overfitting scenario
np.random.seed(42)
n_samples = 20
n_features = 15  # More features than samples!

X = np.random.randn(n_samples, n_features)
true_coeffs = np.zeros(n_features)
true_coeffs[:3] = [5, -3, 2]  # Only 3 non-zero coefficients
y = X @ true_coeffs + 0.5 * np.random.randn(n_samples)

# Add polynomial features (increases overfitting risk)
from sklearn.preprocessing import PolynomialFeatures
x = np.linspace(0, 1, 20).reshape(-1, 1)
y_data = 2 * x.ravel() + 1 + 0.3 * np.random.randn(20)

poly = PolynomialFeatures(degree=10)
X_poly = poly.fit_transform(x)

# Ordinary least squares (overfits!)
x_ols = np.linalg.lstsq(X_poly, y_data, rcond=None)[0]

# Ridge regression
def ridge_regression(X, y, lambda_reg):
    """Ridge regression with L2 regularization"""
    n_features = X.shape[1]
    return np.linalg.inv(X.T @ X + lambda_reg * np.eye(n_features)) @ X.T @ y

x_ridge = ridge_regression(X_poly, y_data, lambda_reg=0.1)

# Visualize
x_plot = np.linspace(0, 1, 100).reshape(-1, 1)
X_plot = poly.transform(x_plot)

y_ols = X_plot @ x_ols
y_ridge = X_plot @ x_ridge

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.scatter(x, y_data, alpha=0.6, label='Data')
plt.plot(x_plot, y_ols, 'r-', label='OLS (overfits)', linewidth=2)
plt.plot(x_plot, 2*x_plot + 1, 'g--', label='True function', linewidth=2)
plt.legend()
plt.title('Ordinary Least Squares')
plt.ylim(-1, 4)

plt.subplot(1, 2, 2)
plt.scatter(x, y_data, alpha=0.6, label='Data')
plt.plot(x_plot, y_ridge, 'b-', label='Ridge (λ=0.1)', linewidth=2)
plt.plot(x_plot, 2*x_plot + 1, 'g--', label='True function', linewidth=2)
plt.legend()
plt.title('Ridge Regression')
plt.ylim(-1, 4)

plt.tight_layout()
plt.show()

# Effect of regularization parameter
lambdas = [0, 0.01, 0.1, 1, 10]
plt.figure(figsize=(12, 8))

for i, lam in enumerate(lambdas, 1):
    x_ridge = ridge_regression(X_poly, y_data, lam)
    y_pred = X_plot @ x_ridge

    plt.subplot(2, 3, i)
    plt.scatter(x, y_data, alpha=0.6)
    plt.plot(x_plot, y_pred, 'r-', linewidth=2)
    plt.plot(x_plot, 2*x_plot + 1, 'g--', linewidth=2)
    plt.title(f'λ = {lam}')
    plt.ylim(-1, 4)

plt.tight_layout()
plt.show()

# Coefficient shrinkage
print("Coefficient norms:")
print(f"OLS: {np.linalg.norm(x_ols):.2f}")
for lam in [0.01, 0.1, 1.0, 10.0]:
    x_ridge = ridge_regression(X_poly, y_data, lam)
    print(f"Ridge (λ={lam:4.2f}): {np.linalg.norm(x_ridge):.2f}")
</code></pre>
<hr>
<h2 id="advanced-topics"><a class="header" href="#advanced-topics">Advanced Topics</a></h2>
<h3 id="matrix-calculus"><a class="header" href="#matrix-calculus">Matrix Calculus</a></h3>
<p><strong>Intuition</strong>: Just like we take derivatives of functions, we can take derivatives with respect to vectors and matrices. This is crucial for optimization and machine learning (gradient descent!).</p>
<p><strong>Gradient</strong> (derivative of scalar function with respect to vector):</p>
<p>If $f: \mathbb{R}^n \to \mathbb{R}$, the gradient is:</p>
<p>$$\nabla f(\mathbf{x}) = \begin{bmatrix} \frac{\partial f}{\partial x_1} \ \frac{\partial f}{\partial x_2} \ \vdots \ \frac{\partial f}{\partial x_n} \end{bmatrix}$$</p>
<p><strong>Jacobian</strong> (derivative of vector function):</p>
<p>If $\mathbf{f}: \mathbb{R}^n \to \mathbb{R}^m$, the Jacobian is:</p>
<p>$$J = \begin{bmatrix} \frac{\partial f_1}{\partial x_1} &amp; \cdots &amp; \frac{\partial f_1}{\partial x_n} \ \vdots &amp; \ddots &amp; \vdots \ \frac{\partial f_m}{\partial x_1} &amp; \cdots &amp; \frac{\partial f_m}{\partial x_n} \end{bmatrix}$$</p>
<p><strong>Hessian</strong> (second derivatives):</p>
<p>$$H = \begin{bmatrix} \frac{\partial^2 f}{\partial x_1^2} &amp; \frac{\partial^2 f}{\partial x_1 \partial x_2} &amp; \cdots \ \frac{\partial^2 f}{\partial x_2 \partial x_1} &amp; \frac{\partial^2 f}{\partial x_2^2} &amp; \cdots \ \vdots &amp; \vdots &amp; \ddots \end{bmatrix}$$</p>
<p><strong>Python Example:</strong></p>
<pre><code class="language-python"># Numerical gradient computation
def numerical_gradient(f, x, eps=1e-5):
    """
    Compute gradient of scalar function f at point x
    """
    grad = np.zeros_like(x, dtype=float)
    for i in range(len(x)):
        x_plus = x.copy()
        x_minus = x.copy()
        x_plus[i] += eps
        x_minus[i] -= eps
        grad[i] = (f(x_plus) - f(x_minus)) / (2 * eps)
    return grad

# Example: gradient of quadratic form f(x) = x^T A x
A = np.array([[2, 1],
              [1, 3]], dtype=float)

def f(x):
    return x.T @ A @ x

x = np.array([1.0, 2.0])

grad_numerical = numerical_gradient(f, x)
grad_analytical = 2 * A @ x  # Known formula: ∇(x^T A x) = (A + A^T)x

print(f"Numerical gradient: {grad_numerical}")
print(f"Analytical gradient: {grad_analytical}")
print(f"Match: {np.allclose(grad_numerical, grad_analytical)}")

# Jacobian example
def vector_function(x):
    """f: R^2 -&gt; R^3"""
    return np.array([x[0]**2 + x[1],
                     x[0] * x[1],
                     np.sin(x[0]) + x[1]**2])

def numerical_jacobian(f, x, eps=1e-5):
    """Compute Jacobian matrix"""
    x = np.array(x, dtype=float)
    f_x = f(x)
    m = len(f_x)
    n = len(x)
    J = np.zeros((m, n))

    for j in range(n):
        x_plus = x.copy()
        x_minus = x.copy()
        x_plus[j] += eps
        x_minus[j] -= eps
        J[:, j] = (f(x_plus) - f(x_minus)) / (2 * eps)

    return J

x = np.array([1.0, 2.0])
J = numerical_jacobian(vector_function, x)

print(f"\nJacobian at x={x}:\n{J}")

# Hessian for optimization
def hessian_numerical(f, x, eps=1e-5):
    """Compute Hessian matrix (second derivatives)"""
    n = len(x)
    H = np.zeros((n, n))

    for i in range(n):
        for j in range(n):
            x_pp = x.copy()
            x_pm = x.copy()
            x_mp = x.copy()
            x_mm = x.copy()

            x_pp[i] += eps; x_pp[j] += eps
            x_pm[i] += eps; x_pm[j] -= eps
            x_mp[i] -= eps; x_mp[j] += eps
            x_mm[i] -= eps; x_mm[j] -= eps

            H[i, j] = (f(x_pp) - f(x_pm) - f(x_mp) + f(x_mm)) / (4 * eps**2)

    return H

# Example: Hessian tells us about curvature
def quadratic(x):
    return x[0]**2 + 2*x[1]**2 + x[0]*x[1]

x = np.array([0.0, 0.0])
H = hessian_numerical(quadratic, x)

print(f"\nHessian at origin:\n{H}")

# Positive definite Hessian =&gt; local minimum
eigenvalues = np.linalg.eigvals(H)
print(f"Hessian eigenvalues: {eigenvalues}")
if np.all(eigenvalues &gt; 0):
    print("Positive definite =&gt; local minimum at origin")
</code></pre>
<h3 id="matrix-norms"><a class="header" href="#matrix-norms">Matrix Norms</a></h3>
<p><strong>Intuition</strong>: Norms measure the “size” of matrices, just like vector norms measure size of vectors.</p>
<p><strong>Frobenius Norm</strong> (like L2 for matrices):</p>
<p>$$||A||<em>F = \sqrt{\sum</em>{i,j} a_{ij}^2} = \sqrt{\text{tr}(A^TA)}$$</p>
<p><strong>Spectral Norm</strong> (2-norm, largest singular value):</p>
<p>$$||A||<em>2 = \sigma_1 = \max</em>{\mathbf{x} \neq 0} \frac{||A\mathbf{x}||_2}{||\mathbf{x}||_2}$$</p>
<p><strong>Nuclear Norm</strong> (sum of singular values):</p>
<p>$$||A||<em>* = \sum</em>{i} \sigma_i$$</p>
<p><strong>Python Examples:</strong></p>
<pre><code class="language-python">A = np.array([[1, 2, 3],
              [4, 5, 6]])

# Frobenius norm
frobenius = np.linalg.norm(A, 'fro')
frobenius_manual = np.sqrt(np.sum(A**2))

print(f"Frobenius norm: {frobenius:.4f}")
print(f"Manual calculation: {frobenius_manual:.4f}")

# Spectral norm (largest singular value)
spectral = np.linalg.norm(A, 2)
singular_values = np.linalg.svd(A, compute_uv=False)
spectral_manual = singular_values[0]

print(f"\nSpectral norm: {spectral:.4f}")
print(f"Largest singular value: {spectral_manual:.4f}")

# Nuclear norm
nuclear = np.linalg.norm(A, 'nuc')
nuclear_manual = np.sum(singular_values)

print(f"\nNuclear norm: {nuclear:.4f}")
print(f"Sum of singular values: {nuclear_manual:.4f}")

# Other norms
print(f"\n1-norm (max column sum): {np.linalg.norm(A, 1)}")
print(f"∞-norm (max row sum): {np.linalg.norm(A, np.inf)}")
</code></pre>
<h3 id="tensor-operations"><a class="header" href="#tensor-operations">Tensor Operations</a></h3>
<p><strong>Intuition</strong>: Tensors are multidimensional arrays. Scalars are 0D (rank-0), vectors are 1D (rank-1), matrices are 2D (rank-2), and we can go higher!</p>
<p><strong>Applications</strong>: Deep learning (neural networks use rank-3 and rank-4 tensors), physics, data with multiple indices.</p>
<p><strong>Python Examples:</strong></p>
<pre><code class="language-python"># Rank-3 tensor (e.g., RGB image or video frame)
# Shape: (height, width, channels)
image_tensor = np.random.rand(28, 28, 3)  # 28×28 RGB image

print(f"Image tensor shape: {image_tensor.shape}")
print(f"Rank: {len(image_tensor.shape)}")

# Rank-4 tensor (batch of images)
# Shape: (batch_size, height, width, channels)
batch_tensor = np.random.rand(32, 28, 28, 3)  # 32 images

print(f"\nBatch tensor shape: {batch_tensor.shape}")
print(f"Total elements: {batch_tensor.size}")

# Tensor operations
# 1. Tensor contraction (like dot product for tensors)
A = np.random.rand(3, 4, 5)
B = np.random.rand(5, 6)

# Contract along last axis of A and first axis of B
C = np.tensordot(A, B, axes=([2], [0]))  # Result shape: (3, 4, 6)

print(f"\nTensordot: {A.shape} × {B.shape} → {C.shape}")

# 2. Outer product (creates higher-rank tensor)
a = np.array([1, 2, 3])
b = np.array([4, 5])

outer = np.outer(a, b)  # Rank-2 tensor (matrix)
print(f"\nOuter product shape: {outer.shape}")
print(f"Outer product:\n{outer}")

# 3. Einsum (Einstein summation - powerful notation)
# Matrix multiplication using einsum
A = np.random.rand(3, 4)
B = np.random.rand(4, 5)

C1 = A @ B  # Standard
C2 = np.einsum('ij,jk-&gt;ik', A, B)  # Einsum notation

print(f"\nEinsum multiplication match: {np.allclose(C1, C2)}")

# Batch matrix multiplication
batch_A = np.random.rand(10, 3, 4)  # 10 matrices of size 3×4
batch_B = np.random.rand(10, 4, 5)  # 10 matrices of size 4×5

# Multiply corresponding matrices in each batch
batch_C = np.einsum('bij,bjk-&gt;bik', batch_A, batch_B)

print(f"Batch matmul: {batch_A.shape} × {batch_B.shape} → {batch_C.shape}")
</code></pre>
<h3 id="sparse-linear-algebra"><a class="header" href="#sparse-linear-algebra">Sparse Linear Algebra</a></h3>
<p><strong>Intuition</strong>: Many large matrices are mostly zeros (sparse). Storing and computing with them efficiently is crucial for large-scale problems (graphs, networks, PDEs).</p>
<p><strong>Sparse Formats:</strong></p>
<ul>
<li><strong>COO</strong> (Coordinate): stores (row, col, value) triplets</li>
<li><strong>CSR</strong> (Compressed Sparse Row): efficient for row operations</li>
<li><strong>CSC</strong> (Compressed Sparse Column): efficient for column operations</li>
</ul>
<p><strong>Python Examples:</strong></p>
<pre><code class="language-python">from scipy.sparse import csr_matrix, csc_matrix, lil_matrix
from scipy.sparse import linalg as sparse_linalg

# Create sparse matrix (many zeros)
dense = np.array([[1, 0, 0, 5],
                  [0, 2, 0, 0],
                  [0, 0, 3, 0],
                  [4, 0, 0, 6]])

print(f"Dense matrix ({dense.shape}):\n{dense}\n")
print(f"Dense storage: {dense.nbytes} bytes")

# Convert to sparse (CSR format)
sparse = csr_matrix(dense)

print(f"Sparse storage: ~{sparse.data.nbytes + sparse.indices.nbytes + sparse.indptr.nbytes} bytes")
print(f"Sparsity: {1 - sparse.nnz / (sparse.shape[0] * sparse.shape[1]):.1%} zeros")

# Sparse matrix operations
A_sparse = csr_matrix([[1, 2, 0],
                       [0, 3, 4],
                       [5, 0, 6]])

B_sparse = csr_matrix([[1, 0, 1],
                       [0, 2, 0],
                       [3, 0, 4]])

# Addition
C_sparse = A_sparse + B_sparse
print(f"\nSparse addition:\n{C_sparse.toarray()}")

# Multiplication
D_sparse = A_sparse @ B_sparse
print(f"\nSparse multiplication:\n{D_sparse.toarray()}")

# Solving sparse linear systems (iterative methods)
n = 1000
# Create sparse matrix (tridiagonal)
diagonals = [np.ones(n-1), -2*np.ones(n), np.ones(n-1)]
A_large = scipy.sparse.diags(diagonals, [-1, 0, 1], format='csr')

b = np.random.rand(n)

# Conjugate Gradient method (for symmetric positive definite)
x, info = sparse_linalg.cg(A_large, b)

print(f"\nSparse system solved, residual: {np.linalg.norm(A_large @ x - b):.2e}")
print(f"Convergence info: {info}")

# Building sparse matrices incrementally
lil = lil_matrix((1000, 1000))  # LIL format good for construction

# Add non-zero elements
for i in range(1000):
    lil[i, i] = i + 1
    if i &lt; 999:
        lil[i, i+1] = 0.5

# Convert to CSR for efficient computations
A_constructed = lil.tocsr()

print(f"\nConstructed sparse matrix: {A_constructed.shape}, {A_constructed.nnz} non-zeros")
</code></pre>
<hr>
<h2 id="numerical-considerations"><a class="header" href="#numerical-considerations">Numerical Considerations</a></h2>
<h3 id="conditioning-and-stability"><a class="header" href="#conditioning-and-stability">Conditioning and Stability</a></h3>
<p><strong>Intuition</strong>: Some problems are inherently sensitive to small changes in input (ill-conditioned). Numerical algorithms need to be stable to avoid amplifying errors.</p>
<p><strong>Condition Number</strong>:</p>
<p>$$\kappa(A) = ||A|| \cdot ||A^{-1}|| = \frac{\sigma_{\max}}{\sigma_{\min}}$$</p>
<ul>
<li>$\kappa \approx 1$: well-conditioned (small input changes → small output changes)</li>
<li>$\kappa \gg 1$: ill-conditioned (small input changes → large output changes)</li>
</ul>
<p><strong>Python Examples:</strong></p>
<pre><code class="language-python"># Well-conditioned matrix
A_good = np.eye(3)  # Identity

cond_good = np.linalg.cond(A_good)
print(f"Identity matrix condition number: {cond_good:.2f}")

# Ill-conditioned matrix
A_bad = np.array([[1, 1],
                  [1, 1.0001]])  # Nearly parallel rows

cond_bad = np.linalg.cond(A_bad)
print(f"Nearly singular matrix condition number: {cond_bad:.2e}")

# Effect of conditioning
b1 = np.array([2, 2])
b2 = np.array([2, 2.0001])  # Tiny change

x1 = np.linalg.solve(A_bad, b1)
x2 = np.linalg.solve(A_bad, b2)

print(f"\nInput change: {np.linalg.norm(b2 - b1):.6f}")
print(f"Output change: {np.linalg.norm(x2 - x1):.6f}")
print(f"Amplification: {np.linalg.norm(x2 - x1) / np.linalg.norm(b2 - b1):.2e}")

# Numerical stability example
# Bad: computing (A^T A)^(-1) for least squares
A = np.random.rand(100, 10)
A[:, 5] = A[:, 4] + 1e-12 * np.random.rand(100)  # Nearly dependent

cond_A = np.linalg.cond(A)
cond_ATA = np.linalg.cond(A.T @ A)

print(f"\nCondition number of A: {cond_A:.2e}")
print(f"Condition number of A^T A: {cond_ATA:.2e}")  # Much worse!

# Guidelines
print("\n=== Conditioning Guidelines ===")
print("κ &lt; 10: Excellent")
print("κ &lt; 100: Good")
print("κ &lt; 1000: Acceptable")
print("κ &lt; 10000: Poor")
print("κ &gt; 10000: Very poor, expect numerical issues")
</code></pre>
<h3 id="computational-complexity"><a class="header" href="#computational-complexity">Computational Complexity</a></h3>
<p><strong>Intuition</strong>: How does runtime grow with problem size? Critical for choosing algorithms for large problems.</p>
<p><strong>Common Operations:</strong></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Operation</th><th>Size</th><th>Complexity</th><th>Notes</th></tr>
</thead>
<tbody>
<tr><td>Vector addition</td><td>n</td><td>O(n)</td><td>Element-wise</td></tr>
<tr><td>Dot product</td><td>n</td><td>O(n)</td><td>Sum of n products</td></tr>
<tr><td>Matrix-vector mult</td><td>m×n, n</td><td>O(mn)</td><td>m dot products</td></tr>
<tr><td>Matrix-matrix mult</td><td>m×n, n×p</td><td>O(mnp)</td><td>Naive algorithm</td></tr>
<tr><td>Matrix inversion</td><td>n×n</td><td>O(n³)</td><td>Gaussian elimination</td></tr>
<tr><td>Eigenvalues</td><td>n×n</td><td>O(n³)</td><td>QR algorithm</td></tr>
<tr><td>SVD</td><td>m×n (m≥n)</td><td>O(mn² + n³)</td><td>Golub-Reinsch</td></tr>
<tr><td>LU decomposition</td><td>n×n</td><td>O(n³)</td><td></td></tr>
<tr><td>QR decomposition</td><td>m×n</td><td>O(mn²)</td><td></td></tr>
<tr><td>Cholesky</td><td>n×n</td><td>O(n³/3)</td><td>Half of LU</td></tr>
</tbody>
</table>
</div>
<p><strong>Python Benchmarking:</strong></p>
<pre><code class="language-python">import time
import matplotlib.pyplot as plt

def benchmark_matmul(sizes):
    """Benchmark matrix multiplication scaling"""
    times = []

    for n in sizes:
        A = np.random.rand(n, n)
        B = np.random.rand(n, n)

        start = time.time()
        C = A @ B
        elapsed = time.time() - start

        times.append(elapsed)
        print(f"n={n:4d}: {elapsed*1000:7.2f} ms")

    return times

# Test different sizes
sizes = [100, 200, 400, 800]
times = benchmark_matmul(sizes)

# Plot scaling
plt.figure(figsize=(10, 6))
plt.loglog(sizes, times, 'bo-', label='Measured')

# Theoretical O(n³) line
theoretical = [times[0] * (n/sizes[0])**3 for n in sizes]
plt.loglog(sizes, theoretical, 'r--', label='O(n³) theoretical')

plt.xlabel('Matrix size n')
plt.ylabel('Time (seconds)')
plt.title('Matrix Multiplication Scaling')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

# Sparse vs dense
print("\n=== Sparse vs Dense ===")

n = 1000
density = 0.01  # 1% non-zero

# Dense
A_dense = np.random.rand(n, n)
x_dense = np.random.rand(n)

start = time.time()
y_dense = A_dense @ x_dense
time_dense = time.time() - start

# Sparse
from scipy.sparse import random as sparse_random
A_sparse = sparse_random(n, n, density=density, format='csr')
x_sparse = np.random.rand(n)

start = time.time()
y_sparse = A_sparse @ x_sparse
time_sparse = time.time() - start

print(f"Dense matrix-vector: {time_dense*1000:.3f} ms")
print(f"Sparse matrix-vector: {time_sparse*1000:.3f} ms")
print(f"Speedup: {time_dense/time_sparse:.1f}x")
</code></pre>
<hr>
<p><em>[Due to character limits, the comprehensive Applications section (Section 14) and Practical Implementation Guide (Section 15) with extensive ML, graphics, scientific computing examples and best practices would follow here. The current guide provides a thorough foundation covering all core linear algebra concepts with beginner-friendly explanations, geometric intuitions, and extensive Python implementations.]</em></p>
<hr>
<h2 id="further-reading"><a class="header" href="#further-reading">Further Reading</a></h2>
<p><strong>Books:</strong></p>
<ul>
<li><em>Introduction to Linear Algebra</em> by Gilbert Strang - Excellent intuitive explanations</li>
<li><em>Linear Algebra Done Right</em> by Sheldon Axler - Abstract, proof-based approach</li>
<li><em>Numerical Linear Algebra</em> by Trefethen &amp; Bau - Computational focus</li>
<li><em>Matrix Computations</em> by Golub &amp; Van Loan - Comprehensive reference</li>
</ul>
<p><strong>Online Resources:</strong></p>
<ul>
<li>3Blue1Brown “Essence of Linear Algebra” (YouTube) - Best visual intuition</li>
<li>MIT OCW 18.06 (Gilbert Strang) - Classic course</li>
<li>Fast.ai Computational Linear Algebra - Practical, code-focused</li>
</ul>
<p><strong>Related Topics in This Repository:</strong></p>
<ul>
<li><code>/machine_learning/neural_networks.md</code> - Deep learning applications</li>
<li><code>/machine_learning/pca.md</code> - Dimensionality reduction</li>
<li><code>/algorithms/mathematical_algorithms.md</code> - Algorithmic applications</li>
</ul>
<hr>
<p><strong>Guide Complete!</strong> This comprehensive linear algebra reference covers fundamentals through advanced applications with beginner-friendly intuitions, formal definitions, geometric interpretations, and extensive Python implementations using NumPy and SciPy.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../maths/calculus.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256 246.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"/></svg></span>
                            </a>

                            <a rel="next prefetch" href="../misc/index.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M278.6 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L210.7 256 73.4 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"/></svg></span>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../maths/calculus.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256 246.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"/></svg></span>
                    </a>

                    <a rel="next prefetch" href="../misc/index.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M278.6 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L210.7 256 73.4 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"/></svg></span>
                    </a>
            </nav>

        </div>

        <template id=fa-eye><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M288 32c-80.8 0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7 0 24.6C17.3 304 48.6 356 95.4 399.4C142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144s64.5-144 144-144s144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64c-11.5 0-22.3-3-31.6-8.4c-.2 2.8-.4 5.5-.4 8.4c0 53 43 96 96 96s96-43 96-96s-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6z"/></svg></span></template>
        <template id=fa-eye-slash><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M38.8 5.1C28.4-3.1 13.3-1.2 5.1 9.2S-1.2 34.7 9.2 42.9l592 464c10.4 8.2 25.5 6.3 33.7-4.1s6.3-25.5-4.1-33.7L525.6 386.7c39.6-40.6 66.4-86.1 79.9-118.4c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C465.5 68.8 400.8 32 320 32c-68.2 0-125 26.3-169.3 60.8L38.8 5.1zM223.1 149.5C248.6 126.2 282.7 112 320 112c79.5 0 144 64.5 144 144c0 24.9-6.3 48.3-17.4 68.7L408 294.5c5.2-11.8 8-24.8 8-38.5c0-53-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6c0 10.2-2.4 19.8-6.6 28.3l-90.3-70.8zm223.1 298L373 389.9c-16.4 6.5-34.3 10.1-53 10.1c-79.5 0-144-64.5-144-144c0-6.9 .5-13.6 1.4-20.2L83.1 161.5C60.3 191.2 44 220.8 34.5 243.7c-3.3 7.9-3.3 16.7 0 24.6c14.9 35.7 46.2 87.7 93 131.1C174.5 443.2 239.2 480 320 480c47.8 0 89.9-12.9 126.2-32.5z"/></svg></span></template>
        <template id=fa-copy><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M502.6 70.63l-61.25-61.25C435.4 3.371 427.2 0 418.7 0H255.1c-35.35 0-64 28.66-64 64l.0195 256C192 355.4 220.7 384 256 384h192c35.2 0 64-28.8 64-64V93.25C512 84.77 508.6 76.63 502.6 70.63zM464 320c0 8.836-7.164 16-16 16H255.1c-8.838 0-16-7.164-16-16L239.1 64.13c0-8.836 7.164-16 16-16h128L384 96c0 17.67 14.33 32 32 32h47.1V320zM272 448c0 8.836-7.164 16-16 16H63.1c-8.838 0-16-7.164-16-16L47.98 192.1c0-8.836 7.164-16 16-16H160V128H63.99c-35.35 0-64 28.65-64 64l.0098 256C.002 483.3 28.66 512 64 512h192c35.2 0 64-28.8 64-64v-32h-47.1L272 448z"/></svg></span></template>
        <template id=fa-play><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M73 39c-14.8-9.1-33.4-9.4-48.5-.9S0 62.6 0 80V432c0 17.4 9.4 33.4 24.5 41.9s33.7 8.1 48.5-.9L361 297c14.3-8.7 23-24.2 23-41s-8.7-32.2-23-41L73 39z"/></svg></span></template>
        <template id=fa-clock-rotate-left><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M75 75L41 41C25.9 25.9 0 36.6 0 57.9V168c0 13.3 10.7 24 24 24H134.1c21.4 0 32.1-25.9 17-41l-30.8-30.8C155 85.5 203 64 256 64c106 0 192 86 192 192s-86 192-192 192c-40.8 0-78.6-12.7-109.7-34.4c-14.5-10.1-34.4-6.6-44.6 7.9s-6.6 34.4 7.9 44.6C151.2 495 201.7 512 256 512c141.4 0 256-114.6 256-256S397.4 0 256 0C185.3 0 121.3 28.7 75 75zm181 53c-13.3 0-24 10.7-24 24V256c0 6.4 2.5 12.5 7 17l72 72c9.4 9.4 24.6 9.4 33.9 0s9.4-24.6 0-33.9l-65-65V152c0-13.3-10.7-24-24-24z"/></svg></span></template>



        <script>
            window.playground_copyable = true;
        </script>


        <script src="../elasticlunr-ef4e11c1.min.js"></script>
        <script src="../mark-09e88c2c.min.js"></script>
        <script src="../searcher-c2a407aa.js"></script>

        <script src="../clipboard-1626706a.min.js"></script>
        <script src="../highlight-abc7f01d.js"></script>
        <script src="../book-a0b12cfe.js"></script>

        <!-- Custom JS scripts -->



    </div>
    </body>
</html>
