<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Phi - My Notes</title>


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="../highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="../tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "../";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "../searchindex.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../toc.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="../toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">My Notes</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <i class="fa fa-spinner fa-spin"></i>
                            </div>
                        </div>
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="microsoft-phi-models"><a class="header" href="#microsoft-phi-models">Microsoft Phi Models</a></h1>
<h2 id="overview"><a class="header" href="#overview">Overview</a></h2>
<p>Microsoft's Phi model family represents a series of Small Language Models (SLMs) that deliver strong performance relative to their size, particularly excelling in reasoning-focused tasks. The Phi models are distinguished by their focus on data quality, strategic use of synthetic data, and efficient architecture that enables deployment on edge devices and local environments.</p>
<p><strong>Key Characteristics:</strong></p>
<ul>
<li>Small model sizes (3.8B to 14B parameters)</li>
<li>Strong reasoning capabilities despite compact size</li>
<li>Open source under MIT license</li>
<li>Optimized for on-device deployment</li>
<li>No cloud connectivity required for inference</li>
</ul>
<h2 id="model-family"><a class="header" href="#model-family">Model Family</a></h2>
<h3 id="phi-4-latest---december-2024"><a class="header" href="#phi-4-latest---december-2024">Phi-4 (Latest - December 2024)</a></h3>
<p><strong>Phi-4 (14B parameters)</strong></p>
<ul>
<li>Architecture: Decoder-only transformer</li>
<li>Parameters: 14 billion</li>
<li>Default context length: 4096 tokens</li>
<li>Extended context: 16K tokens (during midtraining)</li>
<li>Focus: Complex reasoning and mathematical tasks</li>
<li>Training: Centrally focused on data quality with strategic synthetic data incorporation</li>
<li>Performance: Strong performance on reasoning benchmarks relative to size</li>
</ul>
<p><strong>Phi-4-mini (3.8B parameters)</strong></p>
<ul>
<li>Dense, decoder-only transformer</li>
<li>Grouped-query attention mechanism</li>
<li>Vocabulary size: 200,000 tokens</li>
<li>Shared input-output embeddings</li>
<li>Optimized for: Speed and efficiency</li>
<li>Ideal for: Resource-constrained environments</li>
</ul>
<p><strong>Phi-4-multimodal (5.6B parameters)</strong></p>
<ul>
<li>Unified architecture integrating: Speech, Vision, Text</li>
<li>Top performer on Huggingface OpenASR leaderboard (WER: 6.14% as of Feb 2025)</li>
<li>Previous best: 6.5%</li>
<li>Use cases: Multi-modal applications requiring speech and vision understanding</li>
</ul>
<h3 id="phi-3-family"><a class="header" href="#phi-3-family">Phi-3 Family</a></h3>
<p><strong>Phi-3-mini (3.8B parameters)</strong></p>
<ul>
<li>Baseline small model</li>
<li>Optimized for mobile and edge deployment</li>
<li>Capable of running on phones</li>
</ul>
<p><strong>Phi-3-small</strong></p>
<ul>
<li>Hybrid attention mechanism:
<ul>
<li>Alternating dense attention layers</li>
<li>Blocksparse attention layers</li>
</ul>
</li>
<li>Optimizes KV cache savings</li>
<li>Maintains long context retrieval performance</li>
</ul>
<p><strong>Phi-3-medium (14B parameters)</strong></p>
<ul>
<li>Same tokenizer and architecture as Phi-3-mini</li>
<li>Architecture specs:
<ul>
<li>40 attention heads</li>
<li>40 layers</li>
<li>Embedding dimension: 5120</li>
</ul>
</li>
<li>Enhanced capacity for complex tasks</li>
</ul>
<p><strong>Phi-3-MoE (Mixture of Experts)</strong></p>
<ul>
<li>Activated parameters: 6.6B</li>
<li>Total parameters: 42B</li>
<li>Routing: Top-2 among 16 expert networks</li>
<li>Expert architecture: Separate GLU networks</li>
<li>Efficiency: Sparse activation enables large capacity with moderate compute</li>
</ul>
<h2 id="architecture-details"><a class="header" href="#architecture-details">Architecture Details</a></h2>
<h3 id="core-architecture"><a class="header" href="#core-architecture">Core Architecture</a></h3>
<pre><code>Model Type: Decoder-only Transformer
Training Recipe:
├── High-quality curated data
├── Strategic synthetic data generation
├── Multi-stage training curriculum
└── Advanced post-training techniques

Key Features:
├── Grouped Query Attention (GQA)
├── Efficient KV cache management
├── Optimized tokenizer (200K vocabulary for Phi-4-mini)
└── Shared input-output embeddings
</code></pre>
<h3 id="attention-mechanisms"><a class="header" href="#attention-mechanisms">Attention Mechanisms</a></h3>
<p><strong>Standard Dense Attention</strong> (Phi-4, Phi-3-mini, Phi-3-medium)</p>
<ul>
<li>Full attention across all positions</li>
<li>Standard transformer architecture</li>
<li>Grouped-query attention for efficiency</li>
</ul>
<p><strong>Hybrid Attention</strong> (Phi-3-small)</p>
<ul>
<li>Alternates between dense and blocksparse layers</li>
<li>Reduces memory footprint</li>
<li>Maintains performance on long sequences</li>
</ul>
<p><strong>MoE Architecture</strong> (Phi-3-MoE)</p>
<ul>
<li>16 expert networks with top-2 routing</li>
<li>Each token processed by 2 of 16 experts</li>
<li>Sparse activation reduces compute requirements</li>
</ul>
<h2 id="fine-tuning"><a class="header" href="#fine-tuning">Fine-Tuning</a></h2>
<h3 id="when-to-fine-tune"><a class="header" href="#when-to-fine-tune">When to Fine-Tune</a></h3>
<p>Fine-tune Phi models when:</p>
<ol>
<li>Domain-specific language or terminology is required</li>
<li>Task-specific behavior needs optimization</li>
<li>Custom instruction following is needed</li>
<li>Adapting to proprietary data or workflows</li>
<li>Improving performance on specific benchmark tasks</li>
</ol>
<h3 id="fine-tuning-approaches"><a class="header" href="#fine-tuning-approaches">Fine-Tuning Approaches</a></h3>
<h4 id="1-full-fine-tuning"><a class="header" href="#1-full-fine-tuning">1. Full Fine-Tuning</a></h4>
<ul>
<li>Updates all model parameters</li>
<li>Highest accuracy potential</li>
<li>Requires significant compute resources</li>
<li>Memory intensive</li>
</ul>
<h4 id="2-lora-low-rank-adaptation"><a class="header" href="#2-lora-low-rank-adaptation">2. LoRA (Low-Rank Adaptation)</a></h4>
<ul>
<li>Adds trainable low-rank matrices to attention layers</li>
<li>Freezes base model weights</li>
<li>Memory efficient</li>
<li>Recommended approach for most use cases</li>
</ul>
<h4 id="3-qlora-quantized-lora"><a class="header" href="#3-qlora-quantized-lora">3. QLoRA (Quantized LoRA)</a></h4>
<ul>
<li>Combines 4-bit quantization with LoRA</li>
<li>Quantizes base model to 4-bit</li>
<li>Trains only LoRA adapters in higher precision</li>
<li>Minimal memory footprint</li>
<li>Ideal for consumer GPUs</li>
</ul>
<h3 id="lora-configuration-best-practices"><a class="header" href="#lora-configuration-best-practices">LoRA Configuration Best Practices</a></h3>
<h4 id="rank-and-alpha-settings"><a class="header" href="#rank-and-alpha-settings">Rank and Alpha Settings</a></h4>
<pre><code class="language-python">from peft import LoraConfig, get_peft_model

lora_config = LoraConfig(
    r=16,                    # LoRA rank (8-16 is sufficient baseline)
    lora_alpha=16,           # Alpha = rank for small datasets
    target_modules=[
        "q_proj",            # Query projection
        "k_proj",            # Key projection
        "v_proj",            # Value projection
        "o_proj",            # Output projection
        "gate_proj",         # MLP gate
        "down_proj",         # MLP down
        "up_proj"            # MLP up
    ],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)
</code></pre>
<p><strong>Key Guidelines:</strong></p>
<ul>
<li><strong>Rank</strong>: 8-16 is sufficient for most tasks (higher ranks not necessarily better)</li>
<li><strong>Alpha</strong>: Set <code>alpha = rank</code> for small datasets</li>
<li><strong>Avoid</strong>: Using <code>2*rank</code> or <code>4*rank</code> on small datasets (often unstable)</li>
<li><strong>Target Modules</strong>: Include all attention and MLP projection layers</li>
</ul>
<h4 id="phi-2-specific-configuration"><a class="header" href="#phi-2-specific-configuration">Phi-2 Specific Configuration</a></h4>
<pre><code class="language-python"># Phi-2 uses Wqkv instead of separate q/k/v projections
lora_config_phi2 = LoraConfig(
    r=16,
    lora_alpha=16,
    target_modules=["Wqkv", "out_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)
</code></pre>
<h3 id="training-hyperparameters"><a class="header" href="#training-hyperparameters">Training Hyperparameters</a></h3>
<h4 id="learning-rate"><a class="header" href="#learning-rate">Learning Rate</a></h4>
<pre><code class="language-python">from transformers import TrainingArguments

training_args = TrainingArguments(
    learning_rate=2e-5,              # Start conservative
    lr_scheduler_type="constant",     # Constant schedule works well
    warmup_steps=100,
    max_steps=1000,
    # Alternative learning rates to try:
    # 5e-5: More aggressive
    # 8e-4: Maximum recommended for LoRA
)
</code></pre>
<p><strong>Guidelines:</strong></p>
<ul>
<li><strong>DO NOT</strong> use high learning rates (1e-3, 2e-4) with LoRA</li>
<li><strong>Recommended range</strong>: 2e-5 to 8e-4</li>
<li><strong>Start with</strong>: 2e-5 or 5e-5 for safety</li>
<li><strong>Schedule</strong>: Constant learning rate (per QLoRA author Tim Dettmers)</li>
<li><strong>Warmup</strong>: 100-500 steps helps stabilization</li>
</ul>
<h4 id="precision-and-memory-management"><a class="header" href="#precision-and-memory-management">Precision and Memory Management</a></h4>
<pre><code class="language-python">from transformers import TrainingArguments, BitsAndBytesConfig
import torch

# Use bfloat16 for training (NOT fp16)
training_args = TrainingArguments(
    bf16=True,                        # Use bfloat16
    fp16=False,                       # Avoid fp16 (causes NaN errors)
    gradient_checkpointing=True,      # Reduce memory usage
    gradient_accumulation_steps=4,    # Effective batch size = batch * accum
    per_device_train_batch_size=1,    # Adjust based on memory
    optim="paged_adamw_8bit",        # Memory-efficient optimizer
)

# QLoRA quantization config
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)
</code></pre>
<p><strong>Key Points:</strong></p>
<ul>
<li><strong>Use bfloat16</strong>: Better dynamic range, fewer NaN issues than fp16</li>
<li><strong>Avoid fp16</strong>: Known to cause NaN errors with Phi-2</li>
<li><strong>Gradient Checkpointing</strong>: Trades compute for memory</li>
<li><strong>Gradient Accumulation</strong>: Simulates larger batch sizes</li>
<li><strong>Optimizer Choices</strong>:
<ul>
<li><code>paged_adamw_8bit</code>: Best balance (recommended)</li>
<li><code>adamw_torch</code>: Standard but memory intensive</li>
<li><code>sgd</code>: Memory efficient but slower convergence</li>
</ul>
</li>
</ul>
<h4 id="batch-size-strategy"><a class="header" href="#batch-size-strategy">Batch Size Strategy</a></h4>
<pre><code class="language-python"># Strategy 1: Small batch with gradient accumulation
per_device_train_batch_size = 1
gradient_accumulation_steps = 8
# Effective batch size = 1 * 8 = 8

# Strategy 2: Larger batch if memory allows
per_device_train_batch_size = 4
gradient_accumulation_steps = 2
# Effective batch size = 4 * 2 = 8
</code></pre>
<p><strong>Considerations:</strong></p>
<ul>
<li>Check GPU memory with long context lengths (4K, 8K tokens)</li>
<li>OOM errors common with large context + large batch</li>
<li>Use gradient checkpointing if memory constrained</li>
<li>Monitor actual GPU utilization</li>
</ul>
<h3 id="complete-fine-tuning-example"><a class="header" href="#complete-fine-tuning-example">Complete Fine-Tuning Example</a></h3>
<pre><code class="language-python">from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    BitsAndBytesConfig
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from datasets import load_dataset
import torch

# 1. Quantization configuration
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

# 2. Load model and tokenizer
model_id = "microsoft/phi-4"
tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True
)

# 3. Prepare model for k-bit training
model = prepare_model_for_kbit_training(model)

# 4. Configure LoRA
lora_config = LoraConfig(
    r=16,
    lora_alpha=16,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                   "gate_proj", "down_proj", "up_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

# 5. Prepare dataset
dataset = load_dataset("your-dataset")

def preprocess_function(examples):
    return tokenizer(examples["text"], truncation=True, max_length=2048)

tokenized_dataset = dataset.map(preprocess_function, batched=True)

# 6. Training arguments
training_args = TrainingArguments(
    output_dir="./phi-4-finetuned",
    learning_rate=2e-5,
    lr_scheduler_type="constant",
    warmup_steps=100,
    max_steps=1000,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=8,
    gradient_checkpointing=True,
    bf16=True,
    logging_steps=10,
    save_steps=100,
    save_total_limit=3,
    optim="paged_adamw_8bit",
    report_to="tensorboard"
)

# 7. Train
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"],
)

trainer.train()

# 8. Save
model.save_pretrained("./phi-4-lora-adapters")
tokenizer.save_pretrained("./phi-4-lora-adapters")
</code></pre>
<h3 id="data-preparation-best-practices"><a class="header" href="#data-preparation-best-practices">Data Preparation Best Practices</a></h3>
<h4 id="dataset-format"><a class="header" href="#dataset-format">Dataset Format</a></h4>
<pre><code class="language-python"># Instruction-following format
data = [
    {
        "instruction": "Explain quantum computing",
        "input": "",
        "output": "Quantum computing is..."
    },
    {
        "instruction": "Translate to French",
        "input": "Hello, how are you?",
        "output": "Bonjour, comment allez-vous?"
    }
]

# Convert to prompt template
def format_instruction(sample):
    return f"""### Instruction:
{sample['instruction']}

### Input:
{sample['input']}

### Response:
{sample['output']}"""

# Apply to dataset
formatted_data = [format_instruction(item) for item in data]
</code></pre>
<h4 id="dataset-size-guidelines"><a class="header" href="#dataset-size-guidelines">Dataset Size Guidelines</a></h4>
<ul>
<li><strong>Minimum</strong>: 100-500 high-quality examples</li>
<li><strong>Optimal</strong>: 1,000-10,000 examples for specialized tasks</li>
<li><strong>Large-scale</strong>: 10,000+ for broad domain adaptation</li>
</ul>
<p><strong>Quality over Quantity:</strong></p>
<ul>
<li>Clean, well-formatted data is critical</li>
<li>Remove duplicates and low-quality samples</li>
<li>Balance class distributions</li>
<li>Include diverse examples</li>
</ul>
<h2 id="common-patterns"><a class="header" href="#common-patterns">Common Patterns</a></h2>
<h3 id="1-prompt-engineering"><a class="header" href="#1-prompt-engineering">1. Prompt Engineering</a></h3>
<h4 id="basic-completion"><a class="header" href="#basic-completion">Basic Completion</a></h4>
<pre><code class="language-python">from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("microsoft/phi-4")
tokenizer = AutoTokenizer.from_pretrained("microsoft/phi-4")

prompt = "Explain the concept of recursion in programming:"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_length=200)
print(tokenizer.decode(outputs[0]))
</code></pre>
<h4 id="instruction-following"><a class="header" href="#instruction-following">Instruction-Following</a></h4>
<pre><code class="language-python">instruction_prompt = """### Instruction:
Write a Python function to calculate the Fibonacci sequence.

### Response:"""

inputs = tokenizer(instruction_prompt, return_tensors="pt")
outputs = model.generate(
    **inputs,
    max_length=500,
    temperature=0.7,
    top_p=0.9,
    do_sample=True
)
</code></pre>
<h4 id="few-shot-learning"><a class="header" href="#few-shot-learning">Few-Shot Learning</a></h4>
<pre><code class="language-python">few_shot_prompt = """Classify the sentiment of movie reviews.

Review: "This movie was amazing! Best film of the year."
Sentiment: Positive

Review: "Terrible acting and boring plot."
Sentiment: Negative

Review: "It was okay, nothing special."
Sentiment: Neutral

Review: "Absolutely loved every minute of it!"
Sentiment:"""

# Model continues with prediction
</code></pre>
<h3 id="2-chain-of-thought-reasoning"><a class="header" href="#2-chain-of-thought-reasoning">2. Chain-of-Thought Reasoning</a></h3>
<pre><code class="language-python">cot_prompt = """Solve this math problem step by step:

Problem: If a train travels 120 miles in 2 hours, and then 180 miles in 3 hours, what is its average speed for the entire journey?

Let's solve this step by step:
"""

# Phi-4 excels at mathematical reasoning with CoT prompts
</code></pre>
<h3 id="3-context-window-management"><a class="header" href="#3-context-window-management">3. Context Window Management</a></h3>
<pre><code class="language-python"># For long documents, use sliding window approach
def process_long_document(text, chunk_size=3000, overlap=500):
    chunks = []
    for i in range(0, len(text), chunk_size - overlap):
        chunk = text[i:i + chunk_size]
        chunks.append(chunk)

    results = []
    for chunk in chunks:
        prompt = f"Summarize the following text:\n\n{chunk}"
        # Process each chunk
        results.append(generate(prompt))

    return results
</code></pre>
<h3 id="4-multi-modal-applications-phi-4-multimodal"><a class="header" href="#4-multi-modal-applications-phi-4-multimodal">4. Multi-Modal Applications (Phi-4-multimodal)</a></h3>
<pre><code class="language-python"># Phi-4-multimodal supports vision, speech, and text
from transformers import AutoProcessor, AutoModelForVision2Seq

model = AutoModelForVision2Seq.from_pretrained("microsoft/phi-4-multimodal")
processor = AutoProcessor.from_pretrained("microsoft/phi-4-multimodal")

# Image + Text
image = load_image("path/to/image.jpg")
prompt = "Describe what you see in this image"
inputs = processor(text=prompt, images=image, return_tensors="pt")
outputs = model.generate(**inputs)

# Speech + Text
audio = load_audio("path/to/audio.wav")
prompt = "Transcribe this audio"
inputs = processor(text=prompt, audio=audio, return_tensors="pt")
outputs = model.generate(**inputs)
</code></pre>
<h3 id="5-retrieval-augmented-generation-rag"><a class="header" href="#5-retrieval-augmented-generation-rag">5. Retrieval-Augmented Generation (RAG)</a></h3>
<pre><code class="language-python">from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.llms import HuggingFacePipeline

# Setup vector store
embeddings = HuggingFaceEmbeddings()
vectorstore = FAISS.from_documents(documents, embeddings)

# Setup Phi model
phi_pipeline = HuggingFacePipeline.from_model_id(
    model_id="microsoft/phi-4",
    task="text-generation",
    device=0
)

# RAG pipeline
def rag_query(question):
    # Retrieve relevant context
    docs = vectorstore.similarity_search(question, k=3)
    context = "\n".join([doc.page_content for doc in docs])

    # Generate answer with context
    prompt = f"""Use the following context to answer the question.

Context:
{context}

Question: {question}

Answer:"""

    return phi_pipeline(prompt)
</code></pre>
<h3 id="6-streaming-generation"><a class="header" href="#6-streaming-generation">6. Streaming Generation</a></h3>
<pre><code class="language-python">from transformers import TextIteratorStreamer
from threading import Thread

streamer = TextIteratorStreamer(tokenizer, skip_special_tokens=True)

# Generate in separate thread
generation_kwargs = dict(
    inputs=input_ids,
    streamer=streamer,
    max_length=500,
    temperature=0.7
)

thread = Thread(target=model.generate, kwargs=generation_kwargs)
thread.start()

# Stream output
for text in streamer:
    print(text, end="", flush=True)

thread.join()
</code></pre>
<h2 id="operations"><a class="header" href="#operations">Operations</a></h2>
<h3 id="deployment-options"><a class="header" href="#deployment-options">Deployment Options</a></h3>
<h4 id="1-local-deployment-pytorch"><a class="header" href="#1-local-deployment-pytorch">1. Local Deployment (PyTorch)</a></h4>
<pre><code class="language-python">from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Standard loading
model = AutoModelForCausalLM.from_pretrained(
    "microsoft/phi-4",
    torch_dtype=torch.float16,
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("microsoft/phi-4")

# Inference
def generate_response(prompt, max_length=200):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(
        **inputs,
        max_length=max_length,
        temperature=0.7,
        top_p=0.9,
        do_sample=True
    )
    return tokenizer.decode(outputs[0], skip_special_tokens=True)
</code></pre>
<h4 id="2-quantized-deployment"><a class="header" href="#2-quantized-deployment">2. Quantized Deployment</a></h4>
<pre><code class="language-python">from transformers import BitsAndBytesConfig

# 4-bit quantization
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

model = AutoModelForCausalLM.from_pretrained(
    "microsoft/phi-4",
    quantization_config=bnb_config,
    device_map="auto"
)

# Memory usage:
# - Phi-2 unquantized: ~6.5GB VRAM
# - Phi-2 4-bit NF4: ~2.1GB loading, ~5GB during inference
# - Phi-4 unquantized: ~14.96GB
# - Phi-4 4-bit: ~5.42GB (~64% reduction)
</code></pre>
<h4 id="3-gguf-format-llamacpp"><a class="header" href="#3-gguf-format-llamacpp">3. GGUF Format (llama.cpp)</a></h4>
<pre><code class="language-bash"># Download GGUF quantized models
# Available quantizations: Q2_K, Q4_K, Q6_K, Q8_0

# Using llama.cpp
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
make

# Run inference
./main -m phi-2-Q4_K.gguf -p "Your prompt here" -n 200
</code></pre>
<h4 id="4-onnx-runtime"><a class="header" href="#4-onnx-runtime">4. ONNX Runtime</a></h4>
<pre><code class="language-python">from optimum.onnxruntime import ORTModelForCausalLM

# Export to ONNX
model = ORTModelForCausalLM.from_pretrained(
    "microsoft/phi-4",
    export=True
)

# Optimized inference
model.save_pretrained("phi-4-onnx")
</code></pre>
<h4 id="5-mobileedge-deployment"><a class="header" href="#5-mobileedge-deployment">5. Mobile/Edge Deployment</a></h4>
<pre><code class="language-python"># Platform-specific optimizations:

# Intel OpenVINO (x86 processors)
# - Supports INT4, INT8, FP16, FP32
# - Optimized for Intel CPUs and GPUs

# Qualcomm QNN (Snapdragon)
# - Optimized for mobile ARM processors
# - Hardware acceleration support

# Apple MLX (Apple Silicon)
# - Native M1/M2/M3 optimization
# - Metal acceleration

# NVIDIA CUDA (NVIDIA GPUs)
# - Full GPU acceleration
# - TensorRT optimization
</code></pre>
<h3 id="inference-optimization"><a class="header" href="#inference-optimization">Inference Optimization</a></h3>
<h4 id="1-batch-processing"><a class="header" href="#1-batch-processing">1. Batch Processing</a></h4>
<pre><code class="language-python"># Process multiple prompts efficiently
prompts = [
    "Translate to Spanish: Hello world",
    "Summarize: The quick brown fox...",
    "Calculate: 15 * 24"
]

# Batch tokenization
inputs = tokenizer(prompts, return_tensors="pt", padding=True)

# Batch generation
outputs = model.generate(
    **inputs,
    max_length=100,
    pad_token_id=tokenizer.pad_token_id
)

# Decode all
results = [tokenizer.decode(out, skip_special_tokens=True) for out in outputs]
</code></pre>
<h4 id="2-kv-cache-optimization"><a class="header" href="#2-kv-cache-optimization">2. KV Cache Optimization</a></h4>
<pre><code class="language-python"># Enable past_key_values caching for faster generation
def generate_with_cache(prompt, max_new_tokens=100):
    inputs = tokenizer(prompt, return_tensors="pt")

    # First token
    outputs = model(**inputs, use_cache=True)
    past_key_values = outputs.past_key_values
    next_token = outputs.logits[:, -1:].argmax(dim=-1)

    generated = [next_token.item()]

    # Subsequent tokens use cache
    for _ in range(max_new_tokens - 1):
        outputs = model(
            input_ids=next_token,
            past_key_values=past_key_values,
            use_cache=True
        )
        past_key_values = outputs.past_key_values
        next_token = outputs.logits[:, -1:].argmax(dim=-1)
        generated.append(next_token.item())

        if next_token == tokenizer.eos_token_id:
            break

    return tokenizer.decode(generated)
</code></pre>
<h4 id="3-flash-attention"><a class="header" href="#3-flash-attention">3. Flash Attention</a></h4>
<pre><code class="language-python"># Use Flash Attention 2 for faster inference
model = AutoModelForCausalLM.from_pretrained(
    "microsoft/phi-4",
    torch_dtype=torch.float16,
    attn_implementation="flash_attention_2",  # Requires flash-attn package
    device_map="auto"
)

# Significant speedup for long sequences
</code></pre>
<h4 id="4-speculative-decoding"><a class="header" href="#4-speculative-decoding">4. Speculative Decoding</a></h4>
<pre><code class="language-python"># Use smaller model for draft, larger for verification
draft_model = AutoModelForCausalLM.from_pretrained("microsoft/phi-3-mini")
target_model = AutoModelForCausalLM.from_pretrained("microsoft/phi-4")

# Can achieve 2-3x speedup with quality preservation
</code></pre>
<h3 id="quantization-deep-dive"><a class="header" href="#quantization-deep-dive">Quantization Deep Dive</a></h3>
<h4 id="quantization-methods"><a class="header" href="#quantization-methods">Quantization Methods</a></h4>
<p><strong>1. Post-Training Quantization (PTQ)</strong></p>
<ul>
<li>No retraining required</li>
<li>Quick conversion process</li>
<li>Slight accuracy degradation</li>
<li>Methods: Dynamic, Static, Weight-only</li>
</ul>
<p><strong>2. Quantization-Aware Training (QAT)</strong></p>
<ul>
<li>Retrains with quantization in mind</li>
<li>Better accuracy preservation</li>
<li>Longer process</li>
<li>More compute intensive</li>
</ul>
<h4 id="quantization-formats-comparison"><a class="header" href="#quantization-formats-comparison">Quantization Formats Comparison</a></h4>
<div class="table-wrapper"><table><thead><tr><th>Format</th><th>Bits</th><th>Size (Phi-4)</th><th>Speed</th><th>Quality</th><th>Use Case</th></tr></thead><tbody>
<tr><td>FP32</td><td>32</td><td>~56GB</td><td>Baseline</td><td>Best</td><td>Training</td></tr>
<tr><td>FP16</td><td>16</td><td>~28GB</td><td>1.5-2x</td><td>Excellent</td><td>GPU inference</td></tr>
<tr><td>BF16</td><td>16</td><td>~28GB</td><td>1.5-2x</td><td>Excellent</td><td>Training &amp; inference</td></tr>
<tr><td>INT8</td><td>8</td><td>~14GB</td><td>2-3x</td><td>Very Good</td><td>Production</td></tr>
<tr><td>NF4</td><td>4</td><td>~5.4GB</td><td>1.3x*</td><td>Good</td><td>Memory-constrained</td></tr>
<tr><td>Q4_K</td><td>4</td><td>~5.5GB</td><td>2-4x</td><td>Good</td><td>Edge devices</td></tr>
<tr><td>Q2_K</td><td>2</td><td>~3GB</td><td>3-5x</td><td>Fair</td><td>Extreme edge</td></tr>
</tbody></table>
</div>
<p>*4-bit inference slower than FP16 but enables larger models in limited memory</p>
<h4 id="implementation-examples"><a class="header" href="#implementation-examples">Implementation Examples</a></h4>
<p><strong>BitsAndBytes Quantization</strong></p>
<pre><code class="language-python">from transformers import BitsAndBytesConfig
import torch

# 8-bit quantization
config_8bit = BitsAndBytesConfig(load_in_8bit=True)

# 4-bit quantization (NF4)
config_4bit = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,      # Double quantization
    bnb_4bit_quant_type="nf4",            # NormalFloat4
    bnb_4bit_compute_dtype=torch.bfloat16 # Compute dtype
)

model = AutoModelForCausalLM.from_pretrained(
    "microsoft/phi-4",
    quantization_config=config_4bit,
    device_map="auto"
)
</code></pre>
<p><strong>GPTQ Quantization (Auto-Round)</strong></p>
<pre><code class="language-python">from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig

# Configure quantization
quantize_config = BaseQuantizeConfig(
    bits=4,
    group_size=128,
    desc_act=False
)

# Load and quantize
model = AutoGPTQForCausalLM.from_pretrained(
    "microsoft/phi-4",
    quantize_config=quantize_config
)

# Quantize with calibration data
model.quantize(calibration_data)

# Save
model.save_quantized("phi-4-gptq")
</code></pre>
<p><strong>AWQ Quantization</strong></p>
<pre><code class="language-python">from awq import AutoAWQForCausalLM

# Load model
model = AutoAWQForCausalLM.from_pretrained("microsoft/phi-4")

# Quantize
model.quantize(tokenizer, quant_config={"zero_point": True, "q_group_size": 128})

# Save
model.save_quantized("phi-4-awq")
</code></pre>
<h3 id="performance-benchmarks"><a class="header" href="#performance-benchmarks">Performance Benchmarks</a></h3>
<h4 id="decoding-speed-phi-2"><a class="header" href="#decoding-speed-phi-2">Decoding Speed (Phi-2)</a></h4>
<div class="table-wrapper"><table><thead><tr><th>Configuration</th><th>Tokens/Second</th><th>Memory (VRAM)</th></tr></thead><tbody>
<tr><td>FP16</td><td>21</td><td>6.5GB</td></tr>
<tr><td>4-bit NF4</td><td>15.7</td><td>2.1GB (load), 5GB (inference)</td></tr>
</tbody></table>
</div>
<h4 id="memory-footprint-phi-4"><a class="header" href="#memory-footprint-phi-4">Memory Footprint (Phi-4)</a></h4>
<div class="table-wrapper"><table><thead><tr><th>Configuration</th><th>Memory</th><th>Reduction</th></tr></thead><tbody>
<tr><td>Unquantized</td><td>14.96GB</td><td>-</td></tr>
<tr><td>4-bit</td><td>5.42GB</td><td>64%</td></tr>
<tr><td>2-bit</td><td>~3GB</td><td>80%</td></tr>
</tbody></table>
</div>
<h3 id="serving-architecture"><a class="header" href="#serving-architecture">Serving Architecture</a></h3>
<h4 id="single-model-serving"><a class="header" href="#single-model-serving">Single Model Serving</a></h4>
<pre><code class="language-python">from fastapi import FastAPI
from pydantic import BaseModel
import uvicorn

app = FastAPI()
model = AutoModelForCausalLM.from_pretrained("microsoft/phi-4")
tokenizer = AutoTokenizer.from_pretrained("microsoft/phi-4")

class GenerationRequest(BaseModel):
    prompt: str
    max_length: int = 200
    temperature: float = 0.7

@app.post("/generate")
async def generate(request: GenerationRequest):
    inputs = tokenizer(request.prompt, return_tensors="pt")
    outputs = model.generate(
        **inputs,
        max_length=request.max_length,
        temperature=request.temperature
    )
    text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return {"generated_text": text}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
</code></pre>
<h4 id="vllm-high-performance-serving"><a class="header" href="#vllm-high-performance-serving">vLLM High-Performance Serving</a></h4>
<pre><code class="language-python">from vllm import LLM, SamplingParams

# Initialize with PagedAttention
llm = LLM(
    model="microsoft/phi-4",
    tensor_parallel_size=1,
    dtype="half",
    max_model_len=4096
)

# Sampling parameters
sampling_params = SamplingParams(
    temperature=0.7,
    top_p=0.9,
    max_tokens=200
)

# Batch inference
prompts = ["Prompt 1", "Prompt 2", "Prompt 3"]
outputs = llm.generate(prompts, sampling_params)

for output in outputs:
    print(output.outputs[0].text)
</code></pre>
<h4 id="text-generation-inference-tgi"><a class="header" href="#text-generation-inference-tgi">Text Generation Inference (TGI)</a></h4>
<pre><code class="language-bash"># Run with Docker
docker run --gpus all \
  -p 8080:80 \
  -v $(pwd)/data:/data \
  ghcr.io/huggingface/text-generation-inference:latest \
  --model-id microsoft/phi-4 \
  --max-total-tokens 4096 \
  --max-input-length 3584

# Query the endpoint
curl http://localhost:8080/generate \
  -X POST \
  -d '{"inputs":"What is machine learning?","parameters":{"max_new_tokens":200}}' \
  -H 'Content-Type: application/json'
</code></pre>
<h3 id="monitoring-and-evaluation"><a class="header" href="#monitoring-and-evaluation">Monitoring and Evaluation</a></h3>
<h4 id="performance-metrics"><a class="header" href="#performance-metrics">Performance Metrics</a></h4>
<pre><code class="language-python">import time
import torch

def benchmark_model(model, tokenizer, prompt, runs=10):
    # Warmup
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    _ = model.generate(**inputs, max_length=100)

    # Benchmark
    times = []
    for _ in range(runs):
        torch.cuda.synchronize() if torch.cuda.is_available() else None
        start = time.time()

        outputs = model.generate(**inputs, max_length=100)

        torch.cuda.synchronize() if torch.cuda.is_available() else None
        end = time.time()

        times.append(end - start)

    avg_time = sum(times) / len(times)
    tokens_generated = len(outputs[0]) - len(inputs.input_ids[0])
    tokens_per_sec = tokens_generated / avg_time

    return {
        "avg_time": avg_time,
        "tokens_generated": tokens_generated,
        "tokens_per_second": tokens_per_sec
    }

# Run benchmark
results = benchmark_model(model, tokenizer, "Explain quantum computing:")
print(f"Average generation time: {results['avg_time']:.2f}s")
print(f"Tokens per second: {results['tokens_per_second']:.2f}")
</code></pre>
<h4 id="quality-evaluation"><a class="header" href="#quality-evaluation">Quality Evaluation</a></h4>
<pre><code class="language-python">from evaluate import load

# Perplexity
perplexity = load("perplexity")
results = perplexity.compute(predictions=predictions, model_id="microsoft/phi-4")

# BLEU score (for translation tasks)
bleu = load("bleu")
results = bleu.compute(predictions=predictions, references=references)

# ROUGE score (for summarization)
rouge = load("rouge")
results = rouge.compute(predictions=predictions, references=references)
</code></pre>
<h2 id="advanced-techniques"><a class="header" href="#advanced-techniques">Advanced Techniques</a></h2>
<h3 id="1-model-merging"><a class="header" href="#1-model-merging">1. Model Merging</a></h3>
<pre><code class="language-python">from transformers import AutoModelForCausalLM
import torch

# Load base and fine-tuned models
base_model = AutoModelForCausalLM.from_pretrained("microsoft/phi-4")
ft_model = AutoModelForCausalLM.from_pretrained("./phi-4-finetuned")

# Merge with weighted average
alpha = 0.7  # Weight for fine-tuned model

for name, param in base_model.named_parameters():
    if name in ft_model.state_dict():
        param.data = alpha * ft_model.state_dict()[name] + (1 - alpha) * param.data

base_model.save_pretrained("./phi-4-merged")
</code></pre>
<h3 id="2-multi-adapter-loading"><a class="header" href="#2-multi-adapter-loading">2. Multi-Adapter Loading</a></h3>
<pre><code class="language-python">from peft import PeftModel

# Load base model
base_model = AutoModelForCausalLM.from_pretrained("microsoft/phi-4")

# Load multiple adapters
model_with_adapter1 = PeftModel.from_pretrained(base_model, "./adapter1")
model_with_adapter2 = PeftModel.from_pretrained(base_model, "./adapter2")

# Switch between adapters dynamically
def generate_with_adapter(prompt, adapter_name):
    if adapter_name == "adapter1":
        model = model_with_adapter1
    else:
        model = model_with_adapter2

    # Generate
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(**inputs)
    return tokenizer.decode(outputs[0])
</code></pre>
<h3 id="3-constrained-generation"><a class="header" href="#3-constrained-generation">3. Constrained Generation</a></h3>
<pre><code class="language-python">from transformers import LogitsProcessor

class ForceWordsLogitsProcessor(LogitsProcessor):
    def __init__(self, force_word_ids):
        self.force_word_ids = force_word_ids

    def __call__(self, input_ids, scores):
        if len(input_ids[0]) in self.force_word_ids:
            word_ids = self.force_word_ids[len(input_ids[0])]
            mask = torch.full_like(scores, float('-inf'))
            mask[:, word_ids] = 0
            scores = scores + mask
        return scores

# Use in generation
logits_processor = LogitsProcessorList([
    ForceWordsLogitsProcessor({5: [tokenizer.encode("yes")[0]]})
])

outputs = model.generate(
    **inputs,
    logits_processor=logits_processor
)
</code></pre>
<h2 id="troubleshooting"><a class="header" href="#troubleshooting">Troubleshooting</a></h2>
<h3 id="common-issues-and-solutions"><a class="header" href="#common-issues-and-solutions">Common Issues and Solutions</a></h3>
<h4 id="1-out-of-memory-oom"><a class="header" href="#1-out-of-memory-oom">1. Out of Memory (OOM)</a></h4>
<p><strong>Solutions:</strong></p>
<ul>
<li>Enable gradient checkpointing</li>
<li>Reduce batch size</li>
<li>Increase gradient accumulation steps</li>
<li>Use quantization (4-bit or 8-bit)</li>
<li>Reduce sequence length</li>
<li>Use DeepSpeed ZeRO optimization</li>
</ul>
<pre><code class="language-python"># Example fix
training_args = TrainingArguments(
    per_device_train_batch_size=1,        # Reduce from 4 to 1
    gradient_accumulation_steps=16,        # Increase from 4 to 16
    gradient_checkpointing=True,           # Enable checkpointing
    deepspeed="ds_config.json"            # Use DeepSpeed
)
</code></pre>
<h4 id="2-nan-loss-during-training"><a class="header" href="#2-nan-loss-during-training">2. NaN Loss During Training</a></h4>
<p><strong>Causes:</strong></p>
<ul>
<li>Using fp16 instead of bfloat16</li>
<li>Learning rate too high</li>
<li>Gradient explosion</li>
</ul>
<p><strong>Solutions:</strong></p>
<pre><code class="language-python"># Use bfloat16
training_args = TrainingArguments(
    bf16=True,
    fp16=False,
    learning_rate=2e-5,              # Lower learning rate
    max_grad_norm=1.0,               # Gradient clipping
)
</code></pre>
<h4 id="3-slow-inference"><a class="header" href="#3-slow-inference">3. Slow Inference</a></h4>
<p><strong>Solutions:</strong></p>
<ul>
<li>Use quantization</li>
<li>Enable Flash Attention 2</li>
<li>Batch requests</li>
<li>Use KV cache</li>
<li>Consider vLLM or TGI for serving</li>
</ul>
<h4 id="4-poor-fine-tuning-results"><a class="header" href="#4-poor-fine-tuning-results">4. Poor Fine-Tuning Results</a></h4>
<p><strong>Diagnostics:</strong></p>
<ul>
<li>Check data quality and format</li>
<li>Verify learning rate and schedule</li>
<li>Monitor training loss curve</li>
<li>Evaluate on validation set</li>
<li>Check for overfitting</li>
</ul>
<p><strong>Solutions:</strong></p>
<ul>
<li>Increase dataset size</li>
<li>Adjust learning rate</li>
<li>Add regularization (dropout)</li>
<li>Use early stopping</li>
<li>Try different LoRA ranks</li>
</ul>
<h2 id="best-practices-summary"><a class="header" href="#best-practices-summary">Best Practices Summary</a></h2>
<h3 id="training"><a class="header" href="#training">Training</a></h3>
<ol>
<li>✅ Use bfloat16 for training (not fp16)</li>
<li>✅ Start with learning rate 2e-5 to 5e-5</li>
<li>✅ Use constant learning rate schedule</li>
<li>✅ Set LoRA rank 8-16 (higher not always better)</li>
<li>✅ Enable gradient checkpointing for memory</li>
<li>✅ Use QLoRA for consumer GPUs</li>
<li>✅ Monitor validation metrics to prevent overfitting</li>
</ol>
<h3 id="inference"><a class="header" href="#inference">Inference</a></h3>
<ol>
<li>✅ Quantize to 4-bit for memory-constrained devices</li>
<li>✅ Use Flash Attention 2 for long sequences</li>
<li>✅ Enable KV cache for faster generation</li>
<li>✅ Batch requests when possible</li>
<li>✅ Use vLLM or TGI for production serving</li>
<li>✅ Profile and monitor performance metrics</li>
</ol>
<h3 id="deployment"><a class="header" href="#deployment">Deployment</a></h3>
<ol>
<li>✅ Choose quantization based on hardware/accuracy tradeoff</li>
<li>✅ Use platform-specific optimizations (OpenVINO, QNN, MLX)</li>
<li>✅ Implement proper error handling and retries</li>
<li>✅ Monitor memory usage and latency</li>
<li>✅ Cache frequent requests when appropriate</li>
<li>✅ Set appropriate timeout values</li>
</ol>
<h2 id="resources"><a class="header" href="#resources">Resources</a></h2>
<h3 id="official-links"><a class="header" href="#official-links">Official Links</a></h3>
<ul>
<li><strong>Hugging Face Hub</strong>: https://huggingface.co/microsoft/phi-4</li>
<li><strong>Azure AI</strong>: https://azure.microsoft.com/en-us/products/phi</li>
<li><strong>Phi-4 Technical Report</strong>: https://www.microsoft.com/en-us/research/publication/phi-4-technical-report/</li>
<li><strong>Phi-3 Technical Report</strong>: https://arxiv.org/abs/2404.14219</li>
</ul>
<h3 id="tools-and-libraries"><a class="header" href="#tools-and-libraries">Tools and Libraries</a></h3>
<ul>
<li><strong>Transformers</strong>: https://github.com/huggingface/transformers</li>
<li><strong>PEFT</strong>: https://github.com/huggingface/peft</li>
<li><strong>BitsAndBytes</strong>: https://github.com/TimDettmers/bitsandbytes</li>
<li><strong>vLLM</strong>: https://github.com/vllm-project/vllm</li>
<li><strong>llama.cpp</strong>: https://github.com/ggerganov/llama.cpp</li>
<li><strong>Text Generation Inference</strong>: https://github.com/huggingface/text-generation-inference</li>
</ul>
<h3 id="community"><a class="header" href="#community">Community</a></h3>
<ul>
<li><strong>Phi-3 Cookbook</strong>: https://github.com/microsoft/Phi-3CookBook</li>
<li><strong>Discussions</strong>: https://huggingface.co/microsoft/phi-4/discussions</li>
<li><strong>Issues</strong>: https://github.com/microsoft/phi-4/issues</li>
</ul>
<h2 id="license"><a class="header" href="#license">License</a></h2>
<p>Microsoft Phi models are released under the <strong>MIT License</strong>, allowing commercial use, modification, and distribution with minimal restrictions.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../ai/whisper.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../cloud/index.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../ai/whisper.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../cloud/index.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="../elasticlunr.min.js"></script>
        <script src="../mark.min.js"></script>
        <script src="../searcher.js"></script>

        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->



    </div>
    </body>
</html>
